654	def _accumulateFrequencyCounts ( values , freqCounts = None ) : values = numpy . array ( values ) numEntries = values . max ( ) + 1 if freqCounts is not None : numEntries = max ( numEntries , freqCounts . size ) if freqCounts is not None : if freqCounts . size != numEntries : newCounts = numpy . zeros ( numEntries , dtype = 'int32' ) newCounts [ 0 : freqCounts . size ] = freqCounts else : newCounts = freqCounts else : newCounts = numpy . zeros ( numEntries , dtype = 'int32' ) for v in values : newCounts [ v ] += 1 return newCounts
6316	def reload_programs ( self ) : print ( "Reloading programs:" ) for name , program in self . _programs . items ( ) : if getattr ( program , 'program' , None ) : print ( " - {}" . format ( program . meta . label ) ) program . program = resources . programs . load ( program . meta )
3881	async def _sync ( self ) : logger . info ( 'Syncing events since {}' . format ( self . _sync_timestamp ) ) try : res = await self . _client . sync_all_new_events ( hangouts_pb2 . SyncAllNewEventsRequest ( request_header = self . _client . get_request_header ( ) , last_sync_timestamp = parsers . to_timestamp ( self . _sync_timestamp ) , max_response_size_bytes = 1048576 , ) ) except exceptions . NetworkError as e : logger . warning ( 'Failed to sync events, some events may be lost: {}' . format ( e ) ) else : for conv_state in res . conversation_state : conv_id = conv_state . conversation_id . id conv = self . _conv_dict . get ( conv_id , None ) if conv is not None : conv . update_conversation ( conv_state . conversation ) for event_ in conv_state . event : timestamp = parsers . from_timestamp ( event_ . timestamp ) if timestamp > self . _sync_timestamp : await self . _on_event ( event_ ) else : self . _add_conversation ( conv_state . conversation , conv_state . event , conv_state . event_continuation_token )
6608	def wait ( self ) : sleep = 5 while True : if self . clusterprocids_outstanding : self . poll ( ) if not self . clusterprocids_outstanding : break time . sleep ( sleep ) return self . clusterprocids_finished
10970	def get_group_name ( id_group ) : group = Group . query . get ( id_group ) if group is not None : return group . name
6382	def sim_hamming ( src , tar , diff_lens = True ) : return Hamming ( ) . sim ( src , tar , diff_lens )
8907	def list_services ( self ) : my_services = [ ] for service in self . collection . find ( ) . sort ( 'name' , pymongo . ASCENDING ) : my_services . append ( Service ( service ) ) return my_services
1284	def footnote_ref ( self , key , index ) : html = ( '<sup class="footnote-ref" id="fnref-%s">' '<a href="#fn-%s">%d</a></sup>' ) % ( escape ( key ) , escape ( key ) , index ) return html
10120	def rectangle ( cls , vertices , ** kwargs ) : bottom_left , top_right = vertices top_left = [ bottom_left [ 0 ] , top_right [ 1 ] ] bottom_right = [ top_right [ 0 ] , bottom_left [ 1 ] ] return cls ( [ bottom_left , bottom_right , top_right , top_left ] , ** kwargs )
1794	def NEG ( cpu , dest ) : source = dest . read ( ) res = dest . write ( - source ) cpu . _calculate_logic_flags ( dest . size , res ) cpu . CF = source != 0 cpu . AF = ( res & 0x0f ) != 0x00
10049	def create_error_handlers ( blueprint ) : blueprint . errorhandler ( PIDInvalidAction ) ( create_api_errorhandler ( status = 403 , message = 'Invalid action' ) ) records_rest_error_handlers ( blueprint )
11228	def before ( self , dt , inc = False ) : if self . _cache_complete : gen = self . _cache else : gen = self last = None if inc : for i in gen : if i > dt : break last = i else : for i in gen : if i >= dt : break last = i return last
1146	def copy ( x ) : cls = type ( x ) copier = _copy_dispatch . get ( cls ) if copier : return copier ( x ) copier = getattr ( cls , "__copy__" , None ) if copier : return copier ( x ) reductor = dispatch_table . get ( cls ) if reductor : rv = reductor ( x ) else : reductor = getattr ( x , "__reduce_ex__" , None ) if reductor : rv = reductor ( 2 ) else : reductor = getattr ( x , "__reduce__" , None ) if reductor : rv = reductor ( ) else : raise Error ( "un(shallow)copyable object of type %s" % cls ) return _reconstruct ( x , rv , 0 )
12852	def scan ( xml ) : if xml . tag is et . Comment : yield { 'type' : COMMENT , 'text' : xml . text } return if xml . tag is et . PI : if xml . text : yield { 'type' : PI , 'target' : xml . target , 'text' : xml . text } else : yield { 'type' : PI , 'target' : xml . target } return obj = _elt2obj ( xml ) obj [ 'type' ] = ENTER yield obj assert type ( xml . tag ) is str , xml if xml . text : yield { 'type' : TEXT , 'text' : xml . text } for c in xml : for x in scan ( c ) : yield x if c . tail : yield { 'type' : TEXT , 'text' : c . tail } yield { 'type' : EXIT }
568	def _matchReportKeys ( reportKeyREs = [ ] , allReportKeys = [ ] ) : matchingReportKeys = [ ] for keyRE in reportKeyREs : matchObj = re . compile ( keyRE ) found = False for keyName in allReportKeys : match = matchObj . match ( keyName ) if match and match . end ( ) == len ( keyName ) : matchingReportKeys . append ( keyName ) found = True if not found : raise _BadKeyError ( keyRE ) return matchingReportKeys
5898	def check_mdrun_success ( logfile ) : if not os . path . exists ( logfile ) : return None with open ( logfile , 'rb' ) as log : log . seek ( - 1024 , 2 ) for line in log : line = line . decode ( 'ASCII' ) if line . startswith ( "Finished mdrun on" ) : return True return False
13152	def log_error ( error , result ) : p = { 'error' : error , 'result' : result } _log ( TYPE_CODES . ERROR , p )
1472	def main ( ) : shell_env = os . environ . copy ( ) shell_env [ "PEX_ROOT" ] = os . path . join ( os . path . abspath ( '.' ) , ".pex" ) executor = HeronExecutor ( sys . argv , shell_env ) executor . initialize ( ) start ( executor )
4880	def create_switch ( apps , schema_editor ) : Switch = apps . get_model ( 'waffle' , 'Switch' ) Switch . objects . update_or_create ( name = ENTERPRISE_ROLE_BASED_ACCESS_CONTROL_SWITCH , defaults = { 'active' : False } )
8710	def __write_chunk ( self , chunk ) : log . debug ( 'writing %d bytes chunk' , len ( chunk ) ) data = BLOCK_START + chr ( len ( chunk ) ) + chunk if len ( chunk ) < 128 : padding = 128 - len ( chunk ) log . debug ( 'pad with %d characters' , padding ) data = data + ( ' ' * padding ) log . debug ( "packet size %d" , len ( data ) ) self . __write ( data ) self . _port . flush ( ) return self . __got_ack ( )
1441	def serialize_data_tuple ( self , stream_id , latency_in_ns ) : self . update_count ( self . TUPLE_SERIALIZATION_TIME_NS , incr_by = latency_in_ns , key = stream_id )
4028	def create_cookie ( host , path , secure , expires , name , value ) : return http . cookiejar . Cookie ( 0 , name , value , None , False , host , host . startswith ( '.' ) , host . startswith ( '.' ) , path , True , secure , expires , False , None , None , { } )
10077	def create ( cls , data , id_ = None ) : data . setdefault ( '$schema' , current_jsonschemas . path_to_url ( current_app . config [ 'DEPOSIT_DEFAULT_JSONSCHEMA' ] ) ) if '_deposit' not in data : id_ = id_ or uuid . uuid4 ( ) cls . deposit_minter ( id_ , data ) data [ '_deposit' ] . setdefault ( 'owners' , list ( ) ) if current_user and current_user . is_authenticated : creator_id = int ( current_user . get_id ( ) ) if creator_id not in data [ '_deposit' ] [ 'owners' ] : data [ '_deposit' ] [ 'owners' ] . append ( creator_id ) data [ '_deposit' ] [ 'created_by' ] = creator_id return super ( Deposit , cls ) . create ( data , id_ = id_ )
2859	def transfer ( self , data ) : if ( len ( data ) > 65536 ) : print ( 'the FTDI chip is limited to 65536 bytes (64 KB) of input/output per command!' ) print ( 'use for loops for larger reads' ) exit ( 1 ) command = 0x30 | ( self . lsbfirst << 3 ) | ( self . read_clock_ve << 2 ) | self . write_clock_ve logger . debug ( 'SPI transfer with command {0:2X}.' . format ( command ) ) data1 = data [ : len ( data ) / 2 ] data2 = data [ len ( data ) / 2 : ] len_low1 = ( len ( data1 ) - 1 ) & 0xFF len_high1 = ( ( len ( data1 ) - 1 ) >> 8 ) & 0xFF len_low2 = ( len ( data2 ) - 1 ) & 0xFF len_high2 = ( ( len ( data2 ) - 1 ) >> 8 ) & 0xFF payload1 = '' payload2 = '' self . _assert_cs ( ) if len ( data1 ) > 0 : self . _ft232h . _write ( str ( bytearray ( ( command , len_low1 , len_high1 ) ) ) ) self . _ft232h . _write ( str ( bytearray ( data1 ) ) ) payload1 = self . _ft232h . _poll_read ( len ( data1 ) ) if len ( data2 ) > 0 : self . _ft232h . _write ( str ( bytearray ( ( command , len_low2 , len_high2 ) ) ) ) self . _ft232h . _write ( str ( bytearray ( data2 ) ) ) payload2 = self . _ft232h . _poll_read ( len ( data2 ) ) self . _deassert_cs ( ) return bytearray ( payload1 + payload2 )
8994	def relative_folder ( self , module , folder ) : folder = self . _relative_to_absolute ( module , folder ) return self . folder ( folder )
10327	def statistics ( graph , ps , spanning_cluster = True , model = 'bond' , alpha = alpha_1sigma , runs = 40 ) : my_microcanonical_averages = microcanonical_averages ( graph = graph , runs = runs , spanning_cluster = spanning_cluster , model = model , alpha = alpha ) my_microcanonical_averages_arrays = microcanonical_averages_arrays ( my_microcanonical_averages ) return canonical_averages ( ps , my_microcanonical_averages_arrays )
13670	def get_indices_list ( s : Any ) -> List [ str ] : indices = get_indices ( s ) return [ indices [ i ] for i in sorted ( indices , key = int ) ]
7585	def dstat ( inarr , taxdict , mindict = 1 , nboots = 1000 , name = 0 ) : if isinstance ( inarr , list ) : arr , _ = _loci_to_arr ( inarr , taxdict , mindict ) if arr . shape [ 1 ] == 4 : res , boots = _get_signif_4 ( arr , nboots ) res = pd . DataFrame ( res , columns = [ name ] , index = [ "Dstat" , "bootmean" , "bootstd" , "Z" , "ABBA" , "BABA" , "nloci" ] ) else : res , boots = _get_signif_5 ( arr , nboots ) res = pd . DataFrame ( res , index = [ "p3" , "p4" , "shared" ] , columns = [ "Dstat" , "bootmean" , "bootstd" , "Z" , "ABxxA" , "BAxxA" , "nloci" ] ) return res . T , boots
523	def _updateBoostFactorsLocal ( self ) : targetDensity = numpy . zeros ( self . _numColumns , dtype = realDType ) for i in xrange ( self . _numColumns ) : maskNeighbors = self . _getColumnNeighborhood ( i ) targetDensity [ i ] = numpy . mean ( self . _activeDutyCycles [ maskNeighbors ] ) self . _boostFactors = numpy . exp ( ( targetDensity - self . _activeDutyCycles ) * self . _boostStrength )
4430	async def _remove ( self , ctx , index : int ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . queue : return await ctx . send ( 'Nothing queued.' ) if index > len ( player . queue ) or index < 1 : return await ctx . send ( f'Index has to be **between** 1 and {len(player.queue)}' ) index -= 1 removed = player . queue . pop ( index ) await ctx . send ( f'Removed **{removed.title}** from the queue.' )
12884	def field_type ( self ) : if not self . model : return 'JSON' database = self . model . _meta . database if isinstance ( database , Proxy ) : database = database . obj if Json and isinstance ( database , PostgresqlDatabase ) : return 'JSON' return 'TEXT'
2542	def set_file_chksum ( self , doc , chk_sum ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_chksum_set : self . file_chksum_set = True self . file ( doc ) . chk_sum = checksum . Algorithm ( 'SHA1' , chk_sum ) return True else : raise CardinalityError ( 'File::CheckSum' ) else : raise OrderError ( 'File::CheckSum' )
7088	def jd_to_datetime ( jd , returniso = False ) : tt = astime . Time ( jd , format = 'jd' , scale = 'utc' ) if returniso : return tt . iso else : return tt . datetime
5865	def course_key_is_valid ( course_key ) : if course_key is None : return False try : CourseKey . from_string ( text_type ( course_key ) ) except ( InvalidKeyError , UnicodeDecodeError ) : return False return True
6273	def load ( self , meta : ResourceDescription ) -> Any : self . _check_meta ( meta ) self . resolve_loader ( meta ) return meta . loader_cls ( meta ) . load ( )
3142	def create ( self , data ) : if 'name' not in data : raise KeyError ( 'The file must have a name' ) if 'file_data' not in data : raise KeyError ( 'The file must have file_data' ) response = self . _mc_client . _post ( url = self . _build_path ( ) , data = data ) if response is not None : self . file_id = response [ 'id' ] else : self . file_id = None return response
2043	def current_human_transaction ( self ) : try : tx , _ , _ , _ , _ = self . _callstack [ 0 ] if tx . result is not None : return None assert tx . depth == 0 return tx except IndexError : return None
12788	def load ( self , reload = False , require_load = False ) : if reload : self . config = None if self . config : self . _log . debug ( 'Returning cached config instance. Use ' '``reload=True`` to avoid caching!' ) return path = self . _effective_path ( ) config_filename = self . _effective_filename ( ) self . _active_path = [ join ( _ , config_filename ) for _ in path ] for dirname in path : conf_name = join ( dirname , config_filename ) readable = self . check_file ( conf_name ) if readable : action = 'Updating' if self . _loaded_files else 'Loading initial' self . _log . info ( '%s config from %s' , action , conf_name ) self . read ( conf_name ) if conf_name == expanduser ( "~/.%s/%s/%s" % ( self . group_name , self . app_name , self . filename ) ) : self . _log . warning ( "DEPRECATION WARNING: The file " "'%s/.%s/%s/app.ini' was loaded. The XDG " "Basedir standard requires this file to be in " "'%s/.config/%s/%s/app.ini'! This location " "will no longer be parsed in a future version of " "config_resolver! You can already (and should) move " "the file!" , expanduser ( "~" ) , self . group_name , self . app_name , expanduser ( "~" ) , self . group_name , self . app_name ) self . _loaded_files . append ( conf_name ) if not self . _loaded_files and not require_load : self . _log . warning ( "No config file named %s found! Search path was %r" , config_filename , path ) elif not self . _loaded_files and require_load : raise IOError ( "No config file named %s found! Search path " "was %r" % ( config_filename , path ) )
2192	def renew ( self , cfgstr = None , product = None ) : products = self . _rectify_products ( product ) certificate = { 'timestamp' : util_time . timestamp ( ) , 'product' : products , } if products is not None : if not all ( map ( os . path . exists , products ) ) : raise IOError ( 'The stamped product must exist: {}' . format ( products ) ) certificate [ 'product_file_hash' ] = self . _product_file_hash ( products ) self . cacher . save ( certificate , cfgstr = cfgstr ) return certificate
9518	def count_sequences ( infile ) : seq_reader = sequences . file_reader ( infile ) n = 0 for seq in seq_reader : n += 1 return n
7132	def prepare_docset ( source , dest , name , index_page , enable_js , online_redirect_url ) : resources = os . path . join ( dest , "Contents" , "Resources" ) docs = os . path . join ( resources , "Documents" ) os . makedirs ( resources ) db_conn = sqlite3 . connect ( os . path . join ( resources , "docSet.dsidx" ) ) db_conn . row_factory = sqlite3 . Row db_conn . execute ( "CREATE TABLE searchIndex(id INTEGER PRIMARY KEY, name TEXT, " "type TEXT, path TEXT)" ) db_conn . commit ( ) plist_path = os . path . join ( dest , "Contents" , "Info.plist" ) plist_cfg = { "CFBundleIdentifier" : name , "CFBundleName" : name , "DocSetPlatformFamily" : name . lower ( ) , "DashDocSetFamily" : "python" , "isDashDocset" : True , "isJavaScriptEnabled" : enable_js , } if index_page is not None : plist_cfg [ "dashIndexFilePath" ] = index_page if online_redirect_url is not None : plist_cfg [ "DashDocSetFallbackURL" ] = online_redirect_url write_plist ( plist_cfg , plist_path ) shutil . copytree ( source , docs ) return DocSet ( path = dest , docs = docs , plist = plist_path , db_conn = db_conn )
10233	def _reaction_cartesion_expansion_unqualified_helper ( graph : BELGraph , u : BaseEntity , v : BaseEntity , d : dict , ) -> None : if isinstance ( u , Reaction ) and isinstance ( v , Reaction ) : enzymes = _get_catalysts_in_reaction ( u ) | _get_catalysts_in_reaction ( v ) for reactant , product in chain ( itt . product ( u . reactants , u . products ) , itt . product ( v . reactants , v . products ) ) : if reactant in enzymes or product in enzymes : continue graph . add_unqualified_edge ( reactant , product , INCREASES ) for product , reactant in itt . product ( u . products , u . reactants ) : if reactant in enzymes or product in enzymes : continue graph . add_unqualified_edge ( product , reactant , d [ RELATION ] , ) elif isinstance ( u , Reaction ) : enzymes = _get_catalysts_in_reaction ( u ) for product in u . products : if product in enzymes : continue if v not in u . products and v not in u . reactants : graph . add_unqualified_edge ( product , v , INCREASES ) for reactant in u . reactants : graph . add_unqualified_edge ( reactant , product , INCREASES ) elif isinstance ( v , Reaction ) : enzymes = _get_catalysts_in_reaction ( v ) for reactant in v . reactants : if reactant in enzymes : continue if u not in v . products and u not in v . reactants : graph . add_unqualified_edge ( u , reactant , INCREASES ) for product in v . products : graph . add_unqualified_edge ( reactant , product , INCREASES )
550	def __checkCancelation ( self ) : print >> sys . stderr , "reporter:counter:HypersearchWorker,numRecords,50" jobCancel = self . _jobsDAO . jobGetFields ( self . _jobID , [ 'cancel' ] ) [ 0 ] if jobCancel : self . _cmpReason = ClientJobsDAO . CMPL_REASON_KILLED self . _isCanceled = True self . _logger . info ( "Model %s canceled because Job %s was stopped." , self . _modelID , self . _jobID ) else : stopReason = self . _jobsDAO . modelsGetFields ( self . _modelID , [ 'engStop' ] ) [ 0 ] if stopReason is None : pass elif stopReason == ClientJobsDAO . STOP_REASON_KILLED : self . _cmpReason = ClientJobsDAO . CMPL_REASON_KILLED self . _isKilled = True self . _logger . info ( "Model %s canceled because it was killed by hypersearch" , self . _modelID ) elif stopReason == ClientJobsDAO . STOP_REASON_STOPPED : self . _cmpReason = ClientJobsDAO . CMPL_REASON_STOPPED self . _isCanceled = True self . _logger . info ( "Model %s stopped because hypersearch ended" , self . _modelID ) else : raise RuntimeError ( "Unexpected stop reason encountered: %s" % ( stopReason ) )
12832	def validate_xml_name ( name ) : if len ( name ) == 0 : raise RuntimeError ( 'empty XML name' ) if __INVALID_NAME_CHARS & set ( name ) : raise RuntimeError ( 'XML name contains invalid character' ) if name [ 0 ] in __INVALID_NAME_START_CHARS : raise RuntimeError ( 'XML name starts with invalid character' )
3156	def delete ( self , list_id , segment_id ) : return self . _mc_client . _delete ( url = self . _build_path ( list_id , 'segments' , segment_id ) )
3988	def parallel_task_queue ( pool_size = multiprocessing . cpu_count ( ) ) : task_queue = TaskQueue ( pool_size ) yield task_queue task_queue . execute ( )
3407	def eval_gpr ( expr , knockouts ) : if isinstance ( expr , Expression ) : return eval_gpr ( expr . body , knockouts ) elif isinstance ( expr , Name ) : return expr . id not in knockouts elif isinstance ( expr , BoolOp ) : op = expr . op if isinstance ( op , Or ) : return any ( eval_gpr ( i , knockouts ) for i in expr . values ) elif isinstance ( op , And ) : return all ( eval_gpr ( i , knockouts ) for i in expr . values ) else : raise TypeError ( "unsupported operation " + op . __class__ . __name__ ) elif expr is None : return True else : raise TypeError ( "unsupported operation " + repr ( expr ) )
4974	def verify_edx_resources ( ) : required_methods = { 'ProgramDataExtender' : ProgramDataExtender , } for method in required_methods : if required_methods [ method ] is None : raise NotConnectedToOpenEdX ( _ ( "The following method from the Open edX platform is necessary for this view but isn't available." ) + "\nUnavailable: {method}" . format ( method = method ) )
7707	def save_roster ( self , dest , pretty = True ) : if self . roster is None : raise ValueError ( "No roster" ) element = self . roster . as_xml ( ) if pretty : if len ( element ) : element . text = u'\n ' p_child = None for child in element : if p_child is not None : p_child . tail = u'\n ' if len ( child ) : child . text = u'\n ' p_grand = None for grand in child : if p_grand is not None : p_grand . tail = u'\n ' p_grand = grand if p_grand is not None : p_grand . tail = u'\n ' p_child = child if p_child is not None : p_child . tail = u"\n" tree = ElementTree . ElementTree ( element ) tree . write ( dest , "utf-8" )
1975	def sys_random ( self , cpu , buf , count , rnd_bytes ) : ret = 0 if count != 0 : if count > Decree . CGC_SSIZE_MAX or count < 0 : ret = Decree . CGC_EINVAL else : if buf not in cpu . memory or ( buf + count ) not in cpu . memory : logger . info ( "RANDOM: buf points to invalid address. Returning EFAULT" ) return Decree . CGC_EFAULT with open ( "/dev/urandom" , "rb" ) as f : data = f . read ( count ) self . syscall_trace . append ( ( "_random" , - 1 , data ) ) cpu . write_bytes ( buf , data ) if rnd_bytes : if rnd_bytes not in cpu . memory : logger . info ( "RANDOM: Not valid rnd_bytes. Returning EFAULT" ) return Decree . CGC_EFAULT cpu . write_int ( rnd_bytes , len ( data ) , 32 ) logger . info ( "RANDOM(0x%08x, %d, 0x%08x) -> <%s>)" % ( buf , count , rnd_bytes , repr ( data [ : 10 ] ) ) ) return ret
8933	def run ( cmd , ** kw ) : kw = kw . copy ( ) kw . setdefault ( 'warn' , False ) report_error = kw . pop ( 'report_error' , True ) runner = kw . pop ( 'runner' , invoke_run ) try : return runner ( cmd , ** kw ) except exceptions . Failure as exc : sys . stdout . flush ( ) sys . stderr . flush ( ) if report_error : notify . error ( "Command `{}` failed with RC={}!" . format ( cmd , exc . result . return_code , ) ) raise finally : sys . stdout . flush ( ) sys . stderr . flush ( )
12718	def angle_rates ( self ) : return [ self . ode_obj . getAngleRate ( i ) for i in range ( self . ADOF ) ]
5781	def _create_buffers ( self , number ) : buffers = new ( secur32 , 'SecBuffer[%d]' % number ) for index in range ( 0 , number ) : buffers [ index ] . cbBuffer = 0 buffers [ index ] . BufferType = Secur32Const . SECBUFFER_EMPTY buffers [ index ] . pvBuffer = null ( ) sec_buffer_desc_pointer = struct ( secur32 , 'SecBufferDesc' ) sec_buffer_desc = unwrap ( sec_buffer_desc_pointer ) sec_buffer_desc . ulVersion = Secur32Const . SECBUFFER_VERSION sec_buffer_desc . cBuffers = number sec_buffer_desc . pBuffers = buffers return ( sec_buffer_desc_pointer , buffers )
12262	def add ( self , operator , * args ) : if isinstance ( operator , str ) : op = getattr ( proxops , operator ) ( * args ) elif isinstance ( operator , proxops . ProximalOperatorBaseClass ) : op = operator else : raise ValueError ( "operator must be a string or a subclass of ProximalOperator" ) self . operators . append ( op ) return self
6606	def run_multiple ( self , workingArea , package_indices ) : if not package_indices : return [ ] job_desc = self . _compose_job_desc ( workingArea , package_indices ) clusterprocids = submit_jobs ( job_desc , cwd = workingArea . path ) clusterids = clusterprocids2clusterids ( clusterprocids ) for clusterid in clusterids : change_job_priority ( [ clusterid ] , 10 ) self . clusterprocids_outstanding . extend ( clusterprocids ) return clusterprocids
4103	def setup ( app ) : app . add_config_value ( 'plot_gallery' , True , 'html' ) app . add_config_value ( 'abort_on_example_error' , False , 'html' ) app . add_config_value ( 'sphinx_gallery_conf' , gallery_conf , 'html' ) app . add_stylesheet ( 'gallery.css' ) app . connect ( 'builder-inited' , generate_gallery_rst ) app . connect ( 'build-finished' , embed_code_links )
9123	def belns ( keyword : str , file : TextIO , encoding : Optional [ str ] , use_names : bool ) : directory = get_data_dir ( keyword ) obo_url = f'http://purl.obolibrary.org/obo/{keyword}.obo' obo_path = os . path . join ( directory , f'{keyword}.obo' ) obo_cache_path = os . path . join ( directory , f'{keyword}.obo.pickle' ) obo_getter = make_obo_getter ( obo_url , obo_path , preparsed_path = obo_cache_path ) graph = obo_getter ( ) convert_obo_graph_to_belns ( graph , file = file , encoding = encoding , use_names = use_names , )
9437	def strip_ethernet ( packet ) : if not isinstance ( packet , Ethernet ) : packet = Ethernet ( packet ) payload = packet . payload return payload
2633	def scale_in ( self , blocks ) : status = dict ( zip ( self . engines , self . provider . status ( self . engines ) ) ) to_kill = [ engine for engine in status if status [ engine ] == "RUNNING" ] [ : blocks ] if self . provider : r = self . provider . cancel ( to_kill ) else : logger . error ( "No execution provider available" ) r = None return r
6696	def is_installed ( pkg_name ) : with settings ( hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) , warn_only = True ) : res = run ( "dpkg -s %(pkg_name)s" % locals ( ) ) for line in res . splitlines ( ) : if line . startswith ( "Status: " ) : status = line [ 8 : ] if "installed" in status . split ( ' ' ) : return True return False
4473	def __recursive_transform ( self , jam , steps ) : if len ( steps ) > 0 : head_transformer = steps [ 0 ] [ 1 ] for t_jam in head_transformer . transform ( jam ) : for q in self . __recursive_transform ( t_jam , steps [ 1 : ] ) : yield q else : yield jam
11545	def set_pwm_frequency ( self , frequency , pin = None ) : if pin is None : self . _set_pwm_frequency ( frequency , None ) else : pin_id = self . _pin_mapping . get ( pin , None ) if pin_id : self . _set_pwm_frequency ( frequency , pin_id ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
3461	def single_gene_deletion ( model , gene_list = None , method = "fba" , solution = None , processes = None , ** kwargs ) : return _multi_deletion ( model , 'gene' , element_lists = _element_lists ( model . genes , gene_list ) , method = method , solution = solution , processes = processes , ** kwargs )
6665	def verify_certificate_chain ( self , base = None , crt = None , csr = None , key = None ) : from burlap . common import get_verbose , print_fail , print_success r = self . local_renderer if base : crt = base + '.crt' csr = base + '.csr' key = base + '.key' else : assert crt and csr and key , 'If base not provided, crt and csr and key must be given.' assert os . path . isfile ( crt ) assert os . path . isfile ( csr ) assert os . path . isfile ( key ) csr_md5 = r . local ( 'openssl req -noout -modulus -in %s | openssl md5' % csr , capture = True ) key_md5 = r . local ( 'openssl rsa -noout -modulus -in %s | openssl md5' % key , capture = True ) crt_md5 = r . local ( 'openssl x509 -noout -modulus -in %s | openssl md5' % crt , capture = True ) match = crt_md5 == csr_md5 == key_md5 if self . verbose or not match : print ( 'crt:' , crt_md5 ) print ( 'csr:' , csr_md5 ) print ( 'key:' , key_md5 ) if match : print_success ( 'Files look good!' ) else : print_fail ( 'Files no not match!' ) raise Exception ( 'Files no not match!' )
10386	def match_simple_metapath ( graph , node , simple_metapath ) : if 0 == len ( simple_metapath ) : yield node , else : for neighbor in graph . edges [ node ] : if graph . nodes [ neighbor ] [ FUNCTION ] == simple_metapath [ 0 ] : for path in match_simple_metapath ( graph , neighbor , simple_metapath [ 1 : ] ) : if node not in path : yield ( node , ) + path
12848	def add_safety_checks ( meta , members ) : for member_name , member_value in members . items ( ) : members [ member_name ] = meta . add_safety_check ( member_name , member_value )
12897	def get_mute ( self ) : mute = ( yield from self . handle_int ( self . API . get ( 'mute' ) ) ) return bool ( mute )
4192	def compute_response ( self , ** kargs ) : from numpy . fft import fft , fftshift norm = kargs . get ( 'norm' , self . norm ) NFFT = kargs . get ( 'NFFT' , 2048 ) if NFFT < len ( self . data ) : NFFT = self . data . size * 2 A = fft ( self . data , NFFT ) mag = abs ( fftshift ( A ) ) if norm is True : mag = mag / max ( mag ) response = 20. * stools . log10 ( mag ) self . __response = response
1565	def invoke_hook_spout_ack ( self , message_id , complete_latency_ns ) : if len ( self . task_hooks ) > 0 : spout_ack_info = SpoutAckInfo ( message_id = message_id , spout_task_id = self . get_task_id ( ) , complete_latency_ms = complete_latency_ns * system_constants . NS_TO_MS ) for task_hook in self . task_hooks : task_hook . spout_ack ( spout_ack_info )
3417	def _cell ( x ) : x_no_none = [ i if i is not None else "" for i in x ] return array ( x_no_none , dtype = np_object )
5902	def prehook ( self , ** kwargs ) : cmd = [ 'smpd' , '-s' ] logger . info ( "Starting smpd: " + " " . join ( cmd ) ) rc = subprocess . call ( cmd ) return rc
8045	def parse_docstring ( self ) : self . log . debug ( "parsing docstring, token is %r (%s)" , self . current . kind , self . current . value ) while self . current . kind in ( tk . COMMENT , tk . NEWLINE , tk . NL ) : self . stream . move ( ) self . log . debug ( "parsing docstring, token is %r (%s)" , self . current . kind , self . current . value , ) if self . current . kind == tk . STRING : docstring = self . current . value self . stream . move ( ) return docstring return None
3333	def dynamic_instantiate_middleware ( name , args , expand = None ) : def _expand ( v ) : if expand and compat . is_basestring ( v ) and v . lower ( ) in expand : return expand [ v ] return v try : the_class = dynamic_import_class ( name ) inst = None if type ( args ) in ( tuple , list ) : args = tuple ( map ( _expand , args ) ) inst = the_class ( * args ) else : assert type ( args ) is dict args = { k : _expand ( v ) for k , v in args . items ( ) } inst = the_class ( ** args ) _logger . debug ( "Instantiate {}({}) => {}" . format ( name , args , inst ) ) except Exception : _logger . exception ( "ERROR: Instantiate {}({}) => {}" . format ( name , args , inst ) ) return inst
4039	def _cache ( self , response , key ) : thetime = datetime . datetime . utcnow ( ) . replace ( tzinfo = pytz . timezone ( "GMT" ) ) self . templates [ key ] = { "tmplt" : response . json ( ) , "updated" : thetime } return copy . deepcopy ( response . json ( ) )
9187	def includeme ( config ) : settings = config . registry . settings session_factory = SignedCookieSessionFactory ( settings [ 'session_key' ] ) config . set_session_factory ( session_factory )
7123	def write_config ( config , app_dir , filename = 'configuration.json' ) : path = os . path . join ( app_dir , filename ) with open ( path , 'w' ) as f : json . dump ( config , f , indent = 4 , cls = DetectMissingEncoder , separators = ( ',' , ': ' ) )
317	def perf_stats_bootstrap ( returns , factor_returns = None , return_stats = True , ** kwargs ) : bootstrap_values = OrderedDict ( ) for stat_func in SIMPLE_STAT_FUNCS : stat_name = STAT_FUNC_NAMES [ stat_func . __name__ ] bootstrap_values [ stat_name ] = calc_bootstrap ( stat_func , returns ) if factor_returns is not None : for stat_func in FACTOR_STAT_FUNCS : stat_name = STAT_FUNC_NAMES [ stat_func . __name__ ] bootstrap_values [ stat_name ] = calc_bootstrap ( stat_func , returns , factor_returns = factor_returns ) bootstrap_values = pd . DataFrame ( bootstrap_values ) if return_stats : stats = bootstrap_values . apply ( calc_distribution_stats ) return stats . T [ [ 'mean' , 'median' , '5%' , '95%' ] ] else : return bootstrap_values
5607	def resample_from_array ( in_raster = None , in_affine = None , out_tile = None , in_crs = None , resampling = "nearest" , nodataval = 0 ) : if isinstance ( in_raster , ma . MaskedArray ) : pass if isinstance ( in_raster , np . ndarray ) : in_raster = ma . MaskedArray ( in_raster , mask = in_raster == nodataval ) elif isinstance ( in_raster , ReferencedRaster ) : in_affine = in_raster . affine in_crs = in_raster . crs in_raster = in_raster . data elif isinstance ( in_raster , tuple ) : in_raster = ma . MaskedArray ( data = np . stack ( in_raster ) , mask = np . stack ( [ band . mask if isinstance ( band , ma . masked_array ) else np . where ( band == nodataval , True , False ) for band in in_raster ] ) , fill_value = nodataval ) else : raise TypeError ( "wrong input data type: %s" % type ( in_raster ) ) if in_raster . ndim == 2 : in_raster = ma . expand_dims ( in_raster , axis = 0 ) elif in_raster . ndim == 3 : pass else : raise TypeError ( "input array must have 2 or 3 dimensions" ) if in_raster . fill_value != nodataval : ma . set_fill_value ( in_raster , nodataval ) out_shape = ( in_raster . shape [ 0 ] , ) + out_tile . shape dst_data = np . empty ( out_shape , in_raster . dtype ) in_raster = ma . masked_array ( data = in_raster . filled ( ) , mask = in_raster . mask , fill_value = nodataval ) reproject ( in_raster , dst_data , src_transform = in_affine , src_crs = in_crs if in_crs else out_tile . crs , dst_transform = out_tile . affine , dst_crs = out_tile . crs , resampling = Resampling [ resampling ] ) return ma . MaskedArray ( dst_data , mask = dst_data == nodataval )
11451	def get_collection ( self , journal ) : conference = '' for tag in self . document . getElementsByTagName ( 'conference' ) : conference = xml_to_text ( tag ) if conference or journal == "International Journal of Modern Physics: Conference Series" : return [ ( 'a' , 'HEP' ) , ( 'a' , 'ConferencePaper' ) ] elif self . _get_article_type ( ) == "review-article" : return [ ( 'a' , 'HEP' ) , ( 'a' , 'Review' ) ] else : return [ ( 'a' , 'HEP' ) , ( 'a' , 'Published' ) ]
9338	def map ( self , func , sequence , reduce = None , star = False , minlength = 0 ) : def realreduce ( r ) : if reduce : if isinstance ( r , tuple ) : return reduce ( * r ) else : return reduce ( r ) return r def realfunc ( i ) : if star : return func ( * i ) else : return func ( i ) if len ( sequence ) <= 0 or self . np == 0 or get_debug ( ) : self . local = lambda : None self . local . rank = 0 rt = [ realreduce ( realfunc ( i ) ) for i in sequence ] self . local = None return rt np = min ( [ self . np , len ( sequence ) ] ) Q = self . backend . QueueFactory ( 64 ) R = self . backend . QueueFactory ( 64 ) self . ordered . reset ( ) pg = ProcessGroup ( main = self . _main , np = np , backend = self . backend , args = ( Q , R , sequence , realfunc ) ) pg . start ( ) L = [ ] N = [ ] def feeder ( pg , Q , N ) : j = 0 try : for i , work in enumerate ( sequence ) : if not hasattr ( sequence , '__getitem__' ) : pg . put ( Q , ( i , work ) ) else : pg . put ( Q , ( i , ) ) j = j + 1 N . append ( j ) for i in range ( np ) : pg . put ( Q , None ) except StopProcessGroup : return finally : pass feeder = threading . Thread ( None , feeder , args = ( pg , Q , N ) ) feeder . start ( ) count = 0 try : while True : try : capsule = pg . get ( R ) except queue . Empty : continue except StopProcessGroup : raise pg . get_exception ( ) capsule = capsule [ 0 ] , realreduce ( capsule [ 1 ] ) heapq . heappush ( L , capsule ) count = count + 1 if len ( N ) > 0 and count == N [ 0 ] : break rt = [ ] while len ( L ) > 0 : rt . append ( heapq . heappop ( L ) [ 1 ] ) pg . join ( ) feeder . join ( ) assert N [ 0 ] == len ( rt ) return rt except BaseException as e : pg . killall ( ) pg . join ( ) feeder . join ( ) raise
1615	def ReplaceAll ( pattern , rep , s ) : if pattern not in _regexp_compile_cache : _regexp_compile_cache [ pattern ] = sre_compile . compile ( pattern ) return _regexp_compile_cache [ pattern ] . sub ( rep , s )
5626	def write_json ( path , params ) : logger . debug ( "write %s to %s" , params , path ) if path . startswith ( "s3://" ) : bucket = get_boto3_bucket ( path . split ( "/" ) [ 2 ] ) key = "/" . join ( path . split ( "/" ) [ 3 : ] ) logger . debug ( "upload %s" , key ) bucket . put_object ( Key = key , Body = json . dumps ( params , sort_keys = True , indent = 4 ) ) else : makedirs ( os . path . dirname ( path ) ) with open ( path , 'w' ) as dst : json . dump ( params , dst , sort_keys = True , indent = 4 )
9267	def get_time_of_tag ( self , tag ) : if not tag : raise ChangelogGeneratorError ( "tag is nil" ) name_of_tag = tag [ "name" ] time_for_name = self . tag_times_dict . get ( name_of_tag , None ) if time_for_name : return time_for_name else : time_string = self . fetcher . fetch_date_of_tag ( tag ) try : self . tag_times_dict [ name_of_tag ] = timestring_to_datetime ( time_string ) except UnicodeWarning : print ( "ERROR ERROR:" , tag ) self . tag_times_dict [ name_of_tag ] = timestring_to_datetime ( time_string ) return self . tag_times_dict [ name_of_tag ]
9696	def validate_token ( self , request , consumer , token ) : oauth_server , oauth_request = oauth_provider . utils . initialize_server_request ( request ) oauth_server . verify_request ( oauth_request , consumer , token )
1479	def _wait_process_std_out_err ( self , name , process ) : proc . stream_process_stdout ( process , stdout_log_fn ( name ) ) process . wait ( )
11680	def disconnect ( self ) : logger . info ( u'Disconnecting' ) self . sock . shutdown ( socket . SHUT_RDWR ) self . sock . close ( ) self . state = DISCONNECTED
7951	def wait_for_readability ( self ) : with self . lock : while True : if self . _socket is None or self . _eof : return False if self . _state in ( "connected" , "closing" ) : return True if self . _state == "tls-handshake" and self . _tls_state == "want_read" : return True self . _state_cond . wait ( )
5389	def _datetime_in_range ( self , dt , dt_min = None , dt_max = None ) : dt = dt . replace ( microsecond = 0 ) if dt_min : dt_min = dt_min . replace ( microsecond = 0 ) else : dt_min = dsub_util . replace_timezone ( datetime . datetime . min , pytz . utc ) if dt_max : dt_max = dt_max . replace ( microsecond = 0 ) else : dt_max = dsub_util . replace_timezone ( datetime . datetime . max , pytz . utc ) return dt_min <= dt <= dt_max
7857	def __error ( self , stanza ) : try : self . error ( stanza . get_error ( ) ) except ProtocolError : from . . error import StanzaErrorNode self . error ( StanzaErrorNode ( "undefined-condition" ) )
7572	def clustdealer ( pairdealer , optim ) : ccnt = 0 chunk = [ ] while ccnt < optim : try : taker = itertools . takewhile ( lambda x : x [ 0 ] != "//\n" , pairdealer ) oneclust = [ "" . join ( taker . next ( ) ) ] except StopIteration : return 1 , chunk while 1 : try : oneclust . append ( "" . join ( taker . next ( ) ) ) except StopIteration : break chunk . append ( "" . join ( oneclust ) ) ccnt += 1 return 0 , chunk
9211	def cli ( url , user_agent ) : kwargs = { } if user_agent : kwargs [ 'user_agent' ] = user_agent archive_url = capture ( url , ** kwargs ) click . echo ( archive_url )
3099	def _validate_clientsecrets ( clientsecrets_dict ) : _INVALID_FILE_FORMAT_MSG = ( 'Invalid file format. See ' 'https://developers.google.com/api-client-library/' 'python/guide/aaa_client_secrets' ) if clientsecrets_dict is None : raise InvalidClientSecretsError ( _INVALID_FILE_FORMAT_MSG ) try : ( client_type , client_info ) , = clientsecrets_dict . items ( ) except ( ValueError , AttributeError ) : raise InvalidClientSecretsError ( _INVALID_FILE_FORMAT_MSG + ' ' 'Expected a JSON object with a single property for a "web" or ' '"installed" application' ) if client_type not in VALID_CLIENT : raise InvalidClientSecretsError ( 'Unknown client type: {0}.' . format ( client_type ) ) for prop_name in VALID_CLIENT [ client_type ] [ 'required' ] : if prop_name not in client_info : raise InvalidClientSecretsError ( 'Missing property "{0}" in a client type of "{1}".' . format ( prop_name , client_type ) ) for prop_name in VALID_CLIENT [ client_type ] [ 'string' ] : if client_info [ prop_name ] . startswith ( '[[' ) : raise InvalidClientSecretsError ( 'Property "{0}" is not configured.' . format ( prop_name ) ) return client_type , client_info
8258	def sort_by_distance ( self , reversed = False ) : if len ( self ) == 0 : return ColorList ( ) root = self [ 0 ] for clr in self [ 1 : ] : if clr . brightness < root . brightness : root = clr stack = [ clr for clr in self ] stack . remove ( root ) sorted = [ root ] while len ( stack ) > 1 : closest , distance = stack [ 0 ] , stack [ 0 ] . distance ( sorted [ - 1 ] ) for clr in stack [ 1 : ] : d = clr . distance ( sorted [ - 1 ] ) if d < distance : closest , distance = clr , d stack . remove ( closest ) sorted . append ( closest ) sorted . append ( stack [ 0 ] ) if reversed : _list . reverse ( sorted ) return ColorList ( sorted )
6824	def restart ( self ) : n = 60 sleep_n = int ( self . env . max_restart_wait_minutes / 10. * 60 ) for _ in xrange ( n ) : self . stop ( ) if self . dryrun or not self . is_running ( ) : break print ( 'Waiting for supervisor to stop (%i of %i)...' % ( _ , n ) ) time . sleep ( sleep_n ) self . start ( ) for _ in xrange ( n ) : if self . dryrun or self . is_running ( ) : return print ( 'Waiting for supervisor to start (%i of %i)...' % ( _ , n ) ) time . sleep ( sleep_n ) raise Exception ( 'Failed to restart service %s!' % self . name )
2200	def ensure_app_data_dir ( appname , * args ) : from ubelt import util_path dpath = get_app_data_dir ( appname , * args ) util_path . ensuredir ( dpath ) return dpath
11099	def select_by_mtime ( self , min_time = 0 , max_time = ts_2100 , recursive = True ) : def filters ( p ) : return min_time <= p . mtime <= max_time return self . select_file ( filters , recursive )
2903	def ref ( function , callback = None ) : try : function . __func__ except AttributeError : return _WeakMethodFree ( function , callback ) return _WeakMethodBound ( function , callback )
11248	def median ( data ) : ordered = sorted ( data ) length = len ( ordered ) if length % 2 == 0 : return ( ordered [ math . floor ( length / 2 ) - 1 ] + ordered [ math . floor ( length / 2 ) ] ) / 2.0 elif length % 2 != 0 : return ordered [ math . floor ( length / 2 ) ]
3701	def solubility_eutectic ( T , Tm , Hm , Cpl = 0 , Cps = 0 , gamma = 1 ) : r dCp = Cpl - Cps x = exp ( - Hm / R / T * ( 1 - T / Tm ) + dCp * ( Tm - T ) / R / T - dCp / R * log ( Tm / T ) ) / gamma return x
13823	def end_timing ( self ) : if self . _callback != None : elapsed = time . clock ( ) * 1000 - self . _start self . _callback . end_timing ( self . _counter , elapsed )
9426	def namelist ( self ) : names = [ ] for member in self . filelist : names . append ( member . filename ) return names
783	def jobGetDemand ( self , ) : rows = self . _getMatchingRowsWithRetries ( self . _jobs , dict ( status = self . STATUS_RUNNING ) , [ self . _jobs . pubToDBNameDict [ f ] for f in self . _jobs . jobDemandNamedTuple . _fields ] ) return [ self . _jobs . jobDemandNamedTuple . _make ( r ) for r in rows ]
11466	def ls ( self , folder = '' ) : current_folder = self . _ftp . pwd ( ) self . cd ( folder ) contents = [ ] self . _ftp . retrlines ( 'LIST' , lambda a : contents . append ( a ) ) files = filter ( lambda a : a . split ( ) [ 0 ] . startswith ( '-' ) , contents ) folders = filter ( lambda a : a . split ( ) [ 0 ] . startswith ( 'd' ) , contents ) files = map ( lambda a : ' ' . join ( a . split ( ) [ 8 : ] ) , files ) folders = map ( lambda a : ' ' . join ( a . split ( ) [ 8 : ] ) , folders ) self . _ftp . cwd ( current_folder ) return files , folders
10166	def get_md_device ( self , line , personalities = [ ] ) : ret = { } splitted = split ( '\W+' , line ) ret [ 'status' ] = splitted [ 1 ] if splitted [ 2 ] in personalities : ret [ 'type' ] = splitted [ 2 ] ret [ 'components' ] = self . get_components ( line , with_type = True ) else : ret [ 'type' ] = None ret [ 'components' ] = self . get_components ( line , with_type = False ) return ret
9820	def project ( ctx , project ) : if ctx . invoked_subcommand not in [ 'create' , 'list' ] : ctx . obj = ctx . obj or { } ctx . obj [ 'project' ] = project
2208	def ensuredir ( dpath , mode = 0o1777 , verbose = None ) : r if verbose is None : verbose = 0 if isinstance ( dpath , ( list , tuple ) ) : dpath = join ( * dpath ) if not exists ( dpath ) : if verbose : print ( 'Ensuring new directory (%r)' % dpath ) if sys . version_info . major == 2 : os . makedirs ( normpath ( dpath ) , mode = mode ) else : os . makedirs ( normpath ( dpath ) , mode = mode , exist_ok = True ) else : if verbose : print ( 'Ensuring existing directory (%r)' % dpath ) return dpath
11025	def _get_container_port_mappings ( app ) : container = app [ 'container' ] port_mappings = container . get ( 'portMappings' ) if port_mappings is None and 'docker' in container : port_mappings = container [ 'docker' ] . get ( 'portMappings' ) return port_mappings
761	def modifyBits ( inputVal , maxChanges ) : changes = np . random . random_integers ( 0 , maxChanges , 1 ) [ 0 ] if changes == 0 : return inputVal inputWidth = len ( inputVal ) whatToChange = np . random . random_integers ( 0 , 41 , changes ) runningIndex = - 1 numModsDone = 0 for i in xrange ( inputWidth ) : if numModsDone >= changes : break if inputVal [ i ] == 1 : runningIndex += 1 if runningIndex in whatToChange : if i != 0 and inputVal [ i - 1 ] == 0 : inputVal [ i - 1 ] = 1 inputVal [ i ] = 0 return inputVal
11325	def extract_oembeds ( self , text , maxwidth = None , maxheight = None , resource_type = None ) : parser = text_parser ( ) urls = parser . extract_urls ( text ) return self . handle_extracted_urls ( urls , maxwidth , maxheight , resource_type )
12752	def indices_for_joint ( self , name ) : j = 0 for joint in self . joints : if joint . name == name : return list ( range ( j , j + joint . ADOF ) ) j += joint . ADOF return [ ]
7447	def _step6func ( self , samples , noreverse , force , randomseed , ipyclient , ** kwargs ) : samples = _get_samples ( self , samples ) csamples = self . _samples_precheck ( samples , 6 , force ) if self . _headers : print ( "\n Step 6: Clustering at {} similarity across {} samples" . format ( self . paramsdict [ "clust_threshold" ] , len ( csamples ) ) ) if not csamples : raise IPyradError ( FIRST_RUN_5 ) elif not force : if all ( [ i . stats . state >= 6 for i in csamples ] ) : print ( DATABASE_EXISTS . format ( len ( samples ) ) ) return assemble . cluster_across . run ( self , csamples , noreverse , force , randomseed , ipyclient , ** kwargs )
11272	def register_default_types ( ) : register_type ( type , pipe . map ) register_type ( types . FunctionType , pipe . map ) register_type ( types . MethodType , pipe . map ) register_type ( tuple , seq ) register_type ( list , seq ) register_type ( types . GeneratorType , seq ) register_type ( string_type , sh ) register_type ( unicode_type , sh ) register_type ( file_type , fileobj ) if is_py3 : register_type ( range , seq ) register_type ( map , seq )
9756	def update ( ctx , name , description , tags ) : user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) update_dict = { } if name : update_dict [ 'name' ] = name if description : update_dict [ 'description' ] = description tags = validate_tags ( tags ) if tags : update_dict [ 'tags' ] = tags if not update_dict : Printer . print_warning ( 'No argument was provided to update the experiment.' ) sys . exit ( 0 ) try : response = PolyaxonClient ( ) . experiment . update_experiment ( user , project_name , _experiment , update_dict ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not update experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Experiment updated." ) get_experiment_details ( response )
13288	def read_git_commit_timestamp_for_file ( filepath , repo_path = None , repo = None ) : logger = logging . getLogger ( __name__ ) if repo is None : repo = git . repo . base . Repo ( path = repo_path , search_parent_directories = True ) repo_path = repo . working_tree_dir head_commit = repo . head . commit logger . debug ( 'Using Git repo at %r' , repo_path ) filepath = os . path . relpath ( os . path . abspath ( filepath ) , start = repo_path ) logger . debug ( 'Repo-relative filepath is %r' , filepath ) for commit in head_commit . iter_items ( repo , head_commit , [ filepath ] , skip = 0 ) : return commit . committed_datetime raise IOError ( 'File {} not found' . format ( filepath ) )
12659	def import_pyfile ( filepath , mod_name = None ) : import sys if sys . version_info . major == 3 : import importlib . machinery loader = importlib . machinery . SourceFileLoader ( '' , filepath ) mod = loader . load_module ( mod_name ) else : import imp mod = imp . load_source ( mod_name , filepath ) return mod
47	def shift ( self , x = 0 , y = 0 ) : return self . deepcopy ( self . x + x , self . y + y )
755	def __getDictMetaInfo ( self , inferenceElement , inferenceDict ) : fieldMetaInfo = [ ] inferenceLabel = InferenceElement . getLabel ( inferenceElement ) if InferenceElement . getInputElement ( inferenceElement ) : fieldMetaInfo . append ( FieldMetaInfo ( name = inferenceLabel + ".actual" , type = FieldMetaType . string , special = '' ) ) keys = sorted ( inferenceDict . keys ( ) ) for key in keys : fieldMetaInfo . append ( FieldMetaInfo ( name = inferenceLabel + "." + str ( key ) , type = FieldMetaType . string , special = '' ) ) return fieldMetaInfo
9390	def get_aggregation_timestamp ( self , timestamp , granularity = 'second' ) : if granularity is None or granularity . lower ( ) == 'none' : return int ( timestamp ) , 1 elif granularity == 'hour' : return ( int ( timestamp ) / ( 3600 * 1000 ) ) * 3600 * 1000 , 3600 elif granularity == 'minute' : return ( int ( timestamp ) / ( 60 * 1000 ) ) * 60 * 1000 , 60 else : return ( int ( timestamp ) / 1000 ) * 1000 , 1
1247	def recv ( self , socket_ , encoding = None ) : unpacker = msgpack . Unpacker ( encoding = encoding ) response = socket_ . recv ( 8 ) if response == b"" : raise TensorForceError ( "No data received by socket.recv in call to method `recv` " + "(listener possibly closed)!" ) orig_len = int ( response ) received_len = 0 while True : data = socket_ . recv ( min ( orig_len - received_len , self . max_msg_len ) ) if not data : raise TensorForceError ( "No data of len {} received by socket.recv in call to method `recv`!" . format ( orig_len - received_len ) ) data_len = len ( data ) received_len += data_len unpacker . feed ( data ) if received_len == orig_len : break for message in unpacker : sts = message . get ( "status" , message . get ( b"status" ) ) if sts : if sts == "ok" or sts == b"ok" : return message else : raise TensorForceError ( "RemoteEnvironment server error: {}" . format ( message . get ( "message" , "not specified" ) ) ) else : raise TensorForceError ( "Message without field 'status' received!" ) raise TensorForceError ( "No message encoded in data stream (data stream had len={})" . format ( orig_len ) )
3467	def functional ( self ) : if self . _model : tree , _ = parse_gpr ( self . gene_reaction_rule ) return eval_gpr ( tree , { gene . id for gene in self . genes if not gene . functional } ) return True
10976	def delete ( group_id ) : group = Group . query . get_or_404 ( group_id ) if group . can_edit ( current_user ) : try : group . delete ( ) except Exception as e : flash ( str ( e ) , "error" ) return redirect ( url_for ( ".index" ) ) flash ( _ ( 'Successfully removed group "%(group_name)s"' , group_name = group . name ) , 'success' ) return redirect ( url_for ( ".index" ) ) flash ( _ ( 'You cannot delete the group %(group_name)s' , group_name = group . name ) , 'error' ) return redirect ( url_for ( ".index" ) )
13214	def available ( self , timeout = 5 ) : host = self . _connect_args [ 'host' ] port = self . _connect_args [ 'port' ] try : sock = socket . create_connection ( ( host , port ) , timeout = timeout ) sock . close ( ) return True except socket . error : pass return False
3106	def code_challenge ( verifier ) : digest = hashlib . sha256 ( verifier ) . digest ( ) return base64 . urlsafe_b64encode ( digest ) . rstrip ( b'=' )
11391	def contribute_to_class ( self , cls , name ) : super ( EmbeddedMediaField , self ) . contribute_to_class ( cls , name ) register_field ( cls , self ) cls . _meta . add_virtual_field ( EmbeddedSignalCreator ( self ) )
12679	def can_send ( self , user , notice_type ) : from notification . models import NoticeSetting return NoticeSetting . for_user ( user , notice_type , self . medium_id ) . send
8529	def report ( self ) : self . _output . write ( '\r' ) sort_by = 'avg' results = { } for key , latencies in self . _latencies_by_method . items ( ) : result = { } result [ 'count' ] = len ( latencies ) result [ 'avg' ] = sum ( latencies ) / len ( latencies ) result [ 'min' ] = min ( latencies ) result [ 'max' ] = max ( latencies ) latencies = sorted ( latencies ) result [ 'p90' ] = percentile ( latencies , 0.90 ) result [ 'p95' ] = percentile ( latencies , 0.95 ) result [ 'p99' ] = percentile ( latencies , 0.99 ) result [ 'p999' ] = percentile ( latencies , 0.999 ) results [ key ] = result headers = [ 'method' , 'count' , 'avg' , 'min' , 'max' , 'p90' , 'p95' , 'p99' , 'p999' ] data = [ ] results = sorted ( results . items ( ) , key = lambda it : it [ 1 ] [ sort_by ] , reverse = True ) def row ( key , res ) : data = [ key ] + [ res [ header ] for header in headers [ 1 : ] ] return tuple ( data ) data = [ row ( key , result ) for key , result in results ] self . _output . write ( '%s\n' % tabulate ( data , headers = headers ) ) self . _output . flush ( )
1831	def JBE ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , Operators . OR ( cpu . CF , cpu . ZF ) , target . read ( ) , cpu . PC )
731	def _getW ( self ) : w = self . _w if type ( w ) is list : return w [ self . _random . getUInt32 ( len ( w ) ) ] else : return w
11541	def write ( self , pin , value , pwm = False ) : if type ( pin ) is list : for p in pin : self . write ( p , value , pwm ) return if pwm and type ( value ) is not int and type ( value ) is not float : raise TypeError ( 'pwm is set, but value is not a float or int' ) pin_id = self . _pin_mapping . get ( pin , None ) if pin_id : lpin = self . _pin_lin . get ( pin , None ) if lpin and type ( lpin [ 'write' ] ) is tuple : write_range = lpin [ 'write' ] value = self . _linear_interpolation ( value , * write_range ) self . _write ( pin_id , value , pwm ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
7927	def reorder_srv ( records ) : records = list ( records ) records . sort ( ) ret = [ ] tmp = [ ] for rrecord in records : if not tmp or rrecord . priority == tmp [ 0 ] . priority : tmp . append ( rrecord ) continue ret += shuffle_srv ( tmp ) tmp = [ rrecord ] if tmp : ret += shuffle_srv ( tmp ) return ret
7039	def object_info ( lcc_server , objectid , db_collection_id ) : urlparams = { 'objectid' : objectid , 'collection' : db_collection_id } urlqs = urlencode ( urlparams ) url = '%s/api/object?%s' % ( lcc_server , urlqs ) try : LOGINFO ( 'getting info for %s in collection %s from %s' % ( objectid , db_collection_id , lcc_server ) ) have_apikey , apikey , expires = check_existing_apikey ( lcc_server ) if not have_apikey : apikey , expires = get_new_apikey ( lcc_server ) if apikey : headers = { 'Authorization' : 'Bearer: %s' % apikey } else : headers = { } req = Request ( url , data = None , headers = headers ) resp = urlopen ( req ) objectinfo = json . loads ( resp . read ( ) ) [ 'result' ] return objectinfo except HTTPError as e : if e . code == 404 : LOGERROR ( 'additional info for object %s not ' 'found in collection: %s' % ( objectid , db_collection_id ) ) else : LOGERROR ( 'could not retrieve object info, ' 'URL used: %s, error code: %s, reason: %s' % ( url , e . code , e . reason ) ) return None
13310	def fullStats ( a , b ) : stats = [ [ 'bias' , 'Bias' , bias ( a , b ) ] , [ 'stderr' , 'Standard Deviation Error' , stderr ( a , b ) ] , [ 'mae' , 'Mean Absolute Error' , mae ( a , b ) ] , [ 'rmse' , 'Root Mean Square Error' , rmse ( a , b ) ] , [ 'nmse' , 'Normalized Mean Square Error' , nmse ( a , b ) ] , [ 'mfbe' , 'Mean Fractionalized bias Error' , mfbe ( a , b ) ] , [ 'fa2' , 'Factor of Two' , fa ( a , b , 2 ) ] , [ 'foex' , 'Factor of Exceedance' , foex ( a , b ) ] , [ 'correlation' , 'Correlation R' , correlation ( a , b ) ] , [ 'determination' , 'Coefficient of Determination r2' , determination ( a , b ) ] , [ 'gmb' , 'Geometric Mean Bias' , gmb ( a , b ) ] , [ 'gmv' , 'Geometric Mean Variance' , gmv ( a , b ) ] , [ 'fmt' , 'Figure of Merit in Time' , fmt ( a , b ) ] ] rec = np . rec . fromrecords ( stats , names = ( 'stat' , 'description' , 'result' ) ) df = pd . DataFrame . from_records ( rec , index = 'stat' ) return df
13023	def select ( self , sql_string , cols , * args , ** kwargs ) : working_columns = None if kwargs . get ( 'columns' ) is not None : working_columns = kwargs . pop ( 'columns' ) query = self . _assemble_select ( sql_string , cols , * args , * kwargs ) return self . _execute ( query , working_columns = working_columns )
6359	def sim ( self , src , tar ) : r if src == tar : return 1.0 elif not src or not tar : return 0.0 return len ( self . lcsstr ( src , tar ) ) / max ( len ( src ) , len ( tar ) )
13663	def get_item ( filename , uuid ) : with open ( os . fsencode ( str ( filename ) ) , "r" ) as f : data = json . load ( f ) results = [ i for i in data if i [ "uuid" ] == str ( uuid ) ] if results : return results return None
1696	def clone ( self , num_clones ) : retval = [ ] for i in range ( num_clones ) : retval . append ( self . repartition ( self . get_num_partitions ( ) ) ) return retval
4354	def _save_ack_callback ( self , msgid , callback ) : if msgid in self . ack_callbacks : return False self . ack_callbacks [ msgid ] = callback
5380	def build_pipeline_args ( cls , project , script , job_params , task_params , reserved_labels , preemptible , logging_uri , scopes , keep_alive ) : inputs = { } inputs . update ( { SCRIPT_VARNAME : script } ) inputs . update ( { var . name : var . value for var in job_params [ 'envs' ] | task_params [ 'envs' ] if var . value } ) inputs . update ( { var . name : var . uri for var in job_params [ 'inputs' ] | task_params [ 'inputs' ] if not var . recursive and var . value } ) outputs = { } for var in job_params [ 'outputs' ] | task_params [ 'outputs' ] : if var . recursive or not var . value : continue if '*' in var . uri . basename : outputs [ var . name ] = var . uri . path else : outputs [ var . name ] = var . uri labels = { } labels . update ( { label . name : label . value if label . value else '' for label in ( reserved_labels | job_params [ 'labels' ] | task_params [ 'labels' ] ) } ) args = { 'pipelineArgs' : { 'projectId' : project , 'resources' : { 'preemptible' : preemptible , } , 'inputs' : inputs , 'outputs' : outputs , 'labels' : labels , 'serviceAccount' : { 'email' : 'default' , 'scopes' : scopes , } , 'logging' : { 'gcsPath' : logging_uri } , } } if keep_alive : args [ 'pipelineArgs' ] [ 'keep_vm_alive_on_failure_duration' ] = '%ss' % keep_alive return args
1744	def pythonize_arguments ( arg_str ) : out_args = [ ] if arg_str is None : return out_str args = arg_str . split ( ',' ) for arg in args : components = arg . split ( '=' ) name_and_type = components [ 0 ] . split ( ' ' ) if name_and_type [ - 1 ] == '' and len ( name_and_type ) > 1 : name = name_and_type [ - 2 ] else : name = name_and_type [ - 1 ] if len ( components ) > 1 : name += '=' + components [ 1 ] out_args . append ( name ) return ',' . join ( out_args )
6613	def receive_finished ( self ) : if not self . isopen : logger = logging . getLogger ( __name__ ) logger . warning ( 'the drop box is not open' ) return return self . dropbox . poll ( )
13456	def open_s3 ( bucket ) : conn = boto . connect_s3 ( options . paved . s3 . access_id , options . paved . s3 . secret ) try : bucket = conn . get_bucket ( bucket ) except boto . exception . S3ResponseError : bucket = conn . create_bucket ( bucket ) return bucket
3540	def status_printer ( ) : last_len = [ 0 ] def p ( s ) : s = next ( spinner ) + ' ' + s len_s = len ( s ) output = '\r' + s + ( ' ' * max ( last_len [ 0 ] - len_s , 0 ) ) sys . stdout . write ( output ) sys . stdout . flush ( ) last_len [ 0 ] = len_s return p
13557	def get_all_images ( self ) : self_imgs = self . image_set . all ( ) update_ids = self . update_set . values_list ( 'id' , flat = True ) u_images = UpdateImage . objects . filter ( update__id__in = update_ids ) return list ( chain ( self_imgs , u_images ) )
5915	def _process_range ( self , selection , name = None ) : try : first , last , gmx_atomname = selection except ValueError : try : first , last = selection gmx_atomname = '*' except : logger . error ( "%r is not a valid range selection" , selection ) raise if name is None : name = "{first!s}-{last!s}_{gmx_atomname!s}" . format ( ** vars ( ) ) _first = self . _translate_residue ( first , default_atomname = gmx_atomname ) _last = self . _translate_residue ( last , default_atomname = gmx_atomname ) _selection = 'r {0:d} - {1:d} & & a {2!s}' . format ( _first [ 'resid' ] , _last [ 'resid' ] , gmx_atomname ) cmd = [ 'keep 0' , 'del 0' , _selection , 'name 0 {name!s}' . format ( ** vars ( ) ) , 'q' ] fd , ndx = tempfile . mkstemp ( suffix = '.ndx' , prefix = name + '__' ) rc , out , err = self . make_ndx ( n = self . ndx , o = ndx , input = cmd ) self . check_output ( out , "No atoms found for " "%(selection)r % vars ( ) ) return name , ndx
1262	def import_demonstrations ( self , demonstrations ) : if isinstance ( demonstrations , dict ) : if self . unique_state : demonstrations [ 'states' ] = dict ( state = demonstrations [ 'states' ] ) if self . unique_action : demonstrations [ 'actions' ] = dict ( action = demonstrations [ 'actions' ] ) self . model . import_demo_experience ( ** demonstrations ) else : if self . unique_state : states = dict ( state = list ( ) ) else : states = { name : list ( ) for name in demonstrations [ 0 ] [ 'states' ] } internals = { name : list ( ) for name in demonstrations [ 0 ] [ 'internals' ] } if self . unique_action : actions = dict ( action = list ( ) ) else : actions = { name : list ( ) for name in demonstrations [ 0 ] [ 'actions' ] } terminal = list ( ) reward = list ( ) for demonstration in demonstrations : if self . unique_state : states [ 'state' ] . append ( demonstration [ 'states' ] ) else : for name , state in states . items ( ) : state . append ( demonstration [ 'states' ] [ name ] ) for name , internal in internals . items ( ) : internal . append ( demonstration [ 'internals' ] [ name ] ) if self . unique_action : actions [ 'action' ] . append ( demonstration [ 'actions' ] ) else : for name , action in actions . items ( ) : action . append ( demonstration [ 'actions' ] [ name ] ) terminal . append ( demonstration [ 'terminal' ] ) reward . append ( demonstration [ 'reward' ] ) self . model . import_demo_experience ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward )
11907	def four_blocks ( topleft , topright , bottomleft , bottomright ) : return vstack ( hstack ( topleft , topright ) , hstack ( bottomleft , bottomright ) )
1111	def _qformat ( self , aline , bline , atags , btags ) : r common = min ( _count_leading ( aline , "\t" ) , _count_leading ( bline , "\t" ) ) common = min ( common , _count_leading ( atags [ : common ] , " " ) ) common = min ( common , _count_leading ( btags [ : common ] , " " ) ) atags = atags [ common : ] . rstrip ( ) btags = btags [ common : ] . rstrip ( ) yield "- " + aline if atags : yield "? %s%s\n" % ( "\t" * common , atags ) yield "+ " + bline if btags : yield "? %s%s\n" % ( "\t" * common , btags )
8704	def read_file ( self , filename , destination = '' ) : if not destination : destination = filename log . info ( 'Transferring %s to %s' , filename , destination ) data = self . download_file ( filename ) log . info ( destination ) if not os . path . exists ( os . path . dirname ( destination ) ) : try : os . makedirs ( os . path . dirname ( destination ) ) except OSError as e : if e . errno != errno . EEXIST : raise with open ( destination , 'w' ) as fil : fil . write ( data )
13080	def register ( self ) : if self . app is not None : if not self . blueprint : self . blueprint = self . create_blueprint ( ) self . app . register_blueprint ( self . blueprint ) if self . cache is None : setattr ( self . app . jinja_env , "_fake_cache_extension" , self ) self . app . jinja_env . add_extension ( FakeCacheExtension ) return self . blueprint return None
562	def addEncoder ( self , name , encoder ) : self . encoders . append ( ( name , encoder , self . width ) ) for d in encoder . getDescription ( ) : self . description . append ( ( d [ 0 ] , d [ 1 ] + self . width ) ) self . width += encoder . getWidth ( )
9560	def _apply_assert_methods ( self , i , r , summarize = False , report_unexpected_exceptions = True , context = None ) : for a in dir ( self ) : if a . startswith ( 'assert' ) : rdict = self . _as_dict ( r ) f = getattr ( self , a ) try : f ( rdict ) except AssertionError as e : code = ASSERT_CHECK_FAILED message = MESSAGES [ ASSERT_CHECK_FAILED ] if len ( e . args ) > 0 : custom = e . args [ 0 ] if isinstance ( custom , ( list , tuple ) ) : if len ( custom ) > 0 : code = custom [ 0 ] if len ( custom ) > 1 : message = custom [ 1 ] else : code = custom p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r if context is not None : p [ 'context' ] = context yield p except Exception as e : if report_unexpected_exceptions : p = { 'code' : UNEXPECTED_EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED_EXCEPTION ] % ( e . __class__ . __name__ , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( f . __name__ , f . __doc__ ) if context is not None : p [ 'context' ] = context yield p
2340	def GNN_instance ( x , idx = 0 , device = None , nh = 20 , ** kwargs ) : device = SETTINGS . get_default ( device = device ) xy = scale ( x ) . astype ( 'float32' ) inputx = th . FloatTensor ( xy [ : , [ 0 ] ] ) . to ( device ) target = th . FloatTensor ( xy [ : , [ 1 ] ] ) . to ( device ) GNNXY = GNN_model ( x . shape [ 0 ] , device = device , nh = nh ) . to ( device ) GNNYX = GNN_model ( x . shape [ 0 ] , device = device , nh = nh ) . to ( device ) GNNXY . reset_parameters ( ) GNNYX . reset_parameters ( ) XY = GNNXY . run ( inputx , target , ** kwargs ) YX = GNNYX . run ( target , inputx , ** kwargs ) return [ XY , YX ]
812	def _fixupRandomEncoderParams ( params , minVal , maxVal , minResolution ) : encodersDict = ( params [ "modelConfig" ] [ "modelParams" ] [ "sensorParams" ] [ "encoders" ] ) for encoder in encodersDict . itervalues ( ) : if encoder is not None : if encoder [ "type" ] == "RandomDistributedScalarEncoder" : resolution = max ( minResolution , ( maxVal - minVal ) / encoder . pop ( "numBuckets" ) ) encodersDict [ "c1" ] [ "resolution" ] = resolution
6203	def em_rates_from_E_DA_mix ( em_rates_tot , E_values ) : em_rates_d , em_rates_a = [ ] , [ ] for em_rate_tot , E_value in zip ( em_rates_tot , E_values ) : em_rate_di , em_rate_ai = em_rates_from_E_DA ( em_rate_tot , E_value ) em_rates_d . append ( em_rate_di ) em_rates_a . append ( em_rate_ai ) return em_rates_d , em_rates_a
8135	def down ( self ) : i = self . index ( ) if i != None : del self . canvas . layers [ i ] i = max ( 0 , i - 1 ) self . canvas . layers . insert ( i , self )
2276	def _win32_dir ( path , star = '' ) : from ubelt import util_cmd import re wrapper = 'cmd /S /C "{}"' command = 'dir /-C "{}"{}' . format ( path , star ) wrapped = wrapper . format ( command ) info = util_cmd . cmd ( wrapped , shell = True ) if info [ 'ret' ] != 0 : from ubelt import util_format print ( 'Failed command:' ) print ( info [ 'command' ] ) print ( util_format . repr2 ( info , nl = 1 ) ) raise OSError ( str ( info ) ) lines = info [ 'out' ] . split ( '\n' ) [ 5 : - 3 ] splitter = re . compile ( '( +)' ) for line in lines : parts = splitter . split ( line ) date , sep , time , sep , ampm , sep , type_or_size , sep = parts [ : 8 ] name = '' . join ( parts [ 8 : ] ) if name == '.' or name == '..' : continue if type_or_size in [ '<JUNCTION>' , '<SYMLINKD>' , '<SYMLINK>' ] : pos = name . find ( ':' ) bpos = name [ : pos ] . rfind ( '[' ) name = name [ : bpos - 1 ] pointed = name [ bpos + 1 : - 1 ] yield type_or_size , name , pointed else : yield type_or_size , name , None
11082	def freeze ( value ) : if isinstance ( value , list ) : return FrozenList ( * value ) if isinstance ( value , dict ) : return FrozenDict ( ** value ) return value
3441	def rename_genes ( cobra_model , rename_dict ) : recompute_reactions = set ( ) remove_genes = [ ] for old_name , new_name in iteritems ( rename_dict ) : try : gene_index = cobra_model . genes . index ( old_name ) except ValueError : gene_index = None old_gene_present = gene_index is not None new_gene_present = new_name in cobra_model . genes if old_gene_present and new_gene_present : old_gene = cobra_model . genes . get_by_id ( old_name ) if old_gene is not cobra_model . genes . get_by_id ( new_name ) : remove_genes . append ( old_gene ) recompute_reactions . update ( old_gene . _reaction ) elif old_gene_present and not new_gene_present : gene = cobra_model . genes [ gene_index ] cobra_model . genes . _dict . pop ( gene . id ) gene . id = new_name cobra_model . genes [ gene_index ] = gene elif not old_gene_present and new_gene_present : pass else : pass cobra_model . repair ( ) class Renamer ( NodeTransformer ) : def visit_Name ( self , node ) : node . id = rename_dict . get ( node . id , node . id ) return node gene_renamer = Renamer ( ) for rxn , rule in iteritems ( get_compiled_gene_reaction_rules ( cobra_model ) ) : if rule is not None : rxn . _gene_reaction_rule = ast2str ( gene_renamer . visit ( rule ) ) for rxn in recompute_reactions : rxn . gene_reaction_rule = rxn . _gene_reaction_rule for i in remove_genes : cobra_model . genes . remove ( i )
13410	def addLogbooks ( self , type = None , logs = [ ] , default = "" ) : if type is not None and len ( logs ) != 0 : if type in self . logList : for logbook in logs : if logbook not in self . logList . get ( type ) [ 0 ] : self . logList . get ( type ) [ 0 ] . append ( logbook ) else : self . logList [ type ] = [ ] self . logList [ type ] . append ( logs ) if len ( self . logList [ type ] ) > 1 and default != "" : self . logList . get ( type ) [ 1 ] == default else : self . logList . get ( type ) . append ( default ) self . logType . clear ( ) self . logType . addItems ( list ( self . logList . keys ( ) ) ) self . changeLogType ( )
11461	def add_control_number ( self , tag , value ) : record_add_field ( self . record , tag , controlfield_value = value )
2277	def parse_generator_doubling ( config ) : start = 1 if 'start' in config : start = int ( config [ 'start' ] ) def generator ( ) : val = start while ( True ) : yield val val = val * 2 return generator ( )
207	def draw ( self , size = None , cmap = "jet" ) : heatmaps_uint8 = self . to_uint8 ( ) heatmaps_drawn = [ ] for c in sm . xrange ( heatmaps_uint8 . shape [ 2 ] ) : heatmap_c = heatmaps_uint8 [ ... , c : c + 1 ] if size is not None : heatmap_c_rs = ia . imresize_single_image ( heatmap_c , size , interpolation = "nearest" ) else : heatmap_c_rs = heatmap_c heatmap_c_rs = np . squeeze ( heatmap_c_rs ) . astype ( np . float32 ) / 255.0 if cmap is not None : import matplotlib . pyplot as plt cmap_func = plt . get_cmap ( cmap ) heatmap_cmapped = cmap_func ( heatmap_c_rs ) heatmap_cmapped = np . delete ( heatmap_cmapped , 3 , 2 ) else : heatmap_cmapped = np . tile ( heatmap_c_rs [ ... , np . newaxis ] , ( 1 , 1 , 3 ) ) heatmap_cmapped = np . clip ( heatmap_cmapped * 255 , 0 , 255 ) . astype ( np . uint8 ) heatmaps_drawn . append ( heatmap_cmapped ) return heatmaps_drawn
12545	def div_img ( img1 , div2 ) : if is_img ( div2 ) : return img1 . get_data ( ) / div2 . get_data ( ) elif isinstance ( div2 , ( float , int ) ) : return img1 . get_data ( ) / div2 else : raise NotImplementedError ( 'Cannot divide {}({}) by ' '{}({})' . format ( type ( img1 ) , img1 , type ( div2 ) , div2 ) )
8433	def cubehelix_pal ( start = 0 , rot = .4 , gamma = 1.0 , hue = 0.8 , light = .85 , dark = .15 , reverse = False ) : cdict = mpl . _cm . cubehelix ( gamma , start , rot , hue ) cubehelix_cmap = mpl . colors . LinearSegmentedColormap ( 'cubehelix' , cdict ) def cubehelix_palette ( n ) : values = np . linspace ( light , dark , n ) return [ mcolors . rgb2hex ( cubehelix_cmap ( x ) ) for x in values ] return cubehelix_palette
3868	async def set_typing ( self , typing = hangouts_pb2 . TYPING_TYPE_STARTED ) : try : await self . _client . set_typing ( hangouts_pb2 . SetTypingRequest ( request_header = self . _client . get_request_header ( ) , conversation_id = hangouts_pb2 . ConversationId ( id = self . id_ ) , type = typing , ) ) except exceptions . NetworkError as e : logger . warning ( 'Failed to set typing status: {}' . format ( e ) ) raise
8600	def add_share ( self , group_id , resource_id , ** kwargs ) : properties = { } for attr , value in kwargs . items ( ) : properties [ self . _underscore_to_camelcase ( attr ) ] = value data = { "properties" : properties } response = self . _perform_request ( url = '/um/groups/%s/shares/%s' % ( group_id , resource_id ) , method = 'POST' , data = json . dumps ( data ) ) return response
11092	def select_dir ( self , filters = all_true , recursive = True ) : for p in self . select ( filters , recursive ) : if p . is_dir ( ) : yield p
6431	def dist_abs ( self , src , tar ) : if src == tar : return 6 if src == '' or tar == '' : return 0 src = list ( mra ( src ) ) tar = list ( mra ( tar ) ) if abs ( len ( src ) - len ( tar ) ) > 2 : return 0 length_sum = len ( src ) + len ( tar ) if length_sum < 5 : min_rating = 5 elif length_sum < 8 : min_rating = 4 elif length_sum < 12 : min_rating = 3 else : min_rating = 2 for _ in range ( 2 ) : new_src = [ ] new_tar = [ ] minlen = min ( len ( src ) , len ( tar ) ) for i in range ( minlen ) : if src [ i ] != tar [ i ] : new_src . append ( src [ i ] ) new_tar . append ( tar [ i ] ) src = new_src + src [ minlen : ] tar = new_tar + tar [ minlen : ] src . reverse ( ) tar . reverse ( ) similarity = 6 - max ( len ( src ) , len ( tar ) ) if similarity >= min_rating : return similarity return 0
13295	def convert_lsstdoc_tex ( content , to_fmt , deparagraph = False , mathjax = False , smart = True , extra_args = None ) : augmented_content = '\n' . join ( ( LSSTDOC_MACROS , content ) ) return convert_text ( augmented_content , 'latex' , to_fmt , deparagraph = deparagraph , mathjax = mathjax , smart = smart , extra_args = extra_args )
4672	def newWallet ( self , pwd ) : if self . created ( ) : raise WalletExists ( "You already have created a wallet!" ) self . store . unlock ( pwd )
5939	def _combineargs ( self , * args , ** kwargs ) : d = { arg : True for arg in args } d . update ( kwargs ) return d
10648	def remove_component ( self , name ) : component_to_remove = None for c in self . components : if c . name == name : component_to_remove = c if component_to_remove is not None : self . components . remove ( component_to_remove )
13292	def json_attributes ( self , vfuncs = None ) : vfuncs = vfuncs or [ ] js = { 'global' : { } } for k in self . ncattrs ( ) : js [ 'global' ] [ k ] = self . getncattr ( k ) for varname , var in self . variables . items ( ) : js [ varname ] = { } for k in var . ncattrs ( ) : z = var . getncattr ( k ) try : assert not np . isnan ( z ) . all ( ) js [ varname ] [ k ] = z except AssertionError : js [ varname ] [ k ] = None except TypeError : js [ varname ] [ k ] = z for vf in vfuncs : try : js [ varname ] . update ( vfuncs ( var ) ) except BaseException : logger . exception ( "Could not apply custom variable attribue function" ) return json . loads ( json . dumps ( js , cls = BasicNumpyEncoder ) )
11158	def mirror_to ( self , dst ) : self . assert_is_dir_and_exists ( ) src = self . abspath dst = os . path . abspath ( dst ) if os . path . exists ( dst ) : raise Exception ( "distination already exist!" ) folder_to_create = list ( ) file_to_create = list ( ) for current_folder , _ , file_list in os . walk ( self . abspath ) : current_folder = current_folder . replace ( src , dst ) try : os . mkdir ( current_folder ) except : pass for basename in file_list : abspath = os . path . join ( current_folder , basename ) with open ( abspath , "wb" ) as _ : pass
5114	def clear_data ( self , queues = None , edge = None , edge_type = None ) : queues = _get_queues ( self . g , queues , edge , edge_type ) for k in queues : self . edge2queue [ k ] . data = { }
3640	def tradeStatus ( self , trade_id ) : method = 'GET' url = 'trade/status' if not isinstance ( trade_id , ( list , tuple ) ) : trade_id = ( trade_id , ) trade_id = ( str ( i ) for i in trade_id ) params = { 'tradeIds' : ',' . join ( trade_id ) } rc = self . __request__ ( method , url , params = params ) return [ itemParse ( i , full = False ) for i in rc [ 'auctionInfo' ] ]
9283	def set_login ( self , callsign , passwd = "-1" , skip_login = False ) : self . __dict__ . update ( locals ( ) )
358	def load_npy_to_any ( path = '' , name = 'file.npy' ) : file_path = os . path . join ( path , name ) try : return np . load ( file_path ) . item ( ) except Exception : return np . load ( file_path ) raise Exception ( "[!] Fail to load %s" % file_path )
253	def extract_round_trips ( transactions , portfolio_value = None ) : transactions = _groupby_consecutive ( transactions ) roundtrips = [ ] for sym , trans_sym in transactions . groupby ( 'symbol' ) : trans_sym = trans_sym . sort_index ( ) price_stack = deque ( ) dt_stack = deque ( ) trans_sym [ 'signed_price' ] = trans_sym . price * np . sign ( trans_sym . amount ) trans_sym [ 'abs_amount' ] = trans_sym . amount . abs ( ) . astype ( int ) for dt , t in trans_sym . iterrows ( ) : if t . price < 0 : warnings . warn ( 'Negative price detected, ignoring for' 'round-trip.' ) continue indiv_prices = [ t . signed_price ] * t . abs_amount if ( len ( price_stack ) == 0 ) or ( copysign ( 1 , price_stack [ - 1 ] ) == copysign ( 1 , t . amount ) ) : price_stack . extend ( indiv_prices ) dt_stack . extend ( [ dt ] * len ( indiv_prices ) ) else : pnl = 0 invested = 0 cur_open_dts = [ ] for price in indiv_prices : if len ( price_stack ) != 0 and ( copysign ( 1 , price_stack [ - 1 ] ) != copysign ( 1 , price ) ) : prev_price = price_stack . popleft ( ) prev_dt = dt_stack . popleft ( ) pnl += - ( price + prev_price ) cur_open_dts . append ( prev_dt ) invested += abs ( prev_price ) else : price_stack . append ( price ) dt_stack . append ( dt ) roundtrips . append ( { 'pnl' : pnl , 'open_dt' : cur_open_dts [ 0 ] , 'close_dt' : dt , 'long' : price < 0 , 'rt_returns' : pnl / invested , 'symbol' : sym , } ) roundtrips = pd . DataFrame ( roundtrips ) roundtrips [ 'duration' ] = roundtrips [ 'close_dt' ] . sub ( roundtrips [ 'open_dt' ] ) if portfolio_value is not None : pv = pd . DataFrame ( portfolio_value , columns = [ 'portfolio_value' ] ) . assign ( date = portfolio_value . index ) roundtrips [ 'date' ] = roundtrips . close_dt . apply ( lambda x : x . replace ( hour = 0 , minute = 0 , second = 0 ) ) tmp = roundtrips . join ( pv , on = 'date' , lsuffix = '_' ) roundtrips [ 'returns' ] = tmp . pnl / tmp . portfolio_value roundtrips = roundtrips . drop ( 'date' , axis = 'columns' ) return roundtrips
2777	def add_forwarding_rules ( self , forwarding_rules ) : rules_dict = [ rule . __dict__ for rule in forwarding_rules ] return self . get_data ( "load_balancers/%s/forwarding_rules/" % self . id , type = POST , params = { "forwarding_rules" : rules_dict } )
257	def print_round_trip_stats ( round_trips , hide_pos = False ) : stats = gen_round_trip_stats ( round_trips ) print_table ( stats [ 'summary' ] , float_format = '{:.2f}' . format , name = 'Summary stats' ) print_table ( stats [ 'pnl' ] , float_format = '${:.2f}' . format , name = 'PnL stats' ) print_table ( stats [ 'duration' ] , float_format = '{:.2f}' . format , name = 'Duration stats' ) print_table ( stats [ 'returns' ] * 100 , float_format = '{:.2f}%' . format , name = 'Return stats' ) if not hide_pos : stats [ 'symbols' ] . columns = stats [ 'symbols' ] . columns . map ( format_asset ) print_table ( stats [ 'symbols' ] * 100 , float_format = '{:.2f}%' . format , name = 'Symbol stats' )
3544	def compute_exit_code ( config , exception = None ) : code = 0 if exception is not None : code = code | 1 if config . surviving_mutants > 0 : code = code | 2 if config . surviving_mutants_timeout > 0 : code = code | 4 if config . suspicious_mutants > 0 : code = code | 8 return code
5318	def format ( self , string , * args , ** kwargs ) : return string . format ( c = self , * args , ** kwargs )
3645	def tradepileDelete ( self , trade_id ) : method = 'DELETE' url = 'trade/%s' % trade_id self . __request__ ( method , url ) return True
11289	def strip_xml_namespace ( root ) : try : root . tag = root . tag . split ( '}' ) [ 1 ] except IndexError : pass for element in root . getchildren ( ) : strip_xml_namespace ( element )
2626	def cancel ( self , job_ids ) : if self . linger is True : logger . debug ( "Ignoring cancel requests due to linger mode" ) return [ False for x in job_ids ] try : self . client . terminate_instances ( InstanceIds = list ( job_ids ) ) except Exception as e : logger . error ( "Caught error while attempting to remove instances: {0}" . format ( job_ids ) ) raise e else : logger . debug ( "Removed the instances: {0}" . format ( job_ids ) ) for job_id in job_ids : self . resources [ job_id ] [ "status" ] = "COMPLETED" for job_id in job_ids : self . instances . remove ( job_id ) return [ True for x in job_ids ]
2518	def p_file_cr_text ( self , f_term , predicate ) : try : for _ , _ , cr_text in self . graph . triples ( ( f_term , predicate , None ) ) : self . builder . set_file_copyright ( self . doc , six . text_type ( cr_text ) ) except CardinalityError : self . more_than_one_error ( 'file copyright text' )
12229	def autodiscover_siteprefs ( admin_site = None ) : if admin_site is None : admin_site = admin . site if 'manage' not in sys . argv [ 0 ] or ( len ( sys . argv ) > 1 and sys . argv [ 1 ] in MANAGE_SAFE_COMMANDS ) : import_prefs ( ) Preference . read_prefs ( get_prefs ( ) ) register_admin_models ( admin_site )
5519	def append ( self , name , data , start ) : for throttle in self . throttles . values ( ) : getattr ( throttle , name ) . append ( data , start )
3204	def create ( self , data ) : if 'operations' not in data : raise KeyError ( 'The batch must have operations' ) for op in data [ 'operations' ] : if 'method' not in op : raise KeyError ( 'The batch operation must have a method' ) if op [ 'method' ] not in [ 'GET' , 'POST' , 'PUT' , 'PATCH' , 'DELETE' ] : raise ValueError ( 'The batch operation method must be one of "GET", "POST", "PUT", "PATCH", ' 'or "DELETE", not {0}' . format ( op [ 'method' ] ) ) if 'path' not in op : raise KeyError ( 'The batch operation must have a path' ) return self . _mc_client . _post ( url = self . _build_path ( ) , data = data )
9109	def _create_encrypted_zip ( self , source = 'dirty' , fs_target_dir = None ) : backup_recipients = [ r for r in self . editors if checkRecipient ( self . gpg_context , r ) ] if not backup_recipients : self . status = u'500 no valid keys at all' return self . status fs_backup = join ( self . fs_path , '%s.zip' % source ) if fs_target_dir is None : fs_backup_pgp = join ( self . fs_path , '%s.zip.pgp' % source ) else : fs_backup_pgp = join ( fs_target_dir , '%s.zip.pgp' % self . drop_id ) fs_source = dict ( dirty = self . fs_dirty_attachments , clean = self . fs_cleansed_attachments ) with ZipFile ( fs_backup , 'w' , ZIP_STORED ) as backup : if exists ( join ( self . fs_path , 'message' ) ) : backup . write ( join ( self . fs_path , 'message' ) , arcname = 'message' ) for fs_attachment in fs_source [ source ] : backup . write ( fs_attachment , arcname = split ( fs_attachment ) [ - 1 ] ) with open ( fs_backup , "rb" ) as backup : self . gpg_context . encrypt_file ( backup , backup_recipients , always_trust = True , output = fs_backup_pgp ) remove ( fs_backup ) return fs_backup_pgp
10967	def sync_params ( self ) : def _normalize ( comps , param ) : vals = [ c . get_values ( param ) for c in comps ] diff = any ( [ vals [ i ] != vals [ i + 1 ] for i in range ( len ( vals ) - 1 ) ] ) if diff : for c in comps : c . set_values ( param , vals [ 0 ] ) for param , comps in iteritems ( self . lmap ) : if isinstance ( comps , list ) and len ( comps ) > 1 : _normalize ( comps , param )
13796	def handle_rereduce ( self , reduce_function_names , values ) : reduce_functions = [ ] for reduce_function_name in reduce_function_names : try : reduce_function = get_function ( reduce_function_name ) if getattr ( reduce_function , 'view_decorated' , None ) : reduce_function = reduce_function ( self . log ) reduce_functions . append ( reduce_function ) except Exception , exc : self . log ( repr ( exc ) ) reduce_functions . append ( lambda * args , ** kwargs : None ) results = [ ] for reduce_function in reduce_functions : try : results . append ( reduce_function ( None , values , rereduce = True ) ) except Exception , exc : self . log ( repr ( exc ) ) results . append ( None ) return [ True , results ]
7420	def index_reference_sequence ( data , force = False ) : refseq_file = data . paramsdict [ 'reference_sequence' ] index_files = [ ] if "smalt" in data . _hackersonly [ "aligner" ] : index_files . extend ( [ ".sma" , ".smi" ] ) else : index_files . extend ( [ ".amb" , ".ann" , ".bwt" , ".pac" , ".sa" ] ) index_files . extend ( [ ".fai" ] ) if not force : if all ( [ os . path . isfile ( refseq_file + i ) for i in index_files ] ) : return if "smalt" in data . _hackersonly [ "aligner" ] : cmd1 = [ ipyrad . bins . smalt , "index" , "-k" , str ( data . _hackersonly [ "smalt_index_wordlen" ] ) , refseq_file , refseq_file ] else : cmd1 = [ ipyrad . bins . bwa , "index" , refseq_file ] LOGGER . info ( " " . join ( cmd1 ) ) proc1 = sps . Popen ( cmd1 , stderr = sps . STDOUT , stdout = sps . PIPE ) error1 = proc1 . communicate ( ) [ 0 ] cmd2 = [ ipyrad . bins . samtools , "faidx" , refseq_file ] LOGGER . info ( " " . join ( cmd2 ) ) proc2 = sps . Popen ( cmd2 , stderr = sps . STDOUT , stdout = sps . PIPE ) error2 = proc2 . communicate ( ) [ 0 ] if proc1 . returncode : raise IPyradWarningExit ( error1 ) if error2 : if "please use bgzip" in error2 : raise IPyradWarningExit ( NO_ZIP_BINS . format ( refseq_file ) ) else : raise IPyradWarningExit ( error2 )
7692	def _handle_auth_success ( self , stream , success ) : if not self . _check_authorization ( success . properties , stream ) : element = ElementTree . Element ( FAILURE_TAG ) ElementTree . SubElement ( element , SASL_QNP + "invalid-authzid" ) return True authzid = success . properties . get ( "authzid" ) if authzid : peer = JID ( success . authzid ) elif "username" in success . properties : peer = JID ( success . properties [ "username" ] , stream . me . domain ) else : peer = None stream . set_peer_authenticated ( peer , True )
6245	def set_time ( self , value : float ) : if value < 0 : value = 0 self . controller . row = self . rps * value
1893	def _reset ( self , constraints = None ) : if self . _proc is None : self . _start_proc ( ) else : if self . support_reset : self . _send ( "(reset)" ) for cfg in self . _init : self . _send ( cfg ) else : self . _stop_proc ( ) self . _start_proc ( ) if constraints is not None : self . _send ( constraints )
13560	def decorate ( msg = "" , waitmsg = "Please wait" ) : def decorator ( func ) : @ functools . wraps ( func ) def wrapper ( * args , ** kwargs ) : spin = Spinner ( msg = msg , waitmsg = waitmsg ) spin . start ( ) a = None try : a = func ( * args , ** kwargs ) except Exception as e : spin . msg = "Something went wrong: " spin . stop_spinning ( ) spin . join ( ) raise e spin . stop_spinning ( ) spin . join ( ) return a return wrapper return decorator
13476	def kill ( self ) : assert self . has_started ( ) , "called kill() on a non-active GeventLoop" self . _stop_event . set ( ) self . _greenlet . kill ( ) self . _clear ( )
7135	def filter_dict ( d , exclude ) : ret = { } for key , value in d . items ( ) : if key not in exclude : ret . update ( { key : value } ) return ret
5569	def zoom_index_gen ( mp = None , out_dir = None , zoom = None , geojson = False , gpkg = False , shapefile = False , txt = False , vrt = False , fieldname = "location" , basepath = None , for_gdal = True , threading = False , ) : for zoom in get_zoom_levels ( process_zoom_levels = zoom ) : with ExitStack ( ) as es : index_writers = [ ] if geojson : index_writers . append ( es . enter_context ( VectorFileWriter ( driver = "GeoJSON" , out_path = _index_file_path ( out_dir , zoom , "geojson" ) , crs = mp . config . output_pyramid . crs , fieldname = fieldname ) ) ) if gpkg : index_writers . append ( es . enter_context ( VectorFileWriter ( driver = "GPKG" , out_path = _index_file_path ( out_dir , zoom , "gpkg" ) , crs = mp . config . output_pyramid . crs , fieldname = fieldname ) ) ) if shapefile : index_writers . append ( es . enter_context ( VectorFileWriter ( driver = "ESRI Shapefile" , out_path = _index_file_path ( out_dir , zoom , "shp" ) , crs = mp . config . output_pyramid . crs , fieldname = fieldname ) ) ) if txt : index_writers . append ( es . enter_context ( TextFileWriter ( out_path = _index_file_path ( out_dir , zoom , "txt" ) ) ) ) if vrt : index_writers . append ( es . enter_context ( VRTFileWriter ( out_path = _index_file_path ( out_dir , zoom , "vrt" ) , output = mp . config . output , out_pyramid = mp . config . output_pyramid ) ) ) logger . debug ( "use the following index writers: %s" , index_writers ) def _worker ( tile ) : tile_path = _tile_path ( orig_path = mp . config . output . get_path ( tile ) , basepath = basepath , for_gdal = for_gdal ) indexes = [ i for i in index_writers if not i . entry_exists ( tile = tile , path = tile_path ) ] if indexes : output_exists = mp . config . output . tiles_exist ( output_tile = tile ) else : output_exists = None return tile , tile_path , indexes , output_exists with concurrent . futures . ThreadPoolExecutor ( ) as executor : for task in concurrent . futures . as_completed ( ( executor . submit ( _worker , i ) for i in mp . config . output_pyramid . tiles_from_geom ( mp . config . area_at_zoom ( zoom ) , zoom ) ) ) : tile , tile_path , indexes , output_exists = task . result ( ) if indexes and output_exists : logger . debug ( "%s exists" , tile_path ) logger . debug ( "write to %s indexes" % len ( indexes ) ) for index in indexes : index . write ( tile , tile_path ) yield tile
11703	def set_gender ( self , gender = None ) : if gender and gender in genders : self . gender = gender else : if not self . chromosomes : self . set_chromosomes ( ) self . gender = npchoice ( genders , 1 , p = p_gender [ self . chromosomes ] ) [ 0 ]
5399	def get_filtered_normalized_events ( self ) : user_image = google_v2_operations . get_action_image ( self . _op , _ACTION_USER_COMMAND ) need_ok = google_v2_operations . is_success ( self . _op ) events = { } for event in google_v2_operations . get_events ( self . _op ) : if self . _filter ( event ) : continue mapped , match = self . _map ( event ) name = mapped [ 'name' ] if name == 'ok' : if not need_ok or 'ok' in events : continue if name == 'pulling-image' : if match . group ( 1 ) != user_image : continue events [ name ] = mapped return sorted ( events . values ( ) , key = operator . itemgetter ( 'start-time' ) )
3484	def _create_bound ( model , reaction , bound_type , f_replace , units = None , flux_udef = None ) : value = getattr ( reaction , bound_type ) if value == config . lower_bound : return LOWER_BOUND_ID elif value == 0 : return ZERO_BOUND_ID elif value == config . upper_bound : return UPPER_BOUND_ID elif value == - float ( "Inf" ) : return BOUND_MINUS_INF elif value == float ( "Inf" ) : return BOUND_PLUS_INF else : rid = reaction . id if f_replace and F_REACTION_REV in f_replace : rid = f_replace [ F_REACTION_REV ] ( rid ) pid = rid + "_" + bound_type _create_parameter ( model , pid = pid , value = value , sbo = SBO_FLUX_BOUND , units = units , flux_udef = flux_udef ) return pid
9107	def sanitize_filename ( filename ) : token = generate_drop_id ( ) name , extension = splitext ( filename ) if extension : return '%s%s' % ( token , extension ) else : return token
3826	async def get_self_info ( self , get_self_info_request ) : response = hangouts_pb2 . GetSelfInfoResponse ( ) await self . _pb_request ( 'contacts/getselfinfo' , get_self_info_request , response ) return response
2819	def convert_batchnorm ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting batchnorm ...' ) if names == 'short' : tf_name = 'BN' + random_string ( 6 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) bias_name = '{0}.bias' . format ( w_name ) weights_name = '{0}.weight' . format ( w_name ) mean_name = '{0}.running_mean' . format ( w_name ) var_name = '{0}.running_var' . format ( w_name ) if bias_name in weights : beta = weights [ bias_name ] . numpy ( ) if weights_name in weights : gamma = weights [ weights_name ] . numpy ( ) mean = weights [ mean_name ] . numpy ( ) variance = weights [ var_name ] . numpy ( ) eps = params [ 'epsilon' ] momentum = params [ 'momentum' ] if weights_name not in weights : bn = keras . layers . BatchNormalization ( axis = 1 , momentum = momentum , epsilon = eps , center = False , scale = False , weights = [ mean , variance ] , name = tf_name ) else : bn = keras . layers . BatchNormalization ( axis = 1 , momentum = momentum , epsilon = eps , weights = [ gamma , beta , mean , variance ] , name = tf_name ) layers [ scope_name ] = bn ( layers [ inputs [ 0 ] ] )
13545	def get_user ( self , user_id ) : url = "/2/users/%s" % user_id return self . user_from_json ( self . _get_resource ( url ) [ "user" ] )
5436	def parse_pair_args ( labels , argclass ) : label_data = set ( ) for arg in labels : name , value = split_pair ( arg , '=' , nullable_idx = 1 ) label_data . add ( argclass ( name , value ) ) return label_data
4598	def read_from ( self , data , pad = 0 ) : for i in range ( self . BEGIN , self . END + 1 ) : index = self . index ( i , len ( data ) ) yield pad if index is None else data [ index ]
2929	def write_manifest ( self ) : config = configparser . ConfigParser ( ) config . add_section ( 'Manifest' ) for f in sorted ( self . manifest . keys ( ) ) : config . set ( 'Manifest' , f . replace ( '\\' , '/' ) . lower ( ) , self . manifest [ f ] ) ini = StringIO ( ) config . write ( ini ) self . manifest_data = ini . getvalue ( ) self . package_zip . writestr ( self . MANIFEST_FILE , self . manifest_data )
5249	def bopen ( ** kwargs ) : con = BCon ( ** kwargs ) con . start ( ) try : yield con finally : con . stop ( )
12046	def pickle_save ( thing , fname ) : pickle . dump ( thing , open ( fname , "wb" ) , pickle . HIGHEST_PROTOCOL ) return thing
7140	def transfer ( self , address , amount , priority = prio . NORMAL , payment_id = None , unlock_time = 0 , relay = True ) : return self . accounts [ 0 ] . transfer ( address , amount , priority = priority , payment_id = payment_id , unlock_time = unlock_time , relay = relay )
5732	def parse_response ( gdb_mi_text ) : stream = StringStream ( gdb_mi_text , debug = _DEBUG ) if _GDB_MI_NOTIFY_RE . match ( gdb_mi_text ) : token , message , payload = _get_notify_msg_and_payload ( gdb_mi_text , stream ) return { "type" : "notify" , "message" : message , "payload" : payload , "token" : token , } elif _GDB_MI_RESULT_RE . match ( gdb_mi_text ) : token , message , payload = _get_result_msg_and_payload ( gdb_mi_text , stream ) return { "type" : "result" , "message" : message , "payload" : payload , "token" : token , } elif _GDB_MI_CONSOLE_RE . match ( gdb_mi_text ) : return { "type" : "console" , "message" : None , "payload" : _GDB_MI_CONSOLE_RE . match ( gdb_mi_text ) . groups ( ) [ 0 ] , } elif _GDB_MI_LOG_RE . match ( gdb_mi_text ) : return { "type" : "log" , "message" : None , "payload" : _GDB_MI_LOG_RE . match ( gdb_mi_text ) . groups ( ) [ 0 ] , } elif _GDB_MI_TARGET_OUTPUT_RE . match ( gdb_mi_text ) : return { "type" : "target" , "message" : None , "payload" : _GDB_MI_TARGET_OUTPUT_RE . match ( gdb_mi_text ) . groups ( ) [ 0 ] , } elif response_is_finished ( gdb_mi_text ) : return { "type" : "done" , "message" : None , "payload" : None } else : return { "type" : "output" , "message" : None , "payload" : gdb_mi_text }
3689	def solve_T ( self , P , V ) : r return ( P * V ** 2 * ( V - self . b ) + V * self . a - self . a * self . b ) / ( R * V ** 2 )
2605	def make_hash ( self , task ) : t = [ serialize_object ( task [ 'func_name' ] ) [ 0 ] , serialize_object ( task [ 'fn_hash' ] ) [ 0 ] , serialize_object ( task [ 'args' ] ) [ 0 ] , serialize_object ( task [ 'kwargs' ] ) [ 0 ] , serialize_object ( task [ 'env' ] ) [ 0 ] ] x = b'' . join ( t ) hashedsum = hashlib . md5 ( x ) . hexdigest ( ) return hashedsum
8280	def _render_closure ( self ) : fillcolor = self . fill strokecolor = self . stroke strokewidth = self . strokewidth def _render ( cairo_ctx ) : transform = self . _call_transform_mode ( self . _transform ) if fillcolor is None and strokecolor is None : return cairo_ctx . set_matrix ( transform ) self . _traverse ( cairo_ctx ) cairo_ctx . set_matrix ( cairo . Matrix ( ) ) if fillcolor is not None and strokecolor is not None : if strokecolor [ 3 ] < 1 : cairo_ctx . push_group ( ) cairo_ctx . set_source_rgba ( * fillcolor ) cairo_ctx . fill_preserve ( ) e = cairo_ctx . stroke_extents ( ) cairo_ctx . set_source_rgba ( * strokecolor ) cairo_ctx . set_operator ( cairo . OPERATOR_SOURCE ) cairo_ctx . set_line_width ( strokewidth ) cairo_ctx . stroke ( ) cairo_ctx . pop_group_to_source ( ) cairo_ctx . paint ( ) else : cairo_ctx . set_source_rgba ( * fillcolor ) cairo_ctx . fill_preserve ( ) cairo_ctx . set_source_rgba ( * strokecolor ) cairo_ctx . set_line_width ( strokewidth ) cairo_ctx . stroke ( ) elif fillcolor is not None : cairo_ctx . set_source_rgba ( * fillcolor ) cairo_ctx . fill ( ) elif strokecolor is not None : cairo_ctx . set_source_rgba ( * strokecolor ) cairo_ctx . set_line_width ( strokewidth ) cairo_ctx . stroke ( ) return _render
9851	def _export_python ( self , filename , ** kwargs ) : data = dict ( grid = self . grid , edges = self . edges , metadata = self . metadata ) with open ( filename , 'wb' ) as f : cPickle . dump ( data , f , cPickle . HIGHEST_PROTOCOL )
11678	def run ( self ) : logger . info ( u'Started listening' ) while not self . _stop : xml = self . _readxml ( ) if xml is None : break if not self . modelize : logger . info ( u'Raw xml: %s' % xml ) self . results . put ( xml ) continue if xml . tag == 'RECOGOUT' : sentence = Sentence . from_shypo ( xml . find ( 'SHYPO' ) , self . encoding ) logger . info ( u'Modelized recognition: %r' % sentence ) self . results . put ( sentence ) else : logger . info ( u'Unmodelized xml: %s' % xml ) self . results . put ( xml ) logger . info ( u'Stopped listening' )
4883	def transmit ( self , payload , ** kwargs ) : kwargs [ 'app_label' ] = 'sap_success_factors' kwargs [ 'model_name' ] = 'SapSuccessFactorsLearnerDataTransmissionAudit' kwargs [ 'remote_user_id' ] = 'sapsf_user_id' super ( SapSuccessFactorsLearnerTransmitter , self ) . transmit ( payload , ** kwargs )
6408	def lmean ( nums ) : r if len ( nums ) != len ( set ( nums ) ) : raise AttributeError ( 'No two values in the nums list may be equal' ) rolling_sum = 0 for i in range ( len ( nums ) ) : rolling_prod = 1 for j in range ( len ( nums ) ) : if i != j : rolling_prod *= math . log ( nums [ i ] / nums [ j ] ) rolling_sum += nums [ i ] / rolling_prod return math . factorial ( len ( nums ) - 1 ) * rolling_sum
13822	def update_config ( new_config ) : flask_app . base_config . update ( new_config ) if new_config . has_key ( 'working_directory' ) : wd = os . path . abspath ( new_config [ 'working_directory' ] ) if nbmanager . notebook_dir != wd : if not os . path . exists ( wd ) : raise IOError ( 'Path not found: %s' % wd ) nbmanager . notebook_dir = wd
3871	def next_event ( self , event_id , prev = False ) : i = self . events . index ( self . _events_dict [ event_id ] ) if prev and i > 0 : return self . events [ i - 1 ] elif not prev and i + 1 < len ( self . events ) : return self . events [ i + 1 ] else : return None
8978	def _binary_file ( self , file ) : if self . __text_is_expected : file = TextWrapper ( file , self . __encoding ) self . __dump_to_file ( file )
9631	def send ( self , extra_context = None , ** kwargs ) : message = self . render_to_message ( extra_context = extra_context , ** kwargs ) return message . send ( )
7971	def _run_timeout_threads ( self , handler ) : for dummy , method in inspect . getmembers ( handler , callable ) : if not hasattr ( method , "_pyxmpp_timeout" ) : continue thread = TimeoutThread ( method , daemon = self . daemon , exc_queue = self . exc_queue ) self . timeout_threads . append ( thread ) thread . start ( )
722	def getData ( self , n ) : records = [ self . getNext ( ) for x in range ( n ) ] return records
1707	def run ( command , data = None , timeout = None , kill_timeout = None , env = None , cwd = None ) : command = expand_args ( command ) history = [ ] for c in command : if len ( history ) : data = history [ - 1 ] . std_out [ 0 : 10 * 1024 ] cmd = Command ( c ) try : out , err = cmd . run ( data , timeout , kill_timeout , env , cwd ) status_code = cmd . returncode except OSError as e : out , err = '' , u"\n" . join ( [ e . strerror , traceback . format_exc ( ) ] ) status_code = 127 r = Response ( process = cmd ) r . command = c r . std_out = out r . std_err = err r . status_code = status_code history . append ( r ) r = history . pop ( ) r . history = history return r
3766	def Z_from_virial_pressure_form ( P , * args ) : r return 1 + P * sum ( [ coeff * P ** i for i , coeff in enumerate ( args ) ] )
11297	def _check_for_exceptions ( self , resp , multiple_rates ) : if resp [ 'rCode' ] != 100 : raise exceptions . get_exception_for_code ( resp [ 'rCode' ] ) ( resp ) results = resp [ 'results' ] if len ( results ) == 0 : raise exceptions . ZipTaxNoResults ( 'No results found' ) if len ( results ) > 1 and not multiple_rates : rates = [ result [ 'taxSales' ] for result in results ] if len ( set ( rates ) ) != 1 : raise exceptions . ZipTaxMultipleResults ( 'Multiple results found but requested only one' )
11772	def NaiveBayesLearner ( dataset ) : targetvals = dataset . values [ dataset . target ] target_dist = CountingProbDist ( targetvals ) attr_dists = dict ( ( ( gv , attr ) , CountingProbDist ( dataset . values [ attr ] ) ) for gv in targetvals for attr in dataset . inputs ) for example in dataset . examples : targetval = example [ dataset . target ] target_dist . add ( targetval ) for attr in dataset . inputs : attr_dists [ targetval , attr ] . add ( example [ attr ] ) def predict ( example ) : def class_probability ( targetval ) : return ( target_dist [ targetval ] * product ( attr_dists [ targetval , attr ] [ example [ attr ] ] for attr in dataset . inputs ) ) return argmax ( targetvals , class_probability ) return predict
5610	def _shift_required ( tiles ) : if tiles [ 0 ] [ 0 ] . tile_pyramid . is_global : tile_cols = sorted ( list ( set ( [ t [ 0 ] . col for t in tiles ] ) ) ) if tile_cols == list ( range ( min ( tile_cols ) , max ( tile_cols ) + 1 ) ) : return False else : def gen_groups ( items ) : j = items [ 0 ] group = [ j ] for i in items [ 1 : ] : if i == j + 1 : group . append ( i ) else : yield group group = [ i ] j = i yield group groups = list ( gen_groups ( tile_cols ) ) if len ( groups ) == 1 : return False normal_distance = groups [ - 1 ] [ - 1 ] - groups [ 0 ] [ 0 ] antimeridian_distance = ( groups [ 0 ] [ - 1 ] + tiles [ 0 ] [ 0 ] . tile_pyramid . matrix_width ( tiles [ 0 ] [ 0 ] . zoom ) ) - groups [ - 1 ] [ 0 ] return antimeridian_distance < normal_distance else : return False
6746	def topological_sort ( source ) : if isinstance ( source , dict ) : source = source . items ( ) pending = sorted ( [ ( name , set ( deps ) ) for name , deps in source ] ) emitted = [ ] while pending : next_pending = [ ] next_emitted = [ ] for entry in pending : name , deps = entry deps . difference_update ( emitted ) if deps : next_pending . append ( entry ) else : yield name emitted . append ( name ) next_emitted . append ( name ) if not next_emitted : raise ValueError ( "cyclic or missing dependancy detected: %r" % ( next_pending , ) ) pending = next_pending emitted = next_emitted
28	def mpi_fork ( n , extra_mpi_args = [ ] ) : if n <= 1 : return "child" if os . getenv ( "IN_MPI" ) is None : env = os . environ . copy ( ) env . update ( MKL_NUM_THREADS = "1" , OMP_NUM_THREADS = "1" , IN_MPI = "1" ) args = [ "mpirun" , "-np" , str ( n ) ] + extra_mpi_args + [ sys . executable ] args += sys . argv subprocess . check_call ( args , env = env ) return "parent" else : install_mpi_excepthook ( ) return "child"
8098	def create ( self , stylename , ** kwargs ) : if stylename == "default" : self [ stylename ] = style ( stylename , self . _ctx , ** kwargs ) return self [ stylename ] k = kwargs . get ( "template" , "default" ) s = self [ stylename ] = self [ k ] . copy ( stylename ) for attr in kwargs : if s . __dict__ . has_key ( attr ) : s . __dict__ [ attr ] = kwargs [ attr ] return s
5080	def parse_course_key ( course_identifier ) : try : course_run_key = CourseKey . from_string ( course_identifier ) except InvalidKeyError : return course_identifier return quote_plus ( ' ' . join ( [ course_run_key . org , course_run_key . course ] ) )
7321	def convert_markdown ( message ) : assert message [ 'Content-Type' ] . startswith ( "text/markdown" ) del message [ 'Content-Type' ] message = make_message_multipart ( message ) for payload_item in set ( message . get_payload ( ) ) : if payload_item [ 'Content-Type' ] . startswith ( 'text/plain' ) : original_text = payload_item . get_payload ( ) html_text = markdown . markdown ( original_text ) html_payload = future . backports . email . mime . text . MIMEText ( "<html><body>{}</body></html>" . format ( html_text ) , "html" , ) message . attach ( html_payload ) return message
11953	def _parse_dumb_push_output ( self , string ) : stack = 0 json_list = [ ] tmp_json = '' for char in string : if not char == '\r' and not char == '\n' : tmp_json += char if char == '{' : stack += 1 elif char == '}' : stack -= 1 if stack == 0 : if not len ( tmp_json ) == 0 : json_list . append ( tmp_json ) tmp_json = '' return json_list
9914	def create ( self , validated_data ) : email_query = models . EmailAddress . objects . filter ( email = self . validated_data [ "email" ] ) if email_query . exists ( ) : email = email_query . get ( ) email . send_duplicate_notification ( ) else : email = super ( EmailSerializer , self ) . create ( validated_data ) email . send_confirmation ( ) user = validated_data . get ( "user" ) query = models . EmailAddress . objects . filter ( is_primary = True , user = user ) if not query . exists ( ) : email . set_primary ( ) return email
9712	def heappushpop_max ( heap , item ) : if heap and heap [ 0 ] > item : item , heap [ 0 ] = heap [ 0 ] , item _siftup_max ( heap , 0 ) return item
13121	def get_pipe ( self , object_type ) : for line in sys . stdin : try : data = json . loads ( line . strip ( ) ) obj = object_type ( ** data ) yield obj except ValueError : yield self . id_to_object ( line . strip ( ) )
4873	def create ( self , validated_data ) : enterprise_customer = self . context . get ( 'enterprise_customer' ) lms_user = validated_data . get ( 'lms_user_id' ) tpa_user = validated_data . get ( 'tpa_user_id' ) user_email = validated_data . get ( 'user_email' ) course_run_id = validated_data . get ( 'course_run_id' ) course_mode = validated_data . get ( 'course_mode' ) cohort = validated_data . get ( 'cohort' ) email_students = validated_data . get ( 'email_students' ) is_active = validated_data . get ( 'is_active' ) enterprise_customer_user = lms_user or tpa_user or user_email if isinstance ( enterprise_customer_user , models . EnterpriseCustomerUser ) : validated_data [ 'enterprise_customer_user' ] = enterprise_customer_user try : if is_active : enterprise_customer_user . enroll ( course_run_id , course_mode , cohort = cohort ) else : enterprise_customer_user . unenroll ( course_run_id ) except ( CourseEnrollmentDowngradeError , CourseEnrollmentPermissionError , HttpClientError ) as exc : validated_data [ 'detail' ] = str ( exc ) return validated_data if is_active : track_enrollment ( 'enterprise-customer-enrollment-api' , enterprise_customer_user . user_id , course_run_id ) else : if is_active : enterprise_customer_user = enterprise_customer . enroll_user_pending_registration ( user_email , course_mode , course_run_id , cohort = cohort ) else : enterprise_customer . clear_pending_registration ( user_email , course_run_id ) if email_students : enterprise_customer . notify_enrolled_learners ( self . context . get ( 'request_user' ) , course_run_id , [ enterprise_customer_user ] ) validated_data [ 'detail' ] = 'success' return validated_data
447	def _bias_scale ( x , b , data_format ) : if data_format == 'NHWC' : return x * b elif data_format == 'NCHW' : return x * _to_channel_first_bias ( b ) else : raise ValueError ( 'invalid data_format: %s' % data_format )
6834	def vagrant ( self , name = '' ) : r = self . local_renderer config = self . ssh_config ( name ) extra_args = self . _settings_dict ( config ) r . genv . update ( extra_args )
645	def generateCoincMatrix ( nCoinc = 10 , length = 500 , activity = 50 ) : coincMatrix0 = SM32 ( int ( nCoinc ) , int ( length ) ) theOnes = numpy . array ( [ 1.0 ] * activity , dtype = numpy . float32 ) for rowIdx in xrange ( nCoinc ) : coinc = numpy . array ( random . sample ( xrange ( length ) , activity ) , dtype = numpy . uint32 ) coinc . sort ( ) coincMatrix0 . setRowFromSparse ( rowIdx , coinc , theOnes ) coincMatrix = SM32 ( int ( nCoinc ) , int ( length ) ) coincMatrix . initializeWithFixedNNZR ( activity ) return coincMatrix0
10066	def file_serializer ( obj ) : return { "id" : str ( obj . file_id ) , "filename" : obj . key , "filesize" : obj . file . size , "checksum" : obj . file . checksum , }
2015	def _rollback ( self ) : last_pc , last_gas , last_instruction , last_arguments , fee , allocated = self . _checkpoint_data self . _push_arguments ( last_arguments ) self . _gas = last_gas self . _pc = last_pc self . _allocated = allocated self . _checkpoint_data = None
9648	def parse_log_messages ( self , text ) : regex = r"commit ([0-9a-f]+)\nAuthor: (.*?)\n\n(.*?)(?:\n\n|$)" messages = re . findall ( regex , text , re . DOTALL ) parsed = [ ] for commit , author , message in messages : parsed . append ( ( commit [ : 10 ] , re . sub ( r"\s*<.*?>" , "" , author ) , message . strip ( ) ) ) return parsed
751	def _addAnomalyClassifierRegion ( self , network , params , spEnable , tmEnable ) : allParams = copy . deepcopy ( params ) knnParams = dict ( k = 1 , distanceMethod = 'rawOverlap' , distanceNorm = 1 , doBinarization = 1 , replaceDuplicates = 0 , maxStoredPatterns = 1000 ) allParams . update ( knnParams ) if allParams [ 'trainRecords' ] is None : allParams [ 'trainRecords' ] = DEFAULT_ANOMALY_TRAINRECORDS if allParams [ 'cacheSize' ] is None : allParams [ 'cacheSize' ] = DEFAULT_ANOMALY_CACHESIZE if self . _netInfo is not None and self . _netInfo . net is not None and self . _getAnomalyClassifier ( ) is not None : self . _netInfo . net . removeRegion ( 'AnomalyClassifier' ) network . addRegion ( "AnomalyClassifier" , "py.KNNAnomalyClassifierRegion" , json . dumps ( allParams ) ) if spEnable : network . link ( "SP" , "AnomalyClassifier" , "UniformLink" , "" , srcOutput = "bottomUpOut" , destInput = "spBottomUpOut" ) else : network . link ( "sensor" , "AnomalyClassifier" , "UniformLink" , "" , srcOutput = "dataOut" , destInput = "spBottomUpOut" ) if tmEnable : network . link ( "TM" , "AnomalyClassifier" , "UniformLink" , "" , srcOutput = "topDownOut" , destInput = "tpTopDownOut" ) network . link ( "TM" , "AnomalyClassifier" , "UniformLink" , "" , srcOutput = "lrnActiveStateT" , destInput = "tpLrnActiveStateT" ) else : raise RuntimeError ( "TemporalAnomaly models require a TM region." )
9398	def exit ( self ) : if self . _engine : self . _engine . repl . terminate ( ) self . _engine = None
4062	def show_condition_operators ( self , condition ) : permitted_operators = self . savedsearch . conditions_operators . get ( condition ) permitted_operators_list = set ( [ self . savedsearch . operators . get ( op ) for op in permitted_operators ] ) return permitted_operators_list
12927	def _parse_allele_data ( self ) : return [ Allele ( sequence = x ) for x in [ self . ref_allele ] + self . alt_alleles ]
2466	def set_concluded_license ( self , doc , lic ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_conc_lics_set : self . file_conc_lics_set = True if validations . validate_lics_conc ( lic ) : self . file ( doc ) . conc_lics = lic return True else : raise SPDXValueError ( 'File::ConcludedLicense' ) else : raise CardinalityError ( 'File::ConcludedLicense' ) else : raise OrderError ( 'File::ConcludedLicense' )
7126	def add_download_total ( rows ) : total_row = [ "" ] * len ( rows [ 0 ] ) total_row [ 0 ] = "Total" total_downloads , downloads_column = get_download_total ( rows ) total_row [ downloads_column ] = str ( total_downloads ) rows . append ( total_row ) return rows
856	def seekFromEnd ( self , numRecords ) : self . _file . seek ( self . _getTotalLineCount ( ) - numRecords ) return self . getBookmark ( )
9513	def orfs ( self , frame = 0 , revcomp = False ) : assert frame in [ 0 , 1 , 2 ] if revcomp : self . revcomp ( ) aa_seq = self . translate ( frame = frame ) . seq . rstrip ( 'X' ) if revcomp : self . revcomp ( ) orfs = _orfs_from_aa_seq ( aa_seq ) for i in range ( len ( orfs ) ) : if revcomp : start = len ( self ) - ( orfs [ i ] . end * 3 + 3 ) - frame end = len ( self ) - ( orfs [ i ] . start * 3 ) - 1 - frame else : start = orfs [ i ] . start * 3 + frame end = orfs [ i ] . end * 3 + 2 + frame orfs [ i ] = intervals . Interval ( start , end ) return orfs
9453	def play ( self , call_params ) : path = '/' + self . api_version + '/Play/' method = 'POST' return self . request ( path , method , call_params )
10345	def get_merged_namespace_names ( locations , check_keywords = True ) : resources = { location : get_bel_resource ( location ) for location in locations } if check_keywords : resource_keywords = set ( config [ 'Namespace' ] [ 'Keyword' ] for config in resources . values ( ) ) if 1 != len ( resource_keywords ) : raise ValueError ( 'Tried merging namespaces with different keywords: {}' . format ( resource_keywords ) ) result = { } for resource in resources : result . update ( resource [ 'Values' ] ) return result
7332	async def run_tasks ( self ) : tasks = self . get_tasks ( ) self . _gathered_tasks = asyncio . gather ( * tasks , loop = self . loop ) try : await self . _gathered_tasks except CancelledError : pass
11184	def wrap_state_dict ( self , typename : str , state ) -> Dict [ str , Any ] : return { self . type_key : typename , self . state_key : state }
2058	def _dict_diff ( d1 , d2 ) : d = { } for key in set ( d1 ) . intersection ( set ( d2 ) ) : if d2 [ key ] != d1 [ key ] : d [ key ] = d2 [ key ] for key in set ( d2 ) . difference ( set ( d1 ) ) : d [ key ] = d2 [ key ] return d
7961	def disconnect ( self ) : logger . debug ( "TCPTransport.disconnect()" ) with self . lock : if self . _socket is None : if self . _state != "closed" : self . event ( DisconnectedEvent ( self . _dst_addr ) ) self . _set_state ( "closed" ) return if self . _hup or not self . _serializer : self . _close ( ) else : self . send_stream_tail ( )
5467	def get_action_image ( op , name ) : action = _get_action_by_name ( op , name ) if action : return action . get ( 'imageUri' )
10722	def _wrapper ( func ) : @ functools . wraps ( func ) def the_func ( expr ) : try : return func ( expr ) except ( TypeError , ValueError ) as err : raise IntoDPValueError ( expr , "expr" , "could not be transformed" ) from err return the_func
3072	def init_app ( self , app , scopes = None , client_secrets_file = None , client_id = None , client_secret = None , authorize_callback = None , storage = None , ** kwargs ) : self . app = app self . authorize_callback = authorize_callback self . flow_kwargs = kwargs if storage is None : storage = dictionary_storage . DictionaryStorage ( session , key = _CREDENTIALS_KEY ) self . storage = storage if scopes is None : scopes = app . config . get ( 'GOOGLE_OAUTH2_SCOPES' , _DEFAULT_SCOPES ) self . scopes = scopes self . _load_config ( client_secrets_file , client_id , client_secret ) app . register_blueprint ( self . _create_blueprint ( ) )
6773	def uninstall_blacklisted ( self ) : from burlap . system import distrib_family blacklisted_packages = self . env . blacklisted_packages if not blacklisted_packages : print ( 'No blacklisted packages.' ) return else : family = distrib_family ( ) if family == DEBIAN : self . sudo ( 'DEBIAN_FRONTEND=noninteractive apt-get -yq purge %s' % ' ' . join ( blacklisted_packages ) ) else : raise NotImplementedError ( 'Unknown family: %s' % family )
6412	def agmean ( nums ) : m_a = amean ( nums ) m_g = gmean ( nums ) if math . isnan ( m_a ) or math . isnan ( m_g ) : return float ( 'nan' ) while round ( m_a , 12 ) != round ( m_g , 12 ) : m_a , m_g = ( m_a + m_g ) / 2 , ( m_a * m_g ) ** ( 1 / 2 ) return m_a
6620	def wait ( self ) : finished_pids = [ ] while self . running_procs : finished_pids . extend ( self . poll ( ) ) return finished_pids
7191	def _load_info ( self ) : url = '%s/prefix?duration=36000' % self . base_url r = self . gbdx_connection . get ( url ) r . raise_for_status ( ) return r . json ( )
1184	def match ( self , context ) : while context . remaining_codes ( ) > 0 and context . has_matched is None : opcode = context . peek_code ( ) if not self . dispatch ( opcode , context ) : return None if context . has_matched is None : context . has_matched = False return context . has_matched
967	def resetVector ( x1 , x2 ) : size = len ( x1 ) for i in range ( size ) : x2 [ i ] = x1 [ i ]
2357	def is_element_displayed ( self , strategy , locator ) : return self . driver_adapter . is_element_displayed ( strategy , locator , root = self . root )
1165	def run ( self ) : try : if self . __target : self . __target ( * self . __args , ** self . __kwargs ) finally : del self . __target , self . __args , self . __kwargs
9708	def get_sanitizer ( self ) : sanitizer = self . sanitizer if not sanitizer : default_sanitizer = settings . CONFIG . get ( self . SANITIZER_KEY ) field_settings = getattr ( self , 'field_settings' , None ) if isinstance ( field_settings , six . string_types ) : profiles = settings . CONFIG . get ( self . SANITIZER_PROFILES_KEY , { } ) sanitizer = profiles . get ( field_settings , default_sanitizer ) else : sanitizer = default_sanitizer if isinstance ( sanitizer , six . string_types ) : sanitizer = import_string ( sanitizer ) return sanitizer or noop
4342	def reverse ( self ) : effect_args = [ 'reverse' ] self . effects . extend ( effect_args ) self . effects_log . append ( 'reverse' ) return self
8274	def colors ( self , n = 10 , d = 0.035 ) : s = sum ( [ w for clr , rng , w in self . ranges ] ) colors = colorlist ( ) for i in _range ( n ) : r = random ( ) for clr , rng , weight in self . ranges : if weight / s >= r : break r -= weight / s colors . append ( rng ( clr , d ) ) return colors
4123	def _twosided_zerolag ( data , zerolag ) : res = twosided ( np . insert ( data , 0 , zerolag ) ) return res
12663	def load_mask_data ( image , allow_empty = True ) : mask = load_mask ( image , allow_empty = allow_empty ) return get_img_data ( mask ) , mask . get_affine ( )
1259	def save_component ( self , component_name , save_path ) : component = self . get_component ( component_name = component_name ) self . _validate_savable ( component = component , component_name = component_name ) return component . save ( sess = self . session , save_path = save_path )
3831	async def search_entities ( self , search_entities_request ) : response = hangouts_pb2 . SearchEntitiesResponse ( ) await self . _pb_request ( 'contacts/searchentities' , search_entities_request , response ) return response
8930	def workdir_is_clean ( self , quiet = False ) : self . run ( 'git update-index -q --ignore-submodules --refresh' , ** RUN_KWARGS ) unchanged = True try : self . run ( 'git diff-files --quiet --ignore-submodules --' , report_error = False , ** RUN_KWARGS ) except exceptions . Failure : unchanged = False if not quiet : notify . warning ( 'You have unstaged changes!' ) self . run ( 'git diff-files --name-status -r --ignore-submodules -- >&2' , ** RUN_KWARGS ) try : self . run ( 'git diff-index --cached --quiet HEAD --ignore-submodules --' , report_error = False , ** RUN_KWARGS ) except exceptions . Failure : unchanged = False if not quiet : notify . warning ( 'Your index contains uncommitted changes!' ) self . run ( 'git diff-index --cached --name-status -r --ignore-submodules HEAD -- >&2' , ** RUN_KWARGS ) return unchanged
10824	def query_by_group ( cls , group_or_id , with_invitations = False , ** kwargs ) : if isinstance ( group_or_id , Group ) : id_group = group_or_id . id else : id_group = group_or_id if not with_invitations : return cls . _filter ( cls . query . filter_by ( id_group = id_group ) , ** kwargs ) else : return cls . query . filter ( Membership . id_group == id_group , db . or_ ( Membership . state == MembershipState . PENDING_USER , Membership . state == MembershipState . ACTIVE ) )
1000	def printParameters ( self ) : print "numberOfCols=" , self . numberOfCols print "cellsPerColumn=" , self . cellsPerColumn print "minThreshold=" , self . minThreshold print "newSynapseCount=" , self . newSynapseCount print "activationThreshold=" , self . activationThreshold print print "initialPerm=" , self . initialPerm print "connectedPerm=" , self . connectedPerm print "permanenceInc=" , self . permanenceInc print "permanenceDec=" , self . permanenceDec print "permanenceMax=" , self . permanenceMax print "globalDecay=" , self . globalDecay print print "doPooling=" , self . doPooling print "segUpdateValidDuration=" , self . segUpdateValidDuration print "pamLength=" , self . pamLength
12118	def abfinfo ( self , printToo = False , returnDict = False ) : info = "\n### ABF INFO ###\n" d = { } for thingName in sorted ( dir ( self ) ) : if thingName in [ 'cm' , 'evIs' , 'colormap' , 'dataX' , 'dataY' , 'protoX' , 'protoY' ] : continue if "_" in thingName : continue thing = getattr ( self , thingName ) if type ( thing ) is list and len ( thing ) > 5 : continue thingType = str ( type ( thing ) ) . split ( "'" ) [ 1 ] if "method" in thingType or "neo." in thingType : continue if thingName in [ "header" , "MT" ] : continue info += "%s <%s> %s\n" % ( thingName , thingType , thing ) d [ thingName ] = thing if printToo : print ( ) for line in info . split ( "\n" ) : if len ( line ) < 3 : continue print ( " " , line ) print ( ) if returnDict : return d return info
7309	def with_tz ( request ) : dt = datetime . now ( ) t = Template ( '{% load tz %}{% localtime on %}{% get_current_timezone as TIME_ZONE %}{{ TIME_ZONE }}{% endlocaltime %}' ) c = RequestContext ( request ) response = t . render ( c ) return HttpResponse ( response )
2799	def convert_convtranspose ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting transposed convolution ...' ) if names == 'short' : tf_name = 'C' + random_string ( 7 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) bias_name = '{0}.bias' . format ( w_name ) weights_name = '{0}.weight' . format ( w_name ) if len ( weights [ weights_name ] . numpy ( ) . shape ) == 4 : W = weights [ weights_name ] . numpy ( ) . transpose ( 2 , 3 , 1 , 0 ) height , width , n_filters , channels = W . shape n_groups = params [ 'group' ] if n_groups > 1 : raise AssertionError ( 'Cannot convert conv1d with groups != 1' ) if params [ 'dilations' ] [ 0 ] > 1 : raise AssertionError ( 'Cannot convert conv1d with dilation_rate != 1' ) if bias_name in weights : biases = weights [ bias_name ] . numpy ( ) has_bias = True else : biases = None has_bias = False input_name = inputs [ 0 ] if has_bias : weights = [ W , biases ] else : weights = [ W ] conv = keras . layers . Conv2DTranspose ( filters = n_filters , kernel_size = ( height , width ) , strides = ( params [ 'strides' ] [ 0 ] , params [ 'strides' ] [ 1 ] ) , padding = 'valid' , output_padding = 0 , weights = weights , use_bias = has_bias , activation = None , dilation_rate = params [ 'dilations' ] [ 0 ] , bias_initializer = 'zeros' , kernel_initializer = 'zeros' , name = tf_name ) layers [ scope_name ] = conv ( layers [ input_name ] ) layers [ scope_name ] . set_shape ( layers [ scope_name ] . _keras_shape ) pads = params [ 'pads' ] if pads [ 0 ] > 0 : assert ( len ( pads ) == 2 or ( pads [ 2 ] == pads [ 0 ] and pads [ 3 ] == pads [ 1 ] ) ) crop = keras . layers . Cropping2D ( pads [ : 2 ] , name = tf_name + '_crop' ) layers [ scope_name ] = crop ( layers [ scope_name ] ) else : raise AssertionError ( 'Layer is not supported for now' )
11753	def make_tables ( grammar , precedence ) : ACTION = { } GOTO = { } labels = { } def get_label ( closure ) : if closure not in labels : labels [ closure ] = len ( labels ) return labels [ closure ] def resolve_shift_reduce ( lookahead , s_action , r_action ) : s_assoc , s_level = precedence [ lookahead ] r_assoc , r_level = precedence [ r_action [ 1 ] ] if s_level < r_level : return r_action elif s_level == r_level and r_assoc == LEFT : return r_action else : return s_action initial , closures , goto = grammar . closures ( ) for closure in closures : label = get_label ( closure ) for rule in closure : new_action , lookahead = None , rule . lookahead if not rule . at_end : symbol = rule . rhs [ rule . pos ] is_terminal = symbol in grammar . terminals has_goto = symbol in goto [ closure ] if is_terminal and has_goto : next_state = get_label ( goto [ closure ] [ symbol ] ) new_action , lookahead = ( 'shift' , next_state ) , symbol elif rule . production == grammar . start and rule . at_end : new_action = ( 'accept' , ) elif rule . at_end : new_action = ( 'reduce' , rule . production ) if new_action is None : continue prev_action = ACTION . get ( ( label , lookahead ) ) if prev_action is None or prev_action == new_action : ACTION [ label , lookahead ] = new_action else : types = ( prev_action [ 0 ] , new_action [ 0 ] ) if types == ( 'shift' , 'reduce' ) : chosen = resolve_shift_reduce ( lookahead , prev_action , new_action ) elif types == ( 'reduce' , 'shift' ) : chosen = resolve_shift_reduce ( lookahead , new_action , prev_action ) else : raise TableConflictError ( prev_action , new_action ) ACTION [ label , lookahead ] = chosen for symbol in grammar . nonterminals : if symbol in goto [ closure ] : GOTO [ label , symbol ] = get_label ( goto [ closure ] [ symbol ] ) return get_label ( initial ) , ACTION , GOTO
7159	def ask ( self , error = None ) : q = self . next_question if q is None : return try : answer = q . prompter ( self . get_prompt ( q , error ) , * q . prompter_args , ** q . prompter_kwargs ) except QuestionnaireGoBack as e : steps = e . args [ 0 ] if e . args else 1 if steps == 0 : self . ask ( ) return self . go_back ( steps ) else : if q . _validate : error = q . _validate ( answer ) if error : self . ask ( error ) return if q . _transform : answer = q . _transform ( answer ) self . answers [ q . key ] = answer return answer
6541	def parse_python_file ( filepath ) : with _AST_CACHE_LOCK : if filepath not in _AST_CACHE : source = read_file ( filepath ) _AST_CACHE [ filepath ] = ast . parse ( source , filename = filepath ) return _AST_CACHE [ filepath ]
11663	def as_integer_type ( ary ) : ary = np . asanyarray ( ary ) if is_integer_type ( ary ) : return ary rounded = np . rint ( ary ) if np . any ( rounded != ary ) : raise ValueError ( "argument array must contain only integers" ) return rounded . astype ( int )
219	def get_directories ( self , directory : str = None , packages : typing . List [ str ] = None ) -> typing . List [ str ] : directories = [ ] if directory is not None : directories . append ( directory ) for package in packages or [ ] : spec = importlib . util . find_spec ( package ) assert spec is not None , f"Package {package!r} could not be found." assert ( spec . origin is not None ) , "Directory 'statics' in package {package!r} could not be found." directory = os . path . normpath ( os . path . join ( spec . origin , ".." , "statics" ) ) assert os . path . isdir ( directory ) , "Directory 'statics' in package {package!r} could not be found." directories . append ( directory ) return directories
855	def getBookmark ( self ) : if self . _write and self . _recordCount == 0 : return None rowDict = dict ( filepath = os . path . realpath ( self . _filename ) , currentRow = self . _recordCount ) return json . dumps ( rowDict )
8937	def get_pypi_auth ( configfile = '~/.pypirc' ) : pypi_cfg = ConfigParser ( ) if pypi_cfg . read ( os . path . expanduser ( configfile ) ) : try : user = pypi_cfg . get ( 'pypi' , 'username' ) pwd = pypi_cfg . get ( 'pypi' , 'password' ) return user , pwd except ConfigError : notify . warning ( "No PyPI credentials in '{}'," " will fall back to '~/.netrc'..." . format ( configfile ) ) return None
2013	def _push ( self , value ) : assert isinstance ( value , int ) or isinstance ( value , BitVec ) and value . size == 256 if len ( self . stack ) >= 1024 : raise StackOverflow ( ) if isinstance ( value , int ) : value = value & TT256M1 value = simplify ( value ) if isinstance ( value , Constant ) and not value . taint : value = value . value self . stack . append ( value )
5103	def draw_graph ( self , line_kwargs = None , scatter_kwargs = None , ** kwargs ) : if not HAS_MATPLOTLIB : raise ImportError ( "Matplotlib is required to draw the graph." ) fig = plt . figure ( figsize = kwargs . get ( 'figsize' , ( 7 , 7 ) ) ) ax = fig . gca ( ) mpl_kwargs = { 'line_kwargs' : line_kwargs , 'scatter_kwargs' : scatter_kwargs , 'pos' : kwargs . get ( 'pos' ) } line_kwargs , scatter_kwargs = self . lines_scatter_args ( ** mpl_kwargs ) edge_collection = LineCollection ( ** line_kwargs ) ax . add_collection ( edge_collection ) ax . scatter ( ** scatter_kwargs ) if hasattr ( ax , 'set_facecolor' ) : ax . set_facecolor ( kwargs . get ( 'bgcolor' , [ 1 , 1 , 1 , 1 ] ) ) else : ax . set_axis_bgcolor ( kwargs . get ( 'bgcolor' , [ 1 , 1 , 1 , 1 ] ) ) ax . get_xaxis ( ) . set_visible ( False ) ax . get_yaxis ( ) . set_visible ( False ) if 'fname' in kwargs : new_kwargs = { k : v for k , v in kwargs . items ( ) if k in SAVEFIG_KWARGS } fig . savefig ( kwargs [ 'fname' ] , ** new_kwargs ) else : plt . ion ( ) plt . show ( )
3575	def peripheral_didReadRSSI_error_ ( self , peripheral , rssi , error ) : logger . debug ( 'peripheral_didReadRSSI_error called' ) if error is not None : return device = device_list ( ) . get ( peripheral ) if device is not None : device . _rssi_changed ( rssi )
9099	def write_bel_annotation ( self , file : TextIO ) -> None : if not self . is_populated ( ) : self . populate ( ) values = self . _get_namespace_name_to_encoding ( desc = 'writing names' ) write_annotation ( keyword = self . _get_namespace_keyword ( ) , citation_name = self . _get_namespace_name ( ) , description = '' , values = values , file = file , )
3755	def Carcinogen ( CASRN , AvailableMethods = False , Method = None ) : r methods = [ COMBINED , IARC , NTP ] if AvailableMethods : return methods if not Method : Method = methods [ 0 ] if Method == IARC : if CASRN in IARC_data . index : status = IARC_codes [ IARC_data . at [ CASRN , 'group' ] ] else : status = UNLISTED elif Method == NTP : if CASRN in NTP_data . index : status = NTP_codes [ NTP_data . at [ CASRN , 'Listing' ] ] else : status = UNLISTED elif Method == COMBINED : status = { } for method in methods [ 1 : ] : status [ method ] = Carcinogen ( CASRN , Method = method ) else : raise Exception ( 'Failure in in function' ) return status
7207	def execute ( self ) : self . generate_workflow_description ( ) if self . batch_values : self . id = self . workflow . launch_batch_workflow ( self . definition ) else : self . id = self . workflow . launch ( self . definition ) return self . id
4810	def train_model ( best_processed_path , weight_path = '../weight/model_weight.h5' , verbose = 2 ) : x_train_char , x_train_type , y_train = prepare_feature ( best_processed_path , option = 'train' ) x_test_char , x_test_type , y_test = prepare_feature ( best_processed_path , option = 'test' ) validation_set = False if os . path . isdir ( os . path . join ( best_processed_path , 'val' ) ) : validation_set = True x_val_char , x_val_type , y_val = prepare_feature ( best_processed_path , option = 'val' ) if not os . path . isdir ( os . path . dirname ( weight_path ) ) : os . makedirs ( os . path . dirname ( weight_path ) ) callbacks_list = [ ReduceLROnPlateau ( ) , ModelCheckpoint ( weight_path , save_best_only = True , save_weights_only = True , monitor = 'val_loss' , mode = 'min' , verbose = 1 ) ] model = get_convo_nn2 ( ) train_params = [ ( 10 , 256 ) , ( 3 , 512 ) , ( 3 , 2048 ) , ( 3 , 4096 ) , ( 3 , 8192 ) ] for ( epochs , batch_size ) in train_params : print ( "train with {} epochs and {} batch size" . format ( epochs , batch_size ) ) if validation_set : model . fit ( [ x_train_char , x_train_type ] , y_train , epochs = epochs , batch_size = batch_size , verbose = verbose , callbacks = callbacks_list , validation_data = ( [ x_val_char , x_val_type ] , y_val ) ) else : model . fit ( [ x_train_char , x_train_type ] , y_train , epochs = epochs , batch_size = batch_size , verbose = verbose , callbacks = callbacks_list ) return model
5662	def return_segments ( shape , break_points ) : segs = [ ] bp = 0 bp2 = 0 for i in range ( len ( break_points ) - 1 ) : bp = break_points [ i ] if break_points [ i ] is not None else bp2 bp2 = break_points [ i + 1 ] if break_points [ i + 1 ] is not None else bp segs . append ( shape [ bp : bp2 + 1 ] ) segs . append ( [ ] ) return segs
3777	def calculate_integral ( self , T1 , T2 , method ) : r return float ( quad ( self . calculate , T1 , T2 , args = ( method ) ) [ 0 ] )
7022	def pklc_fovcatalog_objectinfo ( pklcdir , fovcatalog , fovcatalog_columns = [ 0 , 1 , 2 , 6 , 7 , 8 , 9 , 10 , 11 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 ] , fovcatalog_colnames = [ 'objectid' , 'ra' , 'decl' , 'jmag' , 'jmag_err' , 'hmag' , 'hmag_err' , 'kmag' , 'kmag_err' , 'bmag' , 'vmag' , 'rmag' , 'imag' , 'sdssu' , 'sdssg' , 'sdssr' , 'sdssi' , 'sdssz' ] , fovcatalog_colformats = ( 'U20,f8,f8,' 'f8,f8,' 'f8,f8,' 'f8,f8,' 'f8,f8,f8,f8,' 'f8,f8,f8,' 'f8,f8' ) ) : if fovcatalog . endswith ( '.gz' ) : catfd = gzip . open ( fovcatalog ) else : catfd = open ( fovcatalog ) fovcat = np . genfromtxt ( catfd , usecols = fovcatalog_columns , names = fovcatalog_colnames , dtype = fovcatalog_colformats ) catfd . close ( ) pklclist = sorted ( glob . glob ( os . path . join ( pklcdir , '*HAT*-pklc.pkl' ) ) ) updatedpklcs , failedpklcs = [ ] , [ ] for pklc in pklclist : lcdict = read_hatpi_pklc ( pklc ) objectid = lcdict [ 'objectid' ] catind = np . where ( fovcat [ 'objectid' ] == objectid ) if len ( catind ) > 0 and catind [ 0 ] : lcdict [ 'objectinfo' ] . update ( { x : y for x , y in zip ( fovcatalog_colnames , [ np . asscalar ( fovcat [ z ] [ catind ] ) for z in fovcatalog_colnames ] ) } ) with open ( pklc + '-tmp' , 'wb' ) as outfd : pickle . dump ( lcdict , outfd , pickle . HIGHEST_PROTOCOL ) if os . path . exists ( pklc + '-tmp' ) : shutil . move ( pklc + '-tmp' , pklc ) LOGINFO ( 'updated %s with catalog info for %s at %.3f, %.3f OK' % ( pklc , objectid , lcdict [ 'objectinfo' ] [ 'ra' ] , lcdict [ 'objectinfo' ] [ 'decl' ] ) ) updatedpklcs . append ( pklc ) else : failedpklcs . append ( pklc ) return updatedpklcs , failedpklcs
8478	def install ( ) : cmd = CommandHelper ( ) cmd . install ( "npm" ) cmd = CommandHelper ( ) cmd . install ( "nodejs-legacy" ) cmd = CommandHelper ( ) cmd . command = "npm install -g retire" cmd . execute ( ) if cmd . errors : from termcolor import colored print colored ( cmd . errors , "red" ) else : print cmd . output
11849	def percept ( self , agent ) : "By default, agent perceives things within a default radius." return [ self . thing_percept ( thing , agent ) for thing in self . things_near ( agent . location ) ]
3075	def callback_view ( self ) : if 'error' in request . args : reason = request . args . get ( 'error_description' , request . args . get ( 'error' , '' ) ) reason = markupsafe . escape ( reason ) return ( 'Authorization failed: {0}' . format ( reason ) , httplib . BAD_REQUEST ) try : encoded_state = request . args [ 'state' ] server_csrf = session [ _CSRF_KEY ] code = request . args [ 'code' ] except KeyError : return 'Invalid request' , httplib . BAD_REQUEST try : state = json . loads ( encoded_state ) client_csrf = state [ 'csrf_token' ] return_url = state [ 'return_url' ] except ( ValueError , KeyError ) : return 'Invalid request state' , httplib . BAD_REQUEST if client_csrf != server_csrf : return 'Invalid request state' , httplib . BAD_REQUEST flow = _get_flow_for_token ( server_csrf ) if flow is None : return 'Invalid request state' , httplib . BAD_REQUEST try : credentials = flow . step2_exchange ( code ) except client . FlowExchangeError as exchange_error : current_app . logger . exception ( exchange_error ) content = 'An error occurred: {0}' . format ( exchange_error ) return content , httplib . BAD_REQUEST self . storage . put ( credentials ) if self . authorize_callback : self . authorize_callback ( credentials ) return redirect ( return_url )
10899	def update ( self , value = 0 ) : self . _deltas . append ( time . time ( ) ) self . value = value self . _percent = 100.0 * self . value / self . num if self . bar : self . _bars = self . _bar_symbol * int ( np . round ( self . _percent / 100. * self . _barsize ) ) if ( len ( self . _deltas ) < 2 ) or ( self . _deltas [ - 1 ] - self . _deltas [ - 2 ] ) > 1e-1 : self . _estimate_time ( ) self . _draw ( ) if self . value == self . num : self . end ( )
10951	def update ( self , params , values ) : return super ( State , self ) . update ( params , values )
6009	def load_background_noise_map ( background_noise_map_path , background_noise_map_hdu , pixel_scale , convert_background_noise_map_from_weight_map , convert_background_noise_map_from_inverse_noise_map ) : background_noise_map_options = sum ( [ convert_background_noise_map_from_weight_map , convert_background_noise_map_from_inverse_noise_map ] ) if background_noise_map_options == 0 and background_noise_map_path is not None : return NoiseMap . from_fits_with_pixel_scale ( file_path = background_noise_map_path , hdu = background_noise_map_hdu , pixel_scale = pixel_scale ) elif convert_background_noise_map_from_weight_map and background_noise_map_path is not None : weight_map = Array . from_fits ( file_path = background_noise_map_path , hdu = background_noise_map_hdu ) return NoiseMap . from_weight_map ( weight_map = weight_map , pixel_scale = pixel_scale ) elif convert_background_noise_map_from_inverse_noise_map and background_noise_map_path is not None : inverse_noise_map = Array . from_fits ( file_path = background_noise_map_path , hdu = background_noise_map_hdu ) return NoiseMap . from_inverse_noise_map ( inverse_noise_map = inverse_noise_map , pixel_scale = pixel_scale ) else : return None
8469	def getOSName ( self ) : _system = platform . system ( ) if _system in [ self . __class__ . OS_WINDOWS , self . __class__ . OS_MAC , self . __class__ . OS_LINUX ] : if _system == self . __class__ . OS_LINUX : _dist = platform . linux_distribution ( ) [ 0 ] if _dist . lower ( ) == self . __class__ . OS_UBUNTU . lower ( ) : return self . __class__ . OS_UBUNTU elif _dist . lower ( ) == self . __class__ . OS_DEBIAN . lower ( ) : return self . __class__ . OS_DEBIAN elif _dist . lower ( ) == self . __class__ . OS_CENTOS . lower ( ) : return self . __class__ . OS_CENTOS elif _dist . lower ( ) == self . __class__ . OS_REDHAT . lower ( ) : return self . __class__ . OS_REDHAT elif _dist . lower ( ) == self . __class__ . OS_KALI . lower ( ) : return self . __class__ . OS_KALI return _system else : return None
2927	def write_file_to_package_zip ( self , filename , src_filename ) : f = open ( src_filename ) with f : data = f . read ( ) self . manifest [ filename ] = md5hash ( data ) self . package_zip . write ( src_filename , filename )
3152	def delete ( self , list_id , webhook_id ) : self . list_id = list_id self . webhook_id = webhook_id return self . _mc_client . _delete ( url = self . _build_path ( list_id , 'webhooks' , webhook_id ) )
7235	def map ( self , features = None , query = None , styles = None , bbox = [ - 180 , - 90 , 180 , 90 ] , zoom = 10 , center = None , image = None , image_bounds = None , cmap = 'viridis' , api_key = os . environ . get ( 'MAPBOX_API_KEY' , None ) , ** kwargs ) : try : from IPython . display import display except : print ( "IPython is required to produce maps." ) return assert api_key is not None , "No Mapbox API Key found. You can either pass in a key or set the MAPBOX_API_KEY environment variable. Use outside of GBDX Notebooks requires a MapBox API key, sign up for free at https://www.mapbox.com/pricing/" if features is None and query is not None : wkt = box ( * bbox ) . wkt features = self . query ( wkt , query , index = None ) elif features is None and query is None and image is None : print ( 'Must provide either a list of features or a query or an image' ) return if styles is not None and not isinstance ( styles , list ) : styles = [ styles ] geojson = { "type" : "FeatureCollection" , "features" : features } if center is None and features is not None : union = cascaded_union ( [ shape ( f [ 'geometry' ] ) for f in features ] ) lon , lat = union . centroid . coords [ 0 ] elif center is None and image is not None : try : lon , lat = shape ( image ) . centroid . coords [ 0 ] except : lon , lat = box ( * image_bounds ) . centroid . coords [ 0 ] else : lat , lon = center map_id = "map_{}" . format ( str ( int ( time . time ( ) ) ) ) map_data = VectorGeojsonLayer ( geojson , styles = styles , ** kwargs ) image_layer = self . _build_image_layer ( image , image_bounds , cmap ) template = BaseTemplate ( map_id , ** { "lat" : lat , "lon" : lon , "zoom" : zoom , "datasource" : json . dumps ( map_data . datasource ) , "layers" : json . dumps ( map_data . layers ) , "image_layer" : image_layer , "mbkey" : api_key , "token" : 'dummy' } ) template . inject ( )
2738	def assign ( self , droplet_id ) : return self . get_data ( "floating_ips/%s/actions/" % self . ip , type = POST , params = { "type" : "assign" , "droplet_id" : droplet_id } )
1359	def get_argument_instance ( self ) : try : instance = self . get_argument ( constants . PARAM_INSTANCE ) return instance except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
7573	def progressbar ( njobs , finished , msg = "" , spacer = " " ) : if njobs : progress = 100 * ( finished / float ( njobs ) ) else : progress = 100 hashes = '#' * int ( progress / 5. ) nohash = ' ' * int ( 20 - len ( hashes ) ) if not ipyrad . __interactive__ : msg = msg . rsplit ( "|" , 2 ) [ 0 ] args = [ spacer , hashes + nohash , int ( progress ) , msg ] print ( "\r{}[{}] {:>3}% {} " . format ( * args ) , end = "" ) sys . stdout . flush ( )
3305	def _run_gevent ( app , config , mode ) : import gevent import gevent . monkey gevent . monkey . patch_all ( ) from gevent . pywsgi import WSGIServer server_args = { "bind_addr" : ( config [ "host" ] , config [ "port" ] ) , "wsgi_app" : app , "keyfile" : None , "certfile" : None , } protocol = "http" server_args . update ( config . get ( "server_args" , { } ) ) dav_server = WSGIServer ( server_args [ "bind_addr" ] , app ) _logger . info ( "Running {}" . format ( dav_server ) ) _logger . info ( "Serving on {}://{}:{} ..." . format ( protocol , config [ "host" ] , config [ "port" ] ) ) try : gevent . spawn ( dav_server . serve_forever ( ) ) except KeyboardInterrupt : _logger . warning ( "Caught Ctrl-C, shutting down..." ) return
446	def batch_with_dynamic_pad ( images_and_captions , batch_size , queue_capacity , add_summaries = True ) : enqueue_list = [ ] for image , caption in images_and_captions : caption_length = tf . shape ( caption ) [ 0 ] input_length = tf . expand_dims ( tf . subtract ( caption_length , 1 ) , 0 ) input_seq = tf . slice ( caption , [ 0 ] , input_length ) target_seq = tf . slice ( caption , [ 1 ] , input_length ) indicator = tf . ones ( input_length , dtype = tf . int32 ) enqueue_list . append ( [ image , input_seq , target_seq , indicator ] ) images , input_seqs , target_seqs , mask = tf . train . batch_join ( enqueue_list , batch_size = batch_size , capacity = queue_capacity , dynamic_pad = True , name = "batch_and_pad" ) if add_summaries : lengths = tf . add ( tf . reduce_sum ( mask , 1 ) , 1 ) tf . summary . scalar ( "caption_length/batch_min" , tf . reduce_min ( lengths ) ) tf . summary . scalar ( "caption_length/batch_max" , tf . reduce_max ( lengths ) ) tf . summary . scalar ( "caption_length/batch_mean" , tf . reduce_mean ( lengths ) ) return images , input_seqs , target_seqs , mask
6695	def upgrade ( safe = True ) : manager = MANAGER if safe : cmd = 'upgrade' else : cmd = 'dist-upgrade' run_as_root ( "%(manager)s --assume-yes %(cmd)s" % locals ( ) , pty = False )
1011	def trimSegments ( self , minPermanence = None , minNumSyns = None ) : if minPermanence is None : minPermanence = self . connectedPerm if minNumSyns is None : minNumSyns = self . activationThreshold totalSegsRemoved , totalSynsRemoved = 0 , 0 for c , i in itertools . product ( xrange ( self . numberOfCols ) , xrange ( self . cellsPerColumn ) ) : ( segsRemoved , synsRemoved ) = self . _trimSegmentsInCell ( colIdx = c , cellIdx = i , segList = self . cells [ c ] [ i ] , minPermanence = minPermanence , minNumSyns = minNumSyns ) totalSegsRemoved += segsRemoved totalSynsRemoved += synsRemoved if self . verbosity >= 5 : print "Cells, all segments:" self . printCells ( predictedOnly = False ) return totalSegsRemoved , totalSynsRemoved
11350	def merge_kwargs ( self , kwargs ) : if kwargs : self . parser_kwargs . update ( kwargs ) self . parser_kwargs . setdefault ( 'dest' , self . name ) if 'default' in kwargs : self . parser_kwargs [ "default" ] = kwargs [ "default" ] self . parser_kwargs [ "required" ] = False elif 'action' in kwargs : if kwargs [ 'action' ] in set ( [ 'store_false' , 'store_true' ] ) : self . parser_kwargs [ 'required' ] = False elif kwargs [ 'action' ] in set ( [ 'version' ] ) : self . parser_kwargs . pop ( 'required' , False ) else : self . parser_kwargs . setdefault ( "required" , True )
13584	def _obj_display ( obj , display = '' ) : result = '' if not display : result = str ( obj ) else : template = Template ( display ) context = Context ( { 'obj' : obj } ) result = template . render ( context ) return result
2586	def _command_server ( self , kill_event ) : logger . debug ( "[COMMAND] Command Server Starting" ) while not kill_event . is_set ( ) : try : command_req = self . command_channel . recv_pyobj ( ) logger . debug ( "[COMMAND] Received command request: {}" . format ( command_req ) ) if command_req == "OUTSTANDING_C" : outstanding = self . pending_task_queue . qsize ( ) for manager in self . _ready_manager_queue : outstanding += len ( self . _ready_manager_queue [ manager ] [ 'tasks' ] ) reply = outstanding elif command_req == "WORKERS" : num_workers = 0 for manager in self . _ready_manager_queue : num_workers += self . _ready_manager_queue [ manager ] [ 'worker_count' ] reply = num_workers elif command_req == "MANAGERS" : reply = [ ] for manager in self . _ready_manager_queue : resp = { 'manager' : manager . decode ( 'utf-8' ) , 'block_id' : self . _ready_manager_queue [ manager ] [ 'block_id' ] , 'worker_count' : self . _ready_manager_queue [ manager ] [ 'worker_count' ] , 'tasks' : len ( self . _ready_manager_queue [ manager ] [ 'tasks' ] ) , 'active' : self . _ready_manager_queue [ manager ] [ 'active' ] } reply . append ( resp ) elif command_req . startswith ( "HOLD_WORKER" ) : cmd , s_manager = command_req . split ( ';' ) manager = s_manager . encode ( 'utf-8' ) logger . info ( "[CMD] Received HOLD_WORKER for {}" . format ( manager ) ) if manager in self . _ready_manager_queue : self . _ready_manager_queue [ manager ] [ 'active' ] = False reply = True else : reply = False elif command_req == "SHUTDOWN" : logger . info ( "[CMD] Received SHUTDOWN command" ) kill_event . set ( ) reply = True else : reply = None logger . debug ( "[COMMAND] Reply: {}" . format ( reply ) ) self . command_channel . send_pyobj ( reply ) except zmq . Again : logger . debug ( "[COMMAND] is alive" ) continue
7120	def _convert_item ( self , obj ) : if isinstance ( obj , dict ) and not isinstance ( obj , DotDict ) : obj = DotDict ( obj ) elif isinstance ( obj , list ) : for i , item in enumerate ( obj ) : if isinstance ( item , dict ) and not isinstance ( item , DotDict ) : obj [ i ] = DotDict ( item ) return obj
11062	def send_im ( self , user , text ) : if isinstance ( user , SlackUser ) : user = user . id channelid = self . _find_im_channel ( user ) else : channelid = user . id self . send_message ( channelid , text )
9571	def discovery_view ( self , message ) : for handler in self . registered_handlers : if handler . check ( message ) : return handler . view return None
2106	def _echo_setting ( key ) : value = getattr ( settings , key ) secho ( '%s: ' % key , fg = 'magenta' , bold = True , nl = False ) secho ( six . text_type ( value ) , bold = True , fg = 'white' if isinstance ( value , six . text_type ) else 'cyan' , )
4878	def validate ( self , data ) : lms_user_id = data . get ( 'lms_user_id' ) tpa_user_id = data . get ( 'tpa_user_id' ) user_email = data . get ( 'user_email' ) if not lms_user_id and not tpa_user_id and not user_email : raise serializers . ValidationError ( 'At least one of the following fields must be specified and map to an EnterpriseCustomerUser: ' 'lms_user_id, tpa_user_id, user_email' ) return data
8456	def up_to_date ( version = None ) : temple . check . in_git_repo ( ) temple . check . is_temple_project ( ) temple_config = temple . utils . read_temple_config ( ) old_template_version = temple_config [ '_version' ] new_template_version = version or _get_latest_template_version ( temple_config [ '_template' ] ) return new_template_version == old_template_version
9875	def aggregate_tree ( l_tree ) : def _aggregate_phase1 ( tree ) : n_tree = radix . Radix ( ) for prefix in tree . prefixes ( ) : if tree . search_worst ( prefix ) . prefix == prefix : n_tree . add ( prefix ) return n_tree def _aggregate_phase2 ( tree ) : n_tree = radix . Radix ( ) for rnode in tree : p = text ( ip_network ( text ( rnode . prefix ) ) . supernet ( ) ) r = tree . search_covered ( p ) if len ( r ) == 2 : if r [ 0 ] . prefixlen == r [ 1 ] . prefixlen == rnode . prefixlen : n_tree . add ( p ) else : n_tree . add ( rnode . prefix ) else : n_tree . add ( rnode . prefix ) return n_tree l_tree = _aggregate_phase1 ( l_tree ) if len ( l_tree . prefixes ( ) ) == 1 : return l_tree while True : r_tree = _aggregate_phase2 ( l_tree ) if l_tree . prefixes ( ) == r_tree . prefixes ( ) : break else : l_tree = r_tree del r_tree return l_tree
3209	def get_load_balancer ( load_balancer , flags = FLAGS . ALL ^ FLAGS . POLICY_TYPES , ** conn ) : try : basestring except NameError as _ : basestring = str if isinstance ( load_balancer , basestring ) : load_balancer = dict ( LoadBalancerName = load_balancer ) return registry . build_out ( flags , start_with = load_balancer , pass_datastructure = True , ** conn )
2833	def stop ( self , pin ) : if pin not in self . pwm : raise ValueError ( 'Pin {0} is not configured as a PWM. Make sure to first call start for the pin.' . format ( pin ) ) self . pwm [ pin ] . stop ( ) del self . pwm [ pin ]
5643	def get_min_visit_time ( self ) : if not self . visit_events : return float ( 'inf' ) else : return min ( self . visit_events , key = lambda event : event . arr_time_ut ) . arr_time_ut
7517	def snpcount_numba ( superints , snpsarr ) : for iloc in xrange ( superints . shape [ 0 ] ) : for site in xrange ( superints . shape [ 2 ] ) : catg = np . zeros ( 4 , dtype = np . int16 ) ncol = superints [ iloc , : , site ] for idx in range ( ncol . shape [ 0 ] ) : if ncol [ idx ] == 67 : catg [ 0 ] += 1 elif ncol [ idx ] == 65 : catg [ 1 ] += 1 elif ncol [ idx ] == 84 : catg [ 2 ] += 1 elif ncol [ idx ] == 71 : catg [ 3 ] += 1 elif ncol [ idx ] == 82 : catg [ 1 ] += 1 catg [ 3 ] += 1 elif ncol [ idx ] == 75 : catg [ 2 ] += 1 catg [ 3 ] += 1 elif ncol [ idx ] == 83 : catg [ 0 ] += 1 catg [ 3 ] += 1 elif ncol [ idx ] == 89 : catg [ 0 ] += 1 catg [ 2 ] += 1 elif ncol [ idx ] == 87 : catg [ 1 ] += 1 catg [ 2 ] += 1 elif ncol [ idx ] == 77 : catg [ 0 ] += 1 catg [ 1 ] += 1 catg . sort ( ) if not catg [ 2 ] : pass else : if catg [ 2 ] > 1 : snpsarr [ iloc , site , 1 ] = True else : snpsarr [ iloc , site , 0 ] = True return snpsarr
5731	def advance_past_string_with_gdb_escapes ( self , chars_to_remove_gdb_escape = None ) : if chars_to_remove_gdb_escape is None : chars_to_remove_gdb_escape = [ '"' ] buf = "" while True : c = self . raw_text [ self . index ] self . index += 1 logging . debug ( "%s" , fmt_cyan ( c ) ) if c == "\\" : c2 = self . raw_text [ self . index ] self . index += 1 buf += c2 elif c == '"' : break else : buf += c return buf
1841	def JNO ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , False == cpu . OF , target . read ( ) , cpu . PC )
12806	def attach ( self , observer ) : if not observer in self . _observers : self . _observers . append ( observer ) return self
8758	def get_subnet ( context , id , fields = None ) : LOG . info ( "get_subnet %s for tenant %s with fields %s" % ( id , context . tenant_id , fields ) ) subnet = db_api . subnet_find ( context = context , limit = None , page_reverse = False , sorts = [ 'id' ] , marker_obj = None , fields = None , id = id , join_dns = True , join_routes = True , scope = db_api . ONE ) if not subnet : raise n_exc . SubnetNotFound ( subnet_id = id ) cache = subnet . get ( "_allocation_pool_cache" ) if not cache : new_cache = subnet . allocation_pools db_api . subnet_update_set_alloc_pool_cache ( context , subnet , new_cache ) return v . _make_subnet_dict ( subnet )
9438	def load_network ( self , layers = 1 ) : if layers : ctor = payload_type ( self . type ) [ 0 ] if ctor : ctor = ctor payload = self . payload self . payload = ctor ( payload , layers - 1 ) else : pass
9901	def _updateType ( self ) : data = self . _data ( ) if isinstance ( data , dict ) and isinstance ( self , ListFile ) : self . __class__ = DictFile elif isinstance ( data , list ) and isinstance ( self , DictFile ) : self . __class__ = ListFile
9984	def has_lambda ( src ) : module_node = ast . parse ( dedent ( src ) ) lambdaexp = [ node for node in ast . walk ( module_node ) if isinstance ( node , ast . Lambda ) ] return bool ( lambdaexp )
5116	def draw ( self , update_colors = True , line_kwargs = None , scatter_kwargs = None , ** kwargs ) : if not HAS_MATPLOTLIB : raise ImportError ( "matplotlib is necessary to draw the network." ) if update_colors : self . _update_all_colors ( ) if 'bgcolor' not in kwargs : kwargs [ 'bgcolor' ] = self . colors [ 'bgcolor' ] self . g . draw_graph ( line_kwargs = line_kwargs , scatter_kwargs = scatter_kwargs , ** kwargs )
6891	def _starfeatures_worker ( task ) : try : ( lcfile , outdir , kdtree , objlist , lcflist , neighbor_radius_arcsec , deredden , custom_bandpasses , lcformat , lcformatdir ) = task return get_starfeatures ( lcfile , outdir , kdtree , objlist , lcflist , neighbor_radius_arcsec , deredden = deredden , custom_bandpasses = custom_bandpasses , lcformat = lcformat , lcformatdir = lcformatdir ) except Exception as e : return None
3232	def get_user_agent_default ( pkg_name = 'cloudaux' ) : version = '0.0.1' try : import pkg_resources version = pkg_resources . get_distribution ( pkg_name ) . version except pkg_resources . DistributionNotFound : pass except ImportError : pass return 'cloudaux/%s' % ( version )
2932	def package_for_editor_signavio ( self , spec , filename ) : signavio_file = filename [ : - len ( '.bpmn20.xml' ) ] + '.signavio.xml' if os . path . exists ( signavio_file ) : self . write_file_to_package_zip ( "src/" + self . _get_zip_path ( signavio_file ) , signavio_file ) f = open ( signavio_file , 'r' ) try : signavio_tree = ET . parse ( f ) finally : f . close ( ) svg_node = one ( signavio_tree . findall ( './/svg-representation' ) ) self . write_to_package_zip ( "%s.svg" % spec . name , svg_node . text )
9917	def validate ( self , data ) : user = self . _confirmation . email . user if ( app_settings . EMAIL_VERIFICATION_PASSWORD_REQUIRED and not user . check_password ( data [ "password" ] ) ) : raise serializers . ValidationError ( _ ( "The provided password is invalid." ) ) data [ "email" ] = self . _confirmation . email . email return data
5215	def active_futures ( ticker : str , dt ) -> str : t_info = ticker . split ( ) prefix , asset = ' ' . join ( t_info [ : - 1 ] ) , t_info [ - 1 ] info = const . market_info ( f'{prefix[:-1]}1 {asset}' ) f1 , f2 = f'{prefix[:-1]}1 {asset}' , f'{prefix[:-1]}2 {asset}' fut_2 = fut_ticker ( gen_ticker = f2 , dt = dt , freq = info [ 'freq' ] ) fut_1 = fut_ticker ( gen_ticker = f1 , dt = dt , freq = info [ 'freq' ] ) fut_tk = bdp ( tickers = [ fut_1 , fut_2 ] , flds = 'Last_Tradeable_Dt' , cache = True ) if pd . Timestamp ( dt ) . month < pd . Timestamp ( fut_tk . last_tradeable_dt [ 0 ] ) . month : return fut_1 d1 = bdib ( ticker = f1 , dt = dt ) d2 = bdib ( ticker = f2 , dt = dt ) return fut_1 if d1 [ f1 ] . volume . sum ( ) > d2 [ f2 ] . volume . sum ( ) else fut_2
9755	def delete ( ctx ) : user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) if not click . confirm ( "Are sure you want to delete experiment `{}`" . format ( _experiment ) ) : click . echo ( 'Existing without deleting experiment.' ) sys . exit ( 1 ) try : response = PolyaxonClient ( ) . experiment . delete_experiment ( user , project_name , _experiment ) ExperimentManager . purge ( ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not delete experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) if response . status_code == 204 : Printer . print_success ( "Experiment `{}` was delete successfully" . format ( _experiment ) )
12960	def _filter ( filterObj , ** kwargs ) : for key , value in kwargs . items ( ) : if key . endswith ( '__ne' ) : notFilter = True key = key [ : - 4 ] else : notFilter = False if key not in filterObj . indexedFields : raise ValueError ( 'Field "' + key + '" is not in INDEXED_FIELDS array. Filtering is only supported on indexed fields.' ) if notFilter is False : filterObj . filters . append ( ( key , value ) ) else : filterObj . notFilters . append ( ( key , value ) ) return filterObj
9430	def _extract_members ( self , members , targetpath , pwd ) : archive = unrarlib . RAROpenArchiveDataEx ( self . filename , mode = constants . RAR_OM_EXTRACT ) handle = self . _open ( archive ) password = pwd or self . pwd if password is not None : unrarlib . RARSetPassword ( handle , b ( password ) ) try : rarinfo = self . _read_header ( handle ) while rarinfo is not None : if rarinfo . filename in members : self . _process_current ( handle , constants . RAR_EXTRACT , targetpath ) else : self . _process_current ( handle , constants . RAR_SKIP ) rarinfo = self . _read_header ( handle ) except unrarlib . MissingPassword : raise RuntimeError ( "File is encrypted, password required" ) except unrarlib . BadPassword : raise RuntimeError ( "Bad password for File" ) except unrarlib . BadDataError : raise RuntimeError ( "File CRC Error" ) except unrarlib . UnrarException as e : raise BadRarFile ( "Bad RAR archive data: %s" % str ( e ) ) finally : self . _close ( handle )
10673	def load_data_factsage ( path = '' ) : compounds . clear ( ) if path == '' : path = default_data_path if not os . path . exists ( path ) : warnings . warn ( 'The specified data file path does not exist. (%s)' % path ) return files = glob . glob ( os . path . join ( path , 'Compound_*.txt' ) ) for file in files : compound = Compound ( _read_compound_from_factsage_file_ ( file ) ) compounds [ compound . formula ] = compound
11002	def pack_args ( self ) : mapper = { 'psf-kfki' : 'kfki' , 'psf-alpha' : 'alpha' , 'psf-n2n1' : 'n2n1' , 'psf-sigkf' : 'sigkf' , 'psf-sph6-ab' : 'sph6_ab' , 'psf-laser-wavelength' : 'laser_wavelength' , 'psf-pinhole-width' : 'pinhole_width' } bads = [ self . zscale , 'psf-zslab' ] d = { } for k , v in iteritems ( mapper ) : if k in self . param_dict : d [ v ] = self . param_dict [ k ] d . update ( { 'polar_angle' : self . polar_angle , 'normalize' : self . normalize , 'include_K3_det' : self . use_J1 } ) if self . polychromatic : d . update ( { 'nkpts' : self . nkpts } ) d . update ( { 'k_dist' : self . k_dist } ) if self . do_pinhole : d . update ( { 'nlpts' : self . num_line_pts } ) d . update ( { 'use_laggauss' : True } ) return d
861	def getMaxDelay ( inferences ) : maxDelay = 0 for inferenceElement , inference in inferences . iteritems ( ) : if isinstance ( inference , dict ) : for key in inference . iterkeys ( ) : maxDelay = max ( InferenceElement . getTemporalDelay ( inferenceElement , key ) , maxDelay ) else : maxDelay = max ( InferenceElement . getTemporalDelay ( inferenceElement ) , maxDelay ) return maxDelay
1829	def JA ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , Operators . AND ( cpu . CF == False , cpu . ZF == False ) , target . read ( ) , cpu . PC )
13634	def _parseAccept ( headers ) : def sort ( value ) : return float ( value [ 1 ] . get ( 'q' , 1 ) ) return OrderedDict ( sorted ( _splitHeaders ( headers ) , key = sort , reverse = True ) )
13783	def _MakeFieldDescriptor ( self , field_proto , message_name , index , is_extension = False ) : if message_name : full_name = '.' . join ( ( message_name , field_proto . name ) ) else : full_name = field_proto . name return descriptor . FieldDescriptor ( name = field_proto . name , full_name = full_name , index = index , number = field_proto . number , type = field_proto . type , cpp_type = None , message_type = None , enum_type = None , containing_type = None , label = field_proto . label , has_default_value = False , default_value = None , is_extension = is_extension , extension_scope = None , options = field_proto . options )
8342	def _convertEntities ( self , match ) : x = match . group ( 1 ) if self . convertHTMLEntities and x in name2codepoint : return unichr ( name2codepoint [ x ] ) elif x in self . XML_ENTITIES_TO_SPECIAL_CHARS : if self . convertXMLEntities : return self . XML_ENTITIES_TO_SPECIAL_CHARS [ x ] else : return u'&%s;' % x elif len ( x ) > 0 and x [ 0 ] == '#' : if len ( x ) > 1 and x [ 1 ] == 'x' : return unichr ( int ( x [ 2 : ] , 16 ) ) else : return unichr ( int ( x [ 1 : ] ) ) elif self . escapeUnrecognizedEntities : return u'&amp;%s;' % x else : return u'&%s;' % x
4942	def enterprise_customer_uuid ( self ) : try : enterprise_user = EnterpriseCustomerUser . objects . get ( user_id = self . user . id ) except ObjectDoesNotExist : LOGGER . warning ( 'User {} has a {} assignment but is not linked to an enterprise!' . format ( self . __class__ , self . user . id ) ) return None except MultipleObjectsReturned : LOGGER . warning ( 'User {} is linked to multiple enterprises, which is not yet supported!' . format ( self . user . id ) ) return None return str ( enterprise_user . enterprise_customer . uuid )
1295	def import_demo_experience ( self , states , internals , actions , terminal , reward ) : fetches = self . import_demo_experience_output feed_dict = self . get_feed_dict ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward ) self . monitored_session . run ( fetches = fetches , feed_dict = feed_dict )
5150	def parse ( self , native ) : if not hasattr ( self , 'parser' ) or not self . parser : raise NotImplementedError ( 'Parser class not specified' ) parser = self . parser ( native ) self . intermediate_data = parser . intermediate_data del parser self . to_netjson ( )
1373	def defaults_cluster_role_env ( cluster_role_env ) : if len ( cluster_role_env [ 1 ] ) == 0 and len ( cluster_role_env [ 2 ] ) == 0 : return ( cluster_role_env [ 0 ] , getpass . getuser ( ) , ENVIRON ) return ( cluster_role_env [ 0 ] , cluster_role_env [ 1 ] , cluster_role_env [ 2 ] )
4600	def detach ( self , overlay ) : for i , a in enumerate ( self . animations ) : a . layout = a . layout . clone ( ) if overlay and i : a . preclear = False
13445	def create_admin ( username = 'admin' , email = 'admin@admin.com' , password = 'admin' ) : admin = User . objects . create_user ( username , email , password ) admin . is_staff = True admin . is_superuser = True admin . save ( ) return admin
10715	def sendCommands ( comPort , commands ) : mutex . acquire ( ) try : try : port = serial . Serial ( port = comPort ) header = '11010101 10101010' footer = '10101101' for command in _translateCommands ( commands ) : _sendBinaryData ( port , header + command + footer ) except serial . SerialException : print ( 'Unable to open serial port %s' % comPort ) print ( '' ) raise finally : mutex . release ( )
10452	def waittillguiexist ( self , window_name , object_name = '' , guiTimeOut = 30 , state = '' ) : timeout = 0 while timeout < guiTimeOut : if self . guiexist ( window_name , object_name ) : return 1 time . sleep ( 1 ) timeout += 1 return 0
11260	def match ( prev , pattern , * args , ** kw ) : to = 'to' in kw and kw . pop ( 'to' ) pattern_obj = re . compile ( pattern , * args , ** kw ) if to is dict : for data in prev : match = pattern_obj . match ( data ) if match is not None : yield match . groupdict ( ) elif to is tuple : for data in prev : match = pattern_obj . match ( data ) if match is not None : yield match . groups ( ) elif to is list : for data in prev : match = pattern_obj . match ( data ) if match is not None : yield list ( match . groups ( ) ) else : for data in prev : match = pattern_obj . match ( data ) if match is not None : yield match
11429	def record_strip_controlfields ( rec ) : for tag in rec . keys ( ) : if tag [ : 2 ] == '00' and rec [ tag ] [ 0 ] [ 3 ] : del rec [ tag ]
1390	def get_status ( self ) : status = None if self . physical_plan and self . physical_plan . topology : status = self . physical_plan . topology . state if status == 1 : return "Running" elif status == 2 : return "Paused" elif status == 3 : return "Killed" else : return "Unknown"
1643	def CheckSectionSpacing ( filename , clean_lines , class_info , linenum , error ) : if ( class_info . last_line - class_info . starting_linenum <= 24 or linenum <= class_info . starting_linenum ) : return matched = Match ( r'\s*(public|protected|private):' , clean_lines . lines [ linenum ] ) if matched : prev_line = clean_lines . lines [ linenum - 1 ] if ( not IsBlankLine ( prev_line ) and not Search ( r'\b(class|struct)\b' , prev_line ) and not Search ( r'\\$' , prev_line ) ) : end_class_head = class_info . starting_linenum for i in range ( class_info . starting_linenum , linenum ) : if Search ( r'\{\s*$' , clean_lines . lines [ i ] ) : end_class_head = i break if end_class_head < linenum - 1 : error ( filename , linenum , 'whitespace/blank_line' , 3 , '"%s:" should be preceded by a blank line' % matched . group ( 1 ) )
6216	def buffers_exist ( self ) : for buff in self . buffers : if not buff . is_separate_file : continue path = self . path . parent / buff . uri if not os . path . exists ( path ) : raise FileNotFoundError ( "Buffer {} referenced in {} not found" . format ( path , self . path ) )
11828	def exact_sqrt ( n2 ) : "If n2 is a perfect square, return its square root, else raise error." n = int ( math . sqrt ( n2 ) ) assert n * n == n2 return n
5468	def get_event_of_type ( op , event_type ) : events = get_events ( op ) if not events : return None return [ e for e in events if e . get ( 'details' , { } ) . get ( '@type' ) == event_type ]
1393	def getTopologyByClusterRoleEnvironAndName ( self , cluster , role , environ , topologyName ) : topologies = list ( filter ( lambda t : t . name == topologyName and t . cluster == cluster and ( not role or t . execution_state . role == role ) and t . environ == environ , self . topologies ) ) if not topologies or len ( topologies ) > 1 : if role is not None : raise Exception ( "Topology not found for {0}, {1}, {2}, {3}" . format ( cluster , role , environ , topologyName ) ) else : raise Exception ( "Topology not found for {0}, {1}, {2}" . format ( cluster , environ , topologyName ) ) return topologies [ 0 ]
9197	def get ( self , key , default = _sentinel ) : tup = self . _data . get ( key . lower ( ) ) if tup is not None : return tup [ 1 ] elif default is not _sentinel : return default else : return None
2621	def security_group ( self , vpc ) : sg = vpc . create_security_group ( GroupName = "private-subnet" , Description = "security group for remote executors" ) ip_ranges = [ { 'CidrIp' : '10.0.0.0/16' } ] in_permissions = [ { 'IpProtocol' : 'TCP' , 'FromPort' : 0 , 'ToPort' : 65535 , 'IpRanges' : ip_ranges , } , { 'IpProtocol' : 'UDP' , 'FromPort' : 0 , 'ToPort' : 65535 , 'IpRanges' : ip_ranges , } , { 'IpProtocol' : 'ICMP' , 'FromPort' : - 1 , 'ToPort' : - 1 , 'IpRanges' : [ { 'CidrIp' : '0.0.0.0/0' } ] , } , { 'IpProtocol' : 'TCP' , 'FromPort' : 22 , 'ToPort' : 22 , 'IpRanges' : [ { 'CidrIp' : '0.0.0.0/0' } ] , } ] out_permissions = [ { 'IpProtocol' : 'TCP' , 'FromPort' : 0 , 'ToPort' : 65535 , 'IpRanges' : [ { 'CidrIp' : '0.0.0.0/0' } ] , } , { 'IpProtocol' : 'TCP' , 'FromPort' : 0 , 'ToPort' : 65535 , 'IpRanges' : ip_ranges , } , { 'IpProtocol' : 'UDP' , 'FromPort' : 0 , 'ToPort' : 65535 , 'IpRanges' : ip_ranges , } , ] sg . authorize_ingress ( IpPermissions = in_permissions ) sg . authorize_egress ( IpPermissions = out_permissions ) self . sg_id = sg . id return sg
10802	def _c2x ( self , c ) : return 0.5 * ( self . window [ 0 ] + self . window [ 1 ] + c * ( self . window [ 1 ] - self . window [ 0 ] ) )
12950	def connectAlt ( cls , redisConnectionParams ) : if not isinstance ( redisConnectionParams , dict ) : raise ValueError ( 'redisConnectionParams must be a dictionary!' ) hashVal = hashDictOneLevel ( redisConnectionParams ) modelDictCopy = copy . deepcopy ( dict ( cls . __dict__ ) ) modelDictCopy [ 'REDIS_CONNECTION_PARAMS' ] = redisConnectionParams ConnectedIndexedRedisModel = type ( 'AltConnect' + cls . __name__ + str ( hashVal ) , cls . __bases__ , modelDictCopy ) return ConnectedIndexedRedisModel
12963	def getPrimaryKeys ( self , sortByAge = False ) : conn = self . _get_connection ( ) numFilters = len ( self . filters ) numNotFilters = len ( self . notFilters ) if numFilters + numNotFilters == 0 : conn = self . _get_connection ( ) matchedKeys = conn . smembers ( self . _get_ids_key ( ) ) elif numNotFilters == 0 : if numFilters == 1 : ( filterFieldName , filterValue ) = self . filters [ 0 ] matchedKeys = conn . smembers ( self . _get_key_for_index ( filterFieldName , filterValue ) ) else : indexKeys = [ self . _get_key_for_index ( filterFieldName , filterValue ) for filterFieldName , filterValue in self . filters ] matchedKeys = conn . sinter ( indexKeys ) else : notIndexKeys = [ self . _get_key_for_index ( filterFieldName , filterValue ) for filterFieldName , filterValue in self . notFilters ] if numFilters == 0 : matchedKeys = conn . sdiff ( self . _get_ids_key ( ) , * notIndexKeys ) else : indexKeys = [ self . _get_key_for_index ( filterFieldName , filterValue ) for filterFieldName , filterValue in self . filters ] tempKey = self . _getTempKey ( ) pipeline = conn . pipeline ( ) pipeline . sinterstore ( tempKey , * indexKeys ) pipeline . sdiff ( tempKey , * notIndexKeys ) pipeline . delete ( tempKey ) matchedKeys = pipeline . execute ( ) [ 1 ] matchedKeys = [ int ( _key ) for _key in matchedKeys ] if sortByAge is False : return list ( matchedKeys ) else : matchedKeys = list ( matchedKeys ) matchedKeys . sort ( ) return matchedKeys
5554	def _raw_at_zoom ( config , zooms ) : params_per_zoom = { } for zoom in zooms : params = { } for name , element in config . items ( ) : if name not in _RESERVED_PARAMETERS : out_element = _element_at_zoom ( name , element , zoom ) if out_element is not None : params [ name ] = out_element params_per_zoom [ zoom ] = params return params_per_zoom
3830	async def rename_conversation ( self , rename_conversation_request ) : response = hangouts_pb2 . RenameConversationResponse ( ) await self . _pb_request ( 'conversations/renameconversation' , rename_conversation_request , response ) return response
5245	def current_missing ( ** kwargs ) -> int : data_path = os . environ . get ( BBG_ROOT , '' ) . replace ( '\\' , '/' ) if not data_path : return 0 return len ( files . all_files ( f'{data_path}/Logs/{missing_info(**kwargs)}' ) )
11777	def replicated_dataset ( dataset , weights , n = None ) : "Copy dataset, replicating each example in proportion to its weight." n = n or len ( dataset . examples ) result = copy . copy ( dataset ) result . examples = weighted_replicate ( dataset . examples , weights , n ) return result
10248	def update_context ( universe : BELGraph , graph : BELGraph ) : for namespace in get_namespaces ( graph ) : if namespace in universe . namespace_url : graph . namespace_url [ namespace ] = universe . namespace_url [ namespace ] elif namespace in universe . namespace_pattern : graph . namespace_pattern [ namespace ] = universe . namespace_pattern [ namespace ] else : log . warning ( 'namespace: %s missing from universe' , namespace ) for annotation in get_annotations ( graph ) : if annotation in universe . annotation_url : graph . annotation_url [ annotation ] = universe . annotation_url [ annotation ] elif annotation in universe . annotation_pattern : graph . annotation_pattern [ annotation ] = universe . annotation_pattern [ annotation ] elif annotation in universe . annotation_list : graph . annotation_list [ annotation ] = universe . annotation_list [ annotation ] else : log . warning ( 'annotation: %s missing from universe' , annotation )
4292	def validate_project ( project_name ) : if '-' in project_name : return None if keyword . iskeyword ( project_name ) : return None if project_name in dir ( __builtins__ ) : return None try : __import__ ( project_name ) return None except ImportError : return project_name
951	def showPredictions ( ) : for k in range ( 6 ) : tm . reset ( ) print "--- " + "ABCDXY" [ k ] + " ---" tm . compute ( set ( seqT [ k ] [ : ] . nonzero ( ) [ 0 ] . tolist ( ) ) , learn = False ) activeColumnsIndices = [ tm . columnForCell ( i ) for i in tm . getActiveCells ( ) ] predictedColumnIndices = [ tm . columnForCell ( i ) for i in tm . getPredictiveCells ( ) ] currentColumns = [ 1 if i in activeColumnsIndices else 0 for i in range ( tm . numberOfColumns ( ) ) ] predictedColumns = [ 1 if i in predictedColumnIndices else 0 for i in range ( tm . numberOfColumns ( ) ) ] print ( "Active cols: " + str ( np . nonzero ( currentColumns ) [ 0 ] ) ) print ( "Predicted cols: " + str ( np . nonzero ( predictedColumns ) [ 0 ] ) ) print ""
4807	def create_char_dataframe ( words ) : char_dict = [ ] for word in words : for i , char in enumerate ( word ) : if i == 0 : char_dict . append ( { 'char' : char , 'type' : CHAR_TYPE_FLATTEN . get ( char , 'o' ) , 'target' : True } ) else : char_dict . append ( { 'char' : char , 'type' : CHAR_TYPE_FLATTEN . get ( char , 'o' ) , 'target' : False } ) return pd . DataFrame ( char_dict )
12987	def keep_kwargs_partial ( func , * args , ** keywords ) : def newfunc ( * fargs , ** fkeywords ) : newkeywords = fkeywords . copy ( ) newkeywords . update ( keywords ) return func ( * ( args + fargs ) , ** newkeywords ) newfunc . func = func newfunc . args = args newfunc . keywords = keywords return newfunc
9093	def _update_namespace ( self , namespace : Namespace ) -> None : old_entry_identifiers = self . _get_old_entry_identifiers ( namespace ) new_count = 0 skip_count = 0 for model in self . _iterate_namespace_models ( ) : if self . _get_identifier ( model ) in old_entry_identifiers : continue entry = self . _create_namespace_entry_from_model ( model , namespace = namespace ) if entry is None or entry . name is None : skip_count += 1 continue new_count += 1 self . session . add ( entry ) t = time . time ( ) log . info ( 'got %d new entries. skipped %d entries missing names. committing models' , new_count , skip_count ) self . session . commit ( ) log . info ( 'committed models in %.2f seconds' , time . time ( ) - t )
8266	def _cache ( self ) : n = self . steps if len ( self . _colors ) == 1 : ColorList . __init__ ( self , [ self . _colors [ 0 ] for i in _range ( n ) ] ) return colors = self . _interpolate ( self . _colors , 40 ) left = colors [ : len ( colors ) / 2 ] right = colors [ len ( colors ) / 2 : ] left . append ( right [ 0 ] ) right . insert ( 0 , left [ - 1 ] ) gradient = self . _interpolate ( left , int ( n * self . spread ) ) [ : - 1 ] gradient . extend ( self . _interpolate ( right , n - int ( n * self . spread ) ) [ 1 : ] ) if self . spread > 1 : gradient = gradient [ : n ] if self . spread < 0 : gradient = gradient [ - n : ] ColorList . __init__ ( self , gradient )
8457	def _needs_new_cc_config_for_update ( old_template , old_version , new_template , new_version ) : if old_template != new_template : return True else : return _cookiecutter_configs_have_changed ( new_template , old_version , new_version )
4231	def run_subcommand ( netgear , args ) : subcommand = args . subcommand if subcommand == "block_device" or subcommand == "allow_device" : return netgear . allow_block_device ( args . mac_addr , BLOCK if subcommand == "block_device" else ALLOW ) if subcommand == "attached_devices" : if args . verbose : return netgear . get_attached_devices_2 ( ) else : return netgear . get_attached_devices ( ) if subcommand == 'traffic_meter' : return netgear . get_traffic_meter ( ) if subcommand == 'login' : return netgear . login ( ) print ( "Unknown subcommand" )
11732	def registerGoodClass ( self , class_ ) : self . _valid_classes . append ( class_ ) for name , cls in class_members ( class_ ) : if self . isValidClass ( cls ) : self . registerGoodClass ( cls )
2028	def CALLDATALOAD ( self , offset ) : if issymbolic ( offset ) : if solver . can_be_true ( self . _constraints , offset == self . _used_calldata_size ) : self . constraints . add ( offset == self . _used_calldata_size ) raise ConcretizeArgument ( 1 , policy = 'SAMPLED' ) self . _use_calldata ( offset , 32 ) data_length = len ( self . data ) bytes = [ ] for i in range ( 32 ) : try : c = Operators . ITEBV ( 8 , offset + i < data_length , self . data [ offset + i ] , 0 ) except IndexError : c = 0 bytes . append ( c ) return Operators . CONCAT ( 256 , * bytes )
9835	def __general ( self ) : while 1 : try : tok = self . __peek ( ) except DXParserNoTokens : if self . currentobject and self . currentobject not in self . objects : self . objects . append ( self . currentobject ) return if tok . iscode ( 'COMMENT' ) : self . set_parser ( 'comment' ) elif tok . iscode ( 'WORD' ) and tok . equals ( 'object' ) : self . set_parser ( 'object' ) elif self . __parser is self . __general : raise DXParseError ( 'Unknown level-1 construct at ' + str ( tok ) ) self . apply_parser ( )
10220	def preprocessing_excel ( path ) : if not os . path . exists ( path ) : raise ValueError ( "Error: %s file not found" % path ) df = pd . read_excel ( path , sheetname = 0 , header = 0 ) df . iloc [ : , 0 ] = pd . Series ( df . iloc [ : , 0 ] ) . fillna ( method = 'ffill' ) df = df [ df . ix [ : , 1 ] . notnull ( ) ] df = df . reset_index ( drop = True ) df . ix [ : , 2 ] . fillna ( 0 , inplace = True ) if ( df . ix [ : , 1 ] . isnull ( ) . sum ( ) ) != 0 : raise ValueError ( "Error: Empty cells in the gene column" ) return df
10912	def vectorize_damping ( params , damping = 1.0 , increase_list = [ [ 'psf-' , 1e4 ] ] ) : damp_vec = np . ones ( len ( params ) ) * damping for nm , fctr in increase_list : for a in range ( damp_vec . size ) : if nm in params [ a ] : damp_vec [ a ] *= fctr return damp_vec
11150	def get_text_fingerprint ( text , hash_meth , encoding = "utf-8" ) : m = hash_meth ( ) m . update ( text . encode ( encoding ) ) return m . hexdigest ( )
11759	def is_variable ( x ) : "A variable is an Expr with no args and a lowercase symbol as the op." return isinstance ( x , Expr ) and not x . args and is_var_symbol ( x . op )
5792	def _cert_callback ( callback , der_cert , reason ) : if not callback : return callback ( x509 . Certificate . load ( der_cert ) , reason )
4760	def wait ( timeout = 300 ) : if env ( ) : cij . err ( "cij.ssh.wait: Invalid SSH environment" ) return 1 timeout_backup = cij . ENV . get ( "SSH_CMD_TIMEOUT" ) try : time_start = time . time ( ) cij . ENV [ "SSH_CMD_TIMEOUT" ] = "3" while True : time_current = time . time ( ) if ( time_current - time_start ) > timeout : cij . err ( "cij.ssh.wait: Timeout" ) return 1 status , _ , _ = command ( [ "exit" ] , shell = True , echo = False ) if not status : break cij . info ( "cij.ssh.wait: Time elapsed: %d seconds" % ( time_current - time_start ) ) finally : if timeout_backup is None : del cij . ENV [ "SSH_CMD_TIMEOUT" ] else : cij . ENV [ "SSH_CMD_TIMEOUT" ] = timeout_backup return 0
10417	def variants_of ( graph : BELGraph , node : Protein , modifications : Optional [ Set [ str ] ] = None , ) -> Set [ Protein ] : if modifications : return _get_filtered_variants_of ( graph , node , modifications ) return { v for u , v , key , data in graph . edges ( keys = True , data = True ) if ( u == node and data [ RELATION ] == HAS_VARIANT and pybel . struct . has_protein_modification ( v ) ) }
12576	def set_mask ( self , mask_img ) : mask = load_mask ( mask_img , allow_empty = True ) check_img_compatibility ( self . img , mask , only_check_3d = True ) self . mask = mask
9732	def get_6d ( self , component_info = None , data = None , component_position = None ) : components = [ ] append_components = components . append for _ in range ( component_info . body_count ) : component_position , position = QRTPacket . _get_exact ( RT6DBodyPosition , data , component_position ) component_position , matrix = QRTPacket . _get_tuple ( RT6DBodyRotation , data , component_position ) append_components ( ( position , matrix ) ) return components
9039	def _dump_to_file ( self , file ) : xmltodict . unparse ( self . object ( ) , file , pretty = True )
3100	def loadfile ( filename , cache = None ) : _SECRET_NAMESPACE = 'oauth2client:secrets#ns' if not cache : return _loadfile ( filename ) obj = cache . get ( filename , namespace = _SECRET_NAMESPACE ) if obj is None : client_type , client_info = _loadfile ( filename ) obj = { client_type : client_info } cache . set ( filename , obj , namespace = _SECRET_NAMESPACE ) return next ( six . iteritems ( obj ) )
13296	def decode_jsonld ( jsonld_text ) : decoder = json . JSONDecoder ( object_pairs_hook = _decode_object_pairs ) return decoder . decode ( jsonld_text )
7342	def set_debug ( ) : logging . basicConfig ( level = logging . WARNING ) peony . logger . setLevel ( logging . DEBUG )
7906	def __groupchat_message ( self , stanza ) : fr = stanza . get_from ( ) key = fr . bare ( ) . as_unicode ( ) rs = self . rooms . get ( key ) if not rs : self . __logger . debug ( "groupchat message from unknown source" ) return False rs . process_groupchat_message ( stanza ) return True
3858	def _on_watermark_notification ( self , notif ) : if self . get_user ( notif . user_id ) . is_self : logger . info ( 'latest_read_timestamp for {} updated to {}' . format ( self . id_ , notif . read_timestamp ) ) self_conversation_state = ( self . _conversation . self_conversation_state ) self_conversation_state . self_read_state . latest_read_timestamp = ( parsers . to_timestamp ( notif . read_timestamp ) ) previous_timestamp = self . _watermarks . get ( notif . user_id , datetime . datetime . min . replace ( tzinfo = datetime . timezone . utc ) ) if notif . read_timestamp > previous_timestamp : logger . info ( ( 'latest_read_timestamp for conv {} participant {}' + ' updated to {}' ) . format ( self . id_ , notif . user_id . chat_id , notif . read_timestamp ) ) self . _watermarks [ notif . user_id ] = notif . read_timestamp
1307	def IsDesktopLocked ( ) -> bool : isLocked = False desk = ctypes . windll . user32 . OpenDesktopW ( ctypes . c_wchar_p ( 'Default' ) , 0 , 0 , 0x0100 ) if desk : isLocked = not ctypes . windll . user32 . SwitchDesktop ( desk ) ctypes . windll . user32 . CloseDesktop ( desk ) return isLocked
12898	def set_mute ( self , value = False ) : mute = ( yield from self . handle_set ( self . API . get ( 'mute' ) , int ( value ) ) ) return bool ( mute )
4393	def adsSyncWriteReqEx ( port , address , index_group , index_offset , value , plc_data_type ) : sync_write_request = _adsDLL . AdsSyncWriteReqEx ams_address_pointer = ctypes . pointer ( address . amsAddrStruct ( ) ) index_group_c = ctypes . c_ulong ( index_group ) index_offset_c = ctypes . c_ulong ( index_offset ) if plc_data_type == PLCTYPE_STRING : data = ctypes . c_char_p ( value . encode ( "utf-8" ) ) data_pointer = data data_length = len ( data_pointer . value ) + 1 else : if type ( plc_data_type ) . __name__ == "PyCArrayType" : data = plc_data_type ( * value ) else : data = plc_data_type ( value ) data_pointer = ctypes . pointer ( data ) data_length = ctypes . sizeof ( data ) error_code = sync_write_request ( port , ams_address_pointer , index_group_c , index_offset_c , data_length , data_pointer , ) if error_code : raise ADSError ( error_code )
8477	def run ( self ) : self . checkProperties ( ) self . debug ( "[*] Iniciando escaneo de AtomShields con las siguientes propiedades. . . " ) self . showScanProperties ( ) self . loadConfig ( ) init_ts = datetime . now ( ) cwd = os . getcwd ( ) os . chdir ( self . path ) issues = self . executeCheckers ( ) os . chdir ( cwd ) end_ts = datetime . now ( ) duration = '{}' . format ( end_ts - init_ts ) for plugin in issues . keys ( ) : value = issues [ plugin ] if isinstance ( value , list ) : map ( self . saveIssue , value ) else : self . saveIssue ( value ) print "" self . executeReports ( ) self . debug ( "" ) self . debug ( "Duration: {t}" . format ( t = duration ) ) self . showSummary ( ) return self . issues
8427	def brewer_pal ( type = 'seq' , palette = 1 ) : def full_type_name ( text ) : abbrevs = { 'seq' : 'Sequential' , 'qual' : 'Qualitative' , 'div' : 'Diverging' } text = abbrevs . get ( text , text ) return text . title ( ) def number_to_palette_name ( ctype , n ) : n -= 1 palettes = sorted ( colorbrewer . COLOR_MAPS [ ctype ] . keys ( ) ) if n < len ( palettes ) : return palettes [ n ] raise ValueError ( "There are only '{}' palettes of type {}. " "You requested palette no. {}" . format ( len ( palettes ) , ctype , n + 1 ) ) def max_palette_colors ( type , palette_name ) : if type == 'Sequential' : return 9 elif type == 'Diverging' : return 11 else : qlimit = { 'Accent' : 8 , 'Dark2' : 8 , 'Paired' : 12 , 'Pastel1' : 9 , 'Pastel2' : 8 , 'Set1' : 9 , 'Set2' : 8 , 'Set3' : 12 } return qlimit [ palette_name ] type = full_type_name ( type ) if isinstance ( palette , int ) : palette_name = number_to_palette_name ( type , palette ) else : palette_name = palette nmax = max_palette_colors ( type , palette_name ) def _brewer_pal ( n ) : _n = n if n <= nmax else nmax try : bmap = colorbrewer . get_map ( palette_name , type , _n ) except ValueError as err : if 0 <= _n < 3 : bmap = colorbrewer . get_map ( palette_name , type , 3 ) else : raise err hex_colors = bmap . hex_colors [ : n ] if n > nmax : msg = ( "Warning message:" "Brewer palette {} has a maximum of {} colors" "Returning the palette you asked for with" "that many colors" . format ( palette_name , nmax ) ) warnings . warn ( msg ) hex_colors = hex_colors + [ None ] * ( n - nmax ) return hex_colors return _brewer_pal
7789	def get_item ( self , address , state = 'fresh' ) : self . _lock . acquire ( ) try : item = self . _items . get ( address ) if not item : return None self . update_item ( item ) if _state_values [ state ] >= item . state_value : return item return None finally : self . _lock . release ( )
2366	def walk ( self , * types ) : requested = types if len ( types ) > 0 else [ SuiteFile , ResourceFile , SuiteFolder , Testcase , Keyword ] for thing in self . robot_files : if thing . __class__ in requested : yield thing if isinstance ( thing , SuiteFolder ) : for child in thing . walk ( ) : if child . __class__ in requested : yield child else : for child in thing . walk ( * types ) : yield child
5100	def _dict2dict ( adj_dict ) : item = adj_dict . popitem ( ) adj_dict [ item [ 0 ] ] = item [ 1 ] if not isinstance ( item [ 1 ] , dict ) : new_dict = { } for key , value in adj_dict . items ( ) : new_dict [ key ] = { v : { } for v in value } adj_dict = new_dict return adj_dict
4340	def remix ( self , remix_dictionary = None , num_output_channels = None ) : if not ( isinstance ( remix_dictionary , dict ) or remix_dictionary is None ) : raise ValueError ( "remix_dictionary must be a dictionary or None." ) if remix_dictionary is not None : if not all ( [ isinstance ( i , int ) and i > 0 for i in remix_dictionary . keys ( ) ] ) : raise ValueError ( "remix dictionary must have positive integer keys." ) if not all ( [ isinstance ( v , list ) for v in remix_dictionary . values ( ) ] ) : raise ValueError ( "remix dictionary values must be lists." ) for v_list in remix_dictionary . values ( ) : if not all ( [ isinstance ( v , int ) and v > 0 for v in v_list ] ) : raise ValueError ( "elements of remix dictionary values must " "be positive integers" ) if not ( ( isinstance ( num_output_channels , int ) and num_output_channels > 0 ) or num_output_channels is None ) : raise ValueError ( "num_output_channels must be a positive integer or None." ) effect_args = [ 'remix' ] if remix_dictionary is None : effect_args . append ( '-' ) else : if num_output_channels is None : num_output_channels = max ( remix_dictionary . keys ( ) ) for channel in range ( 1 , num_output_channels + 1 ) : if channel in remix_dictionary . keys ( ) : out_channel = ',' . join ( [ str ( i ) for i in remix_dictionary [ channel ] ] ) else : out_channel = '0' effect_args . append ( out_channel ) self . effects . extend ( effect_args ) self . effects_log . append ( 'remix' ) return self
11959	def _check_nm ( nm , notation ) : _NM_CHECK_FUNCT = { NM_DOT : _dot_to_dec , NM_HEX : _hex_to_dec , NM_BIN : _bin_to_dec , NM_OCT : _oct_to_dec , NM_DEC : _dec_to_dec_long } try : dec = _NM_CHECK_FUNCT [ notation ] ( nm , check = True ) except ValueError : return False if dec in _NETMASKS_VALUES : return True return False
6855	def ismounted ( device ) : with settings ( hide ( 'running' , 'stdout' ) ) : res = run_as_root ( 'mount' ) for line in res . splitlines ( ) : fields = line . split ( ) if fields [ 0 ] == device : return True with settings ( hide ( 'running' , 'stdout' ) ) : res = run_as_root ( 'swapon -s' ) for line in res . splitlines ( ) : fields = line . split ( ) if fields [ 0 ] == device : return True return False
2838	def setup ( self , pin , value ) : self . _validate_pin ( pin ) if value == GPIO . IN : self . iodir [ int ( pin / 8 ) ] |= 1 << ( int ( pin % 8 ) ) elif value == GPIO . OUT : self . iodir [ int ( pin / 8 ) ] &= ~ ( 1 << ( int ( pin % 8 ) ) ) else : raise ValueError ( 'Unexpected value. Must be GPIO.IN or GPIO.OUT.' ) self . write_iodir ( )
1266	def make_game ( ) : return ascii_art . ascii_art_to_game ( GAME_ART , what_lies_beneath = ' ' , sprites = dict ( [ ( 'P' , PlayerSprite ) ] + [ ( c , UpwardLaserBoltSprite ) for c in UPWARD_BOLT_CHARS ] + [ ( c , DownwardLaserBoltSprite ) for c in DOWNWARD_BOLT_CHARS ] ) , drapes = dict ( X = MarauderDrape , B = BunkerDrape ) , update_schedule = [ 'P' , 'B' , 'X' ] + list ( _ALL_BOLT_CHARS ) )
3962	def prep_for_start_local_env ( pull_repos ) : if pull_repos : update_managed_repos ( force = True ) assembled_spec = spec_assembler . get_assembled_specs ( ) if not assembled_spec [ constants . CONFIG_BUNDLES_KEY ] : raise RuntimeError ( 'No bundles are activated. Use `dusty bundles` to activate bundles before running `dusty up`.' ) virtualbox . initialize_docker_vm ( )
7426	def refmap_stats ( data , sample ) : mapf = os . path . join ( data . dirs . refmapping , sample . name + "-mapped-sorted.bam" ) umapf = os . path . join ( data . dirs . refmapping , sample . name + "-unmapped.bam" ) cmd1 = [ ipyrad . bins . samtools , "flagstat" , umapf ] proc1 = sps . Popen ( cmd1 , stderr = sps . STDOUT , stdout = sps . PIPE ) result1 = proc1 . communicate ( ) [ 0 ] cmd2 = [ ipyrad . bins . samtools , "flagstat" , mapf ] proc2 = sps . Popen ( cmd2 , stderr = sps . STDOUT , stdout = sps . PIPE ) result2 = proc2 . communicate ( ) [ 0 ] if "pair" in data . paramsdict [ "datatype" ] : sample . stats [ "refseq_unmapped_reads" ] = int ( result1 . split ( ) [ 0 ] ) / 2 sample . stats [ "refseq_mapped_reads" ] = int ( result2 . split ( ) [ 0 ] ) / 2 else : sample . stats [ "refseq_unmapped_reads" ] = int ( result1 . split ( ) [ 0 ] ) sample . stats [ "refseq_mapped_reads" ] = int ( result2 . split ( ) [ 0 ] ) sample_cleanup ( data , sample )
646	def generateVectors ( numVectors = 100 , length = 500 , activity = 50 ) : vectors = [ ] coinc = numpy . zeros ( length , dtype = 'int32' ) indexList = range ( length ) for i in xrange ( numVectors ) : coinc [ : ] = 0 coinc [ random . sample ( indexList , activity ) ] = 1 vectors . append ( coinc . copy ( ) ) return vectors
10917	def get_residuals_update_tile ( st , padded_tile ) : inner_tile = st . ishape . intersection ( [ st . ishape , padded_tile ] ) return inner_tile . translate ( - st . pad )
1665	def CheckRedundantVirtual ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] virtual = Match ( r'^(.*)(\bvirtual\b)(.*)$' , line ) if not virtual : return if ( Search ( r'\b(public|protected|private)\s+$' , virtual . group ( 1 ) ) or Match ( r'^\s+(public|protected|private)\b' , virtual . group ( 3 ) ) ) : return if Match ( r'^.*[^:]:[^:].*$' , line ) : return end_col = - 1 end_line = - 1 start_col = len ( virtual . group ( 2 ) ) for start_line in xrange ( linenum , min ( linenum + 3 , clean_lines . NumLines ( ) ) ) : line = clean_lines . elided [ start_line ] [ start_col : ] parameter_list = Match ( r'^([^(]*)\(' , line ) if parameter_list : ( _ , end_line , end_col ) = CloseExpression ( clean_lines , start_line , start_col + len ( parameter_list . group ( 1 ) ) ) break start_col = 0 if end_col < 0 : return for i in xrange ( end_line , min ( end_line + 3 , clean_lines . NumLines ( ) ) ) : line = clean_lines . elided [ i ] [ end_col : ] match = Search ( r'\b(override|final)\b' , line ) if match : error ( filename , linenum , 'readability/inheritance' , 4 , ( '"virtual" is redundant since function is ' 'already declared as "%s"' % match . group ( 1 ) ) ) end_col = 0 if Search ( r'[^\w]\s*$' , line ) : break
11897	def _create_index_files ( root_dir , force_no_processing = False ) : created_files = [ ] for here , dirs , files in os . walk ( root_dir ) : print ( 'Processing %s' % here ) dirs = sorted ( dirs ) image_files = [ f for f in files if re . match ( IMAGE_FILE_REGEX , f ) ] image_files = sorted ( image_files ) created_files . append ( _create_index_file ( root_dir , here , image_files , dirs , force_no_processing ) ) return created_files
3190	def update ( self , list_id , segment_id , data ) : self . list_id = list_id self . segment_id = segment_id if 'name' not in data : raise KeyError ( 'The list segment must have a name' ) return self . _mc_client . _patch ( url = self . _build_path ( list_id , 'segments' , segment_id ) , data = data )
11078	def start_timer ( self , duration , func , * args ) : t = threading . Timer ( duration , self . _timer_callback , ( func , args ) ) self . _timer_callbacks [ func ] = t t . start ( ) self . log . info ( "Scheduled call to %s in %ds" , func . __name__ , duration )
5752	def strToBool ( val ) : if isinstance ( val , str ) : val = val . lower ( ) return val in [ 'true' , 'on' , 'yes' , True ]
4824	def get_course_modes ( self , course_id ) : details = self . get_course_details ( course_id ) modes = details . get ( 'course_modes' , [ ] ) return self . _sort_course_modes ( [ mode for mode in modes if mode [ 'slug' ] not in EXCLUDED_COURSE_MODES ] )
9070	def value ( self ) : r from numpy_sugar . linalg import ddot , sum2diag if self . _cache [ "value" ] is not None : return self . _cache [ "value" ] scale = exp ( self . logscale ) delta = 1 / ( 1 + exp ( - self . logitdelta ) ) v0 = scale * ( 1 - delta ) v1 = scale * delta mu = self . eta / self . tau n = len ( mu ) if self . _QS is None : K = zeros ( ( n , n ) ) else : Q0 = self . _QS [ 0 ] [ 0 ] S0 = self . _QS [ 1 ] K = dot ( ddot ( Q0 , S0 ) , Q0 . T ) A = sum2diag ( sum2diag ( v0 * K , v1 ) , 1 / self . tau ) m = mu - self . mean ( ) v = - n * log ( 2 * pi ) v -= slogdet ( A ) [ 1 ] v -= dot ( m , solve ( A , m ) ) self . _cache [ "value" ] = v / 2 return self . _cache [ "value" ]
7978	def _post_auth ( self ) : ClientStream . _post_auth ( self ) if not self . initiator : self . unset_iq_get_handler ( "query" , "jabber:iq:auth" ) self . unset_iq_set_handler ( "query" , "jabber:iq:auth" )
418	def delete_datasets ( self , ** kwargs ) : self . _fill_project_info ( kwargs ) self . db . Dataset . delete_many ( kwargs ) logging . info ( "[Database] Delete Dataset SUCCESS" )
10696	def hsv_to_rgb ( hsv ) : h , s , v = hsv c = v * s h /= 60 x = c * ( 1 - abs ( ( h % 2 ) - 1 ) ) m = v - c if h < 1 : res = ( c , x , 0 ) elif h < 2 : res = ( x , c , 0 ) elif h < 3 : res = ( 0 , c , x ) elif h < 4 : res = ( 0 , x , c ) elif h < 5 : res = ( x , 0 , c ) elif h < 6 : res = ( c , 0 , x ) else : raise ColorException ( "Unable to convert from HSV to RGB" ) r , g , b = res return round ( ( r + m ) * 255 , 3 ) , round ( ( g + m ) * 255 , 3 ) , round ( ( b + m ) * 255 , 3 )
12132	def linspace ( self , start , stop , n ) : if n == 1 : return [ start ] L = [ 0.0 ] * n nm1 = n - 1 nm1inv = 1.0 / nm1 for i in range ( n ) : L [ i ] = nm1inv * ( start * ( nm1 - i ) + stop * i ) return L
4090	def _process_event ( self , key , mask ) : self . _logger . debug ( 'Processing event with key {} and mask {}' . format ( key , mask ) ) fileobj , ( reader , writer ) = key . fileobj , key . data if mask & selectors . EVENT_READ and reader is not None : if reader . _cancelled : self . remove_reader ( fileobj ) else : self . _logger . debug ( 'Invoking reader callback: {}' . format ( reader ) ) reader . _run ( ) if mask & selectors . EVENT_WRITE and writer is not None : if writer . _cancelled : self . remove_writer ( fileobj ) else : self . _logger . debug ( 'Invoking writer callback: {}' . format ( writer ) ) writer . _run ( )
11516	def search_item_by_name_and_folder_name ( self , name , folder_name , token = None ) : parameters = dict ( ) parameters [ 'name' ] = name parameters [ 'folderName' ] = folder_name if token : parameters [ 'token' ] = token response = self . request ( 'midas.item.searchbynameandfoldername' , parameters ) return response [ 'items' ]
2582	def load_checkpoints ( self , checkpointDirs ) : self . memo_lookup_table = None if not checkpointDirs : return { } if type ( checkpointDirs ) is not list : raise BadCheckpoint ( "checkpointDirs expects a list of checkpoints" ) return self . _load_checkpoints ( checkpointDirs )
4773	def contains_only ( self , * items ) : if len ( items ) == 0 : raise ValueError ( 'one or more args must be given' ) else : extra = [ ] for i in self . val : if i not in items : extra . append ( i ) if extra : self . _err ( 'Expected <%s> to contain only %s, but did contain %s.' % ( self . val , self . _fmt_items ( items ) , self . _fmt_items ( extra ) ) ) missing = [ ] for i in items : if i not in self . val : missing . append ( i ) if missing : self . _err ( 'Expected <%s> to contain only %s, but did not contain %s.' % ( self . val , self . _fmt_items ( items ) , self . _fmt_items ( missing ) ) ) return self
11454	def from_source ( cls , source ) : bibrecs = BibRecordPackage ( source ) bibrecs . parse ( ) for bibrec in bibrecs . get_records ( ) : yield cls ( bibrec )
13154	def cursor ( func ) : @ wraps ( func ) def wrapper ( cls , * args , ** kwargs ) : with ( yield from cls . get_cursor ( ) ) as c : return ( yield from func ( cls , c , * args , ** kwargs ) ) return wrapper
3800	def Bahadori_gas ( T , MW ) : r A = [ 4.3931323468E-1 , - 3.88001122207E-2 , 9.28616040136E-4 , - 6.57828995724E-6 ] B = [ - 2.9624238519E-3 , 2.67956145820E-4 , - 6.40171884139E-6 , 4.48579040207E-8 ] C = [ 7.54249790107E-6 , - 6.46636219509E-7 , 1.5124510261E-8 , - 1.0376480449E-10 ] D = [ - 6.0988433456E-9 , 5.20752132076E-10 , - 1.19425545729E-11 , 8.0136464085E-14 ] X , Y = T , MW a = A [ 0 ] + B [ 0 ] * X + C [ 0 ] * X ** 2 + D [ 0 ] * X ** 3 b = A [ 1 ] + B [ 1 ] * X + C [ 1 ] * X ** 2 + D [ 1 ] * X ** 3 c = A [ 2 ] + B [ 2 ] * X + C [ 2 ] * X ** 2 + D [ 2 ] * X ** 3 d = A [ 3 ] + B [ 3 ] * X + C [ 3 ] * X ** 2 + D [ 3 ] * X ** 3 return a + b * Y + c * Y ** 2 + d * Y ** 3
3719	def conductivity ( CASRN = None , AvailableMethods = False , Method = None , full_info = True ) : r def list_methods ( ) : methods = [ ] if CASRN in Lange_cond_pure . index : methods . append ( LANGE_COND ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == LANGE_COND : kappa = float ( Lange_cond_pure . at [ CASRN , 'Conductivity' ] ) if full_info : T = float ( Lange_cond_pure . at [ CASRN , 'T' ] ) elif Method == NONE : kappa , T = None , None else : raise Exception ( 'Failure in in function' ) if full_info : return kappa , T else : return kappa
4911	def ensure_data_exists ( self , request , data , error_message = None ) : if not data : error_message = ( error_message or "Unable to fetch API response from endpoint '{}'." . format ( request . get_full_path ( ) ) ) LOGGER . error ( error_message ) raise NotFound ( error_message )
1550	def _get_spout ( self ) : spout = topology_pb2 . Spout ( ) spout . comp . CopyFrom ( self . _get_base_component ( ) ) self . _add_out_streams ( spout ) return spout
11053	def _issue_cert ( self , domain ) : def errback ( failure ) : failure . trap ( txacme_ServerError ) acme_error = failure . value . message if acme_error . code in [ 'rateLimited' , 'serverInternal' , 'connection' , 'unknownHost' ] : self . log . error ( 'Error ({code}) issuing certificate for "{domain}": ' '{detail}' , code = acme_error . code , domain = domain , detail = acme_error . detail ) else : return failure d = self . txacme_service . issue_cert ( domain ) return d . addErrback ( errback )
4151	def power ( self ) : r if self . scale_by_freq == False : return sum ( self . psd ) * len ( self . psd ) else : return sum ( self . psd ) * self . df / ( 2. * numpy . pi )
5154	def get_copy ( dict_ , key , default = None ) : value = dict_ . get ( key , default ) if value : return deepcopy ( value ) return value
9434	def load_savefile ( input_file , layers = 0 , verbose = False , lazy = False ) : global VERBOSE old_verbose = VERBOSE VERBOSE = verbose __TRACE__ ( '[+] attempting to load {:s}' , ( input_file . name , ) ) header = _load_savefile_header ( input_file ) if __validate_header__ ( header ) : __TRACE__ ( '[+] found valid header' ) if lazy : packets = _generate_packets ( input_file , header , layers ) __TRACE__ ( '[+] created packet generator' ) else : packets = _load_packets ( input_file , header , layers ) __TRACE__ ( '[+] loaded {:d} packets' , ( len ( packets ) , ) ) sfile = pcap_savefile ( header , packets ) __TRACE__ ( '[+] finished loading savefile.' ) else : __TRACE__ ( '[!] invalid savefile' ) sfile = None VERBOSE = old_verbose return sfile
11336	def get_records ( self , url ) : page = urllib2 . urlopen ( url ) pages = [ BeautifulSoup ( page ) ] numpag = pages [ 0 ] . body . findAll ( 'span' , attrs = { 'class' : 'number-of-pages' } ) if len ( numpag ) > 0 : if re . search ( '^\d+$' , numpag [ 0 ] . string ) : for i in range ( int ( numpag [ 0 ] . string ) - 1 ) : page = urllib2 . urlopen ( '%s/page/%i' % ( url , i + 2 ) ) pages . append ( BeautifulSoup ( page ) ) else : print ( "number of pages %s not an integer" % ( numpag [ 0 ] . string ) ) impl = getDOMImplementation ( ) doc = impl . createDocument ( None , "collection" , None ) links = [ ] for page in pages : links += page . body . findAll ( 'p' , attrs = { 'class' : 'title' } ) links += page . body . findAll ( 'h3' , attrs = { 'class' : 'title' } ) for link in links : record = self . _get_record ( link ) doc . firstChild . appendChild ( record ) return doc . toprettyxml ( )
7528	def aligned_indel_filter ( clust , max_internal_indels ) : lclust = clust . split ( ) try : seq1 = [ i . split ( "nnnn" ) [ 0 ] for i in lclust [ 1 : : 2 ] ] seq2 = [ i . split ( "nnnn" ) [ 1 ] for i in lclust [ 1 : : 2 ] ] intindels1 = [ i . rstrip ( "-" ) . lstrip ( "-" ) . count ( "-" ) for i in seq1 ] intindels2 = [ i . rstrip ( "-" ) . lstrip ( "-" ) . count ( "-" ) for i in seq2 ] intindels = intindels1 + intindels2 if max ( intindels ) > max_internal_indels : return 1 except IndexError : seq1 = lclust [ 1 : : 2 ] intindels = [ i . rstrip ( "-" ) . lstrip ( "-" ) . count ( "-" ) for i in seq1 ] if max ( intindels ) > max_internal_indels : return 1 return 0
9108	def cleanup ( self ) : try : remove ( join ( self . fs_path , u'message' ) ) remove ( join ( self . fs_path , 'dirty.zip.pgp' ) ) except OSError : pass shutil . rmtree ( join ( self . fs_path , u'clean' ) , ignore_errors = True ) shutil . rmtree ( join ( self . fs_path , u'attach' ) , ignore_errors = True )
8365	def create_rcontext ( self , size , frame ) : if self . format == 'pdf' : surface = cairo . PDFSurface ( self . _output_file ( frame ) , * size ) elif self . format in ( 'ps' , 'eps' ) : surface = cairo . PSSurface ( self . _output_file ( frame ) , * size ) elif self . format == 'svg' : surface = cairo . SVGSurface ( self . _output_file ( frame ) , * size ) elif self . format == 'surface' : surface = self . target else : surface = cairo . ImageSurface ( cairo . FORMAT_ARGB32 , * size ) return cairo . Context ( surface )
4979	def post ( self , request ) : enterprise_uuid = request . POST . get ( 'enterprise_customer_uuid' ) success_url = request . POST . get ( 'redirect_url' ) failure_url = request . POST . get ( 'failure_url' ) course_id = request . POST . get ( 'course_id' , '' ) program_uuid = request . POST . get ( 'program_uuid' , '' ) enterprise_customer = get_enterprise_customer_or_404 ( enterprise_uuid ) context_data = get_global_context ( request , enterprise_customer ) if not ( enterprise_uuid and success_url and failure_url ) : error_code = 'ENTGDS005' log_message = ( 'Error: one or more of the following values was falsy: ' 'enterprise_uuid: {enterprise_uuid}, ' 'success_url: {success_url}, ' 'failure_url: {failure_url} for course_id {course_id}. ' 'The following error code was reported to the user {userid}: {error_code}' . format ( userid = request . user . id , enterprise_uuid = enterprise_uuid , success_url = success_url , failure_url = failure_url , error_code = error_code , course_id = course_id , ) ) return render_page_with_error_code_message ( request , context_data , error_code , log_message ) if not self . course_or_program_exist ( course_id , program_uuid ) : error_code = 'ENTGDS006' log_message = ( 'Neither the course with course_id: {course_id} ' 'or program with {program_uuid} exist for ' 'enterprise customer {enterprise_uuid}' 'Error code {error_code} presented to user {userid}' . format ( course_id = course_id , program_uuid = program_uuid , error_code = error_code , userid = request . user . id , enterprise_uuid = enterprise_uuid , ) ) return render_page_with_error_code_message ( request , context_data , error_code , log_message ) consent_record = get_data_sharing_consent ( request . user . username , enterprise_uuid , program_uuid = program_uuid , course_id = course_id ) if consent_record is None : error_code = 'ENTGDS007' log_message = ( 'The was a problem with the consent record of user {userid} with ' 'enterprise_uuid {enterprise_uuid}. consent_record has a value ' 'of {consent_record} and a ' 'value for course_id {course_id}. ' 'Error code {error_code} presented to user' . format ( userid = request . user . id , enterprise_uuid = enterprise_uuid , consent_record = consent_record , error_code = error_code , course_id = course_id , ) ) return render_page_with_error_code_message ( request , context_data , error_code , log_message ) defer_creation = request . POST . get ( 'defer_creation' ) consent_provided = bool ( request . POST . get ( 'data_sharing_consent' , False ) ) if defer_creation is None and consent_record . consent_required ( ) : if course_id : enterprise_customer_user , __ = EnterpriseCustomerUser . objects . get_or_create ( enterprise_customer = consent_record . enterprise_customer , user_id = request . user . id ) enterprise_customer_user . update_session ( request ) __ , created = EnterpriseCourseEnrollment . objects . get_or_create ( enterprise_customer_user = enterprise_customer_user , course_id = course_id , ) if created : track_enrollment ( 'data-consent-page-enrollment' , request . user . id , course_id , request . path ) consent_record . granted = consent_provided consent_record . save ( ) return redirect ( success_url if consent_provided else failure_url )
10045	def create ( cls , object_type = None , object_uuid = None , ** kwargs ) : assert 'pid_value' in kwargs kwargs . setdefault ( 'status' , cls . default_status ) return super ( DepositProvider , cls ) . create ( object_type = object_type , object_uuid = object_uuid , ** kwargs )
5259	def _adjust_delay ( self , slot , response ) : if response . status in self . retry_http_codes : new_delay = max ( slot . delay , 1 ) * 4 new_delay = max ( new_delay , self . mindelay ) new_delay = min ( new_delay , self . maxdelay ) slot . delay = new_delay self . stats . inc_value ( 'delay_count' ) elif response . status == 200 : new_delay = max ( slot . delay / 2 , self . mindelay ) if new_delay < 0.01 : new_delay = 0 slot . delay = new_delay
10630	def clone ( self ) : result = copy . copy ( self ) result . _compound_mfrs = copy . deepcopy ( self . _compound_mfrs ) return result
2261	def dict_hist ( item_list , weight_list = None , ordered = False , labels = None ) : if labels is None : hist_ = defaultdict ( lambda : 0 ) else : hist_ = { k : 0 for k in labels } if weight_list is None : weight_list = it . repeat ( 1 ) for item , weight in zip ( item_list , weight_list ) : hist_ [ item ] += weight if ordered : getval = op . itemgetter ( 1 ) hist = OrderedDict ( [ ( key , value ) for ( key , value ) in sorted ( hist_ . items ( ) , key = getval ) ] ) else : hist = dict ( hist_ ) return hist
8281	def _linepoint ( self , t , x0 , y0 , x1 , y1 ) : out_x = x0 + t * ( x1 - x0 ) out_y = y0 + t * ( y1 - y0 ) return ( out_x , out_y )
4921	def list ( self , request ) : catalog_api = CourseCatalogApiClient ( request . user ) catalogs = catalog_api . get_paginated_catalogs ( request . GET ) self . ensure_data_exists ( request , catalogs ) serializer = serializers . ResponsePaginationSerializer ( catalogs ) return get_paginated_response ( serializer . data , request )
2232	def _register_numpy_extensions ( self ) : import numpy as np numpy_floating_types = ( np . float16 , np . float32 , np . float64 ) if hasattr ( np , 'float128' ) : numpy_floating_types = numpy_floating_types + ( np . float128 , ) @ self . add_iterable_check def is_object_ndarray ( data ) : return isinstance ( data , np . ndarray ) and data . dtype . kind == 'O' @ self . register ( np . ndarray ) def hash_numpy_array ( data ) : if data . dtype . kind == 'O' : msg = 'directly hashing ndarrays with dtype=object is unstable' raise TypeError ( msg ) else : header = b'' . join ( _hashable_sequence ( ( len ( data . shape ) , data . shape ) ) ) dtype = b'' . join ( _hashable_sequence ( data . dtype . descr ) ) hashable = header + dtype + data . tobytes ( ) prefix = b'NDARR' return prefix , hashable @ self . register ( ( np . int64 , np . int32 , np . int16 , np . int8 ) + ( np . uint64 , np . uint32 , np . uint16 , np . uint8 ) ) def _hash_numpy_int ( data ) : return _convert_to_hashable ( int ( data ) ) @ self . register ( numpy_floating_types ) def _hash_numpy_float ( data ) : return _convert_to_hashable ( float ( data ) ) @ self . register ( np . random . RandomState ) def _hash_numpy_random_state ( data ) : hashable = b'' . join ( _hashable_sequence ( data . get_state ( ) ) ) prefix = b'RNG' return prefix , hashable
7710	def request_roster ( self , version = None ) : processor = self . stanza_processor request = Iq ( stanza_type = "get" ) request . set_payload ( RosterPayload ( version = version ) ) processor . set_response_handlers ( request , self . _get_success , self . _get_error ) processor . send ( request )
4333	def noisered ( self , profile_path , amount = 0.5 ) : if not os . path . exists ( profile_path ) : raise IOError ( "profile_path {} does not exist." . format ( profile_path ) ) if not is_number ( amount ) or amount < 0 or amount > 1 : raise ValueError ( "amount must be a number between 0 and 1." ) effect_args = [ 'noisered' , profile_path , '{:f}' . format ( amount ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'noisered' ) return self
9997	def del_cells ( self , name ) : if name in self . cells : cells = self . cells [ name ] self . cells . del_item ( name ) self . inherit ( ) self . model . spacegraph . update_subspaces ( self ) elif name in self . dynamic_spaces : cells = self . dynamic_spaces . pop ( name ) self . dynamic_spaces . set_update ( ) else : raise KeyError ( "Cells '%s' does not exist" % name ) NullImpl ( cells )
2302	def orient_undirected_graph ( self , data , graph ) : self . arguments [ '{VERBOSE}' ] = str ( self . verbose ) . upper ( ) self . arguments [ '{SCORE}' ] = self . scores [ self . score ] fe = DataFrame ( nx . adj_matrix ( graph , weight = None ) . todense ( ) ) fg = DataFrame ( 1 - fe . values ) results = self . _run_gies ( data , fixedGaps = fg , verbose = self . verbose ) return nx . relabel_nodes ( nx . DiGraph ( results ) , { idx : i for idx , i in enumerate ( data . columns ) } )
6004	def setup_random_seed ( seed ) : if seed == - 1 : seed = np . random . randint ( 0 , int ( 1e9 ) ) np . random . seed ( seed )
7911	def get ( self , key , local_default = None , required = False ) : if key in self . _settings : return self . _settings [ key ] if local_default is not None : return local_default if key in self . _defs : setting_def = self . _defs [ key ] if setting_def . default is not None : return setting_def . default factory = setting_def . factory if factory is None : return None value = factory ( self ) if setting_def . cache is True : setting_def . default = value return value if required : raise KeyError ( key ) return local_default
7204	def set ( self , ** kwargs ) : for port_name , port_value in kwargs . items ( ) : if hasattr ( port_value , 'value' ) : port_value = port_value . value self . inputs . __setattr__ ( port_name , port_value )
992	def sample ( reader , writer , n , start = None , stop = None , tsCol = None , writeSampleOnly = True ) : rows = list ( reader ) if tsCol is not None : ts = rows [ 0 ] [ tsCol ] inc = rows [ 1 ] [ tsCol ] - ts if start is None : start = 0 if stop is None : stop = len ( rows ) - 1 initialN = stop - start + 1 numDeletes = initialN - n for i in xrange ( numDeletes ) : delIndex = random . randint ( start , stop - i ) del rows [ delIndex ] if writeSampleOnly : rows = rows [ start : start + n ] if tsCol is not None : ts = rows [ 0 ] [ tsCol ] for row in rows : if tsCol is not None : row [ tsCol ] = ts ts += inc writer . appendRecord ( row )
373	def brightness ( x , gamma = 1 , gain = 1 , is_random = False ) : if is_random : gamma = np . random . uniform ( 1 - gamma , 1 + gamma ) x = exposure . adjust_gamma ( x , gamma , gain ) return x
8154	def create ( self , name , overwrite = True ) : self . _name = name . rstrip ( ".db" ) from os import unlink if overwrite : try : unlink ( self . _name + ".db" ) except : pass self . _con = sqlite . connect ( self . _name + ".db" ) self . _cur = self . _con . cursor ( )
2447	def set_pkg_vers ( self , doc , version ) : self . assert_package_exists ( ) if not self . package_vers_set : self . package_vers_set = True doc . package . version = version return True else : raise CardinalityError ( 'Package::Version' )
12386	def split_segments ( text , closing_paren = False ) : buf = StringIO ( ) segments = [ ] combinators = [ ] last_group = False iterator = iter ( text ) last_negation = False for character in iterator : if character in COMBINATORS : if last_negation : buf . write ( constants . OPERATOR_NEGATION ) val = buf . getvalue ( ) reset_stringio ( buf ) if not last_group and not len ( val ) : raise ValueError ( 'Unexpected %s.' % character ) if len ( val ) : segments . append ( parse_segment ( val ) ) combinators . append ( COMBINATORS [ character ] ) elif character == constants . GROUP_BEGIN : if buf . tell ( ) : raise ValueError ( 'Unexpected %s' % character ) seg = split_segments ( iterator , True ) if last_negation : seg = UnarySegmentCombinator ( seg ) segments . append ( seg ) last_group = True continue elif character == constants . GROUP_END : val = buf . getvalue ( ) if not buf . tell ( ) or not closing_paren : raise ValueError ( 'Unexpected %s' % character ) segments . append ( parse_segment ( val ) ) return combine ( segments , combinators ) elif character == constants . OPERATOR_NEGATION and not buf . tell ( ) : last_negation = True continue else : if last_negation : buf . write ( constants . OPERATOR_NEGATION ) if last_group : raise ValueError ( 'Unexpected %s' % character ) buf . write ( character ) last_negation = False last_group = False else : if closing_paren : raise ValueError ( 'Expected %s.' % constants . GROUP_END ) if not last_group : segments . append ( parse_segment ( buf . getvalue ( ) ) ) return combine ( segments , combinators )
11599	def verify ( xml , stream ) : import xmlsec signature_node = xmlsec . tree . find_node ( xml , xmlsec . Node . SIGNATURE ) if signature_node is None : return False ctx = xmlsec . SignatureContext ( ) ctx . register_id ( xml ) for assertion in xml . xpath ( "//*[local-name()='Assertion']" ) : ctx . register_id ( assertion ) key = None for fmt in [ xmlsec . KeyFormat . PEM , xmlsec . KeyFormat . CERT_PEM ] : stream . seek ( 0 ) try : key = xmlsec . Key . from_memory ( stream , fmt ) break except ValueError : pass ctx . key = key try : ctx . verify ( signature_node ) return True except Exception : return False
208	def draw_on_image ( self , image , alpha = 0.75 , cmap = "jet" , resize = "heatmaps" ) : ia . do_assert ( image . ndim == 3 ) ia . do_assert ( image . shape [ 2 ] == 3 ) ia . do_assert ( image . dtype . type == np . uint8 ) ia . do_assert ( 0 - 1e-8 <= alpha <= 1.0 + 1e-8 ) ia . do_assert ( resize in [ "heatmaps" , "image" ] ) if resize == "image" : image = ia . imresize_single_image ( image , self . arr_0to1 . shape [ 0 : 2 ] , interpolation = "cubic" ) heatmaps_drawn = self . draw ( size = image . shape [ 0 : 2 ] if resize == "heatmaps" else None , cmap = cmap ) mix = [ np . clip ( ( 1 - alpha ) * image + alpha * heatmap_i , 0 , 255 ) . astype ( np . uint8 ) for heatmap_i in heatmaps_drawn ] return mix
798	def modelsClearAll ( self ) : self . _logger . info ( 'Deleting all rows from models table %r' , self . modelsTableName ) with ConnectionFactory . get ( ) as conn : query = 'DELETE FROM %s' % ( self . modelsTableName ) conn . cursor . execute ( query )
4719	def tsuite_setup ( trun , declr , enum ) : suite = copy . deepcopy ( TESTSUITE ) suite [ "name" ] = declr . get ( "name" ) if suite [ "name" ] is None : cij . err ( "rnr:tsuite_setup: no testsuite is given" ) return None suite [ "alias" ] = declr . get ( "alias" ) suite [ "ident" ] = "%s_%d" % ( suite [ "name" ] , enum ) suite [ "res_root" ] = os . sep . join ( [ trun [ "conf" ] [ "OUTPUT" ] , suite [ "ident" ] ] ) suite [ "aux_root" ] = os . sep . join ( [ suite [ "res_root" ] , "_aux" ] ) suite [ "evars" ] . update ( copy . deepcopy ( trun [ "evars" ] ) ) suite [ "evars" ] . update ( copy . deepcopy ( declr . get ( "evars" , { } ) ) ) os . makedirs ( suite [ "res_root" ] ) os . makedirs ( suite [ "aux_root" ] ) suite [ "hooks" ] = hooks_setup ( trun , suite , declr . get ( "hooks" ) ) suite [ "hooks_pr_tcase" ] = declr . get ( "hooks_pr_tcase" , [ ] ) suite [ "fname" ] = "%s.suite" % suite [ "name" ] suite [ "fpath" ] = os . sep . join ( [ trun [ "conf" ] [ "TESTSUITES" ] , suite [ "fname" ] ] ) tcase_fpaths = [ ] if os . path . exists ( suite [ "fpath" ] ) : suite_lines = ( l . strip ( ) for l in open ( suite [ "fpath" ] ) . read ( ) . splitlines ( ) ) tcase_fpaths . extend ( ( l for l in suite_lines if len ( l ) > 1 and l [ 0 ] != "#" ) ) else : tcase_fpaths . extend ( declr . get ( "testcases" , [ ] ) ) if len ( set ( tcase_fpaths ) ) != len ( tcase_fpaths ) : cij . err ( "rnr:suite: failed: duplicate tcase in suite not supported" ) return None for tcase_fname in tcase_fpaths : tcase = tcase_setup ( trun , suite , tcase_fname ) if not tcase : cij . err ( "rnr:suite: failed: tcase_setup" ) return None suite [ "testcases" ] . append ( tcase ) return suite
11329	def main ( ) : argparser = ArgumentParser ( ) subparsers = argparser . add_subparsers ( dest = 'selected_subparser' ) all_parser = subparsers . add_parser ( 'all' ) elsevier_parser = subparsers . add_parser ( 'elsevier' ) oxford_parser = subparsers . add_parser ( 'oxford' ) springer_parser = subparsers . add_parser ( 'springer' ) all_parser . add_argument ( '--update-credentials' , action = 'store_true' ) elsevier_parser . add_argument ( '--run-locally' , action = 'store_true' ) elsevier_parser . add_argument ( '--package-name' ) elsevier_parser . add_argument ( '--path' ) elsevier_parser . add_argument ( '--CONSYN' , action = 'store_true' ) elsevier_parser . add_argument ( '--update-credentials' , action = 'store_true' ) elsevier_parser . add_argument ( '--extract-nations' , action = 'store_true' ) oxford_parser . add_argument ( '--dont-empty-ftp' , action = 'store_true' ) oxford_parser . add_argument ( '--package-name' ) oxford_parser . add_argument ( '--path' ) oxford_parser . add_argument ( '--update-credentials' , action = 'store_true' ) oxford_parser . add_argument ( '--extract-nations' , action = 'store_true' ) springer_parser . add_argument ( '--package-name' ) springer_parser . add_argument ( '--path' ) springer_parser . add_argument ( '--update-credentials' , action = 'store_true' ) springer_parser . add_argument ( '--extract-nations' , action = 'store_true' ) settings = Bunch ( vars ( argparser . parse_args ( ) ) ) call_package ( settings )
5134	def generate_pagerank_graph ( num_vertices = 250 , ** kwargs ) : g = minimal_random_graph ( num_vertices , ** kwargs ) r = np . zeros ( num_vertices ) for k , pr in nx . pagerank ( g ) . items ( ) : r [ k ] = pr g = set_types_rank ( g , rank = r , ** kwargs ) return g
13439	def lock_file ( filename ) : lockfile = "%s.lock" % filename if isfile ( lockfile ) : return False else : with open ( lockfile , "w" ) : pass return True
9495	def _parse_document_id ( elm_tree ) : xpath = '//md:content-id/text()' return [ x for x in elm_tree . xpath ( xpath , namespaces = COLLECTION_NSMAP ) ] [ 0 ]
902	def updateAnomalyLikelihoods ( anomalyScores , params , verbosity = 0 ) : if verbosity > 3 : print ( "In updateAnomalyLikelihoods." ) print ( "Number of anomaly scores:" , len ( anomalyScores ) ) print ( "First 20:" , anomalyScores [ 0 : min ( 20 , len ( anomalyScores ) ) ] ) print ( "Params:" , params ) if len ( anomalyScores ) == 0 : raise ValueError ( "Must have at least one anomalyScore" ) if not isValidEstimatorParams ( params ) : raise ValueError ( "'params' is not a valid params structure" ) if "historicalLikelihoods" not in params : params [ "historicalLikelihoods" ] = [ 1.0 ] historicalValues = params [ "movingAverage" ] [ "historicalValues" ] total = params [ "movingAverage" ] [ "total" ] windowSize = params [ "movingAverage" ] [ "windowSize" ] aggRecordList = numpy . zeros ( len ( anomalyScores ) , dtype = float ) likelihoods = numpy . zeros ( len ( anomalyScores ) , dtype = float ) for i , v in enumerate ( anomalyScores ) : newAverage , historicalValues , total = ( MovingAverage . compute ( historicalValues , total , v [ 2 ] , windowSize ) ) aggRecordList [ i ] = newAverage likelihoods [ i ] = tailProbability ( newAverage , params [ "distribution" ] ) likelihoods2 = params [ "historicalLikelihoods" ] + list ( likelihoods ) filteredLikelihoods = _filterLikelihoods ( likelihoods2 ) likelihoods [ : ] = filteredLikelihoods [ - len ( likelihoods ) : ] historicalLikelihoods = likelihoods2 [ - min ( windowSize , len ( likelihoods2 ) ) : ] newParams = { "distribution" : params [ "distribution" ] , "movingAverage" : { "historicalValues" : historicalValues , "total" : total , "windowSize" : windowSize , } , "historicalLikelihoods" : historicalLikelihoods , } assert len ( newParams [ "historicalLikelihoods" ] ) <= windowSize if verbosity > 3 : print ( "Number of likelihoods:" , len ( likelihoods ) ) print ( "First 20 likelihoods:" , likelihoods [ 0 : min ( 20 , len ( likelihoods ) ) ] ) print ( "Leaving updateAnomalyLikelihoods." ) return ( likelihoods , aggRecordList , newParams )
8189	def eigenvector_centrality ( self , normalized = True , reversed = True , rating = { } , start = None , iterations = 100 , tolerance = 0.0001 ) : ec = proximity . eigenvector_centrality ( self , normalized , reversed , rating , start , iterations , tolerance ) for id , w in ec . iteritems ( ) : self [ id ] . _eigenvalue = w return ec
563	def invariant ( self ) : assert isinstance ( self . description , str ) assert isinstance ( self . singleNodeOnly , bool ) assert isinstance ( self . inputs , dict ) assert isinstance ( self . outputs , dict ) assert isinstance ( self . parameters , dict ) assert isinstance ( self . commands , dict ) hasDefaultInput = False for k , v in self . inputs . items ( ) : assert isinstance ( k , str ) assert isinstance ( v , InputSpec ) v . invariant ( ) if v . isDefaultInput : assert not hasDefaultInput hasDefaultInput = True hasDefaultOutput = False for k , v in self . outputs . items ( ) : assert isinstance ( k , str ) assert isinstance ( v , OutputSpec ) v . invariant ( ) if v . isDefaultOutput : assert not hasDefaultOutput hasDefaultOutput = True for k , v in self . parameters . items ( ) : assert isinstance ( k , str ) assert isinstance ( v , ParameterSpec ) v . invariant ( ) for k , v in self . commands . items ( ) : assert isinstance ( k , str ) assert isinstance ( v , CommandSpec ) v . invariant ( )
4251	def country_name_by_name ( self , hostname ) : addr = self . _gethostbyname ( hostname ) return self . country_name_by_addr ( addr )
4156	def arma2psd ( A = None , B = None , rho = 1. , T = 1. , NFFT = 4096 , sides = 'default' , norm = False ) : r if NFFT is None : NFFT = 4096 if A is None and B is None : raise ValueError ( "Either AR or MA model must be provided" ) psd = np . zeros ( NFFT , dtype = complex ) if A is not None : ip = len ( A ) den = np . zeros ( NFFT , dtype = complex ) den [ 0 ] = 1. + 0j for k in range ( 0 , ip ) : den [ k + 1 ] = A [ k ] denf = fft ( den , NFFT ) if B is not None : iq = len ( B ) num = np . zeros ( NFFT , dtype = complex ) num [ 0 ] = 1. + 0j for k in range ( 0 , iq ) : num [ k + 1 ] = B [ k ] numf = fft ( num , NFFT ) if A is not None and B is not None : psd = rho / T * abs ( numf ) ** 2. / abs ( denf ) ** 2. elif A is not None : psd = rho / T / abs ( denf ) ** 2. elif B is not None : psd = rho / T * abs ( numf ) ** 2. psd = np . real ( psd ) if sides != 'default' : from . import tools assert sides in [ 'centerdc' ] if sides == 'centerdc' : psd = tools . twosided_2_centerdc ( psd ) if norm == True : psd /= max ( psd ) return psd
11898	def _get_image_from_file ( dir_path , image_file ) : if not PIL_ENABLED : return None path = os . path . join ( dir_path , image_file ) img = None try : img = Image . open ( path ) except IOError as exptn : print ( 'Error loading image file %s: %s' % ( path , exptn ) ) return img
6522	def add_issues ( self , issues ) : if not isinstance ( issues , ( list , tuple ) ) : issues = [ issues ] with self . _lock : self . _all_issues . extend ( issues ) self . _cleaned_issues = None
10268	def main ( output ) : from hbp_knowledge import get_graph graph = get_graph ( ) text = to_html ( graph ) print ( text , file = output )
10462	def doesmenuitemexist ( self , window_name , object_name ) : try : menu_handle = self . _get_menu_handle ( window_name , object_name , False ) return 1 except LdtpServerException : return 0
5634	def make_toc ( sections , maxdepth = 0 ) : if not sections : return [ ] outer = min ( n for n , t in sections ) refs = [ ] for ind , sec in sections : if maxdepth and ind - outer + 1 > maxdepth : continue ref = sec . lower ( ) ref = ref . replace ( '`' , '' ) ref = ref . replace ( ' ' , '-' ) ref = ref . replace ( '?' , '' ) refs . append ( " " * ( ind - outer ) + "- [%s](#%s)" % ( sec , ref ) ) return refs
7972	def _remove_timeout_handler ( self , handler ) : if handler not in self . timeout_handlers : return self . timeout_handlers . remove ( handler ) for thread in self . timeout_threads : if thread . method . im_self is handler : thread . stop ( )
5238	def shift_time ( start_time , mins ) -> str : s_time = pd . Timestamp ( start_time ) e_time = s_time + np . sign ( mins ) * pd . Timedelta ( f'00:{abs(mins)}:00' ) return e_time . strftime ( '%H:%M' )
5200	def Select ( self , command , index ) : OutstationApplication . process_point_value ( 'Select' , command , index , None ) return opendnp3 . CommandStatus . SUCCESS
6915	def collection_worker ( task ) : lcfile , outdir , kwargs = task try : fakelcresults = make_fakelc ( lcfile , outdir , ** kwargs ) return fakelcresults except Exception as e : LOGEXCEPTION ( 'could not process %s into a fakelc' % lcfile ) return None
39	def discount ( x , gamma ) : assert x . ndim >= 1 return scipy . signal . lfilter ( [ 1 ] , [ 1 , - gamma ] , x [ : : - 1 ] , axis = 0 ) [ : : - 1 ]
11748	def _bundle_exists ( self , path ) : for attached_bundle in self . _attached_bundles : if path == attached_bundle . path : return True return False
10062	def deposit_links_factory ( pid ) : links = default_links_factory ( pid ) def _url ( name , ** kwargs ) : endpoint = '.{0}_{1}' . format ( current_records_rest . default_endpoint_prefixes [ pid . pid_type ] , name , ) return url_for ( endpoint , pid_value = pid . pid_value , _external = True , ** kwargs ) links [ 'files' ] = _url ( 'files' ) ui_endpoint = current_app . config . get ( 'DEPOSIT_UI_ENDPOINT' ) if ui_endpoint is not None : links [ 'html' ] = ui_endpoint . format ( host = request . host , scheme = request . scheme , pid_value = pid . pid_value , ) deposit_cls = Deposit if 'pid_value' in request . view_args : deposit_cls = request . view_args [ 'pid_value' ] . data [ 1 ] . __class__ for action in extract_actions_from_class ( deposit_cls ) : links [ action ] = _url ( 'actions' , action = action ) return links
11945	def _store ( self , messages , response , * args , ** kwargs ) : contrib_messages = [ ] if self . user . is_authenticated ( ) : if not messages : self . backend . inbox_purge ( self . user ) else : for m in messages : try : self . backend . inbox_store ( [ self . user ] , m ) except MessageTypeNotSupported : contrib_messages . append ( m ) super ( StorageMixin , self ) . _store ( contrib_messages , response , * args , ** kwargs )
2253	def unique ( items , key = None ) : seen = set ( ) if key is None : for item in items : if item not in seen : seen . add ( item ) yield item else : for item in items : norm = key ( item ) if norm not in seen : seen . add ( norm ) yield item
13638	def settings ( path = None , with_path = None ) : if path : Settings . bind ( path , with_path = with_path ) return Settings . _wrapped
12607	def find_unique ( table , sample , unique_fields = None ) : res = search_unique ( table , sample , unique_fields ) if res is not None : return res . eid else : return res
2468	def set_file_license_comment ( self , doc , text ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_license_comment_set : self . file_license_comment_set = True if validations . validate_file_lics_comment ( text ) : self . file ( doc ) . license_comment = str_from_text ( text ) else : raise SPDXValueError ( 'File::LicenseComment' ) else : raise CardinalityError ( 'File::LicenseComment' ) else : raise OrderError ( 'File::LicenseComment' )
3856	def unread_events ( self ) : return [ conv_event for conv_event in self . _events if conv_event . timestamp > self . latest_read_timestamp ]
8003	def get_form ( self , form_type = "form" ) : if self . form : if self . form . type != form_type : raise ValueError ( "Bad form type in the jabber:iq:register element" ) return self . form form = Form ( form_type , instructions = self . instructions ) form . add_field ( "FORM_TYPE" , [ u"jabber:iq:register" ] , "hidden" ) for field in legacy_fields : field_type , field_label = legacy_fields [ field ] value = getattr ( self , field ) if value is None : continue if form_type == "form" : if not value : value = None form . add_field ( name = field , field_type = field_type , label = field_label , value = value , required = True ) else : form . add_field ( name = field , value = value ) return form
9688	def read_bin_boundaries ( self ) : config = [ ] data = { } self . cnxn . xfer ( [ 0x33 ] ) sleep ( 10e-3 ) for i in range ( 30 ) : resp = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] config . append ( resp ) for i in range ( 0 , 14 ) : data [ "Bin Boundary {0}" . format ( i ) ] = self . _16bit_unsigned ( config [ 2 * i ] , config [ 2 * i + 1 ] ) return data
13614	def apply_orientation ( im ) : try : kOrientationEXIFTag = 0x0112 if hasattr ( im , '_getexif' ) : e = im . _getexif ( ) if e is not None : orientation = e [ kOrientationEXIFTag ] f = orientation_funcs [ orientation ] return f ( im ) except : pass return im
45	def compute_geometric_median ( X , eps = 1e-5 ) : y = np . mean ( X , 0 ) while True : D = scipy . spatial . distance . cdist ( X , [ y ] ) nonzeros = ( D != 0 ) [ : , 0 ] Dinv = 1 / D [ nonzeros ] Dinvs = np . sum ( Dinv ) W = Dinv / Dinvs T = np . sum ( W * X [ nonzeros ] , 0 ) num_zeros = len ( X ) - np . sum ( nonzeros ) if num_zeros == 0 : y1 = T elif num_zeros == len ( X ) : return y else : R = ( T - y ) * Dinvs r = np . linalg . norm ( R ) rinv = 0 if r == 0 else num_zeros / r y1 = max ( 0 , 1 - rinv ) * T + min ( 1 , rinv ) * y if scipy . spatial . distance . euclidean ( y , y1 ) < eps : return y1 y = y1
7623	def melody ( ref , est , ** kwargs ) : r namespace = 'pitch_contour' ref = coerce_annotation ( ref , namespace ) est = coerce_annotation ( est , namespace ) ref_times , ref_p = ref . to_event_values ( ) est_times , est_p = est . to_event_values ( ) ref_freq = np . asarray ( [ p [ 'frequency' ] * ( - 1 ) ** ( ~ p [ 'voiced' ] ) for p in ref_p ] ) est_freq = np . asarray ( [ p [ 'frequency' ] * ( - 1 ) ** ( ~ p [ 'voiced' ] ) for p in est_p ] ) return mir_eval . melody . evaluate ( ref_times , ref_freq , est_times , est_freq , ** kwargs )
6569	def signature_matches ( func , args = ( ) , kwargs = { } ) : try : sig = inspect . signature ( func ) sig . bind ( * args , ** kwargs ) except TypeError : return False else : return True
11235	def get ( ) : config = { } try : config = _load_config ( ) except IOError : try : _create_default_config ( ) config = _load_config ( ) except IOError as e : raise ConfigError ( _FILE_CREATION_ERROR . format ( e . args [ 0 ] ) ) except SyntaxError as e : raise ConfigError ( _JSON_SYNTAX_ERROR . format ( e . args [ 0 ] ) ) except Exception : raise ConfigError ( _JSON_SYNTAX_ERROR . format ( 'Yaml syntax error..' ) ) try : _validate ( config ) except KeyError as e : raise ConfigError ( _MANDATORY_KEY_ERROR . format ( e . args [ 0 ] ) ) except SyntaxError as e : raise ConfigError ( _INVALID_KEY_ERROR . format ( e . args [ 0 ] ) ) except ValueError as e : raise ConfigError ( _INVALID_VALUE_ERROR . format ( e . args [ 0 ] ) ) config [ 'projects-path' ] = os . path . expanduser ( config [ 'projects-path' ] ) _complete_config ( config ) return config
3046	def _do_refresh_request ( self , http ) : body = self . _generate_refresh_request_body ( ) headers = self . _generate_refresh_request_headers ( ) logger . info ( 'Refreshing access_token' ) resp , content = transport . request ( http , self . token_uri , method = 'POST' , body = body , headers = headers ) content = _helpers . _from_bytes ( content ) if resp . status == http_client . OK : d = json . loads ( content ) self . token_response = d self . access_token = d [ 'access_token' ] self . refresh_token = d . get ( 'refresh_token' , self . refresh_token ) if 'expires_in' in d : delta = datetime . timedelta ( seconds = int ( d [ 'expires_in' ] ) ) self . token_expiry = delta + _UTCNOW ( ) else : self . token_expiry = None if 'id_token' in d : self . id_token = _extract_id_token ( d [ 'id_token' ] ) self . id_token_jwt = d [ 'id_token' ] else : self . id_token = None self . id_token_jwt = None self . invalid = False if self . store : self . store . locked_put ( self ) else : logger . info ( 'Failed to retrieve access token: %s' , content ) error_msg = 'Invalid response {0}.' . format ( resp . status ) try : d = json . loads ( content ) if 'error' in d : error_msg = d [ 'error' ] if 'error_description' in d : error_msg += ': ' + d [ 'error_description' ] self . invalid = True if self . store is not None : self . store . locked_put ( self ) except ( TypeError , ValueError ) : pass raise HttpAccessTokenRefreshError ( error_msg , status = resp . status )
3894	async def _async_main ( example_coroutine , client , args ) : task = asyncio . ensure_future ( client . connect ( ) ) on_connect = asyncio . Future ( ) client . on_connect . add_observer ( lambda : on_connect . set_result ( None ) ) done , _ = await asyncio . wait ( ( on_connect , task ) , return_when = asyncio . FIRST_COMPLETED ) await asyncio . gather ( * done ) try : await example_coroutine ( client , args ) except asyncio . CancelledError : pass finally : await client . disconnect ( ) await task
11818	def expected_utility ( a , s , U , mdp ) : "The expected utility of doing a in state s, according to the MDP and U." return sum ( [ p * U [ s1 ] for ( p , s1 ) in mdp . T ( s , a ) ] )
7444	def _step2func ( self , samples , force , ipyclient ) : if self . _headers : print ( "\n Step 2: Filtering reads " ) if not self . samples . keys ( ) : raise IPyradWarningExit ( FIRST_RUN_1 ) samples = _get_samples ( self , samples ) if not force : if all ( [ i . stats . state >= 2 for i in samples ] ) : print ( EDITS_EXIST . format ( len ( samples ) ) ) return assemble . rawedit . run2 ( self , samples , force , ipyclient )
6985	def parallel_timebin ( lclist , binsizesec , maxobjects = None , outdir = None , lcformat = 'hat-sql' , lcformatdir = None , timecols = None , magcols = None , errcols = None , minbinelems = 7 , nworkers = NCPUS , maxworkertasks = 1000 ) : if outdir and not os . path . exists ( outdir ) : os . mkdir ( outdir ) if maxobjects is not None : lclist = lclist [ : maxobjects ] tasks = [ ( x , binsizesec , { 'outdir' : outdir , 'lcformat' : lcformat , 'lcformatdir' : lcformatdir , 'timecols' : timecols , 'magcols' : magcols , 'errcols' : errcols , 'minbinelems' : minbinelems } ) for x in lclist ] pool = mp . Pool ( nworkers , maxtasksperchild = maxworkertasks ) results = pool . map ( timebinlc_worker , tasks ) pool . close ( ) pool . join ( ) resdict = { os . path . basename ( x ) : y for ( x , y ) in zip ( lclist , results ) } return resdict
1984	def load_value ( self , key , binary = False ) : with self . load_stream ( key , binary = binary ) as s : return s . read ( )
1601	def chain ( cmd_list ) : command = ' | ' . join ( map ( lambda x : ' ' . join ( x ) , cmd_list ) ) chained_proc = functools . reduce ( pipe , [ None ] + cmd_list ) stdout_builder = proc . async_stdout_builder ( chained_proc ) chained_proc . wait ( ) return { 'command' : command , 'stdout' : stdout_builder . result ( ) }
897	def generateFromNumbers ( self , numbers ) : sequence = [ ] for number in numbers : if number == None : sequence . append ( number ) else : pattern = self . patternMachine . get ( number ) sequence . append ( pattern ) return sequence
7716	def remove_item ( self , jid , callback = None , error_callback = None ) : item = self . roster [ jid ] if jid not in self . roster : raise KeyError ( jid ) item = RosterItem ( jid , subscription = "remove" ) self . _roster_set ( item , callback , error_callback )
9679	def ping ( self ) : b = self . cnxn . xfer ( [ 0xCF ] ) [ 0 ] sleep ( 0.1 ) return True if b == 0xF3 else False
7948	def send_stream_head ( self , stanza_namespace , stream_from , stream_to , stream_id = None , version = u'1.0' , language = None ) : with self . lock : self . _serializer = XMPPSerializer ( stanza_namespace , self . settings [ "extra_ns_prefixes" ] ) head = self . _serializer . emit_head ( stream_from , stream_to , stream_id , version , language ) self . _write ( head . encode ( "utf-8" ) )
11206	def gettz ( name ) : warnings . warn ( "zoneinfo.gettz() will be removed in future versions, " "to use the dateutil-provided zoneinfo files, instantiate a " "ZoneInfoFile object and use ZoneInfoFile.zones.get() " "instead. See the documentation for details." , DeprecationWarning ) if len ( _CLASS_ZONE_INSTANCE ) == 0 : _CLASS_ZONE_INSTANCE . append ( ZoneInfoFile ( getzoneinfofile_stream ( ) ) ) return _CLASS_ZONE_INSTANCE [ 0 ] . zones . get ( name )
9228	def get_all_tags ( self ) : verbose = self . options . verbose gh = self . github user = self . options . user repo = self . options . project if verbose : print ( "Fetching tags..." ) tags = [ ] page = 1 while page > 0 : if verbose > 2 : print ( "." , end = "" ) rc , data = gh . repos [ user ] [ repo ] . tags . get ( page = page , per_page = PER_PAGE_NUMBER ) if rc == 200 : tags . extend ( data ) else : self . raise_GitHubError ( rc , data , gh . getheaders ( ) ) page = NextPage ( gh ) if verbose > 2 : print ( "." ) if len ( tags ) == 0 : if not self . options . quiet : print ( "Warning: Can't find any tags in repo. Make sure, that " "you push tags to remote repo via 'git push --tags'" ) exit ( ) if verbose > 1 : print ( "Found {} tag(s)" . format ( len ( tags ) ) ) return tags
7162	def format_answers ( self , fmt = 'obj' ) : fmts = ( 'obj' , 'array' , 'plain' ) if fmt not in fmts : eprint ( "Error: '{}' not in {}" . format ( fmt , fmts ) ) return def stringify ( val ) : if type ( val ) in ( list , tuple ) : return ', ' . join ( str ( e ) for e in val ) return val if fmt == 'obj' : return json . dumps ( self . answers ) elif fmt == 'array' : answers = [ [ k , v ] for k , v in self . answers . items ( ) ] return json . dumps ( answers ) elif fmt == 'plain' : answers = '\n' . join ( '{}: {}' . format ( k , stringify ( v ) ) for k , v in self . answers . items ( ) ) return answers
988	def createTemporalAnomaly ( recordParams , spatialParams = _SP_PARAMS , temporalParams = _TM_PARAMS , verbosity = _VERBOSITY ) : inputFilePath = recordParams [ "inputFilePath" ] scalarEncoderArgs = recordParams [ "scalarEncoderArgs" ] dateEncoderArgs = recordParams [ "dateEncoderArgs" ] scalarEncoder = ScalarEncoder ( ** scalarEncoderArgs ) dateEncoder = DateEncoder ( ** dateEncoderArgs ) encoder = MultiEncoder ( ) encoder . addEncoder ( scalarEncoderArgs [ "name" ] , scalarEncoder ) encoder . addEncoder ( dateEncoderArgs [ "name" ] , dateEncoder ) network = Network ( ) network . addRegion ( "sensor" , "py.RecordSensor" , json . dumps ( { "verbosity" : verbosity } ) ) sensor = network . regions [ "sensor" ] . getSelf ( ) sensor . encoder = encoder sensor . dataSource = FileRecordStream ( streamID = inputFilePath ) spatialParams [ "inputWidth" ] = sensor . encoder . getWidth ( ) network . addRegion ( "spatialPoolerRegion" , "py.SPRegion" , json . dumps ( spatialParams ) ) network . link ( "sensor" , "spatialPoolerRegion" , "UniformLink" , "" ) network . link ( "sensor" , "spatialPoolerRegion" , "UniformLink" , "" , srcOutput = "resetOut" , destInput = "resetIn" ) network . link ( "spatialPoolerRegion" , "sensor" , "UniformLink" , "" , srcOutput = "spatialTopDownOut" , destInput = "spatialTopDownIn" ) network . link ( "spatialPoolerRegion" , "sensor" , "UniformLink" , "" , srcOutput = "temporalTopDownOut" , destInput = "temporalTopDownIn" ) network . addRegion ( "temporalPoolerRegion" , "py.TMRegion" , json . dumps ( temporalParams ) ) network . link ( "spatialPoolerRegion" , "temporalPoolerRegion" , "UniformLink" , "" ) network . link ( "temporalPoolerRegion" , "spatialPoolerRegion" , "UniformLink" , "" , srcOutput = "topDownOut" , destInput = "topDownIn" ) spatialPoolerRegion = network . regions [ "spatialPoolerRegion" ] spatialPoolerRegion . setParameter ( "learningMode" , True ) spatialPoolerRegion . setParameter ( "anomalyMode" , False ) temporalPoolerRegion = network . regions [ "temporalPoolerRegion" ] temporalPoolerRegion . setParameter ( "topDownMode" , True ) temporalPoolerRegion . setParameter ( "learningMode" , True ) temporalPoolerRegion . setParameter ( "inferenceMode" , True ) temporalPoolerRegion . setParameter ( "anomalyMode" , True ) return network
2931	def pre_parse_and_validate_signavio ( self , bpmn , filename ) : self . _check_for_disconnected_boundary_events_signavio ( bpmn , filename ) self . _fix_call_activities_signavio ( bpmn , filename ) return bpmn
12223	def execute ( self , args , kwargs ) : return self . lookup_explicit ( args , kwargs ) ( * args , ** kwargs )
3181	def delete ( self , batch_webhook_id ) : self . batch_webhook_id = batch_webhook_id return self . _mc_client . _delete ( url = self . _build_path ( batch_webhook_id ) )
9876	def _ordinal_metric ( _v1 , _v2 , i1 , i2 , n_v ) : if i1 > i2 : i1 , i2 = i2 , i1 return ( np . sum ( n_v [ i1 : ( i2 + 1 ) ] ) - ( n_v [ i1 ] + n_v [ i2 ] ) / 2 ) ** 2
7600	def get_popular_tournaments ( self , ** params : keys ) : url = self . api . POPULAR + '/tournament' return self . _get_model ( url , PartialTournament , ** params )
10270	def get_unweighted_upstream_leaves ( graph : BELGraph , key : Optional [ str ] = None ) -> Iterable [ BaseEntity ] : if key is None : key = WEIGHT return filter_nodes ( graph , [ node_is_upstream_leaf , data_missing_key_builder ( key ) ] )
1060	def cmp_to_key ( mycmp ) : class K ( object ) : __slots__ = [ 'obj' ] def __init__ ( self , obj , * args ) : self . obj = obj def __lt__ ( self , other ) : return mycmp ( self . obj , other . obj ) < 0 def __gt__ ( self , other ) : return mycmp ( self . obj , other . obj ) > 0 def __eq__ ( self , other ) : return mycmp ( self . obj , other . obj ) == 0 def __le__ ( self , other ) : return mycmp ( self . obj , other . obj ) <= 0 def __ge__ ( self , other ) : return mycmp ( self . obj , other . obj ) >= 0 def __ne__ ( self , other ) : return mycmp ( self . obj , other . obj ) != 0 def __hash__ ( self ) : raise TypeError ( 'hash not implemented' ) return K
4843	def get_common_course_modes ( self , course_run_ids ) : available_course_modes = None for course_run_id in course_run_ids : course_run = self . get_course_run ( course_run_id ) or { } course_run_modes = { seat . get ( 'type' ) for seat in course_run . get ( 'seats' , [ ] ) } if available_course_modes is None : available_course_modes = course_run_modes else : available_course_modes &= course_run_modes if not available_course_modes : return available_course_modes return available_course_modes
849	def getOutputElementCount ( self , name ) : if name == "resetOut" : print ( "WARNING: getOutputElementCount should not have been called with " "resetOut" ) return 1 elif name == "sequenceIdOut" : print ( "WARNING: getOutputElementCount should not have been called with " "sequenceIdOut" ) return 1 elif name == "dataOut" : if self . encoder is None : raise Exception ( "NuPIC requested output element count for 'dataOut' " "on a RecordSensor node, but the encoder has not " "been set" ) return self . encoder . getWidth ( ) elif name == "sourceOut" : if self . encoder is None : raise Exception ( "NuPIC requested output element count for 'sourceOut' " "on a RecordSensor node, " "but the encoder has not been set" ) return len ( self . encoder . getDescription ( ) ) elif name == "bucketIdxOut" : return 1 elif name == "actValueOut" : return 1 elif name == "categoryOut" : return self . numCategories elif name == 'spatialTopDownOut' or name == 'temporalTopDownOut' : if self . encoder is None : raise Exception ( "NuPIC requested output element count for 'sourceOut' " "on a RecordSensor node, " "but the encoder has not been set" ) return len ( self . encoder . getDescription ( ) ) else : raise Exception ( "Unknown output %s" % name )
344	def train_and_validate_to_end ( self , validate_step_size = 50 ) : while not self . _sess . should_stop ( ) : self . train_on_batch ( ) if self . global_step % validate_step_size == 0 : log_str = 'step: %d, ' % self . global_step for n , m in self . validation_metrics : log_str += '%s: %f, ' % ( n . name , m ) logging . info ( log_str )
6683	def require ( self , path = None , contents = None , source = None , url = None , md5 = None , use_sudo = False , owner = None , group = '' , mode = None , verify_remote = True , temp_dir = '/tmp' ) : func = use_sudo and run_as_root or self . run if path and not ( contents or source or url ) : assert path if not self . is_file ( path ) : func ( 'touch "%(path)s"' % locals ( ) ) elif url : if not path : path = os . path . basename ( urlparse ( url ) . path ) if not self . is_file ( path ) or md5 and self . md5sum ( path ) != md5 : func ( 'wget --progress=dot:mega "%(url)s" -O "%(path)s"' % locals ( ) ) else : if source : assert not contents t = None else : fd , source = mkstemp ( ) t = os . fdopen ( fd , 'w' ) t . write ( contents ) t . close ( ) if verify_remote : digest = hashlib . md5 ( ) f = open ( source , 'rb' ) try : while True : d = f . read ( BLOCKSIZE ) if not d : break digest . update ( d ) finally : f . close ( ) else : digest = None if ( not self . is_file ( path , use_sudo = use_sudo ) or ( verify_remote and self . md5sum ( path , use_sudo = use_sudo ) != digest . hexdigest ( ) ) ) : with self . settings ( hide ( 'running' ) ) : self . put ( local_path = source , remote_path = path , use_sudo = use_sudo , temp_dir = temp_dir ) if t is not None : os . unlink ( source ) if use_sudo and owner is None : owner = 'root' if ( owner and self . get_owner ( path , use_sudo ) != owner ) or ( group and self . get_group ( path , use_sudo ) != group ) : func ( 'chown %(owner)s:%(group)s "%(path)s"' % locals ( ) ) if use_sudo and mode is None : mode = oct ( 0o666 & ~ int ( self . umask ( use_sudo = True ) , base = 8 ) ) if mode and self . get_mode ( path , use_sudo ) != mode : func ( 'chmod %(mode)s "%(path)s"' % locals ( ) )
12825	def handle_extends ( self , text ) : match = self . re_extends . match ( text ) if match : extra_text = self . re_extends . sub ( '' , text , count = 1 ) blocks = self . get_blocks ( extra_text ) path = os . path . join ( self . base_dir , match . group ( 'path' ) ) with open ( path , encoding = 'utf-8' ) as fp : return self . replace_blocks_in_extends ( fp . read ( ) , blocks ) else : return None
13036	def read_openke_translation ( filename , delimiter = '\t' , entity_first = True ) : result = { } with open ( filename , "r" ) as f : _ = next ( f ) for line in f : line_slice = line . rstrip ( ) . split ( delimiter ) if not entity_first : line_slice = list ( reversed ( line_slice ) ) result [ line_slice [ 0 ] ] = line_slice [ 1 ] return result
6170	def filter ( self , x ) : y = signal . sosfilt ( self . sos , x ) return y
13264	def get_configuration ( self , key , default = None ) : if key in self . config : return self . config . get ( key ) else : return default
11021	def _generate_circle ( self ) : total_weight = 0 for node in self . nodes : total_weight += self . weights . get ( node , 1 ) for node in self . nodes : weight = 1 if node in self . weights : weight = self . weights . get ( node ) factor = math . floor ( ( 40 * len ( self . nodes ) * weight ) / total_weight ) for j in range ( 0 , int ( factor ) ) : b_key = bytearray ( self . _hash_digest ( '%s-%s' % ( node , j ) ) ) for i in range ( 0 , 3 ) : key = self . _hash_val ( b_key , lambda x : x + i * 4 ) self . ring [ key ] = node self . _sorted_keys . append ( key ) self . _sorted_keys . sort ( )
607	def findRequirements ( ) : requirementsPath = os . path . join ( REPO_DIR , "requirements.txt" ) requirements = parse_file ( requirementsPath ) if nupicBindingsPrereleaseInstalled ( ) : requirements = [ req for req in requirements if "nupic.bindings" not in req ] return requirements
3770	def mixing_simple ( fracs , props ) : r if not none_and_length_check ( [ fracs , props ] ) : return None result = sum ( frac * prop for frac , prop in zip ( fracs , props ) ) return result
4387	def adsAddRoute ( net_id , ip_address ) : add_route = _adsDLL . AdsAddRoute add_route . restype = ctypes . c_long ip_address_p = ctypes . c_char_p ( ip_address . encode ( "utf-8" ) ) error_code = add_route ( net_id , ip_address_p ) if error_code : raise ADSError ( error_code )
1544	def get_logical_plan ( cluster , env , topology , role ) : instance = tornado . ioloop . IOLoop . instance ( ) try : return instance . run_sync ( lambda : API . get_logical_plan ( cluster , env , topology , role ) ) except Exception : Log . debug ( traceback . format_exc ( ) ) raise
10241	def count_authors_by_annotation ( graph : BELGraph , annotation : str = 'Subgraph' ) -> Mapping [ str , typing . Counter [ str ] ] : authors = group_as_dict ( _iter_authors_by_annotation ( graph , annotation = annotation ) ) return count_defaultdict ( authors )
11190	def show ( dataset_uri ) : try : dataset = dtoolcore . ProtoDataSet . from_uri ( uri = dataset_uri , config_path = CONFIG_PATH ) except dtoolcore . DtoolCoreTypeError : dataset = dtoolcore . DataSet . from_uri ( uri = dataset_uri , config_path = CONFIG_PATH ) readme_content = dataset . get_readme_content ( ) click . secho ( readme_content )
9686	def pm ( self ) : resp = [ ] data = { } self . cnxn . xfer ( [ 0x32 ] ) sleep ( 10e-3 ) for i in range ( 12 ) : r = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] resp . append ( r ) data [ 'PM1' ] = self . _calculate_float ( resp [ 0 : 4 ] ) data [ 'PM2.5' ] = self . _calculate_float ( resp [ 4 : 8 ] ) data [ 'PM10' ] = self . _calculate_float ( resp [ 8 : ] ) sleep ( 0.1 ) return data
3964	def stop_apps_or_services ( app_or_service_names = None , rm_containers = False ) : if app_or_service_names : log_to_client ( "Stopping the following apps or services: {}" . format ( ', ' . join ( app_or_service_names ) ) ) else : log_to_client ( "Stopping all running containers associated with Dusty" ) compose . stop_running_services ( app_or_service_names ) if rm_containers : compose . rm_containers ( app_or_service_names )
4989	def redirect ( self , request , * args , ** kwargs ) : enterprise_customer_uuid , course_run_id , course_key , program_uuid = RouterView . get_path_variables ( ** kwargs ) resource_id = course_key or course_run_id or program_uuid path = re . sub ( '{}|{}' . format ( enterprise_customer_uuid , re . escape ( resource_id ) ) , '{}' , request . path ) kwargs . pop ( 'course_key' , None ) return self . VIEWS [ path ] . as_view ( ) ( request , * args , ** kwargs )
3759	def atom_fractions ( self ) : r things = dict ( ) for zi , atoms in zip ( self . zs , self . atomss ) : for atom , count in atoms . iteritems ( ) : if atom in things : things [ atom ] += zi * count else : things [ atom ] = zi * count tot = sum ( things . values ( ) ) return { atom : value / tot for atom , value in things . iteritems ( ) }
7128	def inv_entry_to_path ( data ) : path_tuple = data [ 2 ] . split ( "#" ) if len ( path_tuple ) > 1 : path_str = "#" . join ( ( path_tuple [ 0 ] , path_tuple [ - 1 ] ) ) else : path_str = data [ 2 ] return path_str
9613	def element ( self , using , value ) : return self . _execute ( Command . FIND_CHILD_ELEMENT , { 'using' : using , 'value' : value } )
308	def plot_prob_profit_trade ( round_trips , ax = None ) : x = np . linspace ( 0 , 1. , 500 ) round_trips [ 'profitable' ] = round_trips . pnl > 0 dist = sp . stats . beta ( round_trips . profitable . sum ( ) , ( ~ round_trips . profitable ) . sum ( ) ) y = dist . pdf ( x ) lower_perc = dist . ppf ( .025 ) upper_perc = dist . ppf ( .975 ) lower_plot = dist . ppf ( .001 ) upper_plot = dist . ppf ( .999 ) if ax is None : ax = plt . subplot ( ) ax . plot ( x , y ) ax . axvline ( lower_perc , color = '0.5' ) ax . axvline ( upper_perc , color = '0.5' ) ax . set_xlabel ( 'Probability of making a profitable decision' ) ax . set_ylabel ( 'Belief' ) ax . set_xlim ( lower_plot , upper_plot ) ax . set_ylim ( ( 0 , y . max ( ) + 1. ) ) return ax
9013	def knitting_pattern_set ( self , values ) : self . _start ( ) pattern_collection = self . _new_pattern_collection ( ) self . _fill_pattern_collection ( pattern_collection , values ) self . _create_pattern_set ( pattern_collection , values ) return self . _pattern_set
4033	def parse ( s ) : if IS_PY3 : r = sre_parse . parse ( s , flags = U ) else : r = sre_parse . parse ( s . decode ( 'utf-8' ) , flags = U ) return list ( r )
4093	def addBorrowers ( self , * borrowers ) : self . _borrowers . extend ( borrowers ) debug . logger & debug . flagCompiler and debug . logger ( 'current MIB borrower(s): %s' % ', ' . join ( [ str ( x ) for x in self . _borrowers ] ) ) return self
12703	def _set_params ( target , param , values , dof ) : if not isinstance ( values , ( list , tuple , np . ndarray ) ) : values = [ values ] * dof assert dof == len ( values ) for s , value in zip ( [ '' , '2' , '3' ] [ : dof ] , values ) : target . setParam ( getattr ( ode , 'Param{}{}' . format ( param , s ) ) , value )
13883	def GetFileLines ( filename , newline = None , encoding = None ) : return GetFileContents ( filename , binary = False , encoding = encoding , newline = newline , ) . split ( '\n' )
869	def clear ( cls , persistent = False ) : if persistent : try : os . unlink ( cls . getPath ( ) ) except OSError , e : if e . errno != errno . ENOENT : _getLogger ( ) . exception ( "Error %s while trying to remove dynamic " "configuration file: %s" , e . errno , cls . getPath ( ) ) raise cls . _path = None
2710	def make_sentence ( sent_text ) : lex = [ ] idx = 0 for word in sent_text : if len ( word ) > 0 : if ( idx > 0 ) and not ( word [ 0 ] in ",.:;!?-\"'" ) : lex . append ( " " ) lex . append ( word ) idx += 1 return "" . join ( lex )
6576	def populate_fields ( api_client , instance , data ) : for key , value in instance . __class__ . _fields . items ( ) : default = getattr ( value , "default" , None ) newval = data . get ( value . field , default ) if isinstance ( value , SyntheticField ) : newval = value . formatter ( api_client , data , newval ) setattr ( instance , key , newval ) continue model_class = getattr ( value , "model" , None ) if newval and model_class : if isinstance ( newval , list ) : newval = model_class . from_json_list ( api_client , newval ) else : newval = model_class . from_json ( api_client , newval ) if newval and value . formatter : newval = value . formatter ( api_client , newval ) setattr ( instance , key , newval )
10894	def filtered_image ( self , im ) : q = np . fft . fftn ( im ) for k , v in self . filters : q [ k ] -= v return np . real ( np . fft . ifftn ( q ) )
7562	def _run_qmc ( self , boot ) : self . _tmp = os . path . join ( self . dirs , ".tmptre" ) cmd = [ ip . bins . qmc , "qrtt=" + self . files . qdump , "otre=" + self . _tmp ] proc = subprocess . Popen ( cmd , stderr = subprocess . STDOUT , stdout = subprocess . PIPE ) res = proc . communicate ( ) if proc . returncode : raise IPyradWarningExit ( res [ 1 ] ) with open ( self . _tmp , 'r' ) as intree : tre = ete3 . Tree ( intree . read ( ) . strip ( ) ) names = tre . get_leaves ( ) for name in names : name . name = self . samples [ int ( name . name ) ] tmptre = tre . write ( format = 9 ) if boot : self . trees . boots = os . path . join ( self . dirs , self . name + ".boots" ) with open ( self . trees . boots , 'a' ) as outboot : outboot . write ( tmptre + "\n" ) else : self . trees . tree = os . path . join ( self . dirs , self . name + ".tree" ) with open ( self . trees . tree , 'w' ) as outtree : outtree . write ( tmptre ) self . _save ( )
4248	def netspeed_by_addr ( self , addr ) : if self . _databaseType == const . NETSPEED_EDITION : return const . NETSPEED_NAMES [ self . id_by_addr ( addr ) ] elif self . _databaseType in ( const . NETSPEED_EDITION_REV1 , const . NETSPEED_EDITION_REV1_V6 ) : ipnum = util . ip2long ( addr ) return self . _get_org ( ipnum ) raise GeoIPError ( 'Invalid database type, expected NetSpeed or NetSpeedCell' )
9536	def enumeration ( * args ) : assert len ( args ) > 0 , 'at least one argument is required' if len ( args ) == 1 : members = args [ 0 ] else : members = args def checker ( value ) : if value not in members : raise ValueError ( value ) return checker
11968	def _dec_to_bin ( ip ) : bits = [ ] while ip : bits . append ( _BYTES_TO_BITS [ ip & 255 ] ) ip >>= 8 bits . reverse ( ) return '' . join ( bits ) or 32 * '0'
4321	def biquad ( self , b , a ) : if not isinstance ( b , list ) : raise ValueError ( 'b must be a list.' ) if not isinstance ( a , list ) : raise ValueError ( 'a must be a list.' ) if len ( b ) != 3 : raise ValueError ( 'b must be a length 3 list.' ) if len ( a ) != 3 : raise ValueError ( 'a must be a length 3 list.' ) if not all ( [ is_number ( b_val ) for b_val in b ] ) : raise ValueError ( 'all elements of b must be numbers.' ) if not all ( [ is_number ( a_val ) for a_val in a ] ) : raise ValueError ( 'all elements of a must be numbers.' ) effect_args = [ 'biquad' , '{:f}' . format ( b [ 0 ] ) , '{:f}' . format ( b [ 1 ] ) , '{:f}' . format ( b [ 2 ] ) , '{:f}' . format ( a [ 0 ] ) , '{:f}' . format ( a [ 1 ] ) , '{:f}' . format ( a [ 2 ] ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'biquad' ) return self
8603	def create_user ( self , user ) : data = self . _create_user_dict ( user = user ) response = self . _perform_request ( url = '/um/users' , method = 'POST' , data = json . dumps ( data ) ) return response
6552	def check ( self , solution ) : return self . func ( * ( solution [ v ] for v in self . variables ) )
8888	def fit ( self , X ) : X = iter2array ( X , dtype = ReactionContainer ) self . _train_signatures = { self . __get_signature ( x ) for x in X } return self
10434	def getcellvalue ( self , window_name , object_name , row_index , column = 0 ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) count = len ( object_handle . AXRows ) if row_index < 0 or row_index > count : raise LdtpServerException ( 'Row index out of range: %d' % row_index ) cell = object_handle . AXRows [ row_index ] count = len ( cell . AXChildren ) if column < 0 or column > count : raise LdtpServerException ( 'Column index out of range: %d' % column ) obj = cell . AXChildren [ column ] if not re . search ( "AXColumn" , obj . AXRole ) : obj = cell . AXChildren [ column ] return obj . AXValue
13483	def rsync_docs ( ) : assert options . paved . docs . rsync_location , "Please specify an rsync location in options.paved.docs.rsync_location." sh ( 'rsync -ravz %s/ %s/' % ( path ( options . paved . docs . path ) / options . paved . docs . build_rel , options . paved . docs . rsync_location ) )
2814	def convert_shape ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting shape ...' ) def target_layer ( x ) : import tensorflow as tf return tf . shape ( x ) lambda_layer = keras . layers . Lambda ( target_layer ) layers [ scope_name ] = lambda_layer ( layers [ inputs [ 0 ] ] )
1910	def run ( self , procs = 1 , timeout = None , should_profile = False ) : assert not self . running , "Manticore is already running." self . _start_run ( ) self . _last_run_stats [ 'time_started' ] = time . time ( ) with self . shutdown_timeout ( timeout ) : self . _start_workers ( procs , profiling = should_profile ) self . _join_workers ( ) self . _finish_run ( profiling = should_profile )
12337	def pip_r ( self , requirements , raise_on_error = True ) : cmd = "pip install -r %s" % requirements return self . wait ( cmd , raise_on_error = raise_on_error )
7329	async def request ( self , method , url , future , headers = None , session = None , encoding = None , ** kwargs ) : await self . setup req_kwargs = await self . headers . prepare_request ( method = method , url = url , headers = headers , proxy = self . proxy , ** kwargs ) if encoding is None : encoding = self . encoding session = session if ( session is not None ) else self . _session logger . debug ( "making request with parameters: %s" % req_kwargs ) async with session . request ( ** req_kwargs ) as response : if response . status < 400 : data = await data_processing . read ( response , self . _loads , encoding = encoding ) future . set_result ( data_processing . PeonyResponse ( data = data , headers = response . headers , url = response . url , request = req_kwargs ) ) else : await exceptions . throw ( response , loads = self . _loads , encoding = encoding , url = url )
12725	def cfms ( self , cfms ) : _set_params ( self . ode_obj , 'CFM' , cfms , self . ADOF + self . LDOF )
8282	def _linelength ( self , x0 , y0 , x1 , y1 ) : a = pow ( abs ( x0 - x1 ) , 2 ) b = pow ( abs ( y0 - y1 ) , 2 ) return sqrt ( a + b )
7533	def concat_multiple_edits ( data , sample ) : if len ( sample . files . edits ) > 1 : cmd1 = [ "cat" ] + [ i [ 0 ] for i in sample . files . edits ] conc1 = os . path . join ( data . dirs . edits , sample . name + "_R1_concatedit.fq.gz" ) with open ( conc1 , 'w' ) as cout1 : proc1 = sps . Popen ( cmd1 , stderr = sps . STDOUT , stdout = cout1 , close_fds = True ) res1 = proc1 . communicate ( ) [ 0 ] if proc1 . returncode : raise IPyradWarningExit ( "error in: %s, %s" , cmd1 , res1 ) conc2 = 0 if os . path . exists ( str ( sample . files . edits [ 0 ] [ 1 ] ) ) : cmd2 = [ "cat" ] + [ i [ 1 ] for i in sample . files . edits ] conc2 = os . path . join ( data . dirs . edits , sample . name + "_R2_concatedit.fq.gz" ) with gzip . open ( conc2 , 'w' ) as cout2 : proc2 = sps . Popen ( cmd2 , stderr = sps . STDOUT , stdout = cout2 , close_fds = True ) res2 = proc2 . communicate ( ) [ 0 ] if proc2 . returncode : raise IPyradWarningExit ( "error in: %s, %s" , cmd2 , res2 ) sample . files . edits = [ ( conc1 , conc2 ) ] return sample . files . edits
8576	def get_request ( self , request_id , status = False ) : if status : response = self . _perform_request ( '/requests/' + request_id + '/status' ) else : response = self . _perform_request ( '/requests/%s' % request_id ) return response
11299	def register ( self , provider_class ) : if not issubclass ( provider_class , BaseProvider ) : raise TypeError ( '%s is not a subclass of BaseProvider' % provider_class . __name__ ) if provider_class in self . _registered_providers : raise AlreadyRegistered ( '%s is already registered' % provider_class . __name__ ) if issubclass ( provider_class , DjangoProvider ) : signals . post_save . connect ( self . invalidate_stored_oembeds , sender = provider_class . _meta . model ) self . _registered_providers . append ( provider_class ) self . invalidate_providers ( )
13101	def main ( ) : config = Config ( ) core = HostSearch ( ) hosts = core . get_hosts ( tags = [ '!nessus' ] , up = True ) hosts = [ host for host in hosts ] host_ips = "," . join ( [ str ( host . address ) for host in hosts ] ) url = config . get ( 'nessus' , 'host' ) access = config . get ( 'nessus' , 'access_key' ) secret = config . get ( 'nessus' , 'secret_key' ) template_name = config . get ( 'nessus' , 'template_name' ) nessus = Nessus ( access , secret , url , template_name ) scan_id = nessus . create_scan ( host_ips ) nessus . start_scan ( scan_id ) for host in hosts : host . add_tag ( 'nessus' ) host . save ( ) Logger ( ) . log ( "nessus" , "Nessus scan started on {} hosts" . format ( len ( hosts ) ) , { 'scanned_hosts' : len ( hosts ) } )
8432	def manual_pal ( values ) : max_n = len ( values ) def _manual_pal ( n ) : if n > max_n : msg = ( "Palette can return a maximum of {} values. " "{} were requested from it." ) warnings . warn ( msg . format ( max_n , n ) ) return values [ : n ] return _manual_pal
6873	def given_lc_get_out_of_transit_points ( time , flux , err_flux , blsfit_savpath = None , trapfit_savpath = None , in_out_transit_savpath = None , sigclip = None , magsarefluxes = True , nworkers = 1 , extra_maskfrac = 0.03 ) : tmids_obsd , t_starts , t_ends = ( given_lc_get_transit_tmids_tstarts_tends ( time , flux , err_flux , blsfit_savpath = blsfit_savpath , trapfit_savpath = trapfit_savpath , magsarefluxes = magsarefluxes , nworkers = nworkers , sigclip = sigclip , extra_maskfrac = extra_maskfrac ) ) in_transit = np . zeros_like ( time ) . astype ( bool ) for t_start , t_end in zip ( t_starts , t_ends ) : this_transit = ( ( time > t_start ) & ( time < t_end ) ) in_transit |= this_transit out_of_transit = ~ in_transit if in_out_transit_savpath : _in_out_transit_plot ( time , flux , in_transit , out_of_transit , in_out_transit_savpath ) return time [ out_of_transit ] , flux [ out_of_transit ] , err_flux [ out_of_transit ]
9855	def _read_header ( self , ccp4file ) : bsaflag = self . _detect_byteorder ( ccp4file ) nheader = struct . calcsize ( self . _headerfmt ) names = [ r . key for r in self . _header_struct ] bintopheader = ccp4file . read ( 25 * 4 ) def decode_header ( header , bsaflag = '@' ) : h = dict ( zip ( names , struct . unpack ( bsaflag + self . _headerfmt , header ) ) ) h [ 'bsaflag' ] = bsaflag return h header = decode_header ( bintopheader , bsaflag ) for rec in self . _header_struct : if not rec . is_legal_dict ( header ) : warnings . warn ( "Key %s: Illegal value %r" % ( rec . key , header [ rec . key ] ) ) if ( header [ 'lskflg' ] ) : skewmatrix = np . fromfile ( ccp4file , dtype = np . float32 , count = 9 ) header [ 'skwmat' ] = skewmatrix . reshape ( ( 3 , 3 ) ) header [ 'skwtrn' ] = np . fromfile ( ccp4file , dtype = np . float32 , count = 3 ) else : header [ 'skwmat' ] = header [ 'skwtrn' ] = None ccp4file . seek ( 12 * 4 , 1 ) ccp4file . seek ( 15 * 4 , 1 ) ccp4file . seek ( 4 , 1 ) endiancode = struct . unpack ( bsaflag + '4b' , ccp4file . read ( 4 ) ) header [ 'endianness' ] = 'little' if endiancode == ( 0x44 , 0x41 , 0 , 0 ) else 'big' header [ 'arms' ] = struct . unpack ( bsaflag + 'f' , ccp4file . read ( 4 ) ) [ 0 ] header [ 'nlabl' ] = struct . unpack ( bsaflag + 'I' , ccp4file . read ( 4 ) ) [ 0 ] if header [ 'nlabl' ] : binlabel = ccp4file . read ( 80 * header [ 'nlabl' ] ) flag = bsaflag + str ( 80 * header [ 'nlabl' ] ) + 's' label = struct . unpack ( flag , binlabel ) [ 0 ] header [ 'label' ] = label . decode ( 'utf-8' ) . rstrip ( '\x00' ) else : header [ 'label' ] = None ccp4file . seek ( 256 * 4 ) return header
6572	def register_simple_chooser ( self , model , ** kwargs ) : name = '{}Chooser' . format ( model . _meta . object_name ) attrs = { 'model' : model } attrs . update ( kwargs ) chooser = type ( name , ( Chooser , ) , attrs ) self . register_chooser ( chooser ) return model
10407	def bond_initialize_canonical_averages ( canonical_statistics , ** kwargs ) : spanning_cluster = ( 'percolation_probability' in canonical_statistics . dtype . names ) ret = np . empty_like ( canonical_statistics , dtype = canonical_averages_dtype ( spanning_cluster = spanning_cluster ) , ) ret [ 'number_of_runs' ] = 1 if spanning_cluster : ret [ 'percolation_probability_mean' ] = ( canonical_statistics [ 'percolation_probability' ] ) ret [ 'percolation_probability_m2' ] = 0.0 ret [ 'max_cluster_size_mean' ] = ( canonical_statistics [ 'max_cluster_size' ] ) ret [ 'max_cluster_size_m2' ] = 0.0 ret [ 'moments_mean' ] = canonical_statistics [ 'moments' ] ret [ 'moments_m2' ] = 0.0 return ret
1130	def urljoin ( base , url , allow_fragments = True ) : if not base : return url if not url : return base bscheme , bnetloc , bpath , bparams , bquery , bfragment = urlparse ( base , '' , allow_fragments ) scheme , netloc , path , params , query , fragment = urlparse ( url , bscheme , allow_fragments ) if scheme != bscheme or scheme not in uses_relative : return url if scheme in uses_netloc : if netloc : return urlunparse ( ( scheme , netloc , path , params , query , fragment ) ) netloc = bnetloc if path [ : 1 ] == '/' : return urlunparse ( ( scheme , netloc , path , params , query , fragment ) ) if not path and not params : path = bpath params = bparams if not query : query = bquery return urlunparse ( ( scheme , netloc , path , params , query , fragment ) ) segments = bpath . split ( '/' ) [ : - 1 ] + path . split ( '/' ) if segments [ - 1 ] == '.' : segments [ - 1 ] = '' while '.' in segments : segments . remove ( '.' ) while 1 : i = 1 n = len ( segments ) - 1 while i < n : if ( segments [ i ] == '..' and segments [ i - 1 ] not in ( '' , '..' ) ) : del segments [ i - 1 : i + 1 ] break i = i + 1 else : break if segments == [ '' , '..' ] : segments [ - 1 ] = '' elif len ( segments ) >= 2 and segments [ - 1 ] == '..' : segments [ - 2 : ] = [ '' ] return urlunparse ( ( scheme , netloc , '/' . join ( segments ) , params , query , fragment ) )
10393	def workflow_all_aggregate ( graph : BELGraph , key : Optional [ str ] = None , tag : Optional [ str ] = None , default_score : Optional [ float ] = None , runs : Optional [ int ] = None , aggregator : Optional [ Callable [ [ Iterable [ float ] ] , float ] ] = None , ) : results = { } bioprocess_nodes = list ( get_nodes_by_function ( graph , BIOPROCESS ) ) for bioprocess_node in tqdm ( bioprocess_nodes ) : subgraph = generate_mechanism ( graph , bioprocess_node , key = key ) try : results [ bioprocess_node ] = workflow_aggregate ( graph = subgraph , node = bioprocess_node , key = key , tag = tag , default_score = default_score , runs = runs , aggregator = aggregator ) except Exception : log . exception ( 'could not run on %' , bioprocess_node ) return results
11691	def filter ( self ) : self . content = [ ch for ch in self . xml . getchildren ( ) if get_bounds ( ch ) . intersects ( self . area ) ]
11142	def remove_repository ( self , path = None , removeEmptyDirs = True ) : assert isinstance ( removeEmptyDirs , bool ) , "removeEmptyDirs must be boolean" if path is not None : if path != self . __path : repo = Repository ( ) repo . load_repository ( path ) else : repo = self else : repo = self assert repo . path is not None , "path is not given and repository is not initialized" for fdict in reversed ( repo . get_repository_state ( ) ) : relaPath = list ( fdict ) [ 0 ] realPath = os . path . join ( repo . path , relaPath ) path , name = os . path . split ( realPath ) if fdict [ relaPath ] [ 'type' ] == 'file' : if os . path . isfile ( realPath ) : os . remove ( realPath ) if os . path . isfile ( os . path . join ( repo . path , path , self . __fileInfo % name ) ) : os . remove ( os . path . join ( repo . path , path , self . __fileInfo % name ) ) if os . path . isfile ( os . path . join ( repo . path , path , self . __fileLock % name ) ) : os . remove ( os . path . join ( repo . path , path , self . __fileLock % name ) ) if os . path . isfile ( os . path . join ( repo . path , path , self . __fileClass % name ) ) : os . remove ( os . path . join ( repo . path , path , self . __fileClass % name ) ) elif fdict [ relaPath ] [ 'type' ] == 'dir' : if os . path . isfile ( os . path . join ( realPath , self . __dirInfo ) ) : os . remove ( os . path . join ( realPath , self . __dirInfo ) ) if os . path . isfile ( os . path . join ( realPath , self . __dirLock ) ) : os . remove ( os . path . join ( realPath , self . __dirLock ) ) if not len ( os . listdir ( realPath ) ) and removeEmptyDirs : shutil . rmtree ( realPath ) if os . path . isfile ( os . path . join ( repo . path , self . __repoFile ) ) : os . remove ( os . path . join ( repo . path , self . __repoFile ) ) if os . path . isfile ( os . path . join ( repo . path , self . __repoLock ) ) : os . remove ( os . path . join ( repo . path , self . __repoLock ) )
8470	def execute ( self , shell = True ) : process = Popen ( self . command , stdout = PIPE , stderr = PIPE , shell = shell ) self . output , self . errors = process . communicate ( )
1132	def _replace ( _self , ** kwds ) : 'Return a new SplitResult object replacing specified fields with new values' result = _self . _make ( map ( kwds . pop , ( 'scheme' , 'netloc' , 'path' , 'query' , 'fragment' ) , _self ) ) if kwds : raise ValueError ( 'Got unexpected field names: %r' % kwds . keys ( ) ) return result
6233	def points_random_3d ( count , range_x = ( - 10.0 , 10.0 ) , range_y = ( - 10.0 , 10.0 ) , range_z = ( - 10.0 , 10.0 ) , seed = None ) -> VAO : random . seed ( seed ) def gen ( ) : for _ in range ( count ) : yield random . uniform ( * range_x ) yield random . uniform ( * range_y ) yield random . uniform ( * range_z ) data = numpy . fromiter ( gen ( ) , count = count * 3 , dtype = numpy . float32 ) vao = VAO ( "geometry:points_random_3d" , mode = moderngl . POINTS ) vao . buffer ( data , '3f' , [ 'in_position' ] ) return vao
6738	def get_last_modified_timestamp ( path , ignore = None ) : ignore = ignore or [ ] if not isinstance ( path , six . string_types ) : return ignore_str = '' if ignore : assert isinstance ( ignore , ( tuple , list ) ) ignore_str = ' ' . join ( "! -name '%s'" % _ for _ in ignore ) cmd = 'find "' + path + '" ' + ignore_str + ' -type f -printf "%T@ %p\n" | sort -n | tail -1 | cut -f 1 -d " "' ret = subprocess . check_output ( cmd , shell = True ) try : ret = round ( float ( ret ) , 2 ) except ValueError : return return ret
5289	def forms_invalid ( self , form , inlines ) : return self . render_to_response ( self . get_context_data ( form = form , inlines = inlines ) )
1667	def IsBlockInNameSpace ( nesting_state , is_forward_declaration ) : if is_forward_declaration : return len ( nesting_state . stack ) >= 1 and ( isinstance ( nesting_state . stack [ - 1 ] , _NamespaceInfo ) ) return ( len ( nesting_state . stack ) > 1 and nesting_state . stack [ - 1 ] . check_namespace_indentation and isinstance ( nesting_state . stack [ - 2 ] , _NamespaceInfo ) )
12791	def delete ( self , url = None , post_data = { } , parse_data = False , key = None , parameters = None ) : return self . _fetch ( "DELETE" , url , post_data = post_data , parse_data = parse_data , key = key , parameters = parameters , full_return = True )
8058	def do_play ( self , line ) : if self . pause_speed is None : self . bot . _speed = self . pause_speed self . pause_speed = None self . print_response ( "Play" )
7595	def get_clan ( self , * tags : crtag , ** params : keys ) : url = self . api . CLAN + '/' + ',' . join ( tags ) return self . _get_model ( url , FullClan , ** params )
9554	def _apply_record_length_checks ( self , i , r , summarize = False , context = None ) : for code , message , modulus in self . _record_length_checks : if i % modulus == 0 : if len ( r ) != len ( self . _field_names ) : p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'length' ] = len ( r ) if context is not None : p [ 'context' ] = context yield p
13092	def load_targets ( self ) : ldap_services = [ ] if self . ldap : ldap_services = self . search . get_services ( ports = [ 389 ] , up = True ) self . ldap_strings = [ "ldap://{}" . format ( service . address ) for service in ldap_services ] self . services = self . search . get_services ( tags = [ 'smb_signing_disabled' ] ) self . ips = [ str ( service . address ) for service in self . services ]
3326	def acquire ( self , url , lock_type , lock_scope , lock_depth , lock_owner , timeout , principal , token_list , ) : url = normalize_lock_root ( url ) self . _lock . acquire_write ( ) try : self . _check_lock_permission ( url , lock_type , lock_scope , lock_depth , token_list , principal ) return self . _generate_lock ( principal , lock_type , lock_scope , lock_depth , lock_owner , url , timeout ) finally : self . _lock . release ( )
3737	def molecular_diameter ( Tc = None , Pc = None , Vc = None , Zc = None , omega = None , Vm = None , Vb = None , CASRN = '' , AvailableMethods = False , Method = None ) : r def list_methods ( ) : methods = [ ] if CASRN in MagalhaesLJ_data . index : methods . append ( MAGALHAES ) if Tc and Pc and omega : methods . append ( TEEGOTOSTEWARD4 ) if Tc and Pc : methods . append ( SILVALIUMACEDO ) methods . append ( BSLC2 ) methods . append ( TEEGOTOSTEWARD3 ) if Vc and Zc : methods . append ( STIELTHODOSMD ) if Vc : methods . append ( FLYNN ) methods . append ( BSLC1 ) if Vb : methods . append ( BSLB ) if Vm : methods . append ( BSLM ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == FLYNN : sigma = sigma_Flynn ( Vc ) elif Method == BSLC1 : sigma = sigma_Bird_Stewart_Lightfoot_critical_1 ( Vc ) elif Method == BSLC2 : sigma = sigma_Bird_Stewart_Lightfoot_critical_2 ( Tc , Pc ) elif Method == TEEGOTOSTEWARD3 : sigma = sigma_Tee_Gotoh_Steward_1 ( Tc , Pc ) elif Method == SILVALIUMACEDO : sigma = sigma_Silva_Liu_Macedo ( Tc , Pc ) elif Method == BSLB : sigma = sigma_Bird_Stewart_Lightfoot_boiling ( Vb ) elif Method == BSLM : sigma = sigma_Bird_Stewart_Lightfoot_melting ( Vm ) elif Method == STIELTHODOSMD : sigma = sigma_Stiel_Thodos ( Vc , Zc ) elif Method == TEEGOTOSTEWARD4 : sigma = sigma_Tee_Gotoh_Steward_2 ( Tc , Pc , omega ) elif Method == MAGALHAES : sigma = float ( MagalhaesLJ_data . at [ CASRN , "sigma" ] ) elif Method == NONE : sigma = None else : raise Exception ( 'Failure in in function' ) return sigma
10252	def highlight_edges ( graph : BELGraph , edges = None , color : Optional [ str ] = None ) -> None : color = color or EDGE_HIGHLIGHT_DEFAULT_COLOR for u , v , k , d in edges if edges is not None else graph . edges ( keys = True , data = True ) : graph [ u ] [ v ] [ k ] [ EDGE_HIGHLIGHT ] = color
12915	def filelist ( self ) : if len ( self . _filelist ) == 0 : for item in self . _data : if isinstance ( self . _data [ item ] , filetree ) : self . _filelist . extend ( self . _data [ item ] . filelist ( ) ) else : self . _filelist . append ( self . _data [ item ] ) return self . _filelist
2905	def _add_child ( self , task_spec , state = MAYBE ) : if task_spec is None : raise ValueError ( self , '_add_child() requires a TaskSpec' ) if self . _is_predicted ( ) and state & self . PREDICTED_MASK == 0 : msg = 'Attempt to add non-predicted child to predicted task' raise WorkflowException ( self . task_spec , msg ) task = Task ( self . workflow , task_spec , self , state = state ) task . thread_id = self . thread_id if state == self . READY : task . _ready ( ) return task
2734	def get_object ( cls , api_token , ip ) : floating_ip = cls ( token = api_token , ip = ip ) floating_ip . load ( ) return floating_ip
9766	def clean_outputs ( fn ) : @ wraps ( fn ) def clean_outputs_wrapper ( * args , ** kwargs ) : try : return fn ( * args , ** kwargs ) except SystemExit as e : sys . stdout = StringIO ( ) sys . exit ( e . code ) except Exception as e : sys . stdout = StringIO ( ) raise e return clean_outputs_wrapper
9264	def filter_merged_pull_requests ( self , pull_requests ) : if self . options . verbose : print ( "Fetching merge date for pull requests..." ) closed_pull_requests = self . fetcher . fetch_closed_pull_requests ( ) if not pull_requests : return [ ] pulls = copy . deepcopy ( pull_requests ) for pr in pulls : fetched_pr = None for fpr in closed_pull_requests : if fpr [ 'number' ] == pr [ 'number' ] : fetched_pr = fpr if fetched_pr : pr [ 'merged_at' ] = fetched_pr [ 'merged_at' ] closed_pull_requests . remove ( fetched_pr ) for pr in pulls : if not pr . get ( 'merged_at' ) : pulls . remove ( pr ) return pulls
11724	def init_config ( self , app ) : config_apps = [ 'APP_' , 'RATELIMIT_' ] flask_talisman_debug_mode = [ "'unsafe-inline'" ] for k in dir ( config ) : if any ( [ k . startswith ( prefix ) for prefix in config_apps ] ) : app . config . setdefault ( k , getattr ( config , k ) ) if app . config [ 'DEBUG' ] : app . config . setdefault ( 'APP_DEFAULT_SECURE_HEADERS' , { } ) headers = app . config [ 'APP_DEFAULT_SECURE_HEADERS' ] if headers . get ( 'content_security_policy' ) != { } : headers . setdefault ( 'content_security_policy' , { } ) csp = headers [ 'content_security_policy' ] if csp . get ( 'default-src' ) != [ ] : csp . setdefault ( 'default-src' , [ ] ) csp [ 'default-src' ] += flask_talisman_debug_mode
6464	def usage_palette ( parser ) : parser . print_usage ( ) print ( '' ) print ( 'available palettes:' ) for palette in sorted ( PALETTE ) : print ( ' %-12s' % ( palette , ) ) return 0
3452	def find_essential_genes ( model , threshold = None , processes = None ) : if threshold is None : threshold = model . slim_optimize ( error_value = None ) * 1E-02 deletions = single_gene_deletion ( model , method = 'fba' , processes = processes ) essential = deletions . loc [ deletions [ 'growth' ] . isna ( ) | ( deletions [ 'growth' ] < threshold ) , : ] . index return { model . genes . get_by_id ( g ) for ids in essential for g in ids }
8144	def flip ( self , axis = HORIZONTAL ) : if axis == HORIZONTAL : self . img = self . img . transpose ( Image . FLIP_LEFT_RIGHT ) if axis == VERTICAL : self . img = self . img . transpose ( Image . FLIP_TOP_BOTTOM )
2307	def reset_parameters ( self ) : stdv = 1. / math . sqrt ( self . weight . size ( 1 ) ) self . weight . data . uniform_ ( - stdv , stdv ) if self . bias is not None : self . bias . data . uniform_ ( - stdv , stdv )
7984	def registration_form_received ( self , stanza ) : self . lock . acquire ( ) try : self . __register = Register ( stanza . get_query ( ) ) self . registration_callback ( stanza , self . __register . get_form ( ) ) finally : self . lock . release ( )
9258	def find_issues_to_add ( all_issues , tag_name ) : filtered = [ ] for issue in all_issues : if issue . get ( "milestone" ) : if issue [ "milestone" ] [ "title" ] == tag_name : iss = copy . deepcopy ( issue ) filtered . append ( iss ) return filtered
7980	def auth_timeout ( self ) : self . lock . acquire ( ) try : self . __logger . debug ( "Timeout while waiting for jabber:iq:auth result" ) if self . _auth_methods_left : self . _auth_methods_left . pop ( 0 ) finally : self . lock . release ( )
10518	def setmin ( self , window_name , object_name ) : object_handle = self . _get_object_handle ( window_name , object_name ) object_handle . AXValue = 0 return 1
3796	def setup_a_alpha_and_derivatives ( self , i , T = None ) : r if not hasattr ( self , 'kappas' ) : self . kappas = [ ] for Tc , kappa0 , kappa1 , kappa2 , kappa3 in zip ( self . Tcs , self . kappa0s , self . kappa1s , self . kappa2s , self . kappa3s ) : Tr = T / Tc kappa = kappa0 + ( ( kappa1 + kappa2 * ( kappa3 - Tr ) * ( 1. - Tr ** 0.5 ) ) * ( 1. + Tr ** 0.5 ) * ( 0.7 - Tr ) ) self . kappas . append ( kappa ) ( self . a , self . kappa , self . kappa0 , self . kappa1 , self . kappa2 , self . kappa3 , self . Tc ) = ( self . ais [ i ] , self . kappas [ i ] , self . kappa0s [ i ] , self . kappa1s [ i ] , self . kappa2s [ i ] , self . kappa3s [ i ] , self . Tcs [ i ] )
12079	def figure_sweeps ( self , offsetX = 0 , offsetY = 0 ) : self . log . debug ( "creating overlayed sweeps plot" ) self . figure ( ) for sweep in range ( self . abf . sweeps ) : self . abf . setsweep ( sweep ) self . setColorBySweep ( ) plt . plot ( self . abf . sweepX2 + sweep * offsetX , self . abf . sweepY + sweep * offsetY , ** self . kwargs ) if offsetX : self . marginX = .05 self . decorate ( )
8483	def set ( self , name , value ) : if not self . settings . get ( 'pyconfig.case_sensitive' , False ) : name = name . lower ( ) log . info ( " %s = %s" , name , repr ( value ) ) with self . mut_lock : self . settings [ name ] = value
3875	async def _on_state_update ( self , state_update ) : notification_type = state_update . WhichOneof ( 'state_update' ) if state_update . HasField ( 'conversation' ) : try : await self . _handle_conversation_delta ( state_update . conversation ) except exceptions . NetworkError : logger . warning ( 'Discarding %s for %s: Failed to fetch conversation' , notification_type . replace ( '_' , ' ' ) , state_update . conversation . conversation_id . id ) return if notification_type == 'typing_notification' : await self . _handle_set_typing_notification ( state_update . typing_notification ) elif notification_type == 'watermark_notification' : await self . _handle_watermark_notification ( state_update . watermark_notification ) elif notification_type == 'event_notification' : await self . _on_event ( state_update . event_notification . event )
5159	def _add_tc_script ( self ) : context = dict ( tc_options = self . config . get ( 'tc_options' , [ ] ) ) contents = self . _render_template ( 'tc_script.sh' , context ) self . config . setdefault ( 'files' , [ ] ) self . _add_unique_file ( { "path" : "/tc_script.sh" , "contents" : contents , "mode" : "755" } )
12811	def lineReceived ( self , line ) : while self . _in_header : if line : self . _headers . append ( line ) else : http , status , message = self . _headers [ 0 ] . split ( " " , 2 ) status = int ( status ) if status == 200 : self . factory . get_stream ( ) . connected ( ) else : self . factory . continueTrying = 0 self . transport . loseConnection ( ) self . factory . get_stream ( ) . disconnected ( RuntimeError ( status , message ) ) return self . _in_header = False break else : try : self . _len_expected = int ( line , 16 ) self . setRawMode ( ) except : pass
6332	def decode ( self , code , terminator = '\0' ) : r if code : if terminator not in code : raise ValueError ( 'Specified terminator, {}, absent from code.' . format ( terminator if terminator != '\0' else '\\0' ) ) else : wordlist = [ '' ] * len ( code ) for i in range ( len ( code ) ) : wordlist = sorted ( code [ i ] + wordlist [ i ] for i in range ( len ( code ) ) ) rows = [ w for w in wordlist if w [ - 1 ] == terminator ] [ 0 ] return rows . rstrip ( terminator ) else : return ''
4944	def get_course_data_sharing_consent ( username , course_id , enterprise_customer_uuid ) : DataSharingConsent = apps . get_model ( 'consent' , 'DataSharingConsent' ) return DataSharingConsent . objects . proxied_get ( username = username , course_id = course_id , enterprise_customer__uuid = enterprise_customer_uuid )
10431	def selectrowpartialmatch ( self , window_name , object_name , row_text ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) for cell in object_handle . AXRows : if re . search ( row_text , cell . AXChildren [ 0 ] . AXValue ) : if not cell . AXSelected : object_handle . activate ( ) cell . AXSelected = True else : pass return 1 raise LdtpServerException ( u"Unable to select row: %s" % row_text )
6140	def wait_for_simulation_stop ( self , timeout = None ) : start = datetime . now ( ) while self . get_is_sim_running ( ) : sleep ( 0.5 ) if timeout is not None : if ( datetime . now ( ) - start ) . seconds >= timeout : ret = None break else : ret = self . simulation_info ( ) return ret
5084	def unlink_learners ( self ) : sap_inactive_learners = self . client . get_inactive_sap_learners ( ) enterprise_customer = self . enterprise_configuration . enterprise_customer if not sap_inactive_learners : LOGGER . info ( 'Enterprise customer {%s} has no SAPSF inactive learners' , enterprise_customer . name ) return provider_id = enterprise_customer . identity_provider tpa_provider = get_identity_provider ( provider_id ) if not tpa_provider : LOGGER . info ( 'Enterprise customer {%s} has no associated identity provider' , enterprise_customer . name ) return None for sap_inactive_learner in sap_inactive_learners : social_auth_user = get_user_from_social_auth ( tpa_provider , sap_inactive_learner [ 'studentID' ] ) if not social_auth_user : continue try : EnterpriseCustomerUser . objects . unlink_user ( enterprise_customer = enterprise_customer , user_email = social_auth_user . email , ) except ( EnterpriseCustomerUser . DoesNotExist , PendingEnterpriseCustomerUser . DoesNotExist ) : LOGGER . info ( 'Learner with email {%s} is not associated with Enterprise Customer {%s}' , social_auth_user . email , enterprise_customer . name )
12696	def to_XML ( self ) : marcxml_template = oai_template = leader = self . leader if self . leader is not None else "" if leader : leader = "<leader>" + leader + "</leader>" if self . oai_marc : leader = "" xml_template = oai_template if self . oai_marc else marcxml_template xml_output = Template ( xml_template ) . substitute ( LEADER = leader . strip ( ) , CONTROL_FIELDS = self . _serialize_ctl_fields ( ) . strip ( ) , DATA_FIELDS = self . _serialize_data_fields ( ) . strip ( ) ) return xml_output
13257	def save ( self , entry , with_location = True , debug = False ) : entry_dict = { } if isinstance ( entry , DayOneEntry ) : entry_dict = entry . as_dict ( ) else : entry_dict = entry entry_dict [ 'UUID' ] = uuid . uuid4 ( ) . get_hex ( ) if with_location and not entry_dict [ 'Location' ] : entry_dict [ 'Location' ] = self . get_location ( ) if not all ( ( entry_dict [ 'UUID' ] , entry_dict [ 'Time Zone' ] , entry_dict [ 'Entry Text' ] ) ) : print "You must provide: Time zone, UUID, Creation Date, Entry Text" return False if debug is False : file_path = self . _file_path ( entry_dict [ 'UUID' ] ) plistlib . writePlist ( entry_dict , file_path ) else : plist = plistlib . writePlistToString ( entry_dict ) print plist return True
1838	def JNB ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . CF == False , target . read ( ) , cpu . PC )
5998	def plot_grid ( grid_arcsec , array , units , kpc_per_arcsec , pointsize , zoom_offset_arcsec ) : if grid_arcsec is not None : if zoom_offset_arcsec is not None : grid_arcsec -= zoom_offset_arcsec grid_units = convert_grid_units ( grid_arcsec = grid_arcsec , array = array , units = units , kpc_per_arcsec = kpc_per_arcsec ) plt . scatter ( y = np . asarray ( grid_units [ : , 0 ] ) , x = np . asarray ( grid_units [ : , 1 ] ) , s = pointsize , c = 'k' )
8627	def get_users ( session , query ) : response = make_get_request ( session , 'users' , params_data = query ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise UsersNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
5474	def format_pairs ( self , values ) : return ', ' . join ( '%s=%s' % ( key , value ) for key , value in sorted ( values . items ( ) ) )
4051	def file ( self , item , ** kwargs ) : query_string = "/{t}/{u}/items/{i}/file" . format ( u = self . library_id , t = self . library_type , i = item . upper ( ) ) return self . _build_query ( query_string , no_params = True )
13810	def MakeDescriptor ( desc_proto , package = '' , build_file_if_cpp = True , syntax = None ) : if api_implementation . Type ( ) == 'cpp' and build_file_if_cpp : from typy . google . protobuf import descriptor_pb2 file_descriptor_proto = descriptor_pb2 . FileDescriptorProto ( ) file_descriptor_proto . message_type . add ( ) . MergeFrom ( desc_proto ) proto_name = str ( uuid . uuid4 ( ) ) if package : file_descriptor_proto . name = os . path . join ( package . replace ( '.' , '/' ) , proto_name + '.proto' ) file_descriptor_proto . package = package else : file_descriptor_proto . name = proto_name + '.proto' _message . default_pool . Add ( file_descriptor_proto ) result = _message . default_pool . FindFileByName ( file_descriptor_proto . name ) if _USE_C_DESCRIPTORS : return result . message_types_by_name [ desc_proto . name ] full_message_name = [ desc_proto . name ] if package : full_message_name . insert ( 0 , package ) enum_types = { } for enum_proto in desc_proto . enum_type : full_name = '.' . join ( full_message_name + [ enum_proto . name ] ) enum_desc = EnumDescriptor ( enum_proto . name , full_name , None , [ EnumValueDescriptor ( enum_val . name , ii , enum_val . number ) for ii , enum_val in enumerate ( enum_proto . value ) ] ) enum_types [ full_name ] = enum_desc nested_types = { } for nested_proto in desc_proto . nested_type : full_name = '.' . join ( full_message_name + [ nested_proto . name ] ) nested_desc = MakeDescriptor ( nested_proto , package = '.' . join ( full_message_name ) , build_file_if_cpp = False , syntax = syntax ) nested_types [ full_name ] = nested_desc fields = [ ] for field_proto in desc_proto . field : full_name = '.' . join ( full_message_name + [ field_proto . name ] ) enum_desc = None nested_desc = None if field_proto . HasField ( 'type_name' ) : type_name = field_proto . type_name full_type_name = '.' . join ( full_message_name + [ type_name [ type_name . rfind ( '.' ) + 1 : ] ] ) if full_type_name in nested_types : nested_desc = nested_types [ full_type_name ] elif full_type_name in enum_types : enum_desc = enum_types [ full_type_name ] field = FieldDescriptor ( field_proto . name , full_name , field_proto . number - 1 , field_proto . number , field_proto . type , FieldDescriptor . ProtoTypeToCppProtoType ( field_proto . type ) , field_proto . label , None , nested_desc , enum_desc , None , False , None , options = field_proto . options , has_default_value = False ) fields . append ( field ) desc_name = '.' . join ( full_message_name ) return Descriptor ( desc_proto . name , desc_name , None , None , fields , list ( nested_types . values ( ) ) , list ( enum_types . values ( ) ) , [ ] , options = desc_proto . options )
6154	def position_CD ( Ka , out_type = 'fb_exact' ) : rs = 10 / ( 2 * np . pi ) if out_type . lower ( ) == 'open_loop' : b = np . array ( [ Ka * 4000 * rs ] ) a = np . array ( [ 1 , 1275 , 31250 , 0 ] ) elif out_type . lower ( ) == 'fb_approx' : b = np . array ( [ 3.2 * Ka * rs ] ) a = np . array ( [ 1 , 25 , 3.2 * Ka * rs ] ) elif out_type . lower ( ) == 'fb_exact' : b = np . array ( [ 4000 * Ka * rs ] ) a = np . array ( [ 1 , 1250 + 25 , 25 * 1250 , 4000 * Ka * rs ] ) else : raise ValueError ( 'out_type must be: open_loop, fb_approx, or fc_exact' ) return b , a
4094	def AIC ( N , rho , k ) : r from numpy import log , array res = N * log ( array ( rho ) ) + 2. * ( array ( k ) + 1 ) return res
815	def Indicator ( pos , size , dtype ) : x = numpy . zeros ( size , dtype = dtype ) x [ pos ] = 1 return x
10613	def H ( self , H ) : self . _H = H self . _T = self . _calculate_T ( H )
5059	def send_email_notification_message ( user , enrolled_in , enterprise_customer , email_connection = None ) : if hasattr ( user , 'first_name' ) and hasattr ( user , 'username' ) : user_name = user . first_name if not user_name : user_name = user . username else : user_name = None if hasattr ( user , 'email' ) : user_email = user . email elif hasattr ( user , 'user_email' ) : user_email = user . user_email else : raise TypeError ( _ ( '`user` must have one of either `email` or `user_email`.' ) ) msg_context = { 'user_name' : user_name , 'enrolled_in' : enrolled_in , 'organization_name' : enterprise_customer . name , } try : enterprise_template_config = enterprise_customer . enterprise_enrollment_template except ( ObjectDoesNotExist , AttributeError ) : enterprise_template_config = None plain_msg , html_msg = build_notification_message ( msg_context , enterprise_template_config ) subject_line = get_notification_subject_line ( enrolled_in [ 'name' ] , enterprise_template_config ) from_email_address = get_configuration_value_for_site ( enterprise_customer . site , 'DEFAULT_FROM_EMAIL' , default = settings . DEFAULT_FROM_EMAIL ) return mail . send_mail ( subject_line , plain_msg , from_email_address , [ user_email ] , html_message = html_msg , connection = email_connection )
13845	def get_unique_pathname ( path , root = '' ) : path = os . path . join ( root , path ) potentialPaths = itertools . chain ( ( path , ) , __get_numbered_paths ( path ) ) potentialPaths = six . moves . filterfalse ( os . path . exists , potentialPaths ) return next ( potentialPaths )
7158	def add ( self , * args , ** kwargs ) : if 'question' in kwargs and isinstance ( kwargs [ 'question' ] , Question ) : question = kwargs [ 'question' ] else : question = Question ( * args , ** kwargs ) self . questions . setdefault ( question . key , [ ] ) . append ( question ) return question
2076	def main ( loader , name ) : scores = [ ] raw_scores_ds = { } X , y , mapping = loader ( ) clf = linear_model . LogisticRegression ( solver = 'lbfgs' , multi_class = 'auto' , max_iter = 200 , random_state = 0 ) encoders = ( set ( category_encoders . __all__ ) - { 'WOEEncoder' } ) for encoder_name in encoders : encoder = getattr ( category_encoders , encoder_name ) start_time = time . time ( ) score , stds , raw_scores , dim = score_models ( clf , X , y , encoder ) scores . append ( [ encoder_name , name , dim , score , stds , time . time ( ) - start_time ] ) raw_scores_ds [ encoder_name ] = raw_scores gc . collect ( ) results = pd . DataFrame ( scores , columns = [ 'Encoding' , 'Dataset' , 'Dimensionality' , 'Avg. Score' , 'Score StDev' , 'Elapsed Time' ] ) raw = pd . DataFrame . from_dict ( raw_scores_ds ) ax = raw . plot ( kind = 'box' , return_type = 'axes' ) plt . title ( 'Scores for Encodings on %s Dataset' % ( name , ) ) plt . ylabel ( 'Score (higher is better)' ) for tick in ax . get_xticklabels ( ) : tick . set_rotation ( 90 ) plt . grid ( ) plt . tight_layout ( ) plt . show ( ) return results , raw
690	def encodeValue ( self , value , toBeAdded = True ) : encodedValue = np . array ( self . encoder . encode ( value ) , dtype = realDType ) if toBeAdded : self . encodings . append ( encodedValue ) self . numEncodings += 1 return encodedValue
13463	def add_event ( request ) : form = AddEventForm ( request . POST or None ) if form . is_valid ( ) : instance = form . save ( commit = False ) instance . sites = settings . SITE_ID instance . submitted_by = request . user instance . approved = True instance . slug = slugify ( instance . name ) instance . save ( ) messages . success ( request , 'Your event has been added.' ) return HttpResponseRedirect ( reverse ( 'events_index' ) ) return render ( request , 'happenings/event_form.html' , { 'form' : form , 'form_title' : 'Add an event' } )
1979	def wait ( self , readfds , writefds , timeout ) : logger . info ( "WAIT:" ) logger . info ( "\tProcess %d is going to wait for [ %r %r %r ]" , self . _current , readfds , writefds , timeout ) logger . info ( "\tProcess: %r" , self . procs ) logger . info ( "\tRunning: %r" , self . running ) logger . info ( "\tRWait: %r" , self . rwait ) logger . info ( "\tTWait: %r" , self . twait ) logger . info ( "\tTimers: %r" , self . timers ) for fd in readfds : self . rwait [ fd ] . add ( self . _current ) for fd in writefds : self . twait [ fd ] . add ( self . _current ) if timeout is not None : self . timers [ self . _current ] = self . clocks + timeout else : self . timers [ self . _current ] = None procid = self . _current next_index = ( self . running . index ( procid ) + 1 ) % len ( self . running ) self . _current = self . running [ next_index ] logger . info ( "\tTransfer control from process %d to %d" , procid , self . _current ) logger . info ( "\tREMOVING %r from %r. Current: %r" , procid , self . running , self . _current ) self . running . remove ( procid ) if self . _current not in self . running : logger . info ( "\tCurrent not running. Checking for timers..." ) self . _current = None if all ( [ x is None for x in self . timers ] ) : raise Deadlock ( ) self . check_timers ( )
12646	def set_aad_metadata ( uri , resource , client ) : set_config_value ( 'authority_uri' , uri ) set_config_value ( 'aad_resource' , resource ) set_config_value ( 'aad_client' , client )
13649	def get_fuel_prices_within_radius ( self , latitude : float , longitude : float , radius : int , fuel_type : str , brands : Optional [ List [ str ] ] = None ) -> List [ StationPrice ] : if brands is None : brands = [ ] response = requests . post ( '{}/prices/nearby' . format ( API_URL_BASE ) , json = { 'fueltype' : fuel_type , 'latitude' : latitude , 'longitude' : longitude , 'radius' : radius , 'brand' : brands , } , headers = self . _get_headers ( ) , timeout = self . _timeout , ) if not response . ok : raise FuelCheckError . create ( response ) data = response . json ( ) stations = { station [ 'code' ] : Station . deserialize ( station ) for station in data [ 'stations' ] } station_prices = [ ] for serialized_price in data [ 'prices' ] : price = Price . deserialize ( serialized_price ) station_prices . append ( StationPrice ( price = price , station = stations [ price . station_code ] ) ) return station_prices
82	def SaltAndPepper ( p = 0 , per_channel = False , name = None , deterministic = False , random_state = None ) : if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return ReplaceElementwise ( mask = p , replacement = iap . Beta ( 0.5 , 0.5 ) * 255 , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )
7186	def maybe_replace_any_if_equal ( name , expected , actual ) : is_equal = expected == actual if not is_equal and Config . replace_any : actual_str = minimize_whitespace ( str ( actual ) ) if actual_str and actual_str [ 0 ] in { '"' , "'" } : actual_str = actual_str [ 1 : - 1 ] is_equal = actual_str in { 'Any' , 'typing.Any' , 't.Any' } if not is_equal : expected_annotation = minimize_whitespace ( str ( expected ) ) actual_annotation = minimize_whitespace ( str ( actual ) ) raise ValueError ( f"incompatible existing {name}. " + f"Expected: {expected_annotation!r}, actual: {actual_annotation!r}" ) return expected or actual
7767	def _stream_authenticated ( self , event ) : with self . lock : if event . stream != self . stream : return self . me = event . stream . me self . peer = event . stream . peer handlers = self . _base_handlers [ : ] handlers += self . handlers + [ self ] self . setup_stanza_handlers ( handlers , "post-auth" )
7483	def parse_single_results ( data , sample , res1 ) : sample . stats_dfs . s2 [ "trim_adapter_bp_read1" ] = 0 sample . stats_dfs . s2 [ "trim_quality_bp_read1" ] = 0 sample . stats_dfs . s2 [ "reads_filtered_by_Ns" ] = 0 sample . stats_dfs . s2 [ "reads_filtered_by_minlen" ] = 0 sample . stats_dfs . s2 [ "reads_passed_filter" ] = 0 lines = res1 . strip ( ) . split ( "\n" ) for line in lines : if "Total reads processed:" in line : value = int ( line . split ( ) [ 3 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "reads_raw" ] = value if "Reads with adapters:" in line : value = int ( line . split ( ) [ 3 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "trim_adapter_bp_read1" ] = value if "Quality-trimmed" in line : value = int ( line . split ( ) [ 1 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "trim_quality_bp_read1" ] = value if "Reads that were too short" in line : value = int ( line . split ( ) [ 5 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "reads_filtered_by_minlen" ] = value if "Reads with too many N" in line : value = int ( line . split ( ) [ 5 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "reads_filtered_by_Ns" ] = value if "Reads written (passing filters):" in line : value = int ( line . split ( ) [ 4 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "reads_passed_filter" ] = value if sample . stats_dfs . s2 . reads_passed_filter : sample . stats . state = 2 sample . stats . reads_passed_filter = sample . stats_dfs . s2 . reads_passed_filter sample . files . edits = [ ( OPJ ( data . dirs . edits , sample . name + ".trimmed_R1_.fastq.gz" ) , 0 ) ] LOGGER . info ( res1 ) else : print ( "{}No reads passed filtering in Sample: {}" . format ( data . _spacer , sample . name ) )
10977	def members ( group_id ) : page = request . args . get ( 'page' , 1 , type = int ) per_page = request . args . get ( 'per_page' , 5 , type = int ) q = request . args . get ( 'q' , '' ) s = request . args . get ( 's' , '' ) group = Group . query . get_or_404 ( group_id ) if group . can_see_members ( current_user ) : members = Membership . query_by_group ( group_id , with_invitations = True ) if q : members = Membership . search ( members , q ) if s : members = Membership . order ( members , Membership . state , s ) members = members . paginate ( page , per_page = per_page ) return render_template ( "invenio_groups/members.html" , group = group , members = members , page = page , per_page = per_page , q = q , s = s , ) flash ( _ ( 'You are not allowed to see members of this group %(group_name)s.' , group_name = group . name ) , 'error' ) return redirect ( url_for ( '.index' ) )
4059	def _citation_processor ( self , retrieved ) : items = [ ] for cit in retrieved . entries : items . append ( cit [ "content" ] [ 0 ] [ "value" ] ) self . url_params = None return items
1361	def get_argument_endtime ( self ) : try : endtime = self . get_argument ( constants . PARAM_ENDTIME ) return endtime except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
12820	def _filename ( draw , result_type = None ) : ascii_char = characters ( min_codepoint = 0x01 , max_codepoint = 0x7f ) if os . name == 'nt' : surrogate = characters ( min_codepoint = 0xD800 , max_codepoint = 0xDFFF ) uni_char = characters ( min_codepoint = 0x1 ) text_strategy = text ( alphabet = one_of ( uni_char , surrogate , ascii_char ) ) def text_to_bytes ( path ) : fs_enc = sys . getfilesystemencoding ( ) try : return path . encode ( fs_enc , 'surrogatepass' ) except UnicodeEncodeError : return path . encode ( fs_enc , 'replace' ) bytes_strategy = text_strategy . map ( text_to_bytes ) else : latin_char = characters ( min_codepoint = 0x01 , max_codepoint = 0xff ) bytes_strategy = text ( alphabet = one_of ( latin_char , ascii_char ) ) . map ( lambda t : t . encode ( 'latin-1' ) ) unix_path_text = bytes_strategy . map ( lambda b : b . decode ( sys . getfilesystemencoding ( ) , 'surrogateescape' if PY3 else 'ignore' ) ) text_strategy = permutations ( draw ( unix_path_text ) ) . map ( u"" . join ) if result_type is None : return draw ( one_of ( bytes_strategy , text_strategy ) ) elif result_type is bytes : return draw ( bytes_strategy ) else : return draw ( text_strategy )
12366	def update ( self , id , name ) : return super ( Keys , self ) . update ( id , name = name )
8124	def draw_cornu_flat ( x0 , y0 , t0 , t1 , s0 , c0 , flip , cs , ss , cmd ) : for j in range ( 0 , 100 ) : t = j * .01 s , c = eval_cornu ( t0 + t * ( t1 - t0 ) ) s *= flip s -= s0 c -= c0 x = c * cs - s * ss y = s * cs + c * ss print_pt ( x0 + x , y0 + y , cmd ) cmd = 'lineto' return cmd
6996	def spline_fit_magseries ( times , mags , errs , period , knotfraction = 0.01 , maxknots = 30 , sigclip = 30.0 , plotfit = False , ignoreinitfail = False , magsarefluxes = False , verbose = True ) : if errs is None : errs = npfull_like ( mags , 0.005 ) stimes , smags , serrs = sigclip_magseries ( times , mags , errs , sigclip = sigclip , magsarefluxes = magsarefluxes ) nzind = npnonzero ( serrs ) stimes , smags , serrs = stimes [ nzind ] , smags [ nzind ] , serrs [ nzind ] phase , pmags , perrs , ptimes , mintime = ( get_phased_quantities ( stimes , smags , serrs , period ) ) nobs = len ( phase ) nknots = int ( npfloor ( knotfraction * nobs ) ) nknots = maxknots if nknots > maxknots else nknots splineknots = nplinspace ( phase [ 0 ] + 0.01 , phase [ - 1 ] - 0.01 , num = nknots ) phase_diffs_ind = npdiff ( phase ) > 0.0 incphase_ind = npconcatenate ( ( nparray ( [ True ] ) , phase_diffs_ind ) ) phase , pmags , perrs = ( phase [ incphase_ind ] , pmags [ incphase_ind ] , perrs [ incphase_ind ] ) spl = LSQUnivariateSpline ( phase , pmags , t = splineknots , w = 1.0 / perrs ) fitmags = spl ( phase ) fitchisq = npsum ( ( ( fitmags - pmags ) * ( fitmags - pmags ) ) / ( perrs * perrs ) ) fitredchisq = fitchisq / ( len ( pmags ) - nknots - 1 ) if verbose : LOGINFO ( 'spline fit done. nknots = %s, ' 'chisq = %.5f, reduced chisq = %.5f' % ( nknots , fitchisq , fitredchisq ) ) if not magsarefluxes : fitmagminind = npwhere ( fitmags == npmax ( fitmags ) ) else : fitmagminind = npwhere ( fitmags == npmin ( fitmags ) ) if len ( fitmagminind [ 0 ] ) > 1 : fitmagminind = ( fitmagminind [ 0 ] [ 0 ] , ) magseriesepoch = ptimes [ fitmagminind ] returndict = { 'fittype' : 'spline' , 'fitinfo' : { 'nknots' : nknots , 'fitmags' : fitmags , 'fitepoch' : magseriesepoch } , 'fitchisq' : fitchisq , 'fitredchisq' : fitredchisq , 'fitplotfile' : None , 'magseries' : { 'times' : ptimes , 'phase' : phase , 'mags' : pmags , 'errs' : perrs , 'magsarefluxes' : magsarefluxes } , } if plotfit and isinstance ( plotfit , str ) : make_fit_plot ( phase , pmags , perrs , fitmags , period , mintime , magseriesepoch , plotfit , magsarefluxes = magsarefluxes ) returndict [ 'fitplotfile' ] = plotfit return returndict
8918	def _get_version ( self ) : version = self . _get_param ( param = "version" , allowed_values = allowed_versions [ self . params [ 'service' ] ] , optional = True ) if version is None and self . _get_request_type ( ) != "getcapabilities" : raise OWSMissingParameterValue ( 'Parameter "version" is missing' , value = "version" ) else : return version
12131	def lexsort ( self , * order ) : if order == [ ] : raise Exception ( "Please specify the keys for sorting, use" "'+' prefix for ascending," "'-' for descending.)" ) if not set ( el [ 1 : ] for el in order ) . issubset ( set ( self . varying_keys ) ) : raise Exception ( "Key(s) specified not in the set of varying keys." ) sorted_args = copy . deepcopy ( self ) specs_param = sorted_args . params ( 'specs' ) specs_param . constant = False sorted_args . specs = self . _lexsorted_specs ( order ) specs_param . constant = True sorted_args . _lexorder = order return sorted_args
12350	def get ( self , id ) : info = self . _get_droplet_info ( id ) return DropletActions ( self . api , self , ** info )
10228	def get_separate_unstable_correlation_triples ( graph : BELGraph ) -> Iterable [ NodeTriple ] : cg = get_correlation_graph ( graph ) for a , b , c in get_correlation_triangles ( cg ) : if POSITIVE_CORRELATION in cg [ a ] [ b ] and POSITIVE_CORRELATION in cg [ b ] [ c ] and NEGATIVE_CORRELATION in cg [ a ] [ c ] : yield b , a , c if POSITIVE_CORRELATION in cg [ a ] [ b ] and NEGATIVE_CORRELATION in cg [ b ] [ c ] and POSITIVE_CORRELATION in cg [ a ] [ c ] : yield a , b , c if NEGATIVE_CORRELATION in cg [ a ] [ b ] and POSITIVE_CORRELATION in cg [ b ] [ c ] and POSITIVE_CORRELATION in cg [ a ] [ c ] : yield c , a , b
8539	def get_disk_image_by_name ( pbclient , location , image_name ) : all_images = pbclient . list_images ( ) matching = [ i for i in all_images [ 'items' ] if i [ 'properties' ] [ 'name' ] == image_name and i [ 'properties' ] [ 'imageType' ] == "HDD" and i [ 'properties' ] [ 'location' ] == location ] return matching
9350	def check_digit ( num ) : sum = 0 digits = str ( num ) [ : - 1 ] [ : : - 1 ] for i , n in enumerate ( digits ) : if ( i + 1 ) % 2 != 0 : digit = int ( n ) * 2 if digit > 9 : sum += ( digit - 9 ) else : sum += digit else : sum += int ( n ) return ( ( divmod ( sum , 10 ) [ 0 ] + 1 ) * 10 - sum ) % 10
102	def imresize_single_image ( image , sizes , interpolation = None ) : grayscale = False if image . ndim == 2 : grayscale = True image = image [ : , : , np . newaxis ] do_assert ( len ( image . shape ) == 3 , image . shape ) rs = imresize_many_images ( image [ np . newaxis , : , : , : ] , sizes , interpolation = interpolation ) if grayscale : return np . squeeze ( rs [ 0 , : , : , 0 ] ) else : return rs [ 0 , ... ]
3000	def splitsDF ( symbol , timeframe = 'ytd' , token = '' , version = '' ) : s = splits ( symbol , timeframe , token , version ) df = _splitsToDF ( s ) return df
12876	def many ( parser ) : results = [ ] terminate = object ( ) while local_ps . value : result = optional ( parser , terminate ) if result == terminate : break results . append ( result ) return results
8822	def main ( notify , hour , minute ) : config_opts = [ '--config-file' , '/etc/neutron/neutron.conf' ] config . init ( config_opts ) network_strategy . STRATEGY . load ( ) billing . PUBLIC_NETWORK_ID = network_strategy . STRATEGY . get_public_net_id ( ) config . setup_logging ( ) context = neutron_context . get_admin_context ( ) query = context . session . query ( models . IPAddress ) ( period_start , period_end ) = billing . calc_periods ( hour , minute ) full_day_ips = billing . build_full_day_ips ( query , period_start , period_end ) partial_day_ips = billing . build_partial_day_ips ( query , period_start , period_end ) if notify : for ipaddress in full_day_ips : click . echo ( 'start: {}, end: {}' . format ( period_start , period_end ) ) payload = billing . build_payload ( ipaddress , billing . IP_EXISTS , start_time = period_start , end_time = period_end ) billing . do_notify ( context , billing . IP_EXISTS , payload ) for ipaddress in partial_day_ips : click . echo ( 'start: {}, end: {}' . format ( period_start , period_end ) ) payload = billing . build_payload ( ipaddress , billing . IP_EXISTS , start_time = ipaddress . allocated_at , end_time = period_end ) billing . do_notify ( context , billing . IP_EXISTS , payload ) else : click . echo ( 'Case 1 ({}):\n' . format ( len ( full_day_ips ) ) ) for ipaddress in full_day_ips : pp ( billing . build_payload ( ipaddress , billing . IP_EXISTS , start_time = period_start , end_time = period_end ) ) click . echo ( '\n===============================================\n' ) click . echo ( 'Case 2 ({}):\n' . format ( len ( partial_day_ips ) ) ) for ipaddress in partial_day_ips : pp ( billing . build_payload ( ipaddress , billing . IP_EXISTS , start_time = ipaddress . allocated_at , end_time = period_end ) )
13249	def get_url_from_entry ( entry ) : if 'url' in entry . fields : return entry . fields [ 'url' ] elif entry . type . lower ( ) == 'docushare' : return 'https://ls.st/' + entry . fields [ 'handle' ] elif 'adsurl' in entry . fields : return entry . fields [ 'adsurl' ] elif 'doi' in entry . fields : return 'https://doi.org/' + entry . fields [ 'doi' ] else : raise NoEntryUrlError ( )
3937	def submit_form ( self , form_selector , input_dict ) : logger . info ( 'Submitting form on page %r' , self . _page . url . split ( '?' ) [ 0 ] ) logger . info ( 'Page contains forms: %s' , [ elem . get ( 'id' ) for elem in self . _page . soup . select ( 'form' ) ] ) try : form = self . _page . soup . select ( form_selector ) [ 0 ] except IndexError : raise GoogleAuthError ( 'Failed to find form {!r} in page' . format ( form_selector ) ) logger . info ( 'Page contains inputs: %s' , [ elem . get ( 'id' ) for elem in form . select ( 'input' ) ] ) for selector , value in input_dict . items ( ) : try : form . select ( selector ) [ 0 ] [ 'value' ] = value except IndexError : raise GoogleAuthError ( 'Failed to find input {!r} in form' . format ( selector ) ) try : self . _page = self . _browser . submit ( form , self . _page . url ) self . _page . raise_for_status ( ) except requests . RequestException as e : raise GoogleAuthError ( 'Failed to submit form: {}' . format ( e ) )
8106	def copytree ( src , dst , symlinks = False , ignore = None ) : if not os . path . exists ( dst ) : os . makedirs ( dst ) shutil . copystat ( src , dst ) lst = os . listdir ( src ) if ignore : excl = ignore ( src , lst ) lst = [ x for x in lst if x not in excl ] for item in lst : s = os . path . join ( src , item ) d = os . path . join ( dst , item ) if symlinks and os . path . islink ( s ) : if os . path . lexists ( d ) : os . remove ( d ) os . symlink ( os . readlink ( s ) , d ) try : st = os . lstat ( s ) mode = stat . S_IMODE ( st . st_mode ) os . lchmod ( d , mode ) except : pass elif os . path . isdir ( s ) : copytree ( s , d , symlinks , ignore ) else : shutil . copy2 ( s , d )
7209	def cancel ( self ) : if not self . id : raise WorkflowError ( 'Workflow is not running. Cannot cancel.' ) if self . batch_values : self . workflow . batch_workflow_cancel ( self . id ) else : self . workflow . cancel ( self . id )
4717	def tsuite_exit ( trun , tsuite ) : if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:tsuite:exit" ) rcode = 0 for hook in reversed ( tsuite [ "hooks" ] [ "exit" ] ) : rcode = script_run ( trun , hook ) if rcode : break if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:tsuite:exit { rcode: %r } " % rcode , rcode ) return rcode
6489	def _process_field_queries ( field_dictionary ) : def field_item ( field ) : return { "match" : { field : field_dictionary [ field ] } } return [ field_item ( field ) for field in field_dictionary ]
1252	def add_random_tile ( self ) : x_pos , y_pos = np . where ( self . _state == 0 ) assert len ( x_pos ) != 0 empty_index = np . random . choice ( len ( x_pos ) ) value = np . random . choice ( [ 1 , 2 ] , p = [ 0.9 , 0.1 ] ) self . _state [ x_pos [ empty_index ] , y_pos [ empty_index ] ] = value
5692	def update ( self , new_labels , departure_time_backup = None ) : if self . _closed : raise RuntimeError ( "Profile is closed, no updates can be made" ) try : departure_time = next ( iter ( new_labels ) ) . departure_time except StopIteration : departure_time = departure_time_backup self . _check_dep_time_is_valid ( departure_time ) for new_label in new_labels : assert ( new_label . departure_time == departure_time ) dep_time_index = self . dep_times_to_index [ departure_time ] if dep_time_index > 0 : mod_prev_labels = [ label . get_copy_with_specified_departure_time ( departure_time ) for label in self . _label_bags [ dep_time_index - 1 ] ] else : mod_prev_labels = list ( ) mod_prev_labels += self . _label_bags [ dep_time_index ] walk_label = self . _get_label_to_target ( departure_time ) if walk_label : new_labels = new_labels + [ walk_label ] new_frontier = merge_pareto_frontiers ( new_labels , mod_prev_labels ) self . _label_bags [ dep_time_index ] = new_frontier return True
8846	def update_terminal_colors ( self ) : self . color_scheme = self . create_color_scheme ( background = self . syntax_highlighter . color_scheme . background , foreground = self . syntax_highlighter . color_scheme . formats [ 'normal' ] . foreground ( ) . color ( ) )
12454	def create_env ( env , args , recreate = False , ignore_activated = False , quiet = False ) : cmd = None result = True inside_env = hasattr ( sys , 'real_prefix' ) or os . environ . get ( 'VIRTUAL_ENV' ) env_exists = os . path . isdir ( env ) if not quiet : print_message ( '== Step 1. Create virtual environment ==' ) if ( recreate or ( not inside_env and not env_exists ) ) or ( ignore_activated and not env_exists ) : cmd = ( 'virtualenv' , ) + args + ( env , ) if not cmd and not quiet : if inside_env : message = 'Working inside of virtual environment, done...' else : message = 'Virtual environment {0!r} already created, done...' print_message ( message . format ( env ) ) if cmd : with disable_error_handler ( ) : result = not run_cmd ( cmd , echo = not quiet ) if not quiet : print_message ( ) return result
12742	def get_ISBNs ( self ) : invalid_isbns = set ( self . get_invalid_ISBNs ( ) ) valid_isbns = [ self . _clean_isbn ( isbn ) for isbn in self [ "020a" ] if self . _clean_isbn ( isbn ) not in invalid_isbns ] if valid_isbns : return valid_isbns return [ self . _clean_isbn ( isbn ) for isbn in self [ "901i" ] ]
8548	def delete_firewall_rule ( self , datacenter_id , server_id , nic_id , firewall_rule_id ) : response = self . _perform_request ( url = '/datacenters/%s/servers/%s/nics/%s/firewallrules/%s' % ( datacenter_id , server_id , nic_id , firewall_rule_id ) , method = 'DELETE' ) return response
1971	def check_timers ( self ) : if self . _current is None : advance = min ( [ self . clocks ] + [ x for x in self . timers if x is not None ] ) + 1 logger . debug ( f"Advancing the clock from {self.clocks} to {advance}" ) self . clocks = advance for procid in range ( len ( self . timers ) ) : if self . timers [ procid ] is not None : if self . clocks > self . timers [ procid ] : self . procs [ procid ] . PC += self . procs [ procid ] . instruction . size self . awake ( procid )
10197	def _handle_request ( self , scheme , netloc , path , headers , body = None , method = "GET" ) : backend_url = "{}://{}{}" . format ( scheme , netloc , path ) try : response = self . http_request . request ( backend_url , method = method , body = body , headers = dict ( headers ) ) self . _return_response ( response ) except Exception as e : body = "Invalid response from backend: '{}' Server might be busy" . format ( e . message ) logging . debug ( body ) self . send_error ( httplib . SERVICE_UNAVAILABLE , body )
9928	def authenticate ( self , request , email = None , password = None , username = None ) : email = email or username try : email_instance = models . EmailAddress . objects . get ( is_verified = True , email = email ) except models . EmailAddress . DoesNotExist : return None user = email_instance . user if user . check_password ( password ) : return user return None
10534	def delete_project ( project_id ) : try : res = _pybossa_req ( 'delete' , 'project' , project_id ) if type ( res ) . __name__ == 'bool' : return True else : return res except : raise
8032	def pruneUI ( dupeList , mainPos = 1 , mainLen = 1 ) : dupeList = sorted ( dupeList ) print for pos , val in enumerate ( dupeList ) : print "%d) %s" % ( pos + 1 , val ) while True : choice = raw_input ( "[%s/%s] Keepers: " % ( mainPos , mainLen ) ) . strip ( ) if not choice : print ( "Please enter a space/comma-separated list of numbers or " "'all'." ) continue elif choice . lower ( ) == 'all' : return [ ] try : out = [ int ( x ) - 1 for x in choice . replace ( ',' , ' ' ) . split ( ) ] return [ val for pos , val in enumerate ( dupeList ) if pos not in out ] except ValueError : print ( "Invalid choice. Please enter a space/comma-separated list" "of numbers or 'all'." )
6826	def clone ( self , remote_url , path = None , use_sudo = False , user = None ) : cmd = 'git clone --quiet %s' % remote_url if path is not None : cmd = cmd + ' %s' % path if use_sudo and user is None : run_as_root ( cmd ) elif use_sudo : sudo ( cmd , user = user ) else : run ( cmd )
2395	def quadratic_weighted_kappa ( rater_a , rater_b , min_rating = None , max_rating = None ) : assert ( len ( rater_a ) == len ( rater_b ) ) rater_a = [ int ( a ) for a in rater_a ] rater_b = [ int ( b ) for b in rater_b ] if min_rating is None : min_rating = min ( rater_a + rater_b ) if max_rating is None : max_rating = max ( rater_a + rater_b ) conf_mat = confusion_matrix ( rater_a , rater_b , min_rating , max_rating ) num_ratings = len ( conf_mat ) num_scored_items = float ( len ( rater_a ) ) hist_rater_a = histogram ( rater_a , min_rating , max_rating ) hist_rater_b = histogram ( rater_b , min_rating , max_rating ) numerator = 0.0 denominator = 0.0 if ( num_ratings > 1 ) : for i in range ( num_ratings ) : for j in range ( num_ratings ) : expected_count = ( hist_rater_a [ i ] * hist_rater_b [ j ] / num_scored_items ) d = pow ( i - j , 2.0 ) / pow ( num_ratings - 1 , 2.0 ) numerator += d * conf_mat [ i ] [ j ] / num_scored_items denominator += d * expected_count / num_scored_items return 1.0 - numerator / denominator else : return 1.0
3723	def calculate ( self , T , P , zs , ws , method ) : r if method == SIMPLE : sigmas = [ i ( T ) for i in self . SurfaceTensions ] return mixing_simple ( zs , sigmas ) elif method == DIGUILIOTEJA : return Diguilio_Teja ( T = T , xs = zs , sigmas_Tb = self . sigmas_Tb , Tbs = self . Tbs , Tcs = self . Tcs ) elif method == WINTERFELDSCRIVENDAVIS : sigmas = [ i ( T ) for i in self . SurfaceTensions ] rhoms = [ 1. / i ( T , P ) for i in self . VolumeLiquids ] return Winterfeld_Scriven_Davis ( zs , sigmas , rhoms ) else : raise Exception ( 'Method not valid' )
7156	def get_operator ( self , op ) : if op in self . OPERATORS : return self . OPERATORS . get ( op ) try : n_args = len ( inspect . getargspec ( op ) [ 0 ] ) if n_args != 2 : raise TypeError except : eprint ( 'Error: invalid operator function. Operators must accept two args.' ) raise else : return op
2511	def parse_package ( self , p_term ) : if not ( p_term , self . spdx_namespace [ 'name' ] , None ) in self . graph : self . error = True self . logger . log ( 'Package must have a name.' ) self . builder . create_package ( self . doc , 'dummy_package' ) else : for _s , _p , o in self . graph . triples ( ( p_term , self . spdx_namespace [ 'name' ] , None ) ) : try : self . builder . create_package ( self . doc , six . text_type ( o ) ) except CardinalityError : self . more_than_one_error ( 'Package name' ) break self . p_pkg_vinfo ( p_term , self . spdx_namespace [ 'versionInfo' ] ) self . p_pkg_fname ( p_term , self . spdx_namespace [ 'packageFileName' ] ) self . p_pkg_suppl ( p_term , self . spdx_namespace [ 'supplier' ] ) self . p_pkg_originator ( p_term , self . spdx_namespace [ 'originator' ] ) self . p_pkg_down_loc ( p_term , self . spdx_namespace [ 'downloadLocation' ] ) self . p_pkg_homepg ( p_term , self . doap_namespace [ 'homepage' ] ) self . p_pkg_chk_sum ( p_term , self . spdx_namespace [ 'checksum' ] ) self . p_pkg_src_info ( p_term , self . spdx_namespace [ 'sourceInfo' ] ) self . p_pkg_verif_code ( p_term , self . spdx_namespace [ 'packageVerificationCode' ] ) self . p_pkg_lic_conc ( p_term , self . spdx_namespace [ 'licenseConcluded' ] ) self . p_pkg_lic_decl ( p_term , self . spdx_namespace [ 'licenseDeclared' ] ) self . p_pkg_lics_info_from_files ( p_term , self . spdx_namespace [ 'licenseInfoFromFiles' ] ) self . p_pkg_comments_on_lics ( p_term , self . spdx_namespace [ 'licenseComments' ] ) self . p_pkg_cr_text ( p_term , self . spdx_namespace [ 'copyrightText' ] ) self . p_pkg_summary ( p_term , self . spdx_namespace [ 'summary' ] ) self . p_pkg_descr ( p_term , self . spdx_namespace [ 'description' ] )
1464	def __replace ( config , wildcards , config_file ) : for config_key in config : config_value = config [ config_key ] original_value = config_value if isinstance ( config_value , str ) : for token in wildcards : if wildcards [ token ] : config_value = config_value . replace ( token , wildcards [ token ] ) found = re . findall ( r'\${[A-Z_]+}' , config_value ) if found : raise ValueError ( "%s=%s in file %s contains unsupported or unset wildcard tokens: %s" % ( config_key , original_value , config_file , ", " . join ( found ) ) ) config [ config_key ] = config_value return config
5320	def find_ports ( device ) : bus_id = device . bus dev_id = device . address for dirent in os . listdir ( USB_SYS_PREFIX ) : matches = re . match ( USB_PORTS_STR + '$' , dirent ) if matches : bus_str = readattr ( dirent , 'busnum' ) if bus_str : busnum = float ( bus_str ) else : busnum = None dev_str = readattr ( dirent , 'devnum' ) if dev_str : devnum = float ( dev_str ) else : devnum = None if busnum == bus_id and devnum == dev_id : return str ( matches . groups ( ) [ 1 ] )
6835	def vagrant_settings ( self , name = '' , * args , ** kwargs ) : config = self . ssh_config ( name ) extra_args = self . _settings_dict ( config ) kwargs . update ( extra_args ) return self . settings ( * args , ** kwargs )
7188	def get_offset_and_prefix ( body , skip_assignments = False ) : assert body . type in ( syms . file_input , syms . suite ) _offset = 0 prefix = '' for _offset , child in enumerate ( body . children ) : if child . type == syms . simple_stmt : stmt = child . children [ 0 ] if stmt . type == syms . expr_stmt : expr = stmt . children if not skip_assignments : break if ( len ( expr ) != 2 or expr [ 0 ] . type != token . NAME or expr [ 1 ] . type != syms . annassign or _eq in expr [ 1 ] . children ) : break elif stmt . type not in ( syms . import_name , syms . import_from , token . STRING ) : break elif child . type == token . INDENT : assert isinstance ( child , Leaf ) prefix = child . value elif child . type != token . NEWLINE : break prefix , child . prefix = child . prefix , prefix return _offset , prefix
5806	def parse_alert ( server_handshake_bytes ) : for record_type , _ , record_data in parse_tls_records ( server_handshake_bytes ) : if record_type != b'\x15' : continue if len ( record_data ) != 2 : return None return ( int_from_bytes ( record_data [ 0 : 1 ] ) , int_from_bytes ( record_data [ 1 : 2 ] ) ) return None
1401	def extract_packing_plan ( self , topology ) : packingPlan = { "id" : "" , "container_plans" : [ ] } if not topology . packing_plan : return packingPlan container_plans = topology . packing_plan . container_plans containers = [ ] for container_plan in container_plans : instances = [ ] for instance_plan in container_plan . instance_plans : instance_resources = { "cpu" : instance_plan . resource . cpu , "ram" : instance_plan . resource . ram , "disk" : instance_plan . resource . disk } instance = { "component_name" : instance_plan . component_name , "task_id" : instance_plan . task_id , "component_index" : instance_plan . component_index , "instance_resources" : instance_resources } instances . append ( instance ) required_resource = { "cpu" : container_plan . requiredResource . cpu , "ram" : container_plan . requiredResource . ram , "disk" : container_plan . requiredResource . disk } scheduled_resource = { } if container_plan . scheduledResource : scheduled_resource = { "cpu" : container_plan . scheduledResource . cpu , "ram" : container_plan . scheduledResource . ram , "disk" : container_plan . scheduledResource . disk } container = { "id" : container_plan . id , "instances" : instances , "required_resources" : required_resource , "scheduled_resources" : scheduled_resource } containers . append ( container ) packingPlan [ "id" ] = topology . packing_plan . id packingPlan [ "container_plans" ] = containers return json . dumps ( packingPlan )
12153	def convolve ( signal , kernel ) : pad = np . ones ( len ( kernel ) / 2 ) signal = np . concatenate ( ( pad * signal [ 0 ] , signal , pad * signal [ - 1 ] ) ) signal = np . convolve ( signal , kernel , mode = 'same' ) signal = signal [ len ( pad ) : - len ( pad ) ] return signal
10454	def stateenabled ( self , window_name , object_name ) : try : object_handle = self . _get_object_handle ( window_name , object_name ) if object_handle . AXEnabled : return 1 except LdtpServerException : pass return 0
3058	def _write_credentials_file ( credentials_file , credentials ) : data = { 'file_version' : 2 , 'credentials' : { } } for key , credential in iteritems ( credentials ) : credential_json = credential . to_json ( ) encoded_credential = _helpers . _from_bytes ( base64 . b64encode ( _helpers . _to_bytes ( credential_json ) ) ) data [ 'credentials' ] [ key ] = encoded_credential credentials_file . seek ( 0 ) json . dump ( data , credentials_file ) credentials_file . truncate ( )
7409	def get_order ( tre ) : anode = tre . tree & ">A" sister = anode . get_sisters ( ) [ 0 ] sisters = ( anode . name [ 1 : ] , sister . name [ 1 : ] ) others = [ i for i in list ( "ABCD" ) if i not in sisters ] return sorted ( sisters ) + sorted ( others )
1022	def createTMs ( includeCPP = True , includePy = True , numCols = 100 , cellsPerCol = 4 , activationThreshold = 3 , minThreshold = 3 , newSynapseCount = 3 , initialPerm = 0.6 , permanenceInc = 0.1 , permanenceDec = 0.0 , globalDecay = 0.0 , pamLength = 0 , checkSynapseConsistency = True , maxInfBacktrack = 0 , maxLrnBacktrack = 0 , ** kwargs ) : connectedPerm = 0.5 tms = dict ( ) if includeCPP : if VERBOSITY >= 2 : print "Creating BacktrackingTMCPP instance" cpp_tm = BacktrackingTMCPP ( numberOfCols = numCols , cellsPerColumn = cellsPerCol , initialPerm = initialPerm , connectedPerm = connectedPerm , minThreshold = minThreshold , newSynapseCount = newSynapseCount , permanenceInc = permanenceInc , permanenceDec = permanenceDec , activationThreshold = activationThreshold , globalDecay = globalDecay , burnIn = 1 , seed = SEED , verbosity = VERBOSITY , checkSynapseConsistency = checkSynapseConsistency , collectStats = True , pamLength = pamLength , maxInfBacktrack = maxInfBacktrack , maxLrnBacktrack = maxLrnBacktrack , ) cpp_tm . retrieveLearningStates = True tms [ 'CPP' ] = cpp_tm if includePy : if VERBOSITY >= 2 : print "Creating PY TM instance" py_tm = BacktrackingTM ( numberOfCols = numCols , cellsPerColumn = cellsPerCol , initialPerm = initialPerm , connectedPerm = connectedPerm , minThreshold = minThreshold , newSynapseCount = newSynapseCount , permanenceInc = permanenceInc , permanenceDec = permanenceDec , activationThreshold = activationThreshold , globalDecay = globalDecay , burnIn = 1 , seed = SEED , verbosity = VERBOSITY , collectStats = True , pamLength = pamLength , maxInfBacktrack = maxInfBacktrack , maxLrnBacktrack = maxLrnBacktrack , ) tms [ 'PY ' ] = py_tm return tms
4048	def key_info ( self , ** kwargs ) : query_string = "/keys/{k}" . format ( k = self . api_key ) return self . _build_query ( query_string )
4279	def reduce_opacity ( im , opacity ) : assert opacity >= 0 and opacity <= 1 if im . mode != 'RGBA' : im = im . convert ( 'RGBA' ) else : im = im . copy ( ) alpha = im . split ( ) [ 3 ] alpha = ImageEnhance . Brightness ( alpha ) . enhance ( opacity ) im . putalpha ( alpha ) return im
1666	def CheckRedundantOverrideOrFinal ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] declarator_end = line . rfind ( ')' ) if declarator_end >= 0 : fragment = line [ declarator_end : ] else : if linenum > 1 and clean_lines . elided [ linenum - 1 ] . rfind ( ')' ) >= 0 : fragment = line else : return if Search ( r'\boverride\b' , fragment ) and Search ( r'\bfinal\b' , fragment ) : error ( filename , linenum , 'readability/inheritance' , 4 , ( '"override" is redundant since function is ' 'already declared as "final"' ) )
4144	def CHOLESKY ( A , B , method = 'scipy' ) : if method == 'numpy_solver' : X = _numpy_solver ( A , B ) return X elif method == 'numpy' : X , _L = _numpy_cholesky ( A , B ) return X elif method == 'scipy' : import scipy . linalg L = scipy . linalg . cholesky ( A ) X = scipy . linalg . cho_solve ( ( L , False ) , B ) else : raise ValueError ( 'method must be numpy_solver, numpy_cholesky or cholesky_inplace' ) return X
12021	def add_line_error ( self , line_data , error_info , log_level = logging . ERROR ) : if not error_info : return try : line_data [ 'line_errors' ] . append ( error_info ) except KeyError : line_data [ 'line_errors' ] = [ error_info ] except TypeError : pass try : self . logger . log ( log_level , Gff3 . error_format . format ( current_line_num = line_data [ 'line_index' ] + 1 , error_type = error_info [ 'error_type' ] , message = error_info [ 'message' ] , line = line_data [ 'line_raw' ] . rstrip ( ) ) ) except AttributeError : pass
4317	def info ( filepath ) : info_dictionary = { 'channels' : channels ( filepath ) , 'sample_rate' : sample_rate ( filepath ) , 'bitrate' : bitrate ( filepath ) , 'duration' : duration ( filepath ) , 'num_samples' : num_samples ( filepath ) , 'encoding' : encoding ( filepath ) , 'silent' : silent ( filepath ) } return info_dictionary
11384	def module ( self ) : if not hasattr ( self , '_module' ) : if "__main__" in sys . modules : mod = sys . modules [ "__main__" ] path = self . normalize_path ( mod . __file__ ) if os . path . splitext ( path ) == os . path . splitext ( self . path ) : self . _module = mod else : self . _module = imp . load_source ( 'captain_script' , self . path ) return self . _module
2263	def dict_take ( dict_ , keys , default = util_const . NoParam ) : r if default is util_const . NoParam : for key in keys : yield dict_ [ key ] else : for key in keys : yield dict_ . get ( key , default )
5568	def bounds_at_zoom ( self , zoom = None ) : return ( ) if self . area_at_zoom ( zoom ) . is_empty else Bounds ( * self . area_at_zoom ( zoom ) . bounds )
4376	def write ( self , data ) : args = parse_qs ( self . handler . environ . get ( "QUERY_STRING" ) ) if "i" in args : i = args [ "i" ] else : i = "0" super ( JSONPolling , self ) . write ( "io.j[%s]('%s');" % ( i , data ) )
8268	def color ( self , clr = None , d = 0.035 ) : if clr != None and not isinstance ( clr , Color ) : clr = color ( clr ) if clr != None and not self . grayscale : if clr . is_black : return self . black . color ( clr , d ) if clr . is_white : return self . white . color ( clr , d ) if clr . is_grey : return choice ( ( self . black . color ( clr , d ) , self . white . color ( clr , d ) ) ) h , s , b , a = self . h , self . s , self . b , self . a if clr != None : h , a = clr . h + d * ( random ( ) * 2 - 1 ) , clr . a hsba = [ ] for v in [ h , s , b , a ] : if isinstance ( v , _list ) : min , max = choice ( v ) elif isinstance ( v , tuple ) : min , max = v else : min , max = v , v hsba . append ( min + ( max - min ) * random ( ) ) h , s , b , a = hsba return color ( h , s , b , a , mode = "hsb" )
9564	def create_validator ( ) : field_names = ( 'study_id' , 'patient_id' , 'gender' , 'age_years' , 'age_months' , 'date_inclusion' ) validator = CSVValidator ( field_names ) validator . add_header_check ( 'EX1' , 'bad header' ) validator . add_record_length_check ( 'EX2' , 'unexpected record length' ) validator . add_value_check ( 'study_id' , int , 'EX3' , 'study id must be an integer' ) validator . add_value_check ( 'patient_id' , int , 'EX4' , 'patient id must be an integer' ) validator . add_value_check ( 'gender' , enumeration ( 'M' , 'F' ) , 'EX5' , 'invalid gender' ) validator . add_value_check ( 'age_years' , number_range_inclusive ( 0 , 120 , int ) , 'EX6' , 'invalid age in years' ) validator . add_value_check ( 'date_inclusion' , datetime_string ( '%Y-%m-%d' ) , 'EX7' , 'invalid date' ) def check_age_variables ( r ) : age_years = int ( r [ 'age_years' ] ) age_months = int ( r [ 'age_months' ] ) valid = ( age_months >= age_years * 12 and age_months % age_years < 12 ) if not valid : raise RecordError ( 'EX8' , 'invalid age variables' ) validator . add_record_check ( check_age_variables ) return validator
8647	def accept_milestone_request ( session , milestone_request_id ) : params_data = { 'action' : 'accept' , } endpoint = 'milestone_requests/{}' . format ( milestone_request_id ) response = make_put_request ( session , endpoint , params_data = params_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise MilestoneRequestNotAcceptedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
9864	def get_home ( self , home_id ) : if home_id not in self . _all_home_ids : _LOGGER . error ( "Could not find any Tibber home with id: %s" , home_id ) return None if home_id not in self . _homes . keys ( ) : self . _homes [ home_id ] = TibberHome ( home_id , self ) return self . _homes [ home_id ]
7337	def predict_subsequences ( self , sequence_dict , peptide_lengths = None ) : sequence_dict = check_sequence_dictionary ( sequence_dict ) peptide_lengths = self . _check_peptide_lengths ( peptide_lengths ) binding_predictions = [ ] expected_peptides = set ( [ ] ) normalized_alleles = [ ] for key , amino_acid_sequence in sequence_dict . items ( ) : for l in peptide_lengths : for i in range ( len ( amino_acid_sequence ) - l + 1 ) : expected_peptides . add ( amino_acid_sequence [ i : i + l ] ) self . _check_peptide_inputs ( expected_peptides ) for allele in self . alleles : allele = normalize_allele_name ( allele , omit_dra1 = True ) normalized_alleles . append ( allele ) request = self . _get_iedb_request_params ( amino_acid_sequence , allele ) logger . info ( "Calling IEDB (%s) with request %s" , self . url , request ) response_df = _query_iedb ( request , self . url ) for _ , row in response_df . iterrows ( ) : binding_predictions . append ( BindingPrediction ( source_sequence_name = key , offset = row [ 'start' ] - 1 , allele = row [ 'allele' ] , peptide = row [ 'peptide' ] , affinity = row [ 'ic50' ] , percentile_rank = row [ 'rank' ] , prediction_method_name = "iedb-" + self . prediction_method ) ) self . _check_results ( binding_predictions , alleles = normalized_alleles , peptides = expected_peptides ) return BindingPredictionCollection ( binding_predictions )
12178	def ensureDetection ( self ) : if self . APs == False : self . log . debug ( "analysis attempted before event detection..." ) self . detect ( )
2897	def cancel ( self , success = False ) : self . success = success cancel = [ ] mask = Task . NOT_FINISHED_MASK for task in Task . Iterator ( self . task_tree , mask ) : cancel . append ( task ) for task in cancel : task . cancel ( )
10975	def manage ( group_id ) : group = Group . query . get_or_404 ( group_id ) form = GroupForm ( request . form , obj = group ) if form . validate_on_submit ( ) : if group . can_edit ( current_user ) : try : group . update ( ** form . data ) flash ( _ ( 'Group "%(name)s" was updated' , name = group . name ) , 'success' ) except Exception as e : flash ( str ( e ) , 'error' ) return render_template ( "invenio_groups/new.html" , form = form , group = group , ) else : flash ( _ ( 'You cannot edit group %(group_name)s' , group_name = group . name ) , 'error' ) return render_template ( "invenio_groups/new.html" , form = form , group = group , )
1856	def BSR ( cpu , dest , src ) : value = src . read ( ) flag = Operators . EXTRACT ( value , src . size - 1 , 1 ) == 1 res = 0 for pos in reversed ( range ( 0 , src . size ) ) : res = Operators . ITEBV ( dest . size , flag , res , pos ) flag = Operators . OR ( flag , ( Operators . EXTRACT ( value , pos , 1 ) == 1 ) ) cpu . PF = cpu . _calculate_parity_flag ( res ) cpu . ZF = value == 0 dest . write ( Operators . ITEBV ( dest . size , cpu . ZF , dest . read ( ) , res ) )
8198	def angle ( x1 , y1 , x2 , y2 ) : sign = 1.0 usign = ( x1 * y2 - y1 * x2 ) if usign < 0 : sign = - 1.0 num = x1 * x2 + y1 * y2 den = hypot ( x1 , y1 ) * hypot ( x2 , y2 ) ratio = min ( max ( num / den , - 1.0 ) , 1.0 ) return sign * degrees ( acos ( ratio ) )
2077	def secho ( message , ** kwargs ) : if not settings . color : for key in ( 'fg' , 'bg' , 'bold' , 'blink' ) : kwargs . pop ( key , None ) return click . secho ( message , ** kwargs )
4592	def colors_no_palette ( colors = None , ** kwds ) : if isinstance ( colors , str ) : colors = _split_colors ( colors ) else : colors = to_triplets ( colors or ( ) ) colors = ( color ( c ) for c in colors or ( ) ) return palette . Palette ( colors , ** kwds )
9465	def conference_record_start ( self , call_params ) : path = '/' + self . api_version + '/ConferenceRecordStart/' method = 'POST' return self . request ( path , method , call_params )
13850	def ensure_dir_exists ( func ) : "wrap a function that returns a dir, making sure it exists" @ functools . wraps ( func ) def make_if_not_present ( ) : dir = func ( ) if not os . path . isdir ( dir ) : os . makedirs ( dir ) return dir return make_if_not_present
7137	def format ( obj , options ) : formatters = { float_types : lambda x : '{:.{}g}' . format ( x , options . digits ) , } for _types , fmtr in formatters . items ( ) : if isinstance ( obj , _types ) : return fmtr ( obj ) try : if six . PY2 and isinstance ( obj , six . string_types ) : return str ( obj . encode ( 'utf-8' ) ) return str ( obj ) except : return 'OBJECT'
1093	def split ( pattern , string , maxsplit = 0 , flags = 0 ) : return _compile ( pattern , flags ) . split ( string , maxsplit )
13119	def count ( self , * args , ** kwargs ) : search = self . create_search ( * args , ** kwargs ) try : return search . count ( ) except NotFoundError : print_error ( "The index was not found, have you initialized the index?" ) except ( ConnectionError , TransportError ) : print_error ( "Cannot connect to elasticsearch" )
8722	def operation_list ( uploader ) : files = uploader . file_list ( ) for f in files : log . info ( "{file:30s} {size}" . format ( file = f [ 0 ] , size = f [ 1 ] ) )
3078	def email ( self ) : if not self . credentials : return None try : return self . credentials . id_token [ 'email' ] except KeyError : current_app . logger . error ( 'Invalid id_token {0}' . format ( self . credentials . id_token ) )
12123	def validate_activatable_models ( ) : for model in get_activatable_models ( ) : activatable_field = next ( ( f for f in model . _meta . fields if f . __class__ == models . BooleanField and f . name == model . ACTIVATABLE_FIELD_NAME ) , None ) if activatable_field is None : raise ValidationError ( ( 'Model {0} is an activatable model. It must define an activatable BooleanField that ' 'has a field name of model.ACTIVATABLE_FIELD_NAME (which defaults to is_active)' . format ( model ) ) ) if not model . ALLOW_CASCADE_DELETE : for field in model . _meta . fields : if field . __class__ in ( models . ForeignKey , models . OneToOneField ) : if field . remote_field . on_delete == models . CASCADE : raise ValidationError ( ( 'Model {0} is an activatable model. All ForeignKey and OneToOneFields ' 'must set on_delete methods to something other than CASCADE (the default). ' 'If you want to explicitely allow cascade deletes, then you must set the ' 'ALLOW_CASCADE_DELETE=True class variable on your model.' ) . format ( model ) )
12708	def body_to_world ( self , position ) : return np . array ( self . ode_body . getRelPointPos ( tuple ( position ) ) )
11702	def cosine ( vec1 , vec2 ) : if norm ( vec1 ) > 0 and norm ( vec2 ) > 0 : return dot ( vec1 , vec2 ) / ( norm ( vec1 ) * norm ( vec2 ) ) else : return 0.0
7487	def concat_multiple_inputs ( data , sample ) : if len ( sample . files . fastqs ) > 1 : cmd1 = [ "cat" ] + [ i [ 0 ] for i in sample . files . fastqs ] isgzip = ".gz" if not sample . files . fastqs [ 0 ] [ 0 ] . endswith ( ".gz" ) : isgzip = "" conc1 = os . path . join ( data . dirs . edits , sample . name + "_R1_concat.fq{}" . format ( isgzip ) ) with open ( conc1 , 'w' ) as cout1 : proc1 = sps . Popen ( cmd1 , stderr = sps . STDOUT , stdout = cout1 , close_fds = True ) res1 = proc1 . communicate ( ) [ 0 ] if proc1 . returncode : raise IPyradWarningExit ( "error in: {}, {}" . format ( cmd1 , res1 ) ) conc2 = 0 if "pair" in data . paramsdict [ "datatype" ] : cmd2 = [ "cat" ] + [ i [ 1 ] for i in sample . files . fastqs ] conc2 = os . path . join ( data . dirs . edits , sample . name + "_R2_concat.fq{}" . format ( isgzip ) ) with open ( conc2 , 'w' ) as cout2 : proc2 = sps . Popen ( cmd2 , stderr = sps . STDOUT , stdout = cout2 , close_fds = True ) res2 = proc2 . communicate ( ) [ 0 ] if proc2 . returncode : raise IPyradWarningExit ( "Error concatenating fastq files. Make sure all " + "these files exist: {}\nError message: {}" . format ( cmd2 , proc2 . returncode ) ) sample . files . concat = [ ( conc1 , conc2 ) ] return sample . files . concat
5470	def _prepare_summary_table ( rows ) : if not rows : return [ ] key_field = 'job-name' if key_field not in rows [ 0 ] : key_field = 'job-id' grouped = collections . defaultdict ( lambda : collections . defaultdict ( lambda : [ ] ) ) for row in rows : grouped [ row . get ( key_field , '' ) ] [ row . get ( 'status' , '' ) ] += [ row ] new_rows = [ ] for job_key in sorted ( grouped . keys ( ) ) : group = grouped . get ( job_key , None ) canonical_status = [ 'RUNNING' , 'SUCCESS' , 'FAILURE' , 'CANCEL' ] for status in canonical_status + sorted ( group . keys ( ) ) : if status not in group : continue task_count = len ( group [ status ] ) del group [ status ] if task_count : summary_row = collections . OrderedDict ( ) summary_row [ key_field ] = job_key summary_row [ 'status' ] = status summary_row [ 'task-count' ] = task_count new_rows . append ( summary_row ) return new_rows
8535	def pop ( self , nbytes ) : size = 0 popped = [ ] with self . _lock_packets : while size < nbytes : try : packet = self . _packets . pop ( 0 ) size += len ( packet . data . data ) self . _remaining -= len ( packet . data . data ) popped . append ( packet ) except IndexError : break return popped
518	def _raisePermanenceToThreshold ( self , perm , mask ) : if len ( mask ) < self . _stimulusThreshold : raise Exception ( "This is likely due to a " + "value of stimulusThreshold that is too large relative " + "to the input size. [len(mask) < self._stimulusThreshold]" ) numpy . clip ( perm , self . _synPermMin , self . _synPermMax , out = perm ) while True : numConnected = numpy . nonzero ( perm > self . _synPermConnected - PERMANENCE_EPSILON ) [ 0 ] . size if numConnected >= self . _stimulusThreshold : return perm [ mask ] += self . _synPermBelowStimulusInc
10584	def remove_account ( self , name ) : acc_to_remove = None for a in self . accounts : if a . name == name : acc_to_remove = a if acc_to_remove is not None : self . accounts . remove ( acc_to_remove )
4929	def transform_launch_points ( self , content_metadata_item ) : return [ { 'providerID' : self . enterprise_configuration . provider_id , 'launchURL' : content_metadata_item [ 'enrollment_url' ] , 'contentTitle' : content_metadata_item [ 'title' ] , 'contentID' : self . get_content_id ( content_metadata_item ) , 'launchType' : 3 , 'mobileEnabled' : True , 'mobileLaunchURL' : content_metadata_item [ 'enrollment_url' ] , } ]
8510	def _predict ( self , X , method = 'fprop' ) : import theano X_sym = self . trainer . model . get_input_space ( ) . make_theano_batch ( ) y_sym = getattr ( self . trainer . model , method ) ( X_sym ) f = theano . function ( [ X_sym ] , y_sym , allow_input_downcast = True ) return f ( X )
8446	def switch ( template , version ) : temple . update . update ( new_template = template , new_version = version )
10660	def mass_fractions ( amounts ) : m = masses ( amounts ) m_total = sum ( m . values ( ) ) return { compound : m [ compound ] / m_total for compound in m . keys ( ) }
8546	def delete_datacenter ( self , datacenter_id ) : response = self . _perform_request ( url = '/datacenters/%s' % ( datacenter_id ) , method = 'DELETE' ) return response
2942	def _add_notify ( self , task_spec ) : if task_spec . name in self . task_specs : raise KeyError ( 'Duplicate task spec name: ' + task_spec . name ) self . task_specs [ task_spec . name ] = task_spec task_spec . id = len ( self . task_specs )
10704	def get_device ( _id ) : url = DEVICE_URL % _id arequest = requests . get ( url , headers = HEADERS ) status_code = str ( arequest . status_code ) if status_code == '401' : _LOGGER . error ( "Token expired." ) return False return arequest . json ( )
4052	def dump ( self , itemkey , filename = None , path = None ) : if not filename : filename = self . item ( itemkey ) [ "data" ] [ "filename" ] if path : pth = os . path . join ( path , filename ) else : pth = filename file = self . file ( itemkey ) if self . snapshot : self . snapshot = False pth = pth + ".zip" with open ( pth , "wb" ) as f : f . write ( file )
4043	def _build_query ( self , query_string , no_params = False ) : try : query = quote ( query_string . format ( u = self . library_id , t = self . library_type ) ) except KeyError as err : raise ze . ParamNotPassed ( "There's a request parameter missing: %s" % err ) if no_params is False : if not self . url_params : self . add_parameters ( ) query = "%s?%s" % ( query , self . url_params ) return query
5795	def cf_dictionary_to_dict ( dictionary ) : dict_length = CoreFoundation . CFDictionaryGetCount ( dictionary ) keys = ( CFTypeRef * dict_length ) ( ) values = ( CFTypeRef * dict_length ) ( ) CoreFoundation . CFDictionaryGetKeysAndValues ( dictionary , _cast_pointer_p ( keys ) , _cast_pointer_p ( values ) ) output = { } for index in range ( 0 , dict_length ) : output [ CFHelpers . native ( keys [ index ] ) ] = CFHelpers . native ( values [ index ] ) return output
2807	def convert_gemm ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting Linear ...' ) if names == 'short' : tf_name = 'FC' + random_string ( 6 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) bias_name = '{0}.bias' . format ( w_name ) weights_name = '{0}.weight' . format ( w_name ) W = weights [ weights_name ] . numpy ( ) . transpose ( ) input_channels , output_channels = W . shape keras_weights = [ W ] has_bias = False if bias_name in weights : bias = weights [ bias_name ] . numpy ( ) keras_weights = [ W , bias ] has_bias = True dense = keras . layers . Dense ( output_channels , weights = keras_weights , use_bias = has_bias , name = tf_name , bias_initializer = 'zeros' , kernel_initializer = 'zeros' , ) layers [ scope_name ] = dense ( layers [ inputs [ 0 ] ] )
13276	def update_desc_rsib_path ( desc , sibs_len ) : if ( desc [ 'sib_seq' ] < ( sibs_len - 1 ) ) : rsib_path = copy . deepcopy ( desc [ 'path' ] ) rsib_path [ - 1 ] = desc [ 'sib_seq' ] + 1 desc [ 'rsib_path' ] = rsib_path else : pass return ( desc )
12600	def concat_sheets ( xl_path : str , sheetnames = None , add_tab_names = False ) : xl_path , choice = _check_xl_path ( xl_path ) if sheetnames is None : sheetnames = get_sheet_list ( xl_path ) sheets = pd . read_excel ( xl_path , sheetname = sheetnames ) if add_tab_names : for tab in sheets : sheets [ tab ] [ 'Tab' ] = [ tab ] * len ( sheets [ tab ] ) return pd . concat ( [ sheets [ tab ] for tab in sheets ] )
6187	def get_last_commit ( git_path = None ) : if git_path is None : git_path = GIT_PATH line = get_last_commit_line ( git_path ) revision_id = line . split ( ) [ 1 ] return revision_id
3280	def resolve_provider ( self , path ) : share = None lower_path = path . lower ( ) for r in self . sorted_share_list : if r == "/" : share = r break elif lower_path == r or lower_path . startswith ( r + "/" ) : share = r break if share is None : return None , None return share , self . provider_map . get ( share )
11740	def first ( self , symbols ) : ret = set ( ) if EPSILON in symbols : return set ( [ EPSILON ] ) for symbol in symbols : ret |= self . _first [ symbol ] - set ( [ EPSILON ] ) if EPSILON not in self . _first [ symbol ] : break else : ret . add ( EPSILON ) return ret
3003	def start ( self ) : super ( JupyterTensorboardApp , self ) . start ( ) subcmds = ", " . join ( sorted ( self . subcommands ) ) sys . exit ( "Please supply at least one subcommand: %s" % subcmds )
2311	def b_fit_score ( self , x , y ) : x = np . reshape ( minmax_scale ( x ) , ( - 1 , 1 ) ) y = np . reshape ( minmax_scale ( y ) , ( - 1 , 1 ) ) poly = PolynomialFeatures ( degree = self . degree ) poly_x = poly . fit_transform ( x ) poly_x [ : , 1 ] = 0 poly_x [ : , 2 ] = 0 regressor = LinearRegression ( ) regressor . fit ( poly_x , y ) y_predict = regressor . predict ( poly_x ) error = mean_squared_error ( y_predict , y ) return error
13804	def _generate_token ( self , length = 32 ) : return '' . join ( choice ( ascii_letters + digits ) for x in range ( length ) )
8740	def _create_flip ( context , flip , port_fixed_ips ) : if port_fixed_ips : context . session . begin ( ) try : ports = [ val [ 'port' ] for val in port_fixed_ips . values ( ) ] flip = db_api . port_associate_ip ( context , ports , flip , port_fixed_ips . keys ( ) ) for port_id in port_fixed_ips : fixed_ip = port_fixed_ips [ port_id ] [ 'fixed_ip' ] flip = db_api . floating_ip_associate_fixed_ip ( context , flip , fixed_ip ) flip_driver = registry . DRIVER_REGISTRY . get_driver ( ) flip_driver . register_floating_ip ( flip , port_fixed_ips ) context . session . commit ( ) except Exception : context . session . rollback ( ) raise billing . notify ( context , billing . IP_ASSOC , flip )
3960	def update_local_repo_async ( self , task_queue , force = False ) : self . ensure_local_repo ( ) task_queue . enqueue_task ( self . update_local_repo , force = force )
11518	def generate_upload_token ( self , token , item_id , filename , checksum = None ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'itemid' ] = item_id parameters [ 'filename' ] = filename if checksum is not None : parameters [ 'checksum' ] = checksum response = self . request ( 'midas.upload.generatetoken' , parameters ) return response [ 'token' ]
11044	def create_marathon_acme ( client_creator , cert_store , acme_email , allow_multiple_certs , marathon_addrs , marathon_timeout , sse_timeout , mlb_addrs , group , reactor ) : marathon_client = MarathonClient ( marathon_addrs , timeout = marathon_timeout , sse_kwargs = { 'timeout' : sse_timeout } , reactor = reactor ) marathon_lb_client = MarathonLbClient ( mlb_addrs , reactor = reactor ) return MarathonAcme ( marathon_client , group , cert_store , marathon_lb_client , client_creator , reactor , acme_email , allow_multiple_certs )
11815	def decode ( self , ciphertext ) : "Search for a decoding of the ciphertext." self . ciphertext = ciphertext problem = PermutationDecoderProblem ( decoder = self ) return search . best_first_tree_search ( problem , lambda node : self . score ( node . state ) )
13626	def Delimited ( value , parser = Text , delimiter = u',' , encoding = None ) : value = Text ( value , encoding ) if value is None or value == u'' : return [ ] return map ( parser , value . split ( delimiter ) )
4206	def levup ( acur , knxt , ecur = None ) : if acur [ 0 ] != 1 : raise ValueError ( 'At least one of the reflection coefficients is equal to one.' ) acur = acur [ 1 : ] anxt = numpy . concatenate ( ( acur , [ 0 ] ) ) + knxt * numpy . concatenate ( ( numpy . conj ( acur [ - 1 : : - 1 ] ) , [ 1 ] ) ) enxt = None if ecur is not None : enxt = ( 1. - numpy . dot ( numpy . conj ( knxt ) , knxt ) ) * ecur anxt = numpy . insert ( anxt , 0 , 1 ) return anxt , enxt
6003	def sub_to_pix ( self ) : return mapper_util . voronoi_sub_to_pix_from_grids_and_geometry ( sub_grid = self . grid_stack . sub , regular_to_nearest_pix = self . grid_stack . pix . regular_to_nearest_pix , sub_to_regular = self . grid_stack . sub . sub_to_regular , pixel_centres = self . geometry . pixel_centres , pixel_neighbors = self . geometry . pixel_neighbors , pixel_neighbors_size = self . geometry . pixel_neighbors_size ) . astype ( 'int' )
11302	def provider_for_url ( self , url ) : for provider , regex in self . get_registry ( ) . items ( ) : if re . match ( regex , url ) is not None : return provider raise OEmbedMissingEndpoint ( 'No endpoint matches URL: %s' % url )
10317	def single_run_arrays ( spanning_cluster = True , ** kwargs ) : r kwargs [ 'copy_result' ] = False ret = dict ( ) for n , state in enumerate ( sample_states ( spanning_cluster = spanning_cluster , ** kwargs ) ) : if 'N' in ret : assert ret [ 'N' ] == state [ 'N' ] else : ret [ 'N' ] = state [ 'N' ] if 'M' in ret : assert ret [ 'M' ] == state [ 'M' ] else : ret [ 'M' ] = state [ 'M' ] number_of_states = state [ 'M' ] + 1 max_cluster_size = np . empty ( number_of_states ) if spanning_cluster : has_spanning_cluster = np . empty ( number_of_states , dtype = np . bool ) moments = np . empty ( ( 5 , number_of_states ) ) max_cluster_size [ n ] = state [ 'max_cluster_size' ] for k in range ( 5 ) : moments [ k , n ] = state [ 'moments' ] [ k ] if spanning_cluster : has_spanning_cluster [ n ] = state [ 'has_spanning_cluster' ] ret [ 'max_cluster_size' ] = max_cluster_size ret [ 'moments' ] = moments if spanning_cluster : ret [ 'has_spanning_cluster' ] = has_spanning_cluster return ret
4160	def _get_data ( url ) : if url . startswith ( 'http://' ) : try : resp = urllib . urlopen ( url ) encoding = resp . headers . dict . get ( 'content-encoding' , 'plain' ) except AttributeError : resp = urllib . request . urlopen ( url ) encoding = resp . headers . get ( 'content-encoding' , 'plain' ) data = resp . read ( ) if encoding == 'plain' : pass elif encoding == 'gzip' : data = StringIO ( data ) data = gzip . GzipFile ( fileobj = data ) . read ( ) else : raise RuntimeError ( 'unknown encoding' ) else : with open ( url , 'r' ) as fid : data = fid . read ( ) return data
7127	def find_and_patch_entry ( soup , entry ) : link = soup . find ( "a" , { "class" : "headerlink" } , href = "#" + entry . anchor ) tag = soup . new_tag ( "a" ) tag [ "name" ] = APPLE_REF_TEMPLATE . format ( entry . type , entry . name ) if link : link . parent . insert ( 0 , tag ) return True elif entry . anchor . startswith ( "module-" ) : soup . h1 . parent . insert ( 0 , tag ) return True else : return False
8847	def mouseMoveEvent ( self , e ) : super ( PyInteractiveConsole , self ) . mouseMoveEvent ( e ) cursor = self . cursorForPosition ( e . pos ( ) ) assert isinstance ( cursor , QtGui . QTextCursor ) p = cursor . positionInBlock ( ) usd = cursor . block ( ) . userData ( ) if usd and usd . start_pos_in_block <= p <= usd . end_pos_in_block : if QtWidgets . QApplication . overrideCursor ( ) is None : QtWidgets . QApplication . setOverrideCursor ( QtGui . QCursor ( QtCore . Qt . PointingHandCursor ) ) else : if QtWidgets . QApplication . overrideCursor ( ) is not None : QtWidgets . QApplication . restoreOverrideCursor ( )
11449	def send ( self , recipient , message ) : if self . _logindata [ 'login_rufnummer' ] is None or self . _logindata [ 'login_passwort' ] is None : err_mess = "YesssSMS: Login data required" raise self . LoginError ( err_mess ) if not recipient : raise self . NoRecipientError ( "YesssSMS: recipient number missing" ) if not isinstance ( recipient , str ) : raise ValueError ( "YesssSMS: str expected as recipient number" ) if not message : raise self . EmptyMessageError ( "YesssSMS: message is empty" ) with self . _login ( requests . Session ( ) ) as sess : sms_data = { 'to_nummer' : recipient , 'nachricht' : message } req = sess . post ( self . _websms_url , data = sms_data ) if not ( req . status_code == 200 or req . status_code == 302 ) : raise self . SMSSendingError ( "YesssSMS: error sending SMS" ) if _UNSUPPORTED_CHARS_STRING in req . text : raise self . UnsupportedCharsError ( "YesssSMS: message contains unsupported character(s)" ) if _SMS_SENDING_SUCCESSFUL_STRING not in req . text : raise self . SMSSendingError ( "YesssSMS: error sending SMS" ) sess . get ( self . _logout_url )
10140	def check_max_filesize ( chosen_file , max_size ) : if os . path . getsize ( chosen_file ) > max_size : return False else : return True
5153	def evaluate_vars ( data , context = None ) : context = context or { } if isinstance ( data , ( dict , list ) ) : if isinstance ( data , dict ) : loop_items = data . items ( ) elif isinstance ( data , list ) : loop_items = enumerate ( data ) for key , value in loop_items : data [ key ] = evaluate_vars ( value , context ) elif isinstance ( data , six . string_types ) : vars_found = var_pattern . findall ( data ) for var in vars_found : var = var . strip ( ) if len ( vars_found ) > 1 : pattern = r'\{\{(\s*%s\s*)\}\}' % var else : pattern = var_pattern if var in context : data = re . sub ( pattern , context [ var ] , data ) return data
3790	def property_derivative_P ( self , T , P , zs , ws , order = 1 ) : r sorted_valid_methods = self . select_valid_methods ( T , P , zs , ws ) for method in sorted_valid_methods : try : return self . calculate_derivative_P ( P , T , zs , ws , method , order ) except : pass return None
1129	def Newline ( loc = None ) : @ llrule ( loc , lambda parser : [ "newline" ] ) def rule ( parser ) : result = parser . _accept ( "newline" ) if result is unmatched : return result return [ ] return rule
8217	def trigger_fullscreen_action ( self , fullscreen ) : action = self . action_group . get_action ( 'fullscreen' ) action . set_active ( fullscreen )
6331	def encode ( self , word , terminator = '\0' ) : r if word : if terminator in word : raise ValueError ( 'Specified terminator, {}, already in word.' . format ( terminator if terminator != '\0' else '\\0' ) ) else : word += terminator wordlist = sorted ( word [ i : ] + word [ : i ] for i in range ( len ( word ) ) ) return '' . join ( [ w [ - 1 ] for w in wordlist ] ) else : return terminator
12135	def directory ( cls , directory , root = None , extension = None , ** kwargs ) : root = os . getcwd ( ) if root is None else root suffix = '' if extension is None else '.' + extension . rsplit ( '.' ) [ - 1 ] pattern = directory + os . sep + '*' + suffix key = os . path . join ( root , directory , '*' ) . rsplit ( os . sep ) [ - 2 ] format_parse = list ( string . Formatter ( ) . parse ( key ) ) if not all ( [ el is None for el in zip ( * format_parse ) [ 1 ] ] ) : raise Exception ( 'Directory cannot contain format field specifications' ) return cls ( key , pattern , root , ** kwargs )
2059	def disassemble_instruction ( self , code , pc ) : return next ( self . disasm . disasm ( code , pc ) )
13855	def getTextFromNode ( node ) : t = "" for n in node . childNodes : if n . nodeType == n . TEXT_NODE : t += n . nodeValue else : raise NotTextNodeError return t
10509	def stoplog ( self ) : if self . _file_logger : self . logger . removeHandler ( _file_logger ) self . _file_logger = None return 1
2134	def _get_schema ( self , wfjt_id ) : node_res = get_resource ( 'node' ) node_results = node_res . list ( workflow_job_template = wfjt_id , all_pages = True ) [ 'results' ] return self . _workflow_node_structure ( node_results )
1661	def ExpectingFunctionArgs ( clean_lines , linenum ) : line = clean_lines . elided [ linenum ] return ( Match ( r'^\s*MOCK_(CONST_)?METHOD\d+(_T)?\(' , line ) or ( linenum >= 2 and ( Match ( r'^\s*MOCK_(?:CONST_)?METHOD\d+(?:_T)?\((?:\S+,)?\s*$' , clean_lines . elided [ linenum - 1 ] ) or Match ( r'^\s*MOCK_(?:CONST_)?METHOD\d+(?:_T)?\(\s*$' , clean_lines . elided [ linenum - 2 ] ) or Search ( r'\bstd::m?function\s*\<\s*$' , clean_lines . elided [ linenum - 1 ] ) ) ) )
7240	def window_cover ( self , window_shape , pad = True ) : size_y , size_x = window_shape [ 0 ] , window_shape [ 1 ] _ndepth , _nheight , _nwidth = self . shape nheight , _m = divmod ( _nheight , size_y ) nwidth , _n = divmod ( _nwidth , size_x ) img = self if pad is True : new_height , new_width = _nheight , _nwidth if _m != 0 : new_height = ( nheight + 1 ) * size_y if _n != 0 : new_width = ( nwidth + 1 ) * size_x if ( new_height , new_width ) != ( _nheight , _nwidth ) : bounds = box ( 0 , 0 , new_width , new_height ) geom = ops . transform ( self . __geo_transform__ . fwd , bounds ) img = self [ geom ] row_lims = range ( 0 , img . shape [ 1 ] , size_y ) col_lims = range ( 0 , img . shape [ 2 ] , size_x ) for maxy , maxx in product ( row_lims , col_lims ) : reg = img [ : , maxy : ( maxy + size_y ) , maxx : ( maxx + size_x ) ] if pad is False : if reg . shape [ 1 : ] == window_shape : yield reg else : yield reg
13651	def get_reference_data ( self , modified_since : Optional [ datetime . datetime ] = None ) -> GetReferenceDataResponse : if modified_since is None : modified_since = datetime . datetime ( year = 2010 , month = 1 , day = 1 ) response = requests . get ( '{}/lovs' . format ( API_URL_BASE ) , headers = { 'if-modified-since' : self . _format_dt ( modified_since ) , ** self . _get_headers ( ) , } , timeout = self . _timeout , ) if not response . ok : raise FuelCheckError . create ( response ) return GetReferenceDataResponse . deserialize ( response . json ( ) )
7760	def _call_timeout_handlers ( self ) : sources_handled = 0 now = time . time ( ) schedule = None while self . _timeout_handlers : schedule , handler = self . _timeout_handlers [ 0 ] if schedule <= now : logger . debug ( "About to call a timeout handler: {0!r}" . format ( handler ) ) self . _timeout_handlers = self . _timeout_handlers [ 1 : ] result = handler ( ) logger . debug ( " handler result: {0!r}" . format ( result ) ) rec = handler . _pyxmpp_recurring if rec : logger . debug ( " recurring, restarting in {0} s" . format ( handler . _pyxmpp_timeout ) ) self . _timeout_handlers . append ( ( now + handler . _pyxmpp_timeout , handler ) ) self . _timeout_handlers . sort ( key = lambda x : x [ 0 ] ) elif rec is None and result is not None : logger . debug ( " auto-recurring, restarting in {0} s" . format ( result ) ) self . _timeout_handlers . append ( ( now + result , handler ) ) self . _timeout_handlers . sort ( key = lambda x : x [ 0 ] ) sources_handled += 1 else : break if self . check_events ( ) : return 0 , sources_handled if self . _timeout_handlers and schedule : timeout = schedule - now else : timeout = None return timeout , sources_handled
9198	def pop ( self , key , default = _sentinel ) : if default is not _sentinel : tup = self . _data . pop ( key . lower ( ) , default ) else : tup = self . _data . pop ( key . lower ( ) ) if tup is not default : return tup [ 1 ] else : return default
1545	def get_topology_info ( * args ) : instance = tornado . ioloop . IOLoop . instance ( ) try : return instance . run_sync ( lambda : API . get_topology_info ( * args ) ) except Exception : Log . debug ( traceback . format_exc ( ) ) raise
2557	def clean_attribute ( attribute ) : attribute = { 'cls' : 'class' , 'className' : 'class' , 'class_name' : 'class' , 'fr' : 'for' , 'html_for' : 'for' , 'htmlFor' : 'for' , } . get ( attribute , attribute ) if attribute [ 0 ] == '_' : attribute = attribute [ 1 : ] if attribute in set ( [ 'http_equiv' ] ) or attribute . startswith ( 'data_' ) : attribute = attribute . replace ( '_' , '-' ) . lower ( ) if attribute . split ( '_' ) [ 0 ] in ( 'xlink' , 'xml' , 'xmlns' ) : attribute = attribute . replace ( '_' , ':' , 1 ) . lower ( ) return attribute
7541	def nfilter1 ( data , reps ) : if sum ( reps ) >= data . paramsdict [ "mindepth_majrule" ] and sum ( reps ) <= data . paramsdict [ "maxdepth" ] : return 1 else : return 0
7594	def get_player ( self , * tags : crtag , ** params : keys ) : url = self . api . PLAYER + '/' + ',' . join ( tags ) return self . _get_model ( url , FullPlayer , ** params )
13385	def store_env ( path = None ) : path = path or get_store_env_tmp ( ) env_dict = yaml . safe_dump ( os . environ . data , default_flow_style = False ) with open ( path , 'w' ) as f : f . write ( env_dict ) return path
8872	def match ( self , subsetLines , offsetOfSubset , fileName ) : for ( offset , l ) in enumerate ( subsetLines ) : for t in self . regex : m = t . Regex . search ( l ) if m != None : truePosition = offset + offsetOfSubset _logger . debug ( 'Found match on line {}' . format ( str ( truePosition + 1 ) ) ) _logger . debug ( 'Line is {}' . format ( l ) ) self . failed = True self . matchLocation = CheckFileParser . FileLocation ( fileName , truePosition + 1 ) raise DirectiveException ( self )
7048	def bls_stats_singleperiod ( times , mags , errs , period , magsarefluxes = False , sigclip = 10.0 , perioddeltapercent = 10 , nphasebins = 200 , mintransitduration = 0.01 , maxtransitduration = 0.4 , ingressdurationfraction = 0.1 , verbose = True ) : stimes , smags , serrs = sigclip_magseries ( times , mags , errs , magsarefluxes = magsarefluxes , sigclip = sigclip ) if len ( stimes ) > 9 and len ( smags ) > 9 and len ( serrs ) > 9 : startp = period - perioddeltapercent * period / 100.0 if startp < 0 : startp = period endp = period + perioddeltapercent * period / 100.0 blsres = bls_serial_pfind ( stimes , smags , serrs , verbose = verbose , startp = startp , endp = endp , nphasebins = nphasebins , mintransitduration = mintransitduration , maxtransitduration = maxtransitduration , magsarefluxes = magsarefluxes , get_stats = False , sigclip = None ) if ( not blsres or 'blsresult' not in blsres or blsres [ 'blsresult' ] is None ) : LOGERROR ( "BLS failed during a period-search " "performed around the input best period: %.6f. " "Can't continue. " % period ) return None thistransdepth = blsres [ 'blsresult' ] [ 'transdepth' ] thistransduration = blsres [ 'blsresult' ] [ 'transduration' ] thisbestperiod = blsres [ 'bestperiod' ] thistransingressbin = blsres [ 'blsresult' ] [ 'transingressbin' ] thistransegressbin = blsres [ 'blsresult' ] [ 'transegressbin' ] thisnphasebins = nphasebins stats = _get_bls_stats ( stimes , smags , serrs , thistransdepth , thistransduration , ingressdurationfraction , nphasebins , thistransingressbin , thistransegressbin , thisbestperiod , thisnphasebins , magsarefluxes = magsarefluxes , verbose = verbose ) return stats else : LOGERROR ( 'no good detections for these times and mags, skipping...' ) return None
2049	def create_contract ( self , price = 0 , address = None , caller = None , balance = 0 , init = None , gas = None ) : expected_address = self . create_account ( self . new_address ( sender = caller ) ) if address is None : address = expected_address elif caller is not None and address != expected_address : raise EthereumError ( f"Error: contract created from address {hex(caller)} with nonce {self.get_nonce(caller)} was expected to be at address {hex(expected_address)}, but create_contract was called with address={hex(address)}" ) self . start_transaction ( 'CREATE' , address , price , init , caller , balance , gas = gas ) self . _process_pending_transaction ( ) return address
7908	def __presence_error ( self , stanza ) : fr = stanza . get_from ( ) key = fr . bare ( ) . as_unicode ( ) rs = self . rooms . get ( key ) if not rs : return False rs . process_error_presence ( stanza ) return True
12184	def _add_parsley_ns ( cls , namespace_dict ) : namespace_dict . update ( { 'parslepy' : cls . LOCAL_NAMESPACE , 'parsley' : cls . LOCAL_NAMESPACE , } ) return namespace_dict
10238	def count_citations ( graph : BELGraph , ** annotations ) -> Counter : citations = defaultdict ( set ) annotation_dict_filter = build_edge_data_filter ( annotations ) for u , v , _ , d in filter_edges ( graph , annotation_dict_filter ) : if CITATION not in d : continue citations [ u , v ] . add ( ( d [ CITATION ] [ CITATION_TYPE ] , d [ CITATION ] [ CITATION_REFERENCE ] . strip ( ) ) ) return Counter ( itt . chain . from_iterable ( citations . values ( ) ) )
9505	def intersects ( self , i ) : return self . start <= i . end and i . start <= self . end
7229	def paint ( self ) : snippet = { 'heatmap-radius' : VectorStyle . get_style_value ( self . radius ) , 'heatmap-opacity' : VectorStyle . get_style_value ( self . opacity ) , 'heatmap-color' : VectorStyle . get_style_value ( self . color ) , 'heatmap-intensity' : VectorStyle . get_style_value ( self . intensity ) , 'heatmap-weight' : VectorStyle . get_style_value ( self . weight ) } return snippet
7491	def compute_tree_stats ( self , ipyclient ) : names = self . samples if self . params . nboots : fulltre = ete3 . Tree ( self . trees . tree , format = 0 ) fulltre . unroot ( ) with open ( self . trees . boots , 'r' ) as inboots : bb = [ ete3 . Tree ( i . strip ( ) , format = 0 ) for i in inboots . readlines ( ) ] wboots = [ fulltre ] + bb [ - self . params . nboots : ] wctre , wcounts = consensus_tree ( wboots , names = names ) self . trees . cons = os . path . join ( self . dirs , self . name + ".cons" ) with open ( self . trees . cons , 'w' ) as ocons : ocons . write ( wctre . write ( format = 0 ) ) else : wctre = ete3 . Tree ( self . trees . tree , format = 0 ) wctre . unroot ( ) self . trees . nhx = os . path . join ( self . dirs , self . name + ".nhx" ) with open ( self . files . stats , 'w' ) as ostats : if self . params . nboots : ostats . write ( "## splits observed in {} trees\n" . format ( len ( wboots ) ) ) for i , j in enumerate ( self . samples ) : ostats . write ( "{:<3} {}\n" . format ( i , j ) ) ostats . write ( "\n" ) for split , freq in wcounts : if split . count ( '1' ) > 1 : ostats . write ( "{} {:.2f}\n" . format ( split , round ( freq , 2 ) ) ) ostats . write ( "\n" ) lbview = ipyclient . load_balanced_view ( ) qtots = { } qsamp = { } tots = sum ( 1 for i in wctre . iter_leaves ( ) ) totn = set ( wctre . get_leaf_names ( ) ) for node in wctre . traverse ( ) : qtots [ node ] = lbview . apply ( _get_total , * ( tots , node ) ) qsamp [ node ] = lbview . apply ( _get_sampled , * ( self , totn , node ) ) ipyclient . wait ( ) for node in wctre . traverse ( ) : total = qtots [ node ] . result ( ) sampled = qsamp [ node ] . result ( ) node . add_feature ( "quartets_total" , total ) node . add_feature ( "quartets_sampled" , sampled ) features = [ "quartets_total" , "quartets_sampled" ] with open ( self . trees . nhx , 'w' ) as outtre : outtre . write ( wctre . write ( format = 0 , features = features ) )
7043	def lightcurve_moments ( ftimes , fmags , ferrs ) : ndet = len ( fmags ) if ndet > 9 : series_median = npmedian ( fmags ) series_wmean = ( npsum ( fmags * ( 1.0 / ( ferrs * ferrs ) ) ) / npsum ( 1.0 / ( ferrs * ferrs ) ) ) series_mad = npmedian ( npabs ( fmags - series_median ) ) series_stdev = 1.483 * series_mad series_skew = spskew ( fmags ) series_kurtosis = spkurtosis ( fmags ) series_above1std = len ( fmags [ fmags > ( series_median + series_stdev ) ] ) series_below1std = len ( fmags [ fmags < ( series_median - series_stdev ) ] ) series_beyond1std = ( series_above1std + series_below1std ) / float ( ndet ) series_mag_percentiles = nppercentile ( fmags , [ 5.0 , 10 , 17.5 , 25 , 32.5 , 40 , 60 , 67.5 , 75 , 82.5 , 90 , 95 ] ) return { 'median' : series_median , 'wmean' : series_wmean , 'mad' : series_mad , 'stdev' : series_stdev , 'skew' : series_skew , 'kurtosis' : series_kurtosis , 'beyond1std' : series_beyond1std , 'mag_percentiles' : series_mag_percentiles , 'mag_iqr' : series_mag_percentiles [ 8 ] - series_mag_percentiles [ 3 ] , } else : LOGERROR ( 'not enough detections in this magseries ' 'to calculate light curve moments' ) return None
10200	def run ( self ) : return elasticsearch . helpers . bulk ( self . client , self . actionsiter ( ) , stats_only = True , chunk_size = 50 )
1628	def GetIndentLevel ( line ) : indent = Match ( r'^( *)\S' , line ) if indent : return len ( indent . group ( 1 ) ) else : return 0
3114	def _from_base_type ( self , value ) : if not value : return None try : credentials = client . Credentials . new_from_json ( value ) except ValueError : credentials = None return credentials
3205	def all ( self , get_all = False , ** queryparams ) : self . batch_id = None self . operation_status = None if get_all : return self . _iterate ( url = self . _build_path ( ) , ** queryparams ) else : return self . _mc_client . _get ( url = self . _build_path ( ) , ** queryparams )
13073	def r_assets ( self , filetype , asset ) : if filetype in self . assets and asset in self . assets [ filetype ] and self . assets [ filetype ] [ asset ] : return send_from_directory ( directory = self . assets [ filetype ] [ asset ] , filename = asset ) abort ( 404 )
12169	def _dispatch ( self , event , listener , * args , ** kwargs ) : if ( asyncio . iscoroutinefunction ( listener ) or isinstance ( listener , functools . partial ) and asyncio . iscoroutinefunction ( listener . func ) ) : return self . _dispatch_coroutine ( event , listener , * args , ** kwargs ) return self . _dispatch_function ( event , listener , * args , ** kwargs )
12972	def getOnlyFields ( self , pk , fields , cascadeFetch = False ) : conn = self . _get_connection ( ) key = self . _get_key_for_id ( pk ) res = conn . hmget ( key , fields ) if type ( res ) != list or not len ( res ) : return None objDict = { } numFields = len ( fields ) i = 0 anyNotNone = False while i < numFields : objDict [ fields [ i ] ] = res [ i ] if res [ i ] != None : anyNotNone = True i += 1 if anyNotNone is False : return None objDict [ '_id' ] = pk ret = self . _redisResultToObj ( objDict ) if cascadeFetch is True : self . _doCascadeFetch ( ret ) return ret
5165	def __intermediate_proto ( self , interface , address ) : address_proto = address . pop ( 'proto' , 'static' ) if 'proto' not in interface : return address_proto else : return interface . pop ( 'proto' )
13025	def create_payload ( self , x86_file , x64_file , payload_file ) : sc_x86 = open ( os . path . join ( self . datadir , x86_file ) , 'rb' ) . read ( ) sc_x64 = open ( os . path . join ( self . datadir , x64_file ) , 'rb' ) . read ( ) fp = open ( os . path . join ( self . datadir , payload_file ) , 'wb' ) fp . write ( b'\x31\xc0\x40\x0f\x84' + pack ( '<I' , len ( sc_x86 ) ) ) fp . write ( sc_x86 ) fp . write ( sc_x64 ) fp . close ( )
1892	def _start_proc ( self ) : assert '_proc' not in dir ( self ) or self . _proc is None try : self . _proc = Popen ( shlex . split ( self . _command ) , stdin = PIPE , stdout = PIPE , bufsize = 0 , universal_newlines = True ) except OSError as e : print ( e , "Probably too many cached expressions? visitors._cache..." ) raise Z3NotFoundError for cfg in self . _init : self . _send ( cfg )
2348	def seed_url ( self ) : url = self . base_url if self . URL_TEMPLATE is not None : url = urlparse . urljoin ( self . base_url , self . URL_TEMPLATE . format ( ** self . url_kwargs ) ) if not url : return None url_parts = list ( urlparse . urlparse ( url ) ) query = urlparse . parse_qsl ( url_parts [ 4 ] ) for k , v in self . url_kwargs . items ( ) : if v is None : continue if "{{{}}}" . format ( k ) not in str ( self . URL_TEMPLATE ) : for i in iterable ( v ) : query . append ( ( k , i ) ) url_parts [ 4 ] = urlencode ( query ) return urlparse . urlunparse ( url_parts )
13064	def make_coins ( self , collection , text , subreference = "" , lang = None ) : if lang is None : lang = self . __default_lang__ return "url_ver=Z39.88-2004" "&ctx_ver=Z39.88-2004" "&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" "&rft_id={cid}" "&rft.genre=bookitem" "&rft.btitle={title}" "&rft.edition={edition}" "&rft.au={author}" "&rft.atitle={pages}" "&rft.language={language}" "&rft.pages={pages}" . format ( title = quote ( str ( text . get_title ( lang ) ) ) , author = quote ( str ( text . get_creator ( lang ) ) ) , cid = url_for ( ".r_collection" , objectId = collection . id , _external = True ) , language = collection . lang , pages = quote ( subreference ) , edition = quote ( str ( text . get_description ( lang ) ) ) )
10103	def send ( self , email_id , recipient , email_data = None , sender = None , cc = None , bcc = None , tags = [ ] , headers = { } , esp_account = None , locale = None , email_version_name = None , inline = None , files = [ ] , timeout = None ) : if not email_data : email_data = { } if isinstance ( recipient , string_types ) : warnings . warn ( "Passing email directly for recipient is deprecated" , DeprecationWarning ) recipient = { 'address' : recipient } payload = { 'email_id' : email_id , 'recipient' : recipient , 'email_data' : email_data } if sender : payload [ 'sender' ] = sender if cc : if not type ( cc ) == list : logger . error ( 'kwarg cc must be type(list), got %s' % type ( cc ) ) payload [ 'cc' ] = cc if bcc : if not type ( bcc ) == list : logger . error ( 'kwarg bcc must be type(list), got %s' % type ( bcc ) ) payload [ 'bcc' ] = bcc if tags : if not type ( tags ) == list : logger . error ( 'kwarg tags must be type(list), got %s' % ( type ( tags ) ) ) payload [ 'tags' ] = tags if headers : if not type ( headers ) == dict : logger . error ( 'kwarg headers must be type(dict), got %s' % ( type ( headers ) ) ) payload [ 'headers' ] = headers if esp_account : if not isinstance ( esp_account , string_types ) : logger . error ( 'kwarg esp_account must be a string, got %s' % ( type ( esp_account ) ) ) payload [ 'esp_account' ] = esp_account if locale : if not isinstance ( locale , string_types ) : logger . error ( 'kwarg locale must be a string, got %s' % ( type ( locale ) ) ) payload [ 'locale' ] = locale if email_version_name : if not isinstance ( email_version_name , string_types ) : logger . error ( 'kwarg email_version_name must be a string, got %s' % ( type ( email_version_name ) ) ) payload [ 'version_name' ] = email_version_name if inline : payload [ 'inline' ] = self . _make_file_dict ( inline ) if files : payload [ 'files' ] = [ self . _make_file_dict ( f ) for f in files ] return self . _api_request ( self . SEND_ENDPOINT , self . HTTP_POST , payload = payload , timeout = timeout )
5397	def _delocalize_outputs_commands ( self , task_dir , outputs , user_project ) : commands = [ ] for o in outputs : if o . recursive or not o . value : continue dest_path = o . uri . path local_path = task_dir + '/' + _DATA_SUBDIR + '/' + o . docker_path if o . file_provider == job_model . P_LOCAL : commands . append ( 'mkdir -p "%s"' % dest_path ) if o . file_provider in [ job_model . P_LOCAL , job_model . P_GCS ] : if user_project : command = 'gsutil -u %s -mq cp "%s" "%s"' % ( user_project , local_path , dest_path ) else : command = 'gsutil -mq cp "%s" "%s"' % ( local_path , dest_path ) commands . append ( command ) return '\n' . join ( commands )
8857	def on_run ( self ) : filename = self . tabWidget . current_widget ( ) . file . path wd = os . path . dirname ( filename ) args = Settings ( ) . get_run_config_for_file ( filename ) self . interactiveConsole . start_process ( Settings ( ) . interpreter , args = [ filename ] + args , cwd = wd ) self . dockWidget . show ( ) self . actionRun . setEnabled ( False ) self . actionConfigure_run . setEnabled ( False )
4927	def transform_description ( self , content_metadata_item ) : description_with_locales = [ ] for locale in self . enterprise_configuration . get_locales ( ) : description_with_locales . append ( { 'locale' : locale , 'value' : ( content_metadata_item . get ( 'full_description' ) or content_metadata_item . get ( 'short_description' ) or content_metadata_item . get ( 'title' , '' ) ) } ) return description_with_locales
3164	def all ( self , workflow_id , email_id ) : self . workflow_id = workflow_id self . email_id = email_id self . subscriber_hash = None return self . _mc_client . _get ( url = self . _build_path ( workflow_id , 'emails' , email_id , 'queue' ) )
4696	def env ( ) : if cij . ssh . env ( ) : cij . err ( "board.env: invalid SSH environment" ) return 1 board = cij . env_to_dict ( PREFIX , REQUIRED ) if board is None : cij . err ( "board.env: invalid BOARD environment" ) return 1 board [ "CLASS" ] = "_" . join ( [ board [ r ] for r in REQUIRED [ : - 1 ] ] ) board [ "IDENT" ] = "-" . join ( [ board [ "CLASS" ] , board [ "ALIAS" ] ] ) cij . env_export ( PREFIX , EXPORTED , board ) return 0
4081	def get_directory ( ) : try : language_check_dir = cache [ 'language_check_dir' ] except KeyError : def version_key ( string ) : return [ int ( e ) if e . isdigit ( ) else e for e in re . split ( r"(\d+)" , string ) ] def get_lt_dir ( base_dir ) : paths = [ path for path in glob . glob ( os . path . join ( base_dir , 'LanguageTool*' ) ) if os . path . isdir ( path ) ] return max ( paths , key = version_key ) if paths else None base_dir = os . path . dirname ( sys . argv [ 0 ] ) language_check_dir = get_lt_dir ( base_dir ) if not language_check_dir : try : base_dir = os . path . dirname ( os . path . abspath ( __file__ ) ) except NameError : pass else : language_check_dir = get_lt_dir ( base_dir ) if not language_check_dir : raise PathError ( "can't find LanguageTool directory in {!r}" . format ( base_dir ) ) cache [ 'language_check_dir' ] = language_check_dir return language_check_dir
11647	def fit_transform ( self , X , y = None ) : n = X . shape [ 0 ] if X . shape != ( n , n ) : raise TypeError ( "Input must be a square matrix." ) memory = get_memory ( self . memory ) discard_X = not self . copy and self . negatives_likely vals , vecs = memory . cache ( scipy . linalg . eigh , ignore = [ 'overwrite_a' ] ) ( X , overwrite_a = discard_X ) vals = vals [ : , None ] self . clip_ = np . dot ( vecs , np . sign ( vals ) * vecs . T ) if discard_X or vals [ 0 , 0 ] < 0 : del X np . abs ( vals , out = vals ) X = np . dot ( vecs , vals * vecs . T ) del vals , vecs X = Symmetrize ( copy = False ) . fit_transform ( X ) return X
11776	def WeightedMajority ( predictors , weights ) : "Return a predictor that takes a weighted vote." def predict ( example ) : return weighted_mode ( ( predictor ( example ) for predictor in predictors ) , weights ) return predict
11464	def download ( self , source_file , target_folder = '' ) : current_folder = self . _ftp . pwd ( ) if not target_folder . startswith ( '/' ) : target_folder = join ( getcwd ( ) , target_folder ) folder = os . path . dirname ( source_file ) self . cd ( folder ) if folder . startswith ( "/" ) : folder = folder [ 1 : ] destination_folder = join ( target_folder , folder ) if not os . path . exists ( destination_folder ) : print ( "Creating folder" , destination_folder ) os . makedirs ( destination_folder ) source_file = os . path . basename ( source_file ) destination = join ( destination_folder , source_file ) try : with open ( destination , 'wb' ) as result : self . _ftp . retrbinary ( 'RETR %s' % ( source_file , ) , result . write ) except error_perm as e : print ( e ) remove ( join ( target_folder , source_file ) ) raise self . _ftp . cwd ( current_folder )
11934	def auto_widget ( field ) : info = { 'widget' : field . field . widget . __class__ . __name__ , 'field' : field . field . __class__ . __name__ , 'name' : field . name , } return [ fmt . format ( ** info ) for fmt in ( '{field}_{widget}_{name}' , '{field}_{name}' , '{widget}_{name}' , '{field}_{widget}' , '{name}' , '{widget}' , '{field}' , ) ]
4378	def add_parent ( self , parent ) : parent . children . add ( self ) self . parents . add ( parent )
12397	def gen_method_keys ( self , * args , ** kwargs ) : token = args [ 0 ] for mro_type in type ( token ) . __mro__ [ : - 1 ] : name = mro_type . __name__ yield name
10090	def rst2node ( doc_name , data ) : if not data : return parser = docutils . parsers . rst . Parser ( ) document = docutils . utils . new_document ( '<%s>' % doc_name ) document . settings = docutils . frontend . OptionParser ( ) . get_default_values ( ) document . settings . tab_width = 4 document . settings . pep_references = False document . settings . rfc_references = False document . settings . env = Env ( ) parser . parse ( data , document ) if len ( document . children ) == 1 : return document . children [ 0 ] else : par = docutils . nodes . paragraph ( ) for child in document . children : par += child return par
12942	def hasUnsavedChanges ( self , cascadeObjects = False ) : if not self . _id or not self . _origData : return True for thisField in self . FIELDS : thisVal = object . __getattribute__ ( self , thisField ) if self . _origData . get ( thisField , '' ) != thisVal : return True if cascadeObjects is True and issubclass ( thisField . __class__ , IRForeignLinkFieldBase ) : if thisVal . objHasUnsavedChanges ( ) : return True return False
13881	def MoveDirectory ( source_dir , target_dir ) : if not IsDir ( source_dir ) : from . _exceptions import DirectoryNotFoundError raise DirectoryNotFoundError ( source_dir ) if Exists ( target_dir ) : from . _exceptions import DirectoryAlreadyExistsError raise DirectoryAlreadyExistsError ( target_dir ) from six . moves . urllib . parse import urlparse source_url = urlparse ( source_dir ) target_url = urlparse ( target_dir ) if _UrlIsLocal ( source_url ) and _UrlIsLocal ( target_url ) : import shutil shutil . move ( source_dir , target_dir ) elif source_url . scheme == 'ftp' and target_url . scheme == 'ftp' : from . _exceptions import NotImplementedProtocol raise NotImplementedProtocol ( target_url . scheme ) else : raise NotImplementedError ( 'Can only move directories local->local or ftp->ftp' )
10828	def delete ( cls , group , user ) : with db . session . begin_nested ( ) : cls . query . filter_by ( group = group , user_id = user . get_id ( ) ) . delete ( )
6932	def colormagdiagram_cplist ( cplist , outpkl , color_mag1 = [ 'gaiamag' , 'sdssg' ] , color_mag2 = [ 'kmag' , 'kmag' ] , yaxis_mag = [ 'gaia_absmag' , 'rpmj' ] ) : cplist_objectids = [ ] cplist_mags = [ ] cplist_colors = [ ] for cpf in cplist : cpd = _read_checkplot_picklefile ( cpf ) cplist_objectids . append ( cpd [ 'objectid' ] ) thiscp_mags = [ ] thiscp_colors = [ ] for cm1 , cm2 , ym in zip ( color_mag1 , color_mag2 , yaxis_mag ) : if ( ym in cpd [ 'objectinfo' ] and cpd [ 'objectinfo' ] [ ym ] is not None ) : thiscp_mags . append ( cpd [ 'objectinfo' ] [ ym ] ) else : thiscp_mags . append ( np . nan ) if ( cm1 in cpd [ 'objectinfo' ] and cpd [ 'objectinfo' ] [ cm1 ] is not None and cm2 in cpd [ 'objectinfo' ] and cpd [ 'objectinfo' ] [ cm2 ] is not None ) : thiscp_colors . append ( cpd [ 'objectinfo' ] [ cm1 ] - cpd [ 'objectinfo' ] [ cm2 ] ) else : thiscp_colors . append ( np . nan ) cplist_mags . append ( thiscp_mags ) cplist_colors . append ( thiscp_colors ) cplist_objectids = np . array ( cplist_objectids ) cplist_mags = np . array ( cplist_mags ) cplist_colors = np . array ( cplist_colors ) cmddict = { 'objectids' : cplist_objectids , 'mags' : cplist_mags , 'colors' : cplist_colors , 'color_mag1' : color_mag1 , 'color_mag2' : color_mag2 , 'yaxis_mag' : yaxis_mag } with open ( outpkl , 'wb' ) as outfd : pickle . dump ( cmddict , outfd , pickle . HIGHEST_PROTOCOL ) plt . close ( 'all' ) return cmddict
1014	def _getBestMatchingSegment ( self , c , i , activeState ) : maxActivity , which = self . minThreshold , - 1 for j , s in enumerate ( self . cells [ c ] [ i ] ) : activity = self . _getSegmentActivityLevel ( s , activeState , connectedSynapsesOnly = False ) if activity >= maxActivity : maxActivity , which = activity , j if which == - 1 : return None else : return self . cells [ c ] [ i ] [ which ]
2796	def load ( self , use_slug = False ) : identifier = None if use_slug or not self . id : identifier = self . slug else : identifier = self . id if not identifier : raise NotFoundError ( "One of self.id or self.slug must be set." ) data = self . get_data ( "images/%s" % identifier ) image_dict = data [ 'image' ] for attr in image_dict . keys ( ) : setattr ( self , attr , image_dict [ attr ] ) return self
1625	def FindStartOfExpressionInLine ( line , endpos , stack ) : i = endpos while i >= 0 : char = line [ i ] if char in ')]}' : stack . append ( char ) elif char == '>' : if ( i > 0 and ( line [ i - 1 ] == '-' or Match ( r'\s>=\s' , line [ i - 1 : ] ) or Search ( r'\boperator\s*$' , line [ 0 : i ] ) ) ) : i -= 1 else : stack . append ( '>' ) elif char == '<' : if i > 0 and line [ i - 1 ] == '<' : i -= 1 else : if stack and stack [ - 1 ] == '>' : stack . pop ( ) if not stack : return ( i , None ) elif char in '([{' : while stack and stack [ - 1 ] == '>' : stack . pop ( ) if not stack : return ( - 1 , None ) if ( ( char == '(' and stack [ - 1 ] == ')' ) or ( char == '[' and stack [ - 1 ] == ']' ) or ( char == '{' and stack [ - 1 ] == '}' ) ) : stack . pop ( ) if not stack : return ( i , None ) else : return ( - 1 , None ) elif char == ';' : while stack and stack [ - 1 ] == '>' : stack . pop ( ) if not stack : return ( - 1 , None ) i -= 1 return ( - 1 , stack )
13328	def add ( path ) : click . echo ( '\nAdding {} to cache......' . format ( path ) , nl = False ) try : r = cpenv . resolve ( path ) except Exception as e : click . echo ( bold_red ( 'FAILED' ) ) click . echo ( e ) return if isinstance ( r . resolved [ 0 ] , cpenv . VirtualEnvironment ) : EnvironmentCache . add ( r . resolved [ 0 ] ) EnvironmentCache . save ( ) click . echo ( bold_green ( 'OK!' ) )
5951	def strftime ( self , fmt = "%d:%H:%M:%S" ) : substitutions = { "%d" : str ( self . days ) , "%H" : "{0:02d}" . format ( self . dhours ) , "%h" : str ( 24 * self . days + self . dhours ) , "%M" : "{0:02d}" . format ( self . dminutes ) , "%S" : "{0:02d}" . format ( self . dseconds ) , } s = fmt for search , replacement in substitutions . items ( ) : s = s . replace ( search , replacement ) return s
6477	def render ( self , stream ) : encoding = self . option . encoding or self . term . encoding or "utf8" if self . option . color : ramp = self . color_ramp ( self . size . y ) [ : : - 1 ] else : ramp = None if self . cycle >= 1 and self . lines : stream . write ( self . term . csi ( 'cuu' , self . lines ) ) zero = int ( self . null / 4 ) lines = 0 for y in range ( self . screen . size . y ) : if y == zero and self . size . y > 1 : stream . write ( self . term . csi ( 'smul' ) ) if ramp : stream . write ( ramp [ y ] ) for x in range ( self . screen . size . x ) : point = Point ( ( x , y ) ) if point in self . screen : value = self . screen [ point ] if isinstance ( value , int ) : stream . write ( chr ( self . base + value ) . encode ( encoding ) ) else : stream . write ( self . term . csi ( 'sgr0' ) ) stream . write ( self . term . csi_wrap ( value . encode ( encoding ) , 'bold' ) ) if y == zero and self . size . y > 1 : stream . write ( self . term . csi ( 'smul' ) ) if ramp : stream . write ( ramp [ y ] ) else : stream . write ( b' ' ) if y == zero and self . size . y > 1 : stream . write ( self . term . csi ( 'rmul' ) ) if ramp : stream . write ( self . term . csi ( 'sgr0' ) ) stream . write ( b'\n' ) lines += 1 stream . flush ( ) self . cycle = self . cycle + 1 self . lines = lines
13163	def serialize_text ( out , text ) : padding = len ( out ) add_padding = padding_adder ( padding ) text = add_padding ( text , ignore_first_line = True ) return out + text
9468	def conference_speak ( self , call_params ) : path = '/' + self . api_version + '/ConferenceSpeak/' method = 'POST' return self . request ( path , method , call_params )
11482	def _upload_folder_recursive ( local_folder , parent_folder_id , leaf_folders_as_items = False , reuse_existing = False ) : if leaf_folders_as_items and _has_only_files ( local_folder ) : print ( 'Creating item from {0}' . format ( local_folder ) ) _upload_folder_as_item ( local_folder , parent_folder_id , reuse_existing ) return else : print ( 'Creating folder from {0}' . format ( local_folder ) ) new_folder_id = _create_or_reuse_folder ( local_folder , parent_folder_id , reuse_existing ) for entry in sorted ( os . listdir ( local_folder ) ) : full_entry = os . path . join ( local_folder , entry ) if os . path . islink ( full_entry ) : continue elif os . path . isdir ( full_entry ) : _upload_folder_recursive ( full_entry , new_folder_id , leaf_folders_as_items , reuse_existing ) else : print ( 'Uploading item from {0}' . format ( full_entry ) ) _upload_as_item ( entry , new_folder_id , full_entry , reuse_existing )
1487	def _load_class ( cls , d ) : for k , v in d . items ( ) : if isinstance ( k , tuple ) : typ , k = k if typ == 'property' : v = property ( * v ) elif typ == 'staticmethod' : v = staticmethod ( v ) elif typ == 'classmethod' : v = classmethod ( v ) setattr ( cls , k , v ) return cls
9310	def amz_cano_path ( self , path ) : safe_chars = '/~' qs = '' fixed_path = path if '?' in fixed_path : fixed_path , qs = fixed_path . split ( '?' , 1 ) fixed_path = posixpath . normpath ( fixed_path ) fixed_path = re . sub ( '/+' , '/' , fixed_path ) if path . endswith ( '/' ) and not fixed_path . endswith ( '/' ) : fixed_path += '/' full_path = fixed_path if PY2 : full_path = full_path . encode ( 'utf-8' ) safe_chars = safe_chars . encode ( 'utf-8' ) qs = qs . encode ( 'utf-8' ) if self . service in [ 's3' , 'host' ] : full_path = unquote ( full_path ) full_path = quote ( full_path , safe = safe_chars ) if qs : qm = b'?' if PY2 else '?' full_path = qm . join ( ( full_path , qs ) ) if PY2 : full_path = unicode ( full_path ) return full_path
10242	def get_evidences_by_pmid ( graph : BELGraph , pmids : Union [ str , Iterable [ str ] ] ) : result = defaultdict ( set ) for _ , _ , _ , data in filter_edges ( graph , build_pmid_inclusion_filter ( pmids ) ) : result [ data [ CITATION ] [ CITATION_REFERENCE ] ] . add ( data [ EVIDENCE ] ) return dict ( result )
12281	def run ( self , cmd , * args ) : if self . manager is None : raise Exception ( "Fatal internal error: Missing repository manager" ) if cmd not in dir ( self . manager ) : raise Exception ( "Fatal internal error: Invalid command {} being run" . format ( cmd ) ) func = getattr ( self . manager , cmd ) repo = self return func ( repo , * args )
11425	def record_match_subfields ( rec , tag , ind1 = " " , ind2 = " " , sub_key = None , sub_value = '' , sub_key2 = None , sub_value2 = '' , case_sensitive = True ) : if sub_key is None : raise TypeError ( "None object passed for parameter sub_key." ) if sub_key2 is not None and sub_value2 is '' : raise TypeError ( "Parameter sub_key2 defined but sub_value2 is None, " + "function requires a value for comparrison." ) ind1 , ind2 = _wash_indicators ( ind1 , ind2 ) if not case_sensitive : sub_value = sub_value . lower ( ) sub_value2 = sub_value2 . lower ( ) for field in record_get_field_instances ( rec , tag , ind1 , ind2 ) : subfields = dict ( field_get_subfield_instances ( field ) ) if not case_sensitive : for k , v in subfields . iteritems ( ) : subfields [ k ] = v . lower ( ) if sub_key in subfields : if sub_value is '' : return field [ 4 ] else : if sub_value == subfields [ sub_key ] : if sub_key2 is None : return field [ 4 ] else : if sub_key2 in subfields : if sub_value2 == subfields [ sub_key2 ] : return field [ 4 ] return False
13587	def add_formatted_field ( cls , field , format_string , title = '' ) : global klass_count klass_count += 1 fn_name = 'dyn_fn_%d' % klass_count cls . list_display . append ( fn_name ) if not title : title = field . capitalize ( ) _format_string = format_string def _ref ( self , obj ) : return _format_string % getattr ( obj , field ) _ref . short_description = title _ref . allow_tags = True _ref . admin_order_field = field setattr ( cls , fn_name , _ref )
5500	def get_tweets ( self , url , limit = None ) : try : tweets = self . cache [ url ] [ "tweets" ] self . mark_updated ( ) return sorted ( tweets , reverse = True ) [ : limit ] except KeyError : return [ ]
6199	def simulate_diffusion ( self , save_pos = False , total_emission = True , radial = False , rs = None , seed = 1 , path = './' , wrap_func = wrap_periodic , chunksize = 2 ** 19 , chunkslice = 'times' , verbose = True ) : if rs is None : rs = np . random . RandomState ( seed = seed ) self . open_store_traj ( chunksize = chunksize , chunkslice = chunkslice , radial = radial , path = path ) self . traj_group . _v_attrs [ 'init_random_state' ] = rs . get_state ( ) em_store = self . emission_tot if total_emission else self . emission print ( '- Start trajectories simulation - %s' % ctime ( ) , flush = True ) if verbose : print ( '[PID %d] Diffusion time:' % os . getpid ( ) , end = '' ) i_chunk = 0 t_chunk_size = self . emission . chunkshape [ 1 ] chunk_duration = t_chunk_size * self . t_step par_start_pos = self . particles . positions prev_time = 0 for time_size in iter_chunksize ( self . n_samples , t_chunk_size ) : if verbose : curr_time = int ( chunk_duration * ( i_chunk + 1 ) ) if curr_time > prev_time : print ( ' %ds' % curr_time , end = '' , flush = True ) prev_time = curr_time POS , em = self . _sim_trajectories ( time_size , par_start_pos , rs , total_emission = total_emission , save_pos = save_pos , radial = radial , wrap_func = wrap_func ) em_store . append ( em ) if save_pos : self . position . append ( np . vstack ( POS ) . astype ( 'float32' ) ) i_chunk += 1 self . store . h5file . flush ( ) self . traj_group . _v_attrs [ 'last_random_state' ] = rs . get_state ( ) self . store . h5file . flush ( ) print ( '\n- End trajectories simulation - %s' % ctime ( ) , flush = True )
8802	def do_notify ( context , event_type , payload ) : LOG . debug ( 'IP_BILL: notifying {}' . format ( payload ) ) notifier = n_rpc . get_notifier ( 'network' ) notifier . info ( context , event_type , payload )
8450	def has_env_vars ( * env_vars ) : for env_var in env_vars : if not os . environ . get ( env_var ) : msg = ( 'Must set {} environment variable. View docs for setting up environment at {}' ) . format ( env_var , temple . constants . TEMPLE_DOCS_URL ) raise temple . exceptions . InvalidEnvironmentError ( msg )
10993	def _check_for_inception ( self , root_dict ) : for key in root_dict : if isinstance ( root_dict [ key ] , dict ) : root_dict [ key ] = ResponseObject ( root_dict [ key ] ) return root_dict
13060	def get_reffs ( self , objectId , subreference = None , collection = None , export_collection = False ) : if collection is not None : text = collection else : text = self . get_collection ( objectId ) reffs = self . chunk ( text , lambda level : self . resolver . getReffs ( objectId , level = level , subreference = subreference ) ) if export_collection is True : return text , reffs return reffs
4171	def enbw ( data ) : r N = len ( data ) return N * np . sum ( data ** 2 ) / np . sum ( data ) ** 2
5508	def worker ( f ) : @ functools . wraps ( f ) async def wrapper ( cls , connection , rest ) : try : await f ( cls , connection , rest ) except asyncio . CancelledError : connection . response ( "426" , "transfer aborted" ) connection . response ( "226" , "abort successful" ) return wrapper
3838	async def set_group_link_sharing_enabled ( self , set_group_link_sharing_enabled_request ) : response = hangouts_pb2 . SetGroupLinkSharingEnabledResponse ( ) await self . _pb_request ( 'conversations/setgrouplinksharingenabled' , set_group_link_sharing_enabled_request , response ) return response
11736	def _validate_schema ( obj ) : if obj is not None and not isinstance ( obj , Schema ) : raise IncompatibleSchema ( 'Schema must be of type {0}' . format ( Schema ) ) return obj
1175	def unlock ( self ) : if self . queue : function , argument = self . queue . popleft ( ) function ( argument ) else : self . locked = False
10867	def rmatrix ( self ) : t = self . param_dict [ self . lbl_theta ] r0 = np . array ( [ [ np . cos ( t ) , - np . sin ( t ) , 0 ] , [ np . sin ( t ) , np . cos ( t ) , 0 ] , [ 0 , 0 , 1 ] ] ) p = self . param_dict [ self . lbl_phi ] r1 = np . array ( [ [ np . cos ( p ) , 0 , np . sin ( p ) ] , [ 0 , 1 , 0 ] , [ - np . sin ( p ) , 0 , np . cos ( p ) ] ] ) return np . dot ( r1 , r0 )
1481	def start_process_monitor ( self ) : Log . info ( "Start process monitor" ) while True : if len ( self . processes_to_monitor ) > 0 : ( pid , status ) = os . wait ( ) with self . process_lock : if pid in self . processes_to_monitor . keys ( ) : old_process_info = self . processes_to_monitor [ pid ] name = old_process_info . name command = old_process_info . command Log . info ( "%s (pid=%s) exited with status %d. command=%s" % ( name , pid , status , command ) ) self . _wait_process_std_out_err ( name , old_process_info . process ) if os . path . isfile ( "core.%d" % pid ) : os . system ( "chmod a+r core.%d" % pid ) if old_process_info . attempts >= self . max_runs : Log . info ( "%s exited too many times" % name ) sys . exit ( 1 ) time . sleep ( self . interval_between_runs ) p = self . _run_process ( name , command ) del self . processes_to_monitor [ pid ] self . processes_to_monitor [ p . pid ] = ProcessInfo ( p , name , command , old_process_info . attempts + 1 ) log_pid_for_process ( name , p . pid )
11575	def encoder_data ( self , data ) : prev_val = self . digital_response_table [ data [ self . RESPONSE_TABLE_MODE ] ] [ self . RESPONSE_TABLE_PIN_DATA_VALUE ] val = int ( ( data [ self . MSB ] << 7 ) + data [ self . LSB ] ) if val > 8192 : val -= 16384 pin = data [ 0 ] with self . pymata . data_lock : self . digital_response_table [ data [ self . RESPONSE_TABLE_MODE ] ] [ self . RESPONSE_TABLE_PIN_DATA_VALUE ] = val if prev_val != val : callback = self . digital_response_table [ pin ] [ self . RESPONSE_TABLE_CALLBACK ] if callback is not None : callback ( [ self . pymata . ENCODER , pin , self . digital_response_table [ pin ] [ self . RESPONSE_TABLE_PIN_DATA_VALUE ] ] )
3303	def _read_config_file ( config_file , verbose ) : config_file = os . path . abspath ( config_file ) if not os . path . exists ( config_file ) : raise RuntimeError ( "Couldn't open configuration file '{}'." . format ( config_file ) ) if config_file . endswith ( ".json" ) : with io . open ( config_file , mode = "r" , encoding = "utf-8" ) as json_file : minified = jsmin ( json_file . read ( ) ) conf = json . loads ( minified ) elif config_file . endswith ( ".yaml" ) : with io . open ( config_file , mode = "r" , encoding = "utf-8" ) as yaml_file : conf = yaml . safe_load ( yaml_file ) else : try : import imp conf = { } configmodule = imp . load_source ( "configuration_module" , config_file ) for k , v in vars ( configmodule ) . items ( ) : if k . startswith ( "__" ) : continue elif isfunction ( v ) : continue conf [ k ] = v except Exception : exc_type , exc_value = sys . exc_info ( ) [ : 2 ] exc_info_list = traceback . format_exception_only ( exc_type , exc_value ) exc_text = "\n" . join ( exc_info_list ) print ( "Failed to read configuration file: " + config_file + "\nDue to " + exc_text , file = sys . stderr , ) raise conf [ "_config_file" ] = config_file return conf
12888	def call ( self , path , extra = None ) : try : if not self . __webfsapi : self . __webfsapi = yield from self . get_fsapi_endpoint ( ) if not self . sid : self . sid = yield from self . create_session ( ) if not isinstance ( extra , dict ) : extra = dict ( ) params = dict ( pin = self . pin , sid = self . sid ) params . update ( ** extra ) req_url = ( '%s/%s' % ( self . __webfsapi , path ) ) result = yield from self . __session . get ( req_url , params = params , timeout = self . timeout ) if result . status == 200 : text = yield from result . text ( encoding = 'utf-8' ) else : self . sid = yield from self . create_session ( ) params = dict ( pin = self . pin , sid = self . sid ) params . update ( ** extra ) result = yield from self . __session . get ( req_url , params = params , timeout = self . timeout ) text = yield from result . text ( encoding = 'utf-8' ) return objectify . fromstring ( text ) except Exception as e : logging . info ( 'AFSAPI Exception: ' + traceback . format_exc ( ) ) return None
12845	def _relay_message ( self , message ) : info ( "relaying message: {message}" ) if not message . was_sent_by ( self . _id_factory ) : self . pipe . send ( message ) self . pipe . deliver ( )
2916	def get_dump ( self , indent = 0 , recursive = True ) : dbg = ( ' ' * indent * 2 ) dbg += '%s/' % self . id dbg += '%s:' % self . thread_id dbg += ' Task of %s' % self . get_name ( ) if self . task_spec . description : dbg += ' (%s)' % self . get_description ( ) dbg += ' State: %s' % self . get_state_name ( ) dbg += ' Children: %s' % len ( self . children ) if recursive : for child in self . children : dbg += '\n' + child . get_dump ( indent + 1 ) return dbg
7803	def display_name ( self ) : if self . subject_name : return u", " . join ( [ u", " . join ( [ u"{0}={1}" . format ( k , v ) for k , v in dn_tuple ] ) for dn_tuple in self . subject_name ] ) for name_type in ( "XmppAddr" , "DNS" , "SRV" ) : names = self . alt_names . get ( name_type ) if names : return names [ 0 ] return u"<unknown>"
2592	def get_all_checkpoints ( rundir = "runinfo" ) : if ( not os . path . isdir ( rundir ) ) : return [ ] dirs = sorted ( os . listdir ( rundir ) ) checkpoints = [ ] for runid in dirs : checkpoint = os . path . abspath ( '{}/{}/checkpoint' . format ( rundir , runid ) ) if os . path . isdir ( checkpoint ) : checkpoints . append ( checkpoint ) return checkpoints
3824	async def get_entity_by_id ( self , get_entity_by_id_request ) : response = hangouts_pb2 . GetEntityByIdResponse ( ) await self . _pb_request ( 'contacts/getentitybyid' , get_entity_by_id_request , response ) return response
327	def summarize_paths ( samples , cone_std = ( 1. , 1.5 , 2. ) , starting_value = 1. ) : cum_samples = ep . cum_returns ( samples . T , starting_value = starting_value ) . T cum_mean = cum_samples . mean ( axis = 0 ) cum_std = cum_samples . std ( axis = 0 ) if isinstance ( cone_std , ( float , int ) ) : cone_std = [ cone_std ] cone_bounds = pd . DataFrame ( columns = pd . Float64Index ( [ ] ) ) for num_std in cone_std : cone_bounds . loc [ : , float ( num_std ) ] = cum_mean + cum_std * num_std cone_bounds . loc [ : , float ( - num_std ) ] = cum_mean - cum_std * num_std return cone_bounds
807	def disableTap ( self ) : if self . _tapFileIn is not None : self . _tapFileIn . close ( ) self . _tapFileIn = None if self . _tapFileOut is not None : self . _tapFileOut . close ( ) self . _tapFileOut = None
10435	def gettablerowindex ( self , window_name , object_name , row_text ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) index = 0 for cell in object_handle . AXRows : if re . match ( row_text , cell . AXChildren [ 0 ] . AXValue ) : return index index += 1 raise LdtpServerException ( u"Unable to find row: %s" % row_text )
1641	def _IsType ( clean_lines , nesting_state , expr ) : last_word = Match ( r'^.*(\b\S+)$' , expr ) if last_word : token = last_word . group ( 1 ) else : token = expr if _TYPES . match ( token ) : return True typename_pattern = ( r'\b(?:typename|class|struct)\s+' + re . escape ( token ) + r'\b' ) block_index = len ( nesting_state . stack ) - 1 while block_index >= 0 : if isinstance ( nesting_state . stack [ block_index ] , _NamespaceInfo ) : return False last_line = nesting_state . stack [ block_index ] . starting_linenum next_block_start = 0 if block_index > 0 : next_block_start = nesting_state . stack [ block_index - 1 ] . starting_linenum first_line = last_line while first_line >= next_block_start : if clean_lines . elided [ first_line ] . find ( 'template' ) >= 0 : break first_line -= 1 if first_line < next_block_start : block_index -= 1 continue for i in xrange ( first_line , last_line + 1 , 1 ) : if Search ( typename_pattern , clean_lines . elided [ i ] ) : return True block_index -= 1 return False
1185	def dispatch ( self , opcode , context ) : if id ( context ) in self . executing_contexts : generator = self . executing_contexts [ id ( context ) ] del self . executing_contexts [ id ( context ) ] has_finished = generator . next ( ) else : method = self . DISPATCH_TABLE . get ( opcode , _OpcodeDispatcher . unknown ) has_finished = method ( self , context ) if hasattr ( has_finished , "next" ) : generator = has_finished has_finished = generator . next ( ) if not has_finished : self . executing_contexts [ id ( context ) ] = generator return has_finished
6462	def filter_symlog ( y , base = 10.0 ) : log_base = np . log ( base ) sign = np . sign ( y ) logs = np . log ( np . abs ( y ) / log_base ) return sign * logs
10940	def calc_J ( self ) : del self . J self . J = np . zeros ( [ self . param_vals . size , self . data . size ] ) dp = np . zeros_like ( self . param_vals ) f0 = self . model . copy ( ) for a in range ( self . param_vals . size ) : dp *= 0 dp [ a ] = self . dl [ a ] f1 = self . func ( self . param_vals + dp , * self . func_args , ** self . func_kwargs ) grad_func = ( f1 - f0 ) / dp [ a ] self . J [ a ] = - grad_func
13868	def weekday ( when , weekday , start = mon ) : if isinstance ( when , datetime ) : when = when . date ( ) today = when . weekday ( ) delta = weekday - today if weekday < start and today >= start : delta += 7 elif weekday >= start and today < start : delta -= 7 return when + timedelta ( days = delta )
13608	def unpickle ( filepath ) : arr = [ ] with open ( filepath , 'rb' ) as f : carr = f . read ( blosc . MAX_BUFFERSIZE ) while len ( carr ) > 0 : arr . append ( blosc . decompress ( carr ) ) carr = f . read ( blosc . MAX_BUFFERSIZE ) return pkl . loads ( b"" . join ( arr ) )
7599	def get_popular_players ( self , ** params : keys ) : url = self . api . POPULAR + '/players' return self . _get_model ( url , PartialPlayerClan , ** params )
439	def print_params ( self , details = True , session = None ) : for i , p in enumerate ( self . all_params ) : if details : try : val = p . eval ( session = session ) logging . info ( " param {:3}: {:20} {:15} {} (mean: {:<18}, median: {:<18}, std: {:<18}) " . format ( i , p . name , str ( val . shape ) , p . dtype . name , val . mean ( ) , np . median ( val ) , val . std ( ) ) ) except Exception as e : logging . info ( str ( e ) ) raise Exception ( "Hint: print params details after tl.layers.initialize_global_variables(sess) " "or use network.print_params(False)." ) else : logging . info ( " param {:3}: {:20} {:15} {}" . format ( i , p . name , str ( p . get_shape ( ) ) , p . dtype . name ) ) logging . info ( " num of params: %d" % self . count_params ( ) )
4779	def is_in ( self , * items ) : if len ( items ) == 0 : raise ValueError ( 'one or more args must be given' ) else : for i in items : if self . val == i : return self self . _err ( 'Expected <%s> to be in %s, but was not.' % ( self . val , self . _fmt_items ( items ) ) )
4560	def stop ( self = None ) : if not self : instance = getattr ( Runner . instance ( ) , 'builder' , None ) self = instance and instance ( ) if not self : return self . _runner . stop ( ) if self . project : self . project . stop ( ) self . project = None
2450	def set_pkg_originator ( self , doc , entity ) : self . assert_package_exists ( ) if not self . package_originator_set : self . package_originator_set = True if validations . validate_pkg_originator ( entity ) : doc . package . originator = entity return True else : raise SPDXValueError ( 'Package::Originator' ) else : raise CardinalityError ( 'Package::Originator' )
13251	async def process_sphinx_technote ( session , github_api_token , ltd_product_data , mongo_collection = None ) : logger = logging . getLogger ( __name__ ) github_url = ltd_product_data [ 'doc_repo' ] github_url = normalize_repo_root_url ( github_url ) repo_slug = parse_repo_slug_from_url ( github_url ) try : metadata_yaml = await download_metadata_yaml ( session , github_url ) except aiohttp . ClientResponseError as err : logger . debug ( 'Tried to download %s\'s metadata.yaml, got status %d' , ltd_product_data [ 'slug' ] , err . code ) raise NotSphinxTechnoteError ( ) github_query = GitHubQuery . load ( 'technote_repo' ) github_variables = { "orgName" : repo_slug . owner , "repoName" : repo_slug . repo } github_data = await github_request ( session , github_api_token , query = github_query , variables = github_variables ) try : jsonld = reduce_technote_metadata ( github_url , metadata_yaml , github_data , ltd_product_data ) except Exception as exception : message = "Issue building JSON-LD for technote %s" logger . exception ( message , github_url , exception ) raise if mongo_collection is not None : await _upload_to_mongodb ( mongo_collection , jsonld ) logger . info ( 'Ingested technote %s into MongoDB' , github_url ) return jsonld
11107	def walk_files_relative_path ( self , relativePath = "" ) : def walk_files ( directory , relativePath ) : directories = dict . __getitem__ ( directory , 'directories' ) files = dict . __getitem__ ( directory , 'files' ) for f in sorted ( files ) : yield os . path . join ( relativePath , f ) for k in sorted ( dict . keys ( directories ) ) : path = os . path . join ( relativePath , k ) dir = directories . __getitem__ ( k ) for e in walk_files ( dir , path ) : yield e dir , errorMessage = self . get_directory_info ( relativePath ) assert dir is not None , errorMessage return walk_files ( dir , relativePath = '' )
5704	def _scan_footpaths ( self , stop_id , walk_departure_time ) : for _ , neighbor , data in self . _walk_network . edges_iter ( nbunch = [ stop_id ] , data = True ) : d_walk = data [ "d_walk" ] arrival_time = walk_departure_time + d_walk / self . _walk_speed self . _update_stop_label ( neighbor , arrival_time )
4670	def setKeys ( self , loadkeys ) : log . debug ( "Force setting of private keys. Not using the wallet database!" ) if isinstance ( loadkeys , dict ) : loadkeys = list ( loadkeys . values ( ) ) elif not isinstance ( loadkeys , ( list , set ) ) : loadkeys = [ loadkeys ] for wif in loadkeys : pub = self . publickey_from_wif ( wif ) self . store . add ( str ( wif ) , pub )
6143	def DSP_capture_add_samples_stereo ( self , new_data_left , new_data_right ) : self . capture_sample_count = self . capture_sample_count + len ( new_data_left ) + len ( new_data_right ) if self . Tcapture > 0 : self . data_capture_left = np . hstack ( ( self . data_capture_left , new_data_left ) ) self . data_capture_right = np . hstack ( ( self . data_capture_right , new_data_right ) ) if ( len ( self . data_capture_left ) > self . Ncapture ) : self . data_capture_left = self . data_capture_left [ - self . Ncapture : ] if ( len ( self . data_capture_right ) > self . Ncapture ) : self . data_capture_right = self . data_capture_right [ - self . Ncapture : ]
9097	def drop_bel_namespace ( self ) -> Optional [ Namespace ] : namespace = self . _get_default_namespace ( ) if namespace is not None : for entry in tqdm ( namespace . entries , desc = f'deleting entries in {self._get_namespace_name()}' ) : self . session . delete ( entry ) self . session . delete ( namespace ) log . info ( 'committing deletions' ) self . session . commit ( ) return namespace
4165	def _get_link ( self , cobj ) : fname_idx = None full_name = cobj [ 'module_short' ] + '.' + cobj [ 'name' ] if full_name in self . _searchindex [ 'objects' ] : value = self . _searchindex [ 'objects' ] [ full_name ] if isinstance ( value , dict ) : value = value [ next ( iter ( value . keys ( ) ) ) ] fname_idx = value [ 0 ] elif cobj [ 'module_short' ] in self . _searchindex [ 'objects' ] : value = self . _searchindex [ 'objects' ] [ cobj [ 'module_short' ] ] if cobj [ 'name' ] in value . keys ( ) : fname_idx = value [ cobj [ 'name' ] ] [ 0 ] if fname_idx is not None : fname = self . _searchindex [ 'filenames' ] [ fname_idx ] + '.html' if self . _is_windows : fname = fname . replace ( '/' , '\\' ) link = os . path . join ( self . doc_url , fname ) else : link = posixpath . join ( self . doc_url , fname ) if hasattr ( link , 'decode' ) : link = link . decode ( 'utf-8' , 'replace' ) if link in self . _page_cache : html = self . _page_cache [ link ] else : html = get_data ( link , self . gallery_dir ) self . _page_cache [ link ] = html comb_names = [ cobj [ 'module_short' ] + '.' + cobj [ 'name' ] ] if self . extra_modules_test is not None : for mod in self . extra_modules_test : comb_names . append ( mod + '.' + cobj [ 'name' ] ) url = False if hasattr ( html , 'decode' ) : html = html . decode ( 'utf-8' , 'replace' ) for comb_name in comb_names : if hasattr ( comb_name , 'decode' ) : comb_name = comb_name . decode ( 'utf-8' , 'replace' ) if comb_name in html : url = link + u'#' + comb_name link = url else : link = False return link
6687	def install ( packages , repos = None , yes = None , options = None ) : manager = MANAGER if options is None : options = [ ] elif isinstance ( options , six . string_types ) : options = [ options ] if not isinstance ( packages , six . string_types ) : packages = " " . join ( packages ) if repos : for repo in repos : options . append ( '--enablerepo=%(repo)s' % locals ( ) ) options = " " . join ( options ) if isinstance ( yes , str ) : run_as_root ( 'yes %(yes)s | %(manager)s %(options)s install %(packages)s' % locals ( ) ) else : run_as_root ( '%(manager)s %(options)s install %(packages)s' % locals ( ) )
1054	def seed ( self , a = None ) : super ( Random , self ) . seed ( a ) self . gauss_next = None
5273	def _generalized_word_starts ( self , xs ) : self . word_starts = [ ] i = 0 for n in range ( len ( xs ) ) : self . word_starts . append ( i ) i += len ( xs [ n ] ) + 1
6467	def csi ( self , capname , * args ) : value = curses . tigetstr ( capname ) if value is None : return b'' else : return curses . tparm ( value , * args )
9067	def _lml_optimal_scale ( self ) : assert self . _optimal [ "scale" ] n = len ( self . _y ) lml = - self . _df * log2pi - self . _df - n * log ( self . scale ) lml -= sum ( npsum ( log ( D ) ) for D in self . _D ) return lml / 2
6263	def check_glfw_version ( self ) : print ( "glfw version: {} (python wrapper version {})" . format ( glfw . get_version ( ) , glfw . __version__ ) ) if glfw . get_version ( ) < self . min_glfw_version : raise ValueError ( "Please update glfw binaries to version {} or later" . format ( self . min_glfw_version ) )
5279	def make_pmml_pipeline ( obj , active_fields = None , target_fields = None ) : steps = _filter_steps ( _get_steps ( obj ) ) pipeline = PMMLPipeline ( steps ) if active_fields is not None : pipeline . active_fields = numpy . asarray ( active_fields ) if target_fields is not None : pipeline . target_fields = numpy . asarray ( target_fields ) return pipeline
1779	def AAA ( cpu ) : cpu . AF = Operators . OR ( cpu . AL & 0x0F > 9 , cpu . AF ) cpu . CF = cpu . AF cpu . AH = Operators . ITEBV ( 8 , cpu . AF , cpu . AH + 1 , cpu . AH ) cpu . AL = Operators . ITEBV ( 8 , cpu . AF , cpu . AL + 6 , cpu . AL ) cpu . AL = cpu . AL & 0x0f
953	def closenessScores ( self , expValues , actValues , ** kwargs ) : ratio = 1.0 esum = int ( expValues . sum ( ) ) asum = int ( actValues . sum ( ) ) if asum > esum : diff = asum - esum if diff < esum : ratio = 1 - diff / float ( esum ) else : ratio = 1 / float ( diff ) olap = expValues & actValues osum = int ( olap . sum ( ) ) if esum == 0 : r = 0.0 else : r = osum / float ( esum ) r = r * ratio return numpy . array ( [ r ] )
11231	def replace ( self , ** kwargs ) : new_kwargs = { "interval" : self . _interval , "count" : self . _count , "dtstart" : self . _dtstart , "freq" : self . _freq , "until" : self . _until , "wkst" : self . _wkst , "cache" : False if self . _cache is None else True } new_kwargs . update ( self . _original_rule ) new_kwargs . update ( kwargs ) return rrule ( ** new_kwargs )
1056	def _slotnames ( cls ) : names = cls . __dict__ . get ( "__slotnames__" ) if names is not None : return names names = [ ] if not hasattr ( cls , "__slots__" ) : pass else : for c in cls . __mro__ : if "__slots__" in c . __dict__ : slots = c . __dict__ [ '__slots__' ] if isinstance ( slots , basestring ) : slots = ( slots , ) for name in slots : if name in ( "__dict__" , "__weakref__" ) : continue elif name . startswith ( '__' ) and not name . endswith ( '__' ) : names . append ( '_%s%s' % ( c . __name__ , name ) ) else : names . append ( name ) try : cls . __slotnames__ = names except : pass return names
4845	def _load_data ( self , resource , default = DEFAULT_VALUE_SAFEGUARD , ** kwargs ) : default_val = default if default != self . DEFAULT_VALUE_SAFEGUARD else { } try : return get_edx_api_data ( api_config = CatalogIntegration . current ( ) , resource = resource , api = self . client , ** kwargs ) or default_val except ( SlumberBaseException , ConnectionError , Timeout ) as exc : LOGGER . exception ( 'Failed to load data from resource [%s] with kwargs [%s] due to: [%s]' , resource , kwargs , str ( exc ) ) return default_val
7754	def process_stanza ( self , stanza ) : self . fix_in_stanza ( stanza ) to_jid = stanza . to_jid if not self . process_all_stanzas and to_jid and ( to_jid != self . me and to_jid . bare ( ) != self . me . bare ( ) ) : return self . route_stanza ( stanza ) try : if isinstance ( stanza , Iq ) : if self . process_iq ( stanza ) : return True elif isinstance ( stanza , Message ) : if self . process_message ( stanza ) : return True elif isinstance ( stanza , Presence ) : if self . process_presence ( stanza ) : return True except ProtocolError , err : typ = stanza . stanza_type if typ != 'error' and ( typ != 'result' or stanza . stanza_type != 'iq' ) : response = stanza . make_error_response ( err . xmpp_name ) self . send ( response ) err . log_reported ( ) else : err . log_ignored ( ) return logger . debug ( "Unhandled %r stanza: %r" % ( stanza . stanza_type , stanza . serialize ( ) ) ) return False
3450	def flux_variability_analysis ( model , reaction_list = None , loopless = False , fraction_of_optimum = 1.0 , pfba_factor = None , processes = None ) : if reaction_list is None : reaction_ids = [ r . id for r in model . reactions ] else : reaction_ids = [ r . id for r in model . reactions . get_by_any ( reaction_list ) ] if processes is None : processes = CONFIGURATION . processes num_reactions = len ( reaction_ids ) processes = min ( processes , num_reactions ) fva_result = DataFrame ( { "minimum" : zeros ( num_reactions , dtype = float ) , "maximum" : zeros ( num_reactions , dtype = float ) } , index = reaction_ids ) prob = model . problem with model : model . slim_optimize ( error_value = None , message = "There is no optimal solution for the " "chosen objective!" ) if model . solver . objective . direction == "max" : fva_old_objective = prob . Variable ( "fva_old_objective" , lb = fraction_of_optimum * model . solver . objective . value ) else : fva_old_objective = prob . Variable ( "fva_old_objective" , ub = fraction_of_optimum * model . solver . objective . value ) fva_old_obj_constraint = prob . Constraint ( model . solver . objective . expression - fva_old_objective , lb = 0 , ub = 0 , name = "fva_old_objective_constraint" ) model . add_cons_vars ( [ fva_old_objective , fva_old_obj_constraint ] ) if pfba_factor is not None : if pfba_factor < 1. : warn ( "The 'pfba_factor' should be larger or equal to 1." , UserWarning ) with model : add_pfba ( model , fraction_of_optimum = 0 ) ub = model . slim_optimize ( error_value = None ) flux_sum = prob . Variable ( "flux_sum" , ub = pfba_factor * ub ) flux_sum_constraint = prob . Constraint ( model . solver . objective . expression - flux_sum , lb = 0 , ub = 0 , name = "flux_sum_constraint" ) model . add_cons_vars ( [ flux_sum , flux_sum_constraint ] ) model . objective = Zero for what in ( "minimum" , "maximum" ) : if processes > 1 : chunk_size = len ( reaction_ids ) // processes pool = multiprocessing . Pool ( processes , initializer = _init_worker , initargs = ( model , loopless , what [ : 3 ] ) ) for rxn_id , value in pool . imap_unordered ( _fva_step , reaction_ids , chunksize = chunk_size ) : fva_result . at [ rxn_id , what ] = value pool . close ( ) pool . join ( ) else : _init_worker ( model , loopless , what [ : 3 ] ) for rxn_id , value in map ( _fva_step , reaction_ids ) : fva_result . at [ rxn_id , what ] = value return fva_result [ [ "minimum" , "maximum" ] ]
4126	def spectrum_data ( filename ) : import os import pkg_resources info = pkg_resources . get_distribution ( 'spectrum' ) location = info . location share = os . sep . join ( [ location , "spectrum" , 'data' ] ) filename2 = os . sep . join ( [ share , filename ] ) if os . path . exists ( filename2 ) : return filename2 else : raise Exception ( 'unknown file %s' % filename2 )
13088	def write_config ( self , initialize_indices = False ) : if not os . path . exists ( self . config_dir ) : os . mkdir ( self . config_dir ) with open ( self . config_file , 'w' ) as configfile : self . config . write ( configfile ) if initialize_indices : index = self . get ( 'jackal' , 'index' ) from jackal import Host , Range , Service , User , Credential , Log from jackal . core import create_connection create_connection ( self ) Host . init ( index = "{}-hosts" . format ( index ) ) Range . init ( index = "{}-ranges" . format ( index ) ) Service . init ( index = "{}-services" . format ( index ) ) User . init ( index = "{}-users" . format ( index ) ) Credential . init ( index = "{}-creds" . format ( index ) ) Log . init ( index = "{}-log" . format ( index ) )
1564	def get_metrics_collector ( self ) : if self . metrics_collector is None or not isinstance ( self . metrics_collector , MetricsCollector ) : raise RuntimeError ( "Metrics collector is not registered in this context" ) return self . metrics_collector
4429	async def _queue ( self , ctx , page : int = 1 ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . queue : return await ctx . send ( 'There\'s nothing in the queue! Why not queue something?' ) items_per_page = 10 pages = math . ceil ( len ( player . queue ) / items_per_page ) start = ( page - 1 ) * items_per_page end = start + items_per_page queue_list = '' for index , track in enumerate ( player . queue [ start : end ] , start = start ) : queue_list += f'`{index + 1}.` [**{track.title}**]({track.uri})\n' embed = discord . Embed ( colour = discord . Color . blurple ( ) , description = f'**{len(player.queue)} tracks**\n\n{queue_list}' ) embed . set_footer ( text = f'Viewing page {page}/{pages}' ) await ctx . send ( embed = embed )
5143	def search_for_comment ( self , lineno , default = None ) : if not self . index : self . make_index ( ) block = self . index . get ( lineno , None ) text = getattr ( block , 'text' , default ) return text
1330	def predictions ( self , image , strict = True , return_details = False ) : in_bounds = self . in_bounds ( image ) assert not strict or in_bounds self . _total_prediction_calls += 1 predictions = self . __model . predictions ( image ) is_adversarial , is_best , distance = self . __is_adversarial ( image , predictions , in_bounds ) assert predictions . ndim == 1 if return_details : return predictions , is_adversarial , is_best , distance else : return predictions , is_adversarial
1263	def states ( self ) : screen = self . env . getScreenRGB ( ) return dict ( shape = screen . shape , type = 'int' )
13241	def daily_periods ( self , range_start = datetime . date . min , range_end = datetime . date . max , exclude_dates = tuple ( ) ) : tz = self . timezone period = self . period weekdays = self . weekdays current_date = max ( range_start , self . start_date ) end_date = range_end if self . end_date : end_date = min ( end_date , self . end_date ) while current_date <= end_date : if current_date . weekday ( ) in weekdays and current_date not in exclude_dates : yield Period ( tz . localize ( datetime . datetime . combine ( current_date , period . start ) ) , tz . localize ( datetime . datetime . combine ( current_date , period . end ) ) ) current_date += datetime . timedelta ( days = 1 )
6839	def distrib_release ( ) : with settings ( hide ( 'running' , 'stdout' ) ) : kernel = ( run ( 'uname -s' ) or '' ) . strip ( ) . lower ( ) if kernel == LINUX : return run ( 'lsb_release -r --short' ) elif kernel == SUNOS : return run ( 'uname -v' )
13827	def get_doc ( doc_id , db_name , server_url = 'http://127.0.0.1:5984/' , rev = None ) : db = get_server ( server_url ) [ db_name ] if rev : headers , response = db . resource . get ( doc_id , rev = rev ) return couchdb . client . Document ( response ) return db [ doc_id ]
9959	def restore_python ( self ) : orig = self . orig_settings sys . setrecursionlimit ( orig [ "sys.recursionlimit" ] ) if "sys.tracebacklimit" in orig : sys . tracebacklimit = orig [ "sys.tracebacklimit" ] else : if hasattr ( sys , "tracebacklimit" ) : del sys . tracebacklimit if "showwarning" in orig : warnings . showwarning = orig [ "showwarning" ] orig . clear ( ) threading . stack_size ( )
5374	def _prefix_exists_in_gcs ( gcs_prefix , credentials = None ) : gcs_service = _get_storage_service ( credentials ) bucket_name , prefix = gcs_prefix [ len ( 'gs://' ) : ] . split ( '/' , 1 ) request = gcs_service . objects ( ) . list ( bucket = bucket_name , prefix = prefix , maxResults = 1 ) response = request . execute ( ) return response . get ( 'items' , None )
9687	def read_gsc_sfr ( self ) : config = [ ] data = { } self . cnxn . xfer ( [ 0x33 ] ) sleep ( 10e-3 ) for i in range ( 8 ) : resp = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] config . append ( resp ) data [ "GSC" ] = self . _calculate_float ( config [ 0 : 4 ] ) data [ "SFR" ] = self . _calculate_float ( config [ 4 : ] ) return data
529	def Array ( dtype , size = None , ref = False ) : def getArrayType ( self ) : return self . _dtype if ref : assert size is None index = basicTypes . index ( dtype ) if index == - 1 : raise Exception ( 'Invalid data type: ' + dtype ) if size and size <= 0 : raise Exception ( 'Array size must be positive' ) suffix = 'ArrayRef' if ref else 'Array' arrayFactory = getattr ( engine_internal , dtype + suffix ) arrayFactory . getType = getArrayType if size : a = arrayFactory ( size ) else : a = arrayFactory ( ) a . _dtype = basicTypes [ index ] return a
6479	def _normalised_python ( self ) : dx = ( self . screen . width / float ( len ( self . points ) ) ) oy = ( self . screen . height ) for x , point in enumerate ( self . points ) : y = ( point - self . minimum ) * 4.0 / self . extents * self . size . y yield Point ( ( dx * x , min ( oy , oy - y ) , ) )
13280	def child_end_handler ( self , scache ) : desc = self . desc desc_level = scache . desc_level breadth = desc_level . __len__ ( ) desc [ 'breadth' ] = breadth desc [ 'breadth_path' ] . append ( breadth ) desc_level . append ( desc )
6975	def stellingwerf_pdm_theta ( times , mags , errs , frequency , binsize = 0.05 , minbin = 9 ) : period = 1.0 / frequency fold_time = times [ 0 ] phased = phase_magseries ( times , mags , period , fold_time , wrap = False , sort = True ) phases = phased [ 'phase' ] pmags = phased [ 'mags' ] bins = nparange ( 0.0 , 1.0 , binsize ) binnedphaseinds = npdigitize ( phases , bins ) binvariances = [ ] binndets = [ ] goodbins = 0 for x in npunique ( binnedphaseinds ) : thisbin_inds = binnedphaseinds == x thisbin_mags = pmags [ thisbin_inds ] if thisbin_mags . size > minbin : thisbin_variance = npvar ( thisbin_mags , ddof = 1 ) binvariances . append ( thisbin_variance ) binndets . append ( thisbin_mags . size ) goodbins = goodbins + 1 binvariances = nparray ( binvariances ) binndets = nparray ( binndets ) theta_top = npsum ( binvariances * ( binndets - 1 ) ) / ( npsum ( binndets ) - goodbins ) theta_bot = npvar ( pmags , ddof = 1 ) theta = theta_top / theta_bot return theta
7821	def challenge ( self , challenge ) : if not challenge : logger . debug ( "Empty challenge" ) return Failure ( "bad-challenge" ) if self . _server_first_message : return self . _final_challenge ( challenge ) match = SERVER_FIRST_MESSAGE_RE . match ( challenge ) if not match : logger . debug ( "Bad challenge syntax: {0!r}" . format ( challenge ) ) return Failure ( "bad-challenge" ) self . _server_first_message = challenge mext = match . group ( "mext" ) if mext : logger . debug ( "Unsupported extension received: {0!r}" . format ( mext ) ) return Failure ( "bad-challenge" ) nonce = match . group ( "nonce" ) if not nonce . startswith ( self . _c_nonce ) : logger . debug ( "Nonce does not start with our nonce" ) return Failure ( "bad-challenge" ) salt = match . group ( "salt" ) try : salt = a2b_base64 ( salt ) except ValueError : logger . debug ( "Bad base64 encoding for salt: {0!r}" . format ( salt ) ) return Failure ( "bad-challenge" ) iteration_count = match . group ( "iteration_count" ) try : iteration_count = int ( iteration_count ) except ValueError : logger . debug ( "Bad iteration_count: {0!r}" . format ( iteration_count ) ) return Failure ( "bad-challenge" ) return self . _make_response ( nonce , salt , iteration_count )
6124	def gaussian_prior_model_for_arguments ( self , arguments ) : new_model = copy . deepcopy ( self ) for key , value in filter ( lambda t : isinstance ( t [ 1 ] , pm . PriorModel ) , self . __dict__ . items ( ) ) : setattr ( new_model , key , value . gaussian_prior_model_for_arguments ( arguments ) ) return new_model
4903	def link_to_modal ( link_text , index , autoescape = True ) : link = ( '<a' ' href="#!"' ' class="text-underline view-course-details-link"' ' id="view-course-details-link-{index}"' ' data-toggle="modal"' ' data-target="#course-details-modal-{index}"' '>{link_text}</a>' ) . format ( index = index , link_text = link_text , ) return mark_safe ( link )
1265	def sanity_check_actions ( actions_spec ) : actions = copy . deepcopy ( actions_spec ) is_unique = ( 'type' in actions ) if is_unique : actions = dict ( action = actions ) for name , action in actions . items ( ) : if 'type' not in action : action [ 'type' ] = 'int' if action [ 'type' ] == 'int' : if 'num_actions' not in action : raise TensorForceError ( "Action requires value 'num_actions' set!" ) elif action [ 'type' ] == 'float' : if ( 'min_value' in action ) != ( 'max_value' in action ) : raise TensorForceError ( "Action requires both values 'min_value' and 'max_value' set!" ) if 'shape' not in action : action [ 'shape' ] = ( ) if isinstance ( action [ 'shape' ] , int ) : action [ 'shape' ] = ( action [ 'shape' ] , ) return actions , is_unique
300	def plot_slippage_sensitivity ( returns , positions , transactions , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) avg_returns_given_slippage = pd . Series ( ) for bps in range ( 1 , 100 ) : adj_returns = txn . adjust_returns_for_slippage ( returns , positions , transactions , bps ) avg_returns = ep . annual_return ( adj_returns ) avg_returns_given_slippage . loc [ bps ] = avg_returns avg_returns_given_slippage . plot ( alpha = 1.0 , lw = 2 , ax = ax ) ax . set_title ( 'Average annual returns given additional per-dollar slippage' ) ax . set_xticks ( np . arange ( 0 , 100 , 10 ) ) ax . set_ylabel ( 'Average annual return' ) ax . set_xlabel ( 'Per-dollar slippage (bps)' ) return ax
4114	def rc2lar ( k ) : assert numpy . isrealobj ( k ) , 'Log area ratios not defined for complex reflection coefficients.' if max ( numpy . abs ( k ) ) >= 1 : raise ValueError ( 'All reflection coefficients should have magnitude less than unity.' ) return - 2 * numpy . arctanh ( - numpy . array ( k ) )
6288	def find_commands ( command_dir : str ) -> List [ str ] : if not command_dir : return [ ] return [ name for _ , name , is_pkg in pkgutil . iter_modules ( [ command_dir ] ) if not is_pkg and not name . startswith ( '_' ) ]
10028	def describe_events ( self , environment_name , next_token = None , start_time = None ) : events = self . ebs . describe_events ( application_name = self . app_name , environment_name = environment_name , next_token = next_token , start_time = start_time + 'Z' ) return ( events [ 'DescribeEventsResponse' ] [ 'DescribeEventsResult' ] [ 'Events' ] , events [ 'DescribeEventsResponse' ] [ 'DescribeEventsResult' ] [ 'NextToken' ] )
9200	def _sort_lows_and_highs ( func ) : "Decorator for extract_cycles" @ functools . wraps ( func ) def wrapper ( * args , ** kwargs ) : for low , high , mult in func ( * args , ** kwargs ) : if low < high : yield low , high , mult else : yield high , low , mult return wrapper
12028	def headerHTML ( header , fname ) : html = "<html><body><code>" html += "<h2>%s</h2>" % ( fname ) html += pprint . pformat ( header , indent = 1 ) html = html . replace ( "\n" , '<br>' ) . replace ( " " , "&nbsp;" ) html = html . replace ( r"\x00" , "" ) html += "</code></body></html>" print ( "saving header file:" , fname ) f = open ( fname , 'w' ) f . write ( html ) f . close ( ) webbrowser . open ( fname )
9065	def value ( self ) : if not self . _fix [ "beta" ] : self . _update_beta ( ) if not self . _fix [ "scale" ] : self . _update_scale ( ) return self . lml ( )
2997	def marketOhlcDF ( token = '' , version = '' ) : x = marketOhlc ( token , version ) data = [ ] for key in x : data . append ( x [ key ] ) data [ - 1 ] [ 'symbol' ] = key df = pd . io . json . json_normalize ( data ) _toDatetime ( df ) _reindex ( df , 'symbol' ) return df
3559	def find_service ( self , uuid ) : for service in self . list_services ( ) : if service . uuid == uuid : return service return None
6302	def add_package ( self , name ) : name , cls_name = parse_package_string ( name ) if name in self . package_map : return package = EffectPackage ( name ) package . load ( ) self . packages . append ( package ) self . package_map [ package . name ] = package self . polulate ( package . effect_packages )
6790	def createsuperuser ( self , username = 'admin' , email = None , password = None , site = None ) : r = self . local_renderer site = site or self . genv . SITE self . set_site_specifics ( site ) options = [ '--username=%s' % username ] if email : options . append ( '--email=%s' % email ) if password : options . append ( '--password=%s' % password ) r . env . options_str = ' ' . join ( options ) if self . is_local : r . env . project_dir = r . env . local_project_dir r . genv . SITE = r . genv . SITE or site r . run_or_local ( 'export SITE={SITE}; export ROLE={ROLE}; cd {project_dir}; {manage_cmd} {createsuperuser_cmd} {options_str}' )
7338	def get_args ( func , skip = 0 ) : code = getattr ( func , '__code__' , None ) if code is None : code = func . __call__ . __code__ return code . co_varnames [ skip : code . co_argcount ]
12909	def from_json ( cls , fh ) : if isinstance ( fh , str ) : return cls ( json . loads ( fh ) ) else : return cls ( json . load ( fh ) )
2469	def set_file_copyright ( self , doc , text ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_copytext_set : self . file_copytext_set = True if validations . validate_file_cpyright ( text ) : if isinstance ( text , string_types ) : self . file ( doc ) . copyright = str_from_text ( text ) else : self . file ( doc ) . copyright = text return True else : raise SPDXValueError ( 'File::CopyRight' ) else : raise CardinalityError ( 'File::CopyRight' ) else : raise OrderError ( 'File::CopyRight' )
11729	def flag_inner_classes ( obj ) : for tup in class_members ( obj ) : tup [ 1 ] . _parent = obj tup [ 1 ] . _parent_inst = None tup [ 1 ] . __getattr__ = my_getattr flag_inner_classes ( tup [ 1 ] )
706	def runModel ( self , modelID , jobID , modelParams , modelParamsHash , jobsDAO , modelCheckpointGUID ) : if not self . _createCheckpoints : modelCheckpointGUID = None self . _resultsDB . update ( modelID = modelID , modelParams = modelParams , modelParamsHash = modelParamsHash , metricResult = None , completed = False , completionReason = None , matured = False , numRecords = 0 ) structuredParams = modelParams [ 'structuredParams' ] if self . logger . getEffectiveLevel ( ) <= logging . DEBUG : self . logger . debug ( "Running Model. \nmodelParams: %s, \nmodelID=%s, " % ( pprint . pformat ( modelParams , indent = 4 ) , modelID ) ) cpuTimeStart = time . clock ( ) logLevel = self . logger . getEffectiveLevel ( ) try : if self . _dummyModel is None or self . _dummyModel is False : ( cmpReason , cmpMsg ) = runModelGivenBaseAndParams ( modelID = modelID , jobID = jobID , baseDescription = self . _baseDescription , params = structuredParams , predictedField = self . _predictedField , reportKeys = self . _reportKeys , optimizeKey = self . _optimizeKey , jobsDAO = jobsDAO , modelCheckpointGUID = modelCheckpointGUID , logLevel = logLevel , predictionCacheMaxRecords = self . _predictionCacheMaxRecords ) else : dummyParams = dict ( self . _dummyModel ) dummyParams [ 'permutationParams' ] = structuredParams if self . _dummyModelParamsFunc is not None : permInfo = dict ( structuredParams ) permInfo [ 'generation' ] = modelParams [ 'particleState' ] [ 'genIdx' ] dummyParams . update ( self . _dummyModelParamsFunc ( permInfo ) ) ( cmpReason , cmpMsg ) = runDummyModel ( modelID = modelID , jobID = jobID , params = dummyParams , predictedField = self . _predictedField , reportKeys = self . _reportKeys , optimizeKey = self . _optimizeKey , jobsDAO = jobsDAO , modelCheckpointGUID = modelCheckpointGUID , logLevel = logLevel , predictionCacheMaxRecords = self . _predictionCacheMaxRecords ) jobsDAO . modelSetCompleted ( modelID , completionReason = cmpReason , completionMsg = cmpMsg , cpuTime = time . clock ( ) - cpuTimeStart ) except InvalidConnectionException , e : self . logger . warn ( "%s" , e )
8837	def merge ( * args ) : ret = [ ] for arg in args : if isinstance ( arg , list ) or isinstance ( arg , tuple ) : ret += list ( arg ) else : ret . append ( arg ) return ret
6568	def random_xorsat ( num_variables , num_clauses , vartype = dimod . BINARY , satisfiable = True ) : if num_variables < 3 : raise ValueError ( "a xor problem needs at least 3 variables" ) if num_clauses > 8 * _nchoosek ( num_variables , 3 ) : raise ValueError ( "too many clauses" ) csp = ConstraintSatisfactionProblem ( vartype ) variables = list ( range ( num_variables ) ) constraints = set ( ) if satisfiable : values = tuple ( vartype . value ) planted_solution = { v : choice ( values ) for v in variables } configurations = [ ( 0 , 0 , 0 ) , ( 0 , 1 , 1 ) , ( 1 , 0 , 1 ) , ( 1 , 1 , 0 ) ] while len ( constraints ) < num_clauses : x , y , z = sample ( variables , 3 ) if y > x : x , y = y , x const = xor_gate ( [ x , y , z ] , vartype = vartype ) config = choice ( configurations ) for idx , v in enumerate ( const . variables ) : if config [ idx ] != ( planted_solution [ v ] > 0 ) : const . flip_variable ( v ) assert const . check ( planted_solution ) constraints . add ( const ) else : while len ( constraints ) < num_clauses : x , y , z = sample ( variables , 3 ) if y > x : x , y = y , x const = xor_gate ( [ x , y , z ] , vartype = vartype ) for idx , v in enumerate ( const . variables ) : if random ( ) > .5 : const . flip_variable ( v ) assert const . check ( planted_solution ) constraints . add ( const ) for const in constraints : csp . add_constraint ( const ) for v in variables : csp . add_variable ( v ) return csp
758	def generateRandomInput ( numRecords , elemSize = 400 , numSet = 42 ) : inputs = [ ] for _ in xrange ( numRecords ) : input = np . zeros ( elemSize , dtype = realDType ) for _ in range ( 0 , numSet ) : ind = np . random . random_integers ( 0 , elemSize - 1 , 1 ) [ 0 ] input [ ind ] = 1 while abs ( input . sum ( ) - numSet ) > 0.1 : ind = np . random . random_integers ( 0 , elemSize - 1 , 1 ) [ 0 ] input [ ind ] = 1 inputs . append ( input ) return inputs
2770	def get_firewall ( self , firewall_id ) : return Firewall . get_object ( api_token = self . token , firewall_id = firewall_id , )
3061	def positional ( max_positional_args ) : def positional_decorator ( wrapped ) : @ functools . wraps ( wrapped ) def positional_wrapper ( * args , ** kwargs ) : if len ( args ) > max_positional_args : plural_s = '' if max_positional_args != 1 : plural_s = 's' message = ( '{function}() takes at most {args_max} positional ' 'argument{plural} ({args_given} given)' . format ( function = wrapped . __name__ , args_max = max_positional_args , args_given = len ( args ) , plural = plural_s ) ) if positional_parameters_enforcement == POSITIONAL_EXCEPTION : raise TypeError ( message ) elif positional_parameters_enforcement == POSITIONAL_WARNING : logger . warning ( message ) return wrapped ( * args , ** kwargs ) return positional_wrapper if isinstance ( max_positional_args , six . integer_types ) : return positional_decorator else : args , _ , _ , defaults = inspect . getargspec ( max_positional_args ) return positional ( len ( args ) - len ( defaults ) ) ( max_positional_args )
9489	def generate_bytecode_from_obb ( obb : object , previous : bytes ) -> bytes : if isinstance ( obb , pyte . superclasses . _PyteOp ) : return obb . to_bytes ( previous ) elif isinstance ( obb , ( pyte . superclasses . _PyteAugmentedComparator , pyte . superclasses . _PyteAugmentedValidator . _FakeMathematicalOP ) ) : return obb . to_bytes ( previous ) elif isinstance ( obb , pyte . superclasses . _PyteAugmentedValidator ) : obb . validate ( ) return obb . to_load ( ) elif isinstance ( obb , int ) : return obb . to_bytes ( ( obb . bit_length ( ) + 7 ) // 8 , byteorder = "little" ) or b'' elif isinstance ( obb , bytes ) : return obb else : raise TypeError ( "`{}` was not a valid bytecode-encodable item" . format ( obb ) )
8966	def which ( command , path = None , verbose = 0 , exts = None ) : matched = whichgen ( command , path , verbose , exts ) try : match = next ( matched ) except StopIteration : raise WhichError ( "Could not find '%s' on the path." % command ) else : return match
2685	def fate ( name ) : return cached_download ( 'http://fate.ffmpeg.org/fate-suite/' + name , os . path . join ( 'fate-suite' , name . replace ( '/' , os . path . sep ) ) )
9487	def pack_value ( index : int ) -> bytes : if PY36 : return index . to_bytes ( 1 , byteorder = "little" ) else : return index . to_bytes ( 2 , byteorder = "little" )
1616	def Search ( pattern , s ) : if pattern not in _regexp_compile_cache : _regexp_compile_cache [ pattern ] = sre_compile . compile ( pattern ) return _regexp_compile_cache [ pattern ] . search ( s )
3572	def peripheral_didDiscoverServices_ ( self , peripheral , services ) : logger . debug ( 'peripheral_didDiscoverServices called' ) for service in peripheral . services ( ) : if service_list ( ) . get ( service ) is None : service_list ( ) . add ( service , CoreBluetoothGattService ( service ) ) peripheral . discoverCharacteristics_forService_ ( None , service )
13248	def get_bibliography ( lsst_bib_names = None , bibtex = None ) : bibtex_data = get_lsst_bibtex ( bibtex_filenames = lsst_bib_names ) pybtex_data = [ pybtex . database . parse_string ( _bibtex , 'bibtex' ) for _bibtex in bibtex_data . values ( ) ] if bibtex is not None : pybtex_data . append ( pybtex . database . parse_string ( bibtex , 'bibtex' ) ) bib = pybtex_data [ 0 ] if len ( pybtex_data ) > 1 : for other_bib in pybtex_data [ 1 : ] : for key , entry in other_bib . entries . items ( ) : bib . add_entry ( key , entry ) return bib
2741	def remove_tags ( self , tags ) : return self . get_data ( "firewalls/%s/tags" % self . id , type = DELETE , params = { "tags" : tags } )
448	def _bias_add ( x , b , data_format ) : if data_format == 'NHWC' : return tf . add ( x , b ) elif data_format == 'NCHW' : return tf . add ( x , _to_channel_first_bias ( b ) ) else : raise ValueError ( 'invalid data_format: %s' % data_format )
4296	def parse_config_file ( parser , stdin_args ) : config_args = [ ] required_args = [ ] for action in parser . _actions : if action . required : required_args . append ( action ) action . required = False parsed_args = parser . parse_args ( stdin_args ) for action in required_args : action . required = True if not parsed_args . config_file : return config_args config = ConfigParser ( ) if not config . read ( parsed_args . config_file ) : sys . stderr . write ( 'Config file "{0}" doesn\'t exists\n' . format ( parsed_args . config_file ) ) sys . exit ( 7 ) config_args = _convert_config_to_stdin ( config , parser ) return config_args
806	def enableTap ( self , tapPath ) : self . _tapFileIn = open ( tapPath + '.in' , 'w' ) self . _tapFileOut = open ( tapPath + '.out' , 'w' )
301	def plot_daily_turnover_hist ( transactions , positions , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) turnover = txn . get_turnover ( positions , transactions ) sns . distplot ( turnover , ax = ax , ** kwargs ) ax . set_title ( 'Distribution of daily turnover rates' ) ax . set_xlabel ( 'Turnover rate' ) return ax
11442	def _warning ( code ) : if isinstance ( code , str ) : return code message = '' if isinstance ( code , tuple ) : if isinstance ( code [ 0 ] , str ) : message = code [ 1 ] code = code [ 0 ] return CFG_BIBRECORD_WARNING_MSGS . get ( code , '' ) + message
4447	def add_document ( self , doc_id , nosave = False , score = 1.0 , payload = None , replace = False , partial = False , language = None , ** fields ) : return self . _add_document ( doc_id , conn = None , nosave = nosave , score = score , payload = payload , replace = replace , partial = partial , language = language , ** fields )
5765	def _decrypt_encrypted_data ( encryption_algorithm_info , encrypted_content , password ) : decrypt_func = crypto_funcs [ encryption_algorithm_info . encryption_cipher ] if encryption_algorithm_info . kdf == 'pbkdf2' : if encryption_algorithm_info . encryption_cipher == 'rc5' : raise ValueError ( pretty_message ( ) ) enc_key = pbkdf2 ( encryption_algorithm_info . kdf_hmac , password , encryption_algorithm_info . kdf_salt , encryption_algorithm_info . kdf_iterations , encryption_algorithm_info . key_length ) enc_iv = encryption_algorithm_info . encryption_iv plaintext = decrypt_func ( enc_key , encrypted_content , enc_iv ) elif encryption_algorithm_info . kdf == 'pbkdf1' : derived_output = pbkdf1 ( encryption_algorithm_info . kdf_hmac , password , encryption_algorithm_info . kdf_salt , encryption_algorithm_info . kdf_iterations , encryption_algorithm_info . key_length + 8 ) enc_key = derived_output [ 0 : 8 ] enc_iv = derived_output [ 8 : 16 ] plaintext = decrypt_func ( enc_key , encrypted_content , enc_iv ) elif encryption_algorithm_info . kdf == 'pkcs12_kdf' : enc_key = pkcs12_kdf ( encryption_algorithm_info . kdf_hmac , password , encryption_algorithm_info . kdf_salt , encryption_algorithm_info . kdf_iterations , encryption_algorithm_info . key_length , 1 ) if encryption_algorithm_info . encryption_cipher == 'rc4' : plaintext = decrypt_func ( enc_key , encrypted_content ) else : enc_iv = pkcs12_kdf ( encryption_algorithm_info . kdf_hmac , password , encryption_algorithm_info . kdf_salt , encryption_algorithm_info . kdf_iterations , encryption_algorithm_info . encryption_block_size , 2 ) plaintext = decrypt_func ( enc_key , encrypted_content , enc_iv ) return plaintext
428	def load_and_preprocess_imdb_data ( n_gram = None ) : X_train , y_train , X_test , y_test = tl . files . load_imdb_dataset ( nb_words = VOCAB_SIZE ) if n_gram is not None : X_train = np . array ( [ augment_with_ngrams ( x , VOCAB_SIZE , N_BUCKETS , n = n_gram ) for x in X_train ] ) X_test = np . array ( [ augment_with_ngrams ( x , VOCAB_SIZE , N_BUCKETS , n = n_gram ) for x in X_test ] ) return X_train , y_train , X_test , y_test
13390	def format_pathname ( pathname , max_length ) : if max_length <= 3 : raise ValueError ( "max length must be larger than 3" ) if len ( pathname ) > max_length : pathname = "...{}" . format ( pathname [ - ( max_length - 3 ) : ] ) return pathname
152	def get_intersections ( self ) : if Real is float : return list ( self . intersections . keys ( ) ) else : return [ ( float ( p [ 0 ] ) , float ( p [ 1 ] ) ) for p in self . intersections . keys ( ) ]
2411	def initialize_dictionaries ( self , p_set ) : success = False if not ( hasattr ( p_set , '_type' ) ) : error_message = "needs to be an essay set of the train type." log . exception ( error_message ) raise util_functions . InputError ( p_set , error_message ) if not ( p_set . _type == "train" ) : error_message = "needs to be an essay set of the train type." log . exception ( error_message ) raise util_functions . InputError ( p_set , error_message ) div_length = len ( p_set . _essay_sets ) if div_length == 0 : div_length = 1 max_feats2 = int ( math . floor ( 200 / div_length ) ) for i in xrange ( 0 , len ( p_set . _essay_sets ) ) : self . _extractors . append ( FeatureExtractor ( ) ) self . _extractors [ i ] . initialize_dictionaries ( p_set . _essay_sets [ i ] , max_feats2 = max_feats2 ) self . _initialized = True success = True return success
2129	def configure_display ( self , data , kwargs = None , write = False ) : if settings . format != 'human' : return if write : obj , obj_type , res , res_type = self . obj_res ( kwargs ) data [ 'type' ] = kwargs [ 'type' ] data [ obj_type ] = obj data [ res_type ] = res self . set_display_columns ( set_false = [ 'team' if obj_type == 'user' else 'user' ] , set_true = [ 'target_team' if res_type == 'team' else res_type ] ) else : self . set_display_columns ( set_false = [ 'user' , 'team' ] , set_true = [ 'resource_name' , 'resource_type' ] ) if 'results' in data : for i in range ( len ( data [ 'results' ] ) ) : self . populate_resource_columns ( data [ 'results' ] [ i ] ) else : self . populate_resource_columns ( data )
3995	def check_and_load_ssh_auth ( ) : mac_username = get_config_value ( constants . CONFIG_MAC_USERNAME_KEY ) if not mac_username : logging . info ( "Can't setup ssh authorization; no mac_username specified" ) return if not _running_on_mac ( ) : logging . info ( "Skipping SSH load, we are not running on Mac" ) return if _mac_version_is_post_yosemite ( ) : _load_ssh_auth_post_yosemite ( mac_username ) else : _load_ssh_auth_pre_yosemite ( )
13641	def send ( self , use_open_peers = True , queue = True , ** kw ) : if not use_open_peers : ip = kw . get ( 'ip' ) port = kw . get ( 'port' ) peer = 'http://{}:{}' . format ( ip , port ) res = arky . rest . POST . peer . transactions ( peer = peer , transactions = [ self . tx . tx ] ) else : res = arky . core . sendPayload ( self . tx . tx ) if self . tx . success != '0.0%' : self . tx . error = None self . tx . success = True else : self . tx . error = res [ 'messages' ] self . tx . success = False self . tx . tries += 1 self . tx . res = res if queue : self . tx . send = True self . __save ( ) return res
86	def is_single_float ( val ) : return isinstance ( val , numbers . Real ) and not is_single_integer ( val ) and not isinstance ( val , bool )
837	def getDistances ( self , inputPattern ) : dist = self . _getDistances ( inputPattern ) return ( dist , self . _categoryList )
5155	def type_cast ( self , item , schema = None ) : if schema is None : schema = self . _schema properties = schema [ 'properties' ] for key , value in item . items ( ) : if key not in properties : continue try : json_type = properties [ key ] [ 'type' ] except KeyError : json_type = None if json_type == 'integer' and not isinstance ( value , int ) : value = int ( value ) elif json_type == 'boolean' and not isinstance ( value , bool ) : value = value == '1' item [ key ] = value return item
10835	def all ( self ) : response = self . api . get ( url = PATHS [ 'GET_PROFILES' ] ) for raw_profile in response : self . append ( Profile ( self . api , raw_profile ) ) return self
5437	def args_to_job_params ( envs , labels , inputs , inputs_recursive , outputs , outputs_recursive , mounts , input_file_param_util , output_file_param_util , mount_param_util ) : env_data = parse_pair_args ( envs , job_model . EnvParam ) label_data = parse_pair_args ( labels , job_model . LabelParam ) input_data = set ( ) for ( recursive , args ) in ( ( False , inputs ) , ( True , inputs_recursive ) ) : for arg in args : name , value = split_pair ( arg , '=' , nullable_idx = 0 ) name = input_file_param_util . get_variable_name ( name ) input_data . add ( input_file_param_util . make_param ( name , value , recursive ) ) output_data = set ( ) for ( recursive , args ) in ( ( False , outputs ) , ( True , outputs_recursive ) ) : for arg in args : name , value = split_pair ( arg , '=' , 0 ) name = output_file_param_util . get_variable_name ( name ) output_data . add ( output_file_param_util . make_param ( name , value , recursive ) ) mount_data = set ( ) for arg in mounts : if ' ' in arg : key_value_pair , disk_size = arg . split ( ' ' ) name , value = split_pair ( key_value_pair , '=' , 1 ) mount_data . add ( mount_param_util . make_param ( name , value , disk_size ) ) else : name , value = split_pair ( arg , '=' , 1 ) mount_data . add ( mount_param_util . make_param ( name , value , disk_size = None ) ) return { 'envs' : env_data , 'inputs' : input_data , 'outputs' : output_data , 'labels' : label_data , 'mounts' : mount_data , }
2575	def launch_task ( self , task_id , executable , * args , ** kwargs ) : self . tasks [ task_id ] [ 'time_submitted' ] = datetime . datetime . now ( ) hit , memo_fu = self . memoizer . check_memo ( task_id , self . tasks [ task_id ] ) if hit : logger . info ( "Reusing cached result for task {}" . format ( task_id ) ) return memo_fu executor_label = self . tasks [ task_id ] [ "executor" ] try : executor = self . executors [ executor_label ] except Exception : logger . exception ( "Task {} requested invalid executor {}: config is\n{}" . format ( task_id , executor_label , self . _config ) ) if self . monitoring is not None and self . monitoring . resource_monitoring_enabled : executable = self . monitoring . monitor_wrapper ( executable , task_id , self . monitoring . monitoring_hub_url , self . run_id , self . monitoring . resource_monitoring_interval ) with self . submitter_lock : exec_fu = executor . submit ( executable , * args , ** kwargs ) self . tasks [ task_id ] [ 'status' ] = States . launched if self . monitoring is not None : task_log_info = self . _create_task_log_info ( task_id , 'lazy' ) self . monitoring . send ( MessageType . TASK_INFO , task_log_info ) exec_fu . retries_left = self . _config . retries - self . tasks [ task_id ] [ 'fail_count' ] logger . info ( "Task {} launched on executor {}" . format ( task_id , executor . label ) ) return exec_fu
5180	def base_url ( self ) : return '{proto}://{host}:{port}{url_path}' . format ( proto = self . protocol , host = self . host , port = self . port , url_path = self . url_path , )
9423	def _load_metadata ( self , handle ) : rarinfo = self . _read_header ( handle ) while rarinfo : self . filelist . append ( rarinfo ) self . NameToInfo [ rarinfo . filename ] = rarinfo self . _process_current ( handle , constants . RAR_SKIP ) rarinfo = self . _read_header ( handle )
11377	def _normalize_article_dir_with_dtd ( self , path ) : if exists ( join ( path , 'resolved_main.xml' ) ) : return main_xml_content = open ( join ( path , 'main.xml' ) ) . read ( ) arts = [ 'art501.dtd' , 'art510.dtd' , 'art520.dtd' , 'art540.dtd' ] tmp_extracted = 0 for art in arts : if art in main_xml_content : self . _extract_correct_dtd_package ( art . split ( '.' ) [ 0 ] , path ) tmp_extracted = 1 if not tmp_extracted : message = "It looks like the path " + path message += "does not contain an art501, art510, art520 or art540 in main.xml file" self . logger . error ( message ) raise ValueError ( message ) command = [ "xmllint" , "--format" , "--loaddtd" , join ( path , 'main.xml' ) , "--output" , join ( path , 'resolved_main.xml' ) ] dummy , dummy , cmd_err = run_shell_command ( command ) if cmd_err : message = "Error in cleaning %s: %s" % ( join ( path , 'main.xml' ) , cmd_err ) self . logger . error ( message ) raise ValueError ( message )
9565	def main ( ) : description = 'Validate a CSV data file.' parser = argparse . ArgumentParser ( description = description ) parser . add_argument ( 'file' , metavar = 'FILE' , help = 'a file to be validated' ) parser . add_argument ( '-l' , '--limit' , dest = 'limit' , type = int , action = 'store' , default = 0 , help = 'limit the number of problems reported' ) parser . add_argument ( '-s' , '--summarize' , dest = 'summarize' , action = 'store_true' , default = False , help = 'output only a summary of the different types of problem found' ) parser . add_argument ( '-e' , '--report-unexpected-exceptions' , dest = 'report_unexpected_exceptions' , action = 'store_true' , default = False , help = 'report any unexpected exceptions as problems' ) args = parser . parse_args ( ) if not os . path . isfile ( args . file ) : print '%s is not a file' % args . file sys . exit ( 1 ) with open ( args . file , 'r' ) as f : data = csv . reader ( f , delimiter = '\t' ) validator = create_validator ( ) problems = validator . validate ( data , summarize = args . summarize , report_unexpected_exceptions = args . report_unexpected_exceptions , context = { 'file' : args . file } ) write_problems ( problems , sys . stdout , summarize = args . summarize , limit = args . limit ) if problems : sys . exit ( 1 ) else : sys . exit ( 0 )
13668	def slinky ( filename , seconds_available , bucket_name , aws_key , aws_secret ) : if not os . environ . get ( 'AWS_ACCESS_KEY_ID' ) and os . environ . get ( 'AWS_SECRET_ACCESS_KEY' ) : print 'Need to set environment variables for AWS access and create a slinky bucket.' exit ( ) print create_temp_s3_link ( filename , seconds_available , bucket_name )
6338	def dist_tversky ( src , tar , qval = 2 , alpha = 1 , beta = 1 , bias = None ) : return Tversky ( ) . dist ( src , tar , qval , alpha , beta , bias )
3191	def update ( self , folder_id , data ) : if 'name' not in data : raise KeyError ( 'The template folder must have a name' ) self . folder_id = folder_id return self . _mc_client . _patch ( url = self . _build_path ( folder_id ) , data = data )
576	def tick ( self ) : for act in self . __activities : if not act . iteratorHolder [ 0 ] : continue try : next ( act . iteratorHolder [ 0 ] ) except StopIteration : act . cb ( ) if act . repeating : act . iteratorHolder [ 0 ] = iter ( xrange ( act . period ) ) else : act . iteratorHolder [ 0 ] = None return True
11510	def get_item_metadata ( self , item_id , token = None , revision = None ) : parameters = dict ( ) parameters [ 'id' ] = item_id if token : parameters [ 'token' ] = token if revision : parameters [ 'revision' ] = revision response = self . request ( 'midas.item.getmetadata' , parameters ) return response
2252	def iterable ( obj , strok = False ) : try : iter ( obj ) except Exception : return False else : return strok or not isinstance ( obj , six . string_types )
6461	def _ends_in_cvc ( self , term ) : return len ( term ) > 2 and ( term [ - 1 ] not in self . _vowels and term [ - 2 ] in self . _vowels and term [ - 3 ] not in self . _vowels and term [ - 1 ] not in tuple ( 'wxY' ) )
12814	def startProducing ( self , consumer ) : self . _consumer = consumer self . _current_deferred = defer . Deferred ( ) self . _sent = 0 self . _paused = False if not hasattr ( self , "_chunk_headers" ) : self . _build_chunk_headers ( ) if self . _data : block = "" for field in self . _data : block += self . _chunk_headers [ field ] block += self . _data [ field ] block += "\r\n" self . _send_to_consumer ( block ) if self . _files : self . _files_iterator = self . _files . iterkeys ( ) self . _files_sent = 0 self . _files_length = len ( self . _files ) self . _current_file_path = None self . _current_file_handle = None self . _current_file_length = None self . _current_file_sent = 0 result = self . _produce ( ) if result : return result else : return defer . succeed ( None ) return self . _current_deferred
2242	def split_modpath ( modpath , check = True ) : if six . PY2 : if modpath . endswith ( '.pyc' ) : modpath = modpath [ : - 1 ] modpath_ = abspath ( expanduser ( modpath ) ) if check : if not exists ( modpath_ ) : if not exists ( modpath ) : raise ValueError ( 'modpath={} does not exist' . format ( modpath ) ) raise ValueError ( 'modpath={} is not a module' . format ( modpath ) ) if isdir ( modpath_ ) and not exists ( join ( modpath , '__init__.py' ) ) : raise ValueError ( 'modpath={} is not a module' . format ( modpath ) ) full_dpath , fname_ext = split ( modpath_ ) _relmod_parts = [ fname_ext ] dpath = full_dpath while exists ( join ( dpath , '__init__.py' ) ) : dpath , dname = split ( dpath ) _relmod_parts . append ( dname ) relmod_parts = _relmod_parts [ : : - 1 ] rel_modpath = os . path . sep . join ( relmod_parts ) return dpath , rel_modpath
5506	def screenshot ( url , * args , ** kwargs ) : phantomscript = os . path . join ( os . path . dirname ( __file__ ) , 'take_screenshot.js' ) directory = kwargs . get ( 'save_dir' , '/tmp' ) image_name = kwargs . get ( 'image_name' , None ) or _image_name_from_url ( url ) ext = kwargs . get ( 'format' , 'png' ) . lower ( ) save_path = os . path . join ( directory , image_name ) + '.' + ext crop_to_visible = kwargs . get ( 'crop_to_visible' , False ) cmd_args = [ 'phantomjs' , '--ssl-protocol=any' , phantomscript , url , '--width' , str ( kwargs [ 'width' ] ) , '--height' , str ( kwargs [ 'height' ] ) , '--useragent' , str ( kwargs [ 'user_agent' ] ) , '--dir' , directory , '--ext' , ext , '--name' , str ( image_name ) , ] if crop_to_visible : cmd_args . append ( '--croptovisible' ) output = subprocess . Popen ( cmd_args , stdout = subprocess . PIPE ) . communicate ( ) [ 0 ] return Screenshot ( save_path , directory , image_name + '.' + ext , ext )
5379	def build_pipeline ( cls , project , zones , min_cores , min_ram , disk_size , boot_disk_size , preemptible , accelerator_type , accelerator_count , image , script_name , envs , inputs , outputs , pipeline_name ) : if min_cores is None : min_cores = job_model . DEFAULT_MIN_CORES if min_ram is None : min_ram = job_model . DEFAULT_MIN_RAM if disk_size is None : disk_size = job_model . DEFAULT_DISK_SIZE if boot_disk_size is None : boot_disk_size = job_model . DEFAULT_BOOT_DISK_SIZE if preemptible is None : preemptible = job_model . DEFAULT_PREEMPTIBLE docker_command = cls . _build_pipeline_docker_command ( script_name , inputs , outputs , envs ) input_envs = [ { 'name' : SCRIPT_VARNAME } ] + [ { 'name' : env . name } for env in envs if env . value ] input_files = [ cls . _build_pipeline_input_file_param ( var . name , var . docker_path ) for var in inputs if not var . recursive and var . value ] output_files = [ cls . _build_pipeline_file_param ( var . name , var . docker_path ) for var in outputs if not var . recursive and var . value ] return { 'ephemeralPipeline' : { 'projectId' : project , 'name' : pipeline_name , 'resources' : { 'minimumCpuCores' : min_cores , 'minimumRamGb' : min_ram , 'bootDiskSizeGb' : boot_disk_size , 'preemptible' : preemptible , 'zones' : google_base . get_zones ( zones ) , 'acceleratorType' : accelerator_type , 'acceleratorCount' : accelerator_count , 'disks' : [ { 'name' : 'datadisk' , 'autoDelete' : True , 'sizeGb' : disk_size , 'mountPoint' : providers_util . DATA_MOUNT_POINT , } ] , } , 'inputParameters' : input_envs + input_files , 'outputParameters' : output_files , 'docker' : { 'imageName' : image , 'cmd' : docker_command , } } }
6605	def result_fullpath ( self , package_index ) : ret = os . path . join ( self . path , self . result_relpath ( package_index ) ) return ret
8386	def amend_filename ( filename , amend ) : base , ext = os . path . splitext ( filename ) amended_name = base + amend + ext return amended_name
12439	def serialize ( self , data , response = None , request = None , format = None ) : if isinstance ( self , Resource ) : if not request : request = self . _request Serializer = None if format : Serializer = self . meta . serializers [ format ] if not Serializer : media_ranges = ( request . get ( 'Accept' ) or '*/*' ) . strip ( ) if not media_ranges : media_ranges = '*/*' if media_ranges != '*/*' : media_types = six . iterkeys ( self . _serializer_map ) media_type = mimeparse . best_match ( media_types , media_ranges ) if media_type : format = self . _serializer_map [ media_type ] Serializer = self . meta . serializers [ format ] else : default = self . meta . default_serializer Serializer = self . meta . serializers [ default ] if Serializer : try : serializer = Serializer ( request , response ) return serializer . serialize ( data ) , serializer except ValueError : pass available = { } for name in self . meta . allowed_serializers : Serializer = self . meta . serializers [ name ] instance = Serializer ( request , None ) if instance . can_serialize ( data ) : available [ name ] = Serializer . media_types [ 0 ] raise http . exceptions . NotAcceptable ( available )
11899	def _get_src_from_image ( img , fallback_image_file ) : if img is None : return fallback_image_file target_format = img . format if target_format . lower ( ) in [ 'tif' , 'tiff' ] : target_format = 'JPEG' try : bytesio = io . BytesIO ( ) img . save ( bytesio , target_format ) byte_value = bytesio . getvalue ( ) b64 = base64 . b64encode ( byte_value ) return 'data:image/%s;base64,%s' % ( target_format . lower ( ) , b64 ) except IOError as exptn : print ( 'IOError while saving image bytes: %s' % exptn ) return fallback_image_file
9790	def _remove_trailing_spaces ( line ) : while line . endswith ( ' ' ) and not line . endswith ( '\\ ' ) : line = line [ : - 1 ] return line . replace ( '\\ ' , ' ' )
11782	def check_me ( self ) : "Check that my fields make sense." assert len ( self . attrnames ) == len ( self . attrs ) assert self . target in self . attrs assert self . target not in self . inputs assert set ( self . inputs ) . issubset ( set ( self . attrs ) ) map ( self . check_example , self . examples )
633	def createSynapse ( self , segment , presynapticCell , permanence ) : idx = len ( segment . _synapses ) synapse = Synapse ( segment , presynapticCell , permanence , self . _nextSynapseOrdinal ) self . _nextSynapseOrdinal += 1 segment . _synapses . add ( synapse ) self . _synapsesForPresynapticCell [ presynapticCell ] . add ( synapse ) self . _numSynapses += 1 return synapse
13050	def main ( ) : search = ServiceSearch ( ) services = search . get_services ( up = True , tags = [ '!header_scan' ] ) print_notification ( "Scanning {} services" . format ( len ( services ) ) ) urllib3 . disable_warnings ( urllib3 . exceptions . InsecureRequestWarning ) pool = Pool ( 100 ) count = 0 for service in services : count += 1 if count % 50 == 0 : print_notification ( "Checking {}/{} services" . format ( count , len ( services ) ) ) pool . spawn ( check_service , service ) pool . join ( ) print_notification ( "Completed, 'http' tag added to services that respond to http, 'https' tag added to services that respond to https." )
6754	def all_other_enabled_satchels ( self ) : return dict ( ( name , satchel ) for name , satchel in self . all_satchels . items ( ) if name != self . name . upper ( ) and name . lower ( ) in map ( str . lower , self . genv . services ) )
450	def compute_alpha ( x ) : threshold = _compute_threshold ( x ) alpha1_temp1 = tf . where ( tf . greater ( x , threshold ) , x , tf . zeros_like ( x , tf . float32 ) ) alpha1_temp2 = tf . where ( tf . less ( x , - threshold ) , x , tf . zeros_like ( x , tf . float32 ) ) alpha_array = tf . add ( alpha1_temp1 , alpha1_temp2 , name = None ) alpha_array_abs = tf . abs ( alpha_array ) alpha_array_abs1 = tf . where ( tf . greater ( alpha_array_abs , 0 ) , tf . ones_like ( alpha_array_abs , tf . float32 ) , tf . zeros_like ( alpha_array_abs , tf . float32 ) ) alpha_sum = tf . reduce_sum ( alpha_array_abs ) n = tf . reduce_sum ( alpha_array_abs1 ) alpha = tf . div ( alpha_sum , n ) return alpha
8039	def is_public ( self ) : if self . all is not None : return self . name in self . all else : return not self . name . startswith ( "_" )
611	def _getPropertyValue ( schema , propertyName , options ) : if propertyName not in options : paramsSchema = schema [ 'properties' ] [ propertyName ] if 'default' in paramsSchema : options [ propertyName ] = paramsSchema [ 'default' ] else : options [ propertyName ] = None
5777	def _advapi32_encrypt ( certificate_or_public_key , data , rsa_oaep_padding = False ) : flags = 0 if rsa_oaep_padding : flags = Advapi32Const . CRYPT_OAEP out_len = new ( advapi32 , 'DWORD *' , len ( data ) ) res = advapi32 . CryptEncrypt ( certificate_or_public_key . ex_key_handle , null ( ) , True , flags , null ( ) , out_len , 0 ) handle_error ( res ) buffer_len = deref ( out_len ) buffer = buffer_from_bytes ( buffer_len ) write_to_buffer ( buffer , data ) pointer_set ( out_len , len ( data ) ) res = advapi32 . CryptEncrypt ( certificate_or_public_key . ex_key_handle , null ( ) , True , flags , buffer , out_len , buffer_len ) handle_error ( res ) return bytes_from_buffer ( buffer , deref ( out_len ) ) [ : : - 1 ]
3297	def is_collection ( self , path , environ ) : res = self . get_resource_inst ( path , environ ) return res and res . is_collection
973	def _slowIsSegmentActive ( self , seg , timeStep ) : numSyn = seg . size ( ) numActiveSyns = 0 for synIdx in xrange ( numSyn ) : if seg . getPermanence ( synIdx ) < self . connectedPerm : continue sc , si = self . getColCellIdx ( seg . getSrcCellIdx ( synIdx ) ) if self . infActiveState [ timeStep ] [ sc , si ] : numActiveSyns += 1 if numActiveSyns >= self . activationThreshold : return True return numActiveSyns >= self . activationThreshold
10906	def sim_crb_diff ( std0 , std1 , N = 10000 ) : a = std0 * np . random . randn ( N , len ( std0 ) ) b = std1 * np . random . randn ( N , len ( std1 ) ) return a - b
11216	def valid ( self , time : int = None ) -> bool : if time is None : epoch = datetime ( 1970 , 1 , 1 , 0 , 0 , 0 ) now = datetime . utcnow ( ) time = int ( ( now - epoch ) . total_seconds ( ) ) if isinstance ( self . valid_from , int ) and time < self . valid_from : return False if isinstance ( self . valid_to , int ) and time > self . valid_to : return False return True
12861	def add_months ( self , month_int ) : month_int += self . month while month_int > 12 : self = BusinessDate . add_years ( self , 1 ) month_int -= 12 while month_int < 1 : self = BusinessDate . add_years ( self , - 1 ) month_int += 12 l = monthrange ( self . year , month_int ) [ 1 ] return BusinessDate . from_ymd ( self . year , month_int , min ( l , self . day ) )
3030	def _extract_id_token ( id_token ) : if type ( id_token ) == bytes : segments = id_token . split ( b'.' ) else : segments = id_token . split ( u'.' ) if len ( segments ) != 3 : raise VerifyJwtTokenError ( 'Wrong number of segments in token: {0}' . format ( id_token ) ) return json . loads ( _helpers . _from_bytes ( _helpers . _urlsafe_b64decode ( segments [ 1 ] ) ) )
4420	async def stop ( self ) : await self . _lavalink . ws . send ( op = 'stop' , guildId = self . guild_id ) self . current = None
5063	def get_course_track_selection_url ( course_run , query_parameters ) : try : course_root = reverse ( 'course_modes_choose' , kwargs = { 'course_id' : course_run [ 'key' ] } ) except KeyError : LOGGER . exception ( "KeyError while parsing course run data.\nCourse Run: \n[%s]" , course_run , ) raise url = '{}{}' . format ( settings . LMS_ROOT_URL , course_root ) course_run_url = update_query_parameters ( url , query_parameters ) return course_run_url
5561	def init_bounds ( self ) : if self . _raw [ "init_bounds" ] is None : return self . bounds else : return Bounds ( * _validate_bounds ( self . _raw [ "init_bounds" ] ) )
9219	def _blocks ( self , name ) : i = len ( self ) while i >= 0 : i -= 1 if name in self [ i ] [ '__names__' ] : for b in self [ i ] [ '__blocks__' ] : r = b . raw ( ) if r and r == name : return b else : for b in self [ i ] [ '__blocks__' ] : r = b . raw ( ) if r and name . startswith ( r ) : b = utility . blocksearch ( b , name ) if b : return b return False
7687	def multi_segment ( annotation , sr = 22050 , length = None , ** kwargs ) : PENT = [ 1 , 32. / 27 , 4. / 3 , 3. / 2 , 16. / 9 ] DURATION = 0.1 h_int , _ = hierarchy_flatten ( annotation ) if length is None : length = int ( sr * ( max ( np . max ( _ ) for _ in h_int ) + 1. / DURATION ) + 1 ) y = 0.0 for ints , ( oc , scale ) in zip ( h_int , product ( range ( 3 , 3 + len ( h_int ) ) , PENT ) ) : click = mkclick ( 440.0 * scale * oc , sr = sr , duration = DURATION ) y = y + filter_kwargs ( mir_eval . sonify . clicks , np . unique ( ints ) , fs = sr , length = length , click = click ) return y
1557	def _get_stream_id ( comp_name , stream_id ) : proto_stream_id = topology_pb2 . StreamId ( ) proto_stream_id . id = stream_id proto_stream_id . component_name = comp_name return proto_stream_id
9623	def maybe_decode_header ( header ) : value , encoding = decode_header ( header ) [ 0 ] if encoding : return value . decode ( encoding ) else : return value
12165	def once ( self , event , listener ) : self . emit ( 'new_listener' , event , listener ) self . _once [ event ] . append ( listener ) self . _check_limit ( event ) return self
1360	def get_argument_starttime ( self ) : try : starttime = self . get_argument ( constants . PARAM_STARTTIME ) return starttime except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
6070	def sersic_constant ( self ) : return ( 2 * self . sersic_index ) - ( 1. / 3. ) + ( 4. / ( 405. * self . sersic_index ) ) + ( 46. / ( 25515. * self . sersic_index ** 2 ) ) + ( 131. / ( 1148175. * self . sersic_index ** 3 ) ) - ( 2194697. / ( 30690717750. * self . sersic_index ** 4 ) )
5000	def require_at_least_one_query_parameter ( * query_parameter_names ) : def outer_wrapper ( view ) : @ wraps ( view ) def wrapper ( request , * args , ** kwargs ) : requirement_satisfied = False for query_parameter_name in query_parameter_names : query_parameter_values = request . query_params . getlist ( query_parameter_name ) kwargs [ query_parameter_name ] = query_parameter_values if query_parameter_values : requirement_satisfied = True if not requirement_satisfied : raise ValidationError ( detail = 'You must provide at least one of the following query parameters: {params}.' . format ( params = ', ' . join ( query_parameter_names ) ) ) return view ( request , * args , ** kwargs ) return wrapper return outer_wrapper
13113	def zone_transfer ( address , dns_name ) : ips = [ ] try : print_notification ( "Attempting dns zone transfer for {} on {}" . format ( dns_name , address ) ) z = dns . zone . from_xfr ( dns . query . xfr ( address , dns_name ) ) except dns . exception . FormError : print_notification ( "Zone transfer not allowed" ) return ips names = z . nodes . keys ( ) print_success ( "Zone transfer successfull for {}, found {} entries" . format ( address , len ( names ) ) ) for n in names : node = z [ n ] data = node . get_rdataset ( dns . rdataclass . IN , dns . rdatatype . A ) if data : for item in data . items : address = item . address ips . append ( address ) return ips
2123	def disassociate_failure_node ( self , parent , child ) : return self . _disassoc ( self . _forward_rel_name ( 'failure' ) , parent , child )
106	def avg_pool ( arr , block_size , cval = 0 , preserve_dtype = True ) : return pool ( arr , block_size , np . average , cval = cval , preserve_dtype = preserve_dtype )
12704	def make_quaternion ( theta , * axis ) : x , y , z = axis r = np . sqrt ( x * x + y * y + z * z ) st = np . sin ( theta / 2. ) ct = np . cos ( theta / 2. ) return [ x * st / r , y * st / r , z * st / r , ct ]
9298	def paginate_query ( self , query , count , offset = None , sort = None ) : assert isinstance ( query , peewee . Query ) assert isinstance ( count , int ) assert isinstance ( offset , ( str , int , type ( None ) ) ) assert isinstance ( sort , ( list , set , tuple , type ( None ) ) ) fields = query . model . _meta . get_primary_keys ( ) if len ( fields ) == 0 : raise peewee . ProgrammingError ( 'Cannot apply pagination on model without primary key' ) if len ( fields ) > 1 : raise peewee . ProgrammingError ( 'Cannot apply pagination on model with compound primary key' ) if offset is not None : query = query . where ( fields [ 0 ] >= offset ) order_bys = [ ] if sort : for field , direction in sort : if not isinstance ( direction , str ) : raise ValueError ( "Invalid sort direction on field '{}'" . format ( field ) ) direction = direction . lower ( ) . strip ( ) if direction not in [ 'asc' , 'desc' ] : raise ValueError ( "Invalid sort direction on field '{}'" . format ( field ) ) order_by = peewee . SQL ( field ) order_by = getattr ( order_by , direction ) ( ) order_bys += [ order_by ] order_bys += [ fields [ 0 ] . asc ( ) ] query = query . order_by ( * order_bys ) query = query . limit ( count ) return query
7837	def remove ( self ) : if self . disco is None : return self . xmlnode . unlinkNode ( ) oldns = self . xmlnode . ns ( ) ns = self . xmlnode . newNs ( oldns . getContent ( ) , None ) self . xmlnode . replaceNs ( oldns , ns ) common_root . addChild ( self . xmlnode ( ) ) self . disco = None
10425	def infer_missing_backwards_edge ( graph , u , v , k ) : if u in graph [ v ] : for attr_dict in graph [ v ] [ u ] . values ( ) : if attr_dict == graph [ u ] [ v ] [ k ] : return graph . add_edge ( v , u , key = k , ** graph [ u ] [ v ] [ k ] )
13227	def decorator ( decorator_func ) : assert callable ( decorator_func ) , type ( decorator_func ) def _decorator ( func = None , ** kwargs ) : assert func is None or callable ( func ) , type ( func ) if func : return decorator_func ( func , ** kwargs ) else : def _decorator_helper ( func ) : return decorator_func ( func , ** kwargs ) return _decorator_helper return _decorator
13315	def command ( self ) : cmd = self . config . get ( 'command' , None ) if cmd is None : return cmd = cmd [ platform ] return cmd [ 'path' ] , cmd [ 'args' ]
7529	def setup_dirs ( data ) : pdir = os . path . realpath ( data . paramsdict [ "project_dir" ] ) data . dirs . clusts = os . path . join ( pdir , "{}_clust_{}" . format ( data . name , data . paramsdict [ "clust_threshold" ] ) ) if not os . path . exists ( data . dirs . clusts ) : os . mkdir ( data . dirs . clusts ) data . tmpdir = os . path . abspath ( os . path . expanduser ( os . path . join ( pdir , data . name + '-tmpalign' ) ) ) if not os . path . exists ( data . tmpdir ) : os . mkdir ( data . tmpdir ) if not data . paramsdict [ "assembly_method" ] == "denovo" : data . dirs . refmapping = os . path . join ( pdir , "{}_refmapping" . format ( data . name ) ) if not os . path . exists ( data . dirs . refmapping ) : os . mkdir ( data . dirs . refmapping )
13504	def update ( self , server ) : return server . put ( 'challenge_admin' , self . as_payload ( ) , replacements = { 'slug' : self . slug } )
11559	def i2c_write ( self , address , * args ) : data = [ address , self . I2C_WRITE ] for item in args : data . append ( item & 0x7f ) data . append ( ( item >> 7 ) & 0x7f ) self . _command_handler . send_sysex ( self . _command_handler . I2C_REQUEST , data )
283	def plot_long_short_holdings ( returns , positions , legend_loc = 'upper left' , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) positions = positions . drop ( 'cash' , axis = 'columns' ) positions = positions . replace ( 0 , np . nan ) df_longs = positions [ positions > 0 ] . count ( axis = 1 ) df_shorts = positions [ positions < 0 ] . count ( axis = 1 ) lf = ax . fill_between ( df_longs . index , 0 , df_longs . values , color = 'g' , alpha = 0.5 , lw = 2.0 ) sf = ax . fill_between ( df_shorts . index , 0 , df_shorts . values , color = 'r' , alpha = 0.5 , lw = 2.0 ) bf = patches . Rectangle ( [ 0 , 0 ] , 1 , 1 , color = 'darkgoldenrod' ) leg = ax . legend ( [ lf , sf , bf ] , [ 'Long (max: %s, min: %s)' % ( df_longs . max ( ) , df_longs . min ( ) ) , 'Short (max: %s, min: %s)' % ( df_shorts . max ( ) , df_shorts . min ( ) ) , 'Overlap' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) leg . get_frame ( ) . set_edgecolor ( 'black' ) ax . set_xlim ( ( returns . index [ 0 ] , returns . index [ - 1 ] ) ) ax . set_title ( 'Long and short holdings' ) ax . set_ylabel ( 'Holdings' ) ax . set_xlabel ( '' ) return ax
4656	def broadcast ( self ) : if not self . _is_signed ( ) : self . sign ( ) if "operations" not in self or not self [ "operations" ] : log . warning ( "No operations in transaction! Returning" ) return ret = self . json ( ) if self . blockchain . nobroadcast : log . warning ( "Not broadcasting anything!" ) self . clear ( ) return ret try : if self . blockchain . blocking : ret = self . blockchain . rpc . broadcast_transaction_synchronous ( ret , api = "network_broadcast" ) ret . update ( ** ret . get ( "trx" , { } ) ) else : self . blockchain . rpc . broadcast_transaction ( ret , api = "network_broadcast" ) except Exception as e : raise e finally : self . clear ( ) return ret
3235	def list_buckets ( client = None , ** kwargs ) : buckets = client . list_buckets ( ** kwargs ) return [ b . __dict__ for b in buckets ]
6015	def output_positions ( positions , positions_path ) : with open ( positions_path , 'w' ) as f : for position in positions : f . write ( "%s\n" % position )
12837	def init_async ( self , loop = None ) : self . _loop = loop or asyncio . get_event_loop ( ) self . _async_lock = asyncio . Lock ( loop = loop ) if not self . database == ':memory:' : self . _state = ConnectionLocal ( )
5440	def get_variable_name ( self , name ) : if not name : name = '%s%s' % ( self . _auto_prefix , self . _auto_index ) self . _auto_index += 1 return name
13303	def nmse ( a , b ) : return np . square ( a - b ) . mean ( ) / ( a . mean ( ) * b . mean ( ) )
10538	def create_category ( name , description ) : try : category = dict ( name = name , short_name = name . lower ( ) . replace ( " " , "" ) , description = description ) res = _pybossa_req ( 'post' , 'category' , payload = category ) if res . get ( 'id' ) : return Category ( res ) else : return res except : raise
7536	def derep_concat_split ( data , sample , nthreads , force ) : LOGGER . info ( "INSIDE derep %s" , sample . name ) mergefile = os . path . join ( data . dirs . edits , sample . name + "_merged_.fastq" ) if not force : if not os . path . exists ( mergefile ) : sample . files . edits = concat_multiple_edits ( data , sample ) else : LOGGER . info ( "skipped concat_multiple_edits: {} exists" . format ( mergefile ) ) else : sample . files . edits = concat_multiple_edits ( data , sample ) if 'pair' in data . paramsdict [ 'datatype' ] : if "reference" in data . paramsdict [ "assembly_method" ] : nmerged = merge_pairs ( data , sample . files . edits , mergefile , 0 , 0 ) else : nmerged = merge_pairs ( data , sample . files . edits , mergefile , 1 , 1 ) sample . files . edits = [ ( mergefile , ) ] sample . stats . reads_merged = nmerged if "3rad" in data . paramsdict [ "datatype" ] : declone_3rad ( data , sample ) derep_and_sort ( data , os . path . join ( data . dirs . edits , sample . name + "_declone.fastq" ) , os . path . join ( data . dirs . edits , sample . name + "_derep.fastq" ) , nthreads ) else : derep_and_sort ( data , sample . files . edits [ 0 ] [ 0 ] , os . path . join ( data . dirs . edits , sample . name + "_derep.fastq" ) , nthreads )
1593	def prepare ( self , context ) : for stream_id , targets in self . targets . items ( ) : for target in targets : target . prepare ( context , stream_id )
4774	def contains_sequence ( self , * items ) : if len ( items ) == 0 : raise ValueError ( 'one or more args must be given' ) else : try : for i in xrange ( len ( self . val ) - len ( items ) + 1 ) : for j in xrange ( len ( items ) ) : if self . val [ i + j ] != items [ j ] : break else : return self except TypeError : raise TypeError ( 'val is not iterable' ) self . _err ( 'Expected <%s> to contain sequence %s, but did not.' % ( self . val , self . _fmt_items ( items ) ) )
3124	def _verify_time_range ( payload_dict ) : now = int ( time . time ( ) ) issued_at = payload_dict . get ( 'iat' ) if issued_at is None : raise AppIdentityError ( 'No iat field in token: {0}' . format ( payload_dict ) ) expiration = payload_dict . get ( 'exp' ) if expiration is None : raise AppIdentityError ( 'No exp field in token: {0}' . format ( payload_dict ) ) if expiration >= now + MAX_TOKEN_LIFETIME_SECS : raise AppIdentityError ( 'exp field too far in future: {0}' . format ( payload_dict ) ) earliest = issued_at - CLOCK_SKEW_SECS if now < earliest : raise AppIdentityError ( 'Token used too early, {0} < {1}: {2}' . format ( now , earliest , payload_dict ) ) latest = expiration + CLOCK_SKEW_SECS if now > latest : raise AppIdentityError ( 'Token used too late, {0} > {1}: {2}' . format ( now , latest , payload_dict ) )
9314	def _format_datetime ( dttm ) : if dttm . tzinfo is None or dttm . tzinfo . utcoffset ( dttm ) is None : zoned = pytz . utc . localize ( dttm ) else : zoned = dttm . astimezone ( pytz . utc ) ts = zoned . strftime ( "%Y-%m-%dT%H:%M:%S" ) ms = zoned . strftime ( "%f" ) precision = getattr ( dttm , "precision" , None ) if precision == "second" : pass elif precision == "millisecond" : ts = ts + "." + ms [ : 3 ] elif zoned . microsecond > 0 : ts = ts + "." + ms . rstrip ( "0" ) return ts + "Z"
9441	def reload_config ( self , call_params ) : path = '/' + self . api_version + '/ReloadConfig/' method = 'POST' return self . request ( path , method , call_params )
7257	def get_address_coords ( self , address ) : url = "https://maps.googleapis.com/maps/api/geocode/json?&address=" + address r = requests . get ( url ) r . raise_for_status ( ) results = r . json ( ) [ 'results' ] lat = results [ 0 ] [ 'geometry' ] [ 'location' ] [ 'lat' ] lng = results [ 0 ] [ 'geometry' ] [ 'location' ] [ 'lng' ] return lat , lng
9244	def set_date_from_event ( self , event , issue ) : if not event . get ( 'commit_id' , None ) : issue [ 'actual_date' ] = timestring_to_datetime ( issue [ 'closed_at' ] ) return try : commit = self . fetcher . fetch_commit ( event ) issue [ 'actual_date' ] = timestring_to_datetime ( commit [ 'author' ] [ 'date' ] ) except ValueError : print ( "WARNING: Can't fetch commit {0}. " "It is probably referenced from another repo." . format ( event [ 'commit_id' ] ) ) issue [ 'actual_date' ] = timestring_to_datetime ( issue [ 'closed_at' ] )
3879	async def _handle_set_typing_notification ( self , set_typing_notification ) : conv_id = set_typing_notification . conversation_id . id res = parsers . parse_typing_status_message ( set_typing_notification ) await self . on_typing . fire ( res ) try : conv = await self . _get_or_fetch_conversation ( conv_id ) except exceptions . NetworkError : logger . warning ( 'Failed to fetch conversation for typing notification: %s' , conv_id ) else : await conv . on_typing . fire ( res )
8207	def reflect ( self , x0 , y0 , x , y ) : rx = x0 - ( x - x0 ) ry = y0 - ( y - y0 ) return rx , ry
10335	def build_delete_node_by_hash ( manager : Manager ) -> Callable [ [ BELGraph , str ] , None ] : @ in_place_transformation def delete_node_by_hash ( graph : BELGraph , node_hash : str ) -> None : node = manager . get_dsl_by_hash ( node_hash ) graph . remove_node ( node ) return delete_node_by_hash
13568	def selected_course ( func ) : @ wraps ( func ) def inner ( * args , ** kwargs ) : course = Course . get_selected ( ) return func ( course , * args , ** kwargs ) return inner
6023	def new_psf_with_renormalized_array ( self ) : return PSF ( array = self , pixel_scale = self . pixel_scale , renormalize = True )
3851	async def lookup_entities ( client , args ) : lookup_spec = _get_lookup_spec ( args . entity_identifier ) request = hangups . hangouts_pb2 . GetEntityByIdRequest ( request_header = client . get_request_header ( ) , batch_lookup_spec = [ lookup_spec ] , ) res = await client . get_entity_by_id ( request ) for entity_result in res . entity_result : for entity in entity_result . entity : print ( entity )
986	def mmPrettyPrintConnections ( self ) : text = "" text += ( "Segments: (format => " "(#) [(source cell=permanence ...), ...]\n" ) text += "------------------------------------\n" columns = range ( self . numberOfColumns ( ) ) for column in columns : cells = self . cellsForColumn ( column ) for cell in cells : segmentDict = dict ( ) for seg in self . connections . segmentsForCell ( cell ) : synapseList = [ ] for synapse in self . connections . synapsesForSegment ( seg ) : synapseData = self . connections . dataForSynapse ( synapse ) synapseList . append ( ( synapseData . presynapticCell , synapseData . permanence ) ) synapseList . sort ( ) synapseStringList = [ "{0:3}={1:.2f}" . format ( sourceCell , permanence ) for sourceCell , permanence in synapseList ] segmentDict [ seg ] = "({0})" . format ( " " . join ( synapseStringList ) ) text += ( "Column {0:3} / Cell {1:3}:\t({2}) {3}\n" . format ( column , cell , len ( segmentDict . values ( ) ) , "[{0}]" . format ( ", " . join ( segmentDict . values ( ) ) ) ) ) if column < len ( columns ) - 1 : text += "\n" text += "------------------------------------\n" return text
12485	def get_dict_leaves ( data ) : result = [ ] if isinstance ( data , dict ) : for item in data . values ( ) : result . extend ( get_dict_leaves ( item ) ) elif isinstance ( data , list ) : result . extend ( data ) else : result . append ( data ) return result
2728	def get_action ( self , action_id ) : return Action . get_object ( api_token = self . token , action_id = action_id )
4652	def set_fee_asset ( self , fee_asset ) : if isinstance ( fee_asset , self . amount_class ) : self . fee_asset_id = fee_asset [ "id" ] elif isinstance ( fee_asset , self . asset_class ) : self . fee_asset_id = fee_asset [ "id" ] elif fee_asset : self . fee_asset_id = fee_asset else : self . fee_asset_id = "1.3.0"
2410	def create_essay_set_and_dump_model ( text , score , prompt , model_path , additional_array = None ) : essay_set = create_essay_set ( text , score , prompt ) feature_ext , clf = extract_features_and_generate_model ( essay_set , additional_array ) dump_model_to_file ( prompt , feature_ext , clf , model_path )
5313	def translate_style ( style , colormode , colorpalette ) : style_parts = iter ( style . split ( '_' ) ) ansi_start_sequence = [ ] ansi_end_sequence = [ ] try : part = None for mod_part in style_parts : part = mod_part if part not in ansi . MODIFIERS : break mod_start_code , mod_end_code = resolve_modifier_to_ansi_code ( part , colormode ) ansi_start_sequence . append ( mod_start_code ) ansi_end_sequence . append ( mod_end_code ) else : raise StopIteration ( ) if part != 'on' : ansi_start_code , ansi_end_code = translate_colorname_to_ansi_code ( part , ansi . FOREGROUND_COLOR_OFFSET , colormode , colorpalette ) ansi_start_sequence . append ( ansi_start_code ) ansi_end_sequence . append ( ansi_end_code ) next ( style_parts ) part = next ( style_parts ) ansi_start_code , ansi_end_code = translate_colorname_to_ansi_code ( part , ansi . BACKGROUND_COLOR_OFFSET , colormode , colorpalette ) ansi_start_sequence . append ( ansi_start_code ) ansi_end_sequence . append ( ansi_end_code ) except StopIteration : pass return '' . join ( ansi_start_sequence ) , '' . join ( ansi_end_sequence )
11230	def xafter ( self , dt , count = None , inc = False ) : if self . _cache_complete : gen = self . _cache else : gen = self if inc : def comp ( dc , dtc ) : return dc >= dtc else : def comp ( dc , dtc ) : return dc > dtc n = 0 for d in gen : if comp ( d , dt ) : if count is not None : n += 1 if n > count : break yield d
5912	def combine ( self , name_all = None , out_ndx = None , operation = '|' , defaultgroups = False ) : if not operation in ( '|' , '&' , False ) : raise ValueError ( "Illegal operation {0!r}, only '|' (OR) and '&' (AND) or False allowed." . format ( operation ) ) if name_all is None and operation : name_all = self . name_all or operation . join ( self . indexfiles ) if out_ndx is None : out_ndx = self . output if defaultgroups : fd , default_ndx = tempfile . mkstemp ( suffix = '.ndx' , prefix = 'default__' ) try : self . make_ndx ( o = default_ndx , input = [ 'q' ] ) except : utilities . unlink_gmx ( default_ndx ) raise ndxfiles = [ default_ndx ] else : ndxfiles = [ ] ndxfiles . extend ( self . indexfiles . values ( ) ) if operation : try : fd , tmp_ndx = tempfile . mkstemp ( suffix = '.ndx' , prefix = 'combined__' ) operation = ' ' + operation . strip ( ) + ' ' cmd = [ operation . join ( [ '"{0!s}"' . format ( gname ) for gname in self . indexfiles ] ) , '' , 'q' ] rc , out , err = self . make_ndx ( n = ndxfiles , o = tmp_ndx , input = cmd ) if self . _is_empty_group ( out ) : warnings . warn ( "No atoms found for {cmd!r}" . format ( ** vars ( ) ) , category = BadParameterWarning ) groups = parse_ndxlist ( out ) last = groups [ - 1 ] name_cmd = [ "name {0:d} {1!s}" . format ( last [ 'nr' ] , name_all ) , 'q' ] rc , out , err = self . make_ndx ( n = tmp_ndx , o = out_ndx , input = name_cmd ) finally : utilities . unlink_gmx ( tmp_ndx ) if defaultgroups : utilities . unlink_gmx ( default_ndx ) else : rc , out , err = self . make_ndx ( n = ndxfiles , o = out_ndx , input = [ '' , 'q' ] ) return name_all , out_ndx
7876	def bind ( self , stream , resource ) : self . stream = stream stanza = Iq ( stanza_type = "set" ) payload = ResourceBindingPayload ( resource = resource ) stanza . set_payload ( payload ) self . stanza_processor . set_response_handlers ( stanza , self . _bind_success , self . _bind_error ) stream . send ( stanza ) stream . event ( BindingResourceEvent ( resource ) )
12023	def check_phase ( self ) : plus_minus = set ( [ '+' , '-' ] ) for k , g in groupby ( sorted ( [ line for line in self . lines if line [ 'line_type' ] == 'feature' and line [ 'type' ] == 'CDS' and 'Parent' in line [ 'attributes' ] ] , key = lambda x : x [ 'attributes' ] [ 'Parent' ] ) , key = lambda x : x [ 'attributes' ] [ 'Parent' ] ) : cds_list = list ( g ) strand_set = list ( set ( [ line [ 'strand' ] for line in cds_list ] ) ) if len ( strand_set ) != 1 : for line in cds_list : self . add_line_error ( line , { 'message' : 'Inconsistent CDS strand with parent: {0:s}' . format ( k ) , 'error_type' : 'STRAND' } ) continue if len ( cds_list ) == 1 : if cds_list [ 0 ] [ 'phase' ] != 0 : self . add_line_error ( cds_list [ 0 ] , { 'message' : 'Wrong phase {0:d}, should be {1:d}' . format ( cds_list [ 0 ] [ 'phase' ] , 0 ) , 'error_type' : 'PHASE' } ) continue strand = strand_set [ 0 ] if strand not in plus_minus : continue if strand == '-' : sorted_cds_list = sorted ( cds_list , key = lambda x : x [ 'end' ] , reverse = True ) else : sorted_cds_list = sorted ( cds_list , key = lambda x : x [ 'start' ] ) phase = 0 for line in sorted_cds_list : if line [ 'phase' ] != phase : self . add_line_error ( line , { 'message' : 'Wrong phase {0:d}, should be {1:d}' . format ( line [ 'phase' ] , phase ) , 'error_type' : 'PHASE' } ) phase = ( 3 - ( ( line [ 'end' ] - line [ 'start' ] + 1 - phase ) % 3 ) ) % 3
11312	def update_oai_info ( self ) : for field in record_get_field_instances ( self . record , '909' , ind1 = "C" , ind2 = "O" ) : new_subs = [ ] for tag , value in field [ 0 ] : if tag == "o" : new_subs . append ( ( "a" , value ) ) else : new_subs . append ( ( tag , value ) ) if value in [ "CERN" , "CDS" , "ForCDS" ] : self . tag_as_cern = True record_add_field ( self . record , '024' , ind1 = "8" , subfields = new_subs ) record_delete_fields ( self . record , '909' )
2529	def parse ( self , fil ) : self . error = False self . graph = Graph ( ) self . graph . parse ( file = fil , format = 'xml' ) self . doc = document . Document ( ) for s , _p , o in self . graph . triples ( ( None , RDF . type , self . spdx_namespace [ 'SpdxDocument' ] ) ) : self . parse_doc_fields ( s ) for s , _p , o in self . graph . triples ( ( None , RDF . type , self . spdx_namespace [ 'ExternalDocumentRef' ] ) ) : self . parse_ext_doc_ref ( s ) for s , _p , o in self . graph . triples ( ( None , RDF . type , self . spdx_namespace [ 'CreationInfo' ] ) ) : self . parse_creation_info ( s ) for s , _p , o in self . graph . triples ( ( None , RDF . type , self . spdx_namespace [ 'Package' ] ) ) : self . parse_package ( s ) for s , _p , o in self . graph . triples ( ( None , self . spdx_namespace [ 'referencesFile' ] , None ) ) : self . parse_file ( o ) for s , _p , o in self . graph . triples ( ( None , self . spdx_namespace [ 'reviewed' ] , None ) ) : self . parse_review ( o ) for s , _p , o in self . graph . triples ( ( None , self . spdx_namespace [ 'annotation' ] , None ) ) : self . parse_annotation ( o ) validation_messages = [ ] validation_messages = self . doc . validate ( validation_messages ) if not self . error : if validation_messages : for msg in validation_messages : self . logger . log ( msg ) self . error = True return self . doc , self . error
5531	def get_process_tiles ( self , zoom = None ) : if zoom or zoom == 0 : for tile in self . config . process_pyramid . tiles_from_geom ( self . config . area_at_zoom ( zoom ) , zoom ) : yield tile else : for zoom in reversed ( self . config . zoom_levels ) : for tile in self . config . process_pyramid . tiles_from_geom ( self . config . area_at_zoom ( zoom ) , zoom ) : yield tile
13778	def FindFileContainingSymbol ( self , symbol ) : symbol = _NormalizeFullyQualifiedName ( symbol ) try : return self . _descriptors [ symbol ] . file except KeyError : pass try : return self . _enum_descriptors [ symbol ] . file except KeyError : pass try : file_proto = self . _internal_db . FindFileContainingSymbol ( symbol ) except KeyError as error : if self . _descriptor_db : file_proto = self . _descriptor_db . FindFileContainingSymbol ( symbol ) else : raise error if not file_proto : raise KeyError ( 'Cannot find a file containing %s' % symbol ) return self . _ConvertFileProtoToFileDescriptor ( file_proto )
2355	def find_elements ( self , strategy , locator ) : return self . driver_adapter . find_elements ( strategy , locator , root = self . root )
10882	def aN ( a , dim = 3 , dtype = 'int' ) : if not hasattr ( a , '__iter__' ) : return np . array ( [ a ] * dim , dtype = dtype ) return np . array ( a ) . astype ( dtype )
671	def createNetwork ( dataSource ) : with open ( _PARAMS_PATH , "r" ) as f : modelParams = yaml . safe_load ( f ) [ "modelParams" ] network = Network ( ) network . addRegion ( "sensor" , "py.RecordSensor" , '{}' ) sensorRegion = network . regions [ "sensor" ] . getSelf ( ) sensorRegion . encoder = createEncoder ( modelParams [ "sensorParams" ] [ "encoders" ] ) sensorRegion . dataSource = dataSource modelParams [ "spParams" ] [ "inputWidth" ] = sensorRegion . encoder . getWidth ( ) network . addRegion ( "SP" , "py.SPRegion" , json . dumps ( modelParams [ "spParams" ] ) ) network . addRegion ( "TM" , "py.TMRegion" , json . dumps ( modelParams [ "tmParams" ] ) ) clName = "py.%s" % modelParams [ "clParams" ] . pop ( "regionName" ) network . addRegion ( "classifier" , clName , json . dumps ( modelParams [ "clParams" ] ) ) createSensorToClassifierLinks ( network , "sensor" , "classifier" ) createDataOutLink ( network , "sensor" , "SP" ) createFeedForwardLink ( network , "SP" , "TM" ) createFeedForwardLink ( network , "TM" , "classifier" ) createResetLink ( network , "sensor" , "SP" ) createResetLink ( network , "sensor" , "TM" ) network . initialize ( ) return network
4229	def make_formatter ( format_name ) : if "json" in format_name : from json import dumps import datetime def jsonhandler ( obj ) : obj . isoformat ( ) if isinstance ( obj , ( datetime . datetime , datetime . date ) ) else obj if format_name == "prettyjson" : def jsondumps ( data ) : return dumps ( data , default = jsonhandler , indent = 2 , separators = ( ',' , ': ' ) ) else : def jsondumps ( data ) : return dumps ( data , default = jsonhandler ) def jsonify ( data ) : if isinstance ( data , dict ) : print ( jsondumps ( data ) ) elif isinstance ( data , list ) : print ( jsondumps ( [ device . _asdict ( ) for device in data ] ) ) else : print ( dumps ( { 'result' : data } ) ) return jsonify else : def printer ( data ) : if isinstance ( data , dict ) : print ( data ) else : for row in data : print ( row ) return printer
12919	def save ( self ) : if len ( self ) == 0 : return [ ] mdl = self . getModel ( ) return mdl . saver . save ( self )
7851	def remove_feature ( self , var ) : if not var : raise ValueError ( "var is None" ) if '"' not in var : expr = 'd:feature[@var="%s"]' % ( var , ) elif "'" not in var : expr = "d:feature[@var='%s']" % ( var , ) else : raise ValueError ( "Invalid feature name" ) l = self . xpath_ctxt . xpathEval ( expr ) if not l : return for f in l : f . unlinkNode ( ) f . freeNode ( )
329	def model_returns_t_alpha_beta ( data , bmark , samples = 2000 , progressbar = True ) : data_bmark = pd . concat ( [ data , bmark ] , axis = 1 ) . dropna ( ) with pm . Model ( ) as model : sigma = pm . HalfCauchy ( 'sigma' , beta = 1 ) nu = pm . Exponential ( 'nu_minus_two' , 1. / 10. ) X = data_bmark . iloc [ : , 1 ] y = data_bmark . iloc [ : , 0 ] alpha_reg = pm . Normal ( 'alpha' , mu = 0 , sd = .1 ) beta_reg = pm . Normal ( 'beta' , mu = 0 , sd = 1 ) mu_reg = alpha_reg + beta_reg * X pm . StudentT ( 'returns' , nu = nu + 2 , mu = mu_reg , sd = sigma , observed = y ) trace = pm . sample ( samples , progressbar = progressbar ) return model , trace
2846	def close ( self ) : if self . _ctx is not None : ftdi . free ( self . _ctx ) self . _ctx = None
2312	def predict_proba ( self , a , b , ** kwargs ) : return self . cds_score ( b , a ) - self . cds_score ( a , b )
9798	def update ( ctx , name , description , tags ) : user , project_name , _group = get_project_group_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'group' ) ) update_dict = { } if name : update_dict [ 'name' ] = name if description : update_dict [ 'description' ] = description tags = validate_tags ( tags ) if tags : update_dict [ 'tags' ] = tags if not update_dict : Printer . print_warning ( 'No argument was provided to update the experiment group.' ) sys . exit ( 0 ) try : response = PolyaxonClient ( ) . experiment_group . update_experiment_group ( user , project_name , _group , update_dict ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not update experiment group `{}`.' . format ( _group ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Experiment group updated." ) get_group_details ( response )
13250	def get_authoryear_from_entry ( entry , paren = False ) : def _format_last ( person ) : return ' ' . join ( [ n . strip ( '{}' ) for n in person . last_names ] ) if len ( entry . persons [ 'author' ] ) > 0 : persons = entry . persons [ 'author' ] elif len ( entry . persons [ 'editor' ] ) > 0 : persons = entry . persons [ 'editor' ] else : raise AuthorYearError try : year = entry . fields [ 'year' ] except KeyError : raise AuthorYearError if paren and len ( persons ) == 1 : template = '{author} ({year})' return template . format ( author = _format_last ( persons [ 0 ] ) , year = year ) elif not paren and len ( persons ) == 1 : template = '{author} {year}' return template . format ( author = _format_last ( persons [ 0 ] ) , year = year ) elif paren and len ( persons ) == 2 : template = '{author1} and {author2} ({year})' return template . format ( author1 = _format_last ( persons [ 0 ] ) , author2 = _format_last ( persons [ 1 ] ) , year = year ) elif not paren and len ( persons ) == 2 : template = '{author1} and {author2} {year}' return template . format ( author1 = _format_last ( persons [ 0 ] ) , author2 = _format_last ( persons [ 1 ] ) , year = year ) elif not paren and len ( persons ) > 2 : template = '{author} et al {year}' return template . format ( author = _format_last ( persons [ 0 ] ) , year = year ) elif paren and len ( persons ) > 2 : template = '{author} et al ({year})' return template . format ( author = _format_last ( persons [ 0 ] ) , year = year )
2148	def create ( self , fail_on_found = False , force_on_exists = False , ** kwargs ) : config_item = self . _separate ( kwargs ) jt_id = kwargs . pop ( 'job_template' , None ) status = kwargs . pop ( 'status' , 'any' ) old_endpoint = self . endpoint if jt_id is not None : jt = get_resource ( 'job_template' ) jt . get ( pk = jt_id ) try : nt_id = self . get ( ** copy . deepcopy ( kwargs ) ) [ 'id' ] except exc . NotFound : pass else : if fail_on_found : raise exc . TowerCLIError ( 'Notification template already ' 'exists and fail-on-found is ' 'switched on. Please use' ' "associate_notification" method' ' of job_template instead.' ) else : debug . log ( 'Notification template already exists, ' 'associating with job template.' , header = 'details' ) return jt . associate_notification_template ( jt_id , nt_id , status = status ) self . endpoint = '/job_templates/%d/notification_templates_%s/' % ( jt_id , status ) self . _configuration ( kwargs , config_item ) result = super ( Resource , self ) . create ( ** kwargs ) self . endpoint = old_endpoint return result
9919	def save ( self ) : try : email = models . EmailAddress . objects . get ( email = self . validated_data [ "email" ] , is_verified = True ) except models . EmailAddress . DoesNotExist : return None token = models . PasswordResetToken . objects . create ( email = email ) token . send ( ) return token
4638	def shared_blockchain_instance ( self ) : if not self . _sharedInstance . instance : klass = self . get_instance_class ( ) self . _sharedInstance . instance = klass ( ** self . _sharedInstance . config ) return self . _sharedInstance . instance
2591	def stage_out ( self , file , executor ) : if file . scheme == 'http' or file . scheme == 'https' : raise Exception ( 'HTTP/HTTPS file staging out is not supported' ) elif file . scheme == 'ftp' : raise Exception ( 'FTP file staging out is not supported' ) elif file . scheme == 'globus' : globus_ep = self . _get_globus_endpoint ( executor ) stage_out_app = self . _globus_stage_out_app ( ) return stage_out_app ( globus_ep , inputs = [ file ] ) else : raise Exception ( 'Staging out with unknown file scheme {} is not supported' . format ( file . scheme ) )
8596	def update_group ( self , group_id , ** kwargs ) : properties = { } if 'create_datacenter' in kwargs : kwargs [ 'create_data_center' ] = kwargs . pop ( 'create_datacenter' ) for attr , value in kwargs . items ( ) : properties [ self . _underscore_to_camelcase ( attr ) ] = value data = { "properties" : properties } response = self . _perform_request ( url = '/um/groups/%s' % group_id , method = 'PUT' , data = json . dumps ( data ) ) return response
9792	def is_ignored ( cls , path , patterns ) : status = None for pattern in cls . find_matching ( path , patterns ) : status = pattern . is_exclude return status
7362	async def connect ( self ) : with async_timeout . timeout ( self . timeout ) : self . response = await self . _connect ( ) if self . response . status in range ( 200 , 300 ) : self . _error_timeout = 0 self . state = NORMAL elif self . response . status == 500 : self . state = DISCONNECTION elif self . response . status in range ( 501 , 600 ) : self . state = RECONNECTION elif self . response . status in ( 420 , 429 ) : self . state = ENHANCE_YOUR_CALM else : logger . debug ( "raising error during stream connection" ) raise await exceptions . throw ( self . response , loads = self . client . _loads , url = self . kwargs [ 'url' ] ) logger . debug ( "stream state: %d" % self . state )
3520	def snapengage ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return SnapEngageNode ( )
1283	def autolink ( self , link , is_email = False ) : text = link = escape ( link ) if is_email : link = 'mailto:%s' % link return '<a href="%s">%s</a>' % ( link , text )
891	def _adaptSegment ( cls , connections , segment , prevActiveCells , permanenceIncrement , permanenceDecrement ) : synapsesToDestroy = [ ] for synapse in connections . synapsesForSegment ( segment ) : permanence = synapse . permanence if binSearch ( prevActiveCells , synapse . presynapticCell ) != - 1 : permanence += permanenceIncrement else : permanence -= permanenceDecrement permanence = max ( 0.0 , min ( 1.0 , permanence ) ) if permanence < EPSILON : synapsesToDestroy . append ( synapse ) else : connections . updateSynapsePermanence ( synapse , permanence ) for synapse in synapsesToDestroy : connections . destroySynapse ( synapse ) if connections . numSynapses ( segment ) == 0 : connections . destroySegment ( segment )
12541	def group_dicom_files ( dicom_paths , hdr_field = 'PatientID' ) : dicom_groups = defaultdict ( list ) try : for dcm in dicom_paths : hdr = dicom . read_file ( dcm ) group_key = getattr ( hdr , hdr_field ) dicom_groups [ group_key ] . append ( dcm ) except KeyError as ke : raise KeyError ( 'Error reading field {} from file {}.' . format ( hdr_field , dcm ) ) from ke return dicom_groups
3453	def find_essential_reactions ( model , threshold = None , processes = None ) : if threshold is None : threshold = model . slim_optimize ( error_value = None ) * 1E-02 deletions = single_reaction_deletion ( model , method = 'fba' , processes = processes ) essential = deletions . loc [ deletions [ 'growth' ] . isna ( ) | ( deletions [ 'growth' ] < threshold ) , : ] . index return { model . reactions . get_by_id ( r ) for ids in essential for r in ids }
4818	def parse_lms_api_datetime ( datetime_string , datetime_format = LMS_API_DATETIME_FORMAT ) : if isinstance ( datetime_string , datetime . datetime ) : date_time = datetime_string else : try : date_time = datetime . datetime . strptime ( datetime_string , datetime_format ) except ValueError : date_time = datetime . datetime . strptime ( datetime_string , LMS_API_DATETIME_FORMAT_WITHOUT_TIMEZONE ) if date_time . tzinfo is None : date_time = date_time . replace ( tzinfo = timezone . utc ) return date_time
12269	def evaluate ( self , repo , spec , args ) : status = [ ] with cd ( repo . rootdir ) : files = spec . get ( 'files' , [ '*' ] ) resource_files = repo . find_matching_files ( files ) files = glob2 . glob ( "**/*" ) disk_files = [ f for f in files if os . path . isfile ( f ) and f != "datapackage.json" ] allfiles = list ( set ( resource_files + disk_files ) ) allfiles . sort ( ) for f in allfiles : if f in resource_files and f in disk_files : r = repo . get_resource ( f ) coded_sha256 = r [ 'sha256' ] computed_sha256 = compute_sha256 ( f ) if computed_sha256 != coded_sha256 : status . append ( { 'target' : f , 'rules' : "" , 'validator' : self . name , 'description' : self . description , 'status' : 'ERROR' , 'message' : "Mismatch in checksum on disk and in datapackage.json" } ) else : status . append ( { 'target' : f , 'rules' : "" , 'validator' : self . name , 'description' : self . description , 'status' : 'OK' , 'message' : "" } ) elif f in resource_files : status . append ( { 'target' : f , 'rules' : "" , 'validator' : self . name , 'description' : self . description , 'status' : 'ERROR' , 'message' : "In datapackage.json but not in repo" } ) else : status . append ( { 'target' : f , 'rules' : "" , 'validator' : self . name , 'description' : self . description , 'status' : 'ERROR' , 'message' : "In repo but not in datapackage.json" } ) return status
10898	def _draw ( self ) : if self . display : print ( self . _formatstr . format ( ** self . __dict__ ) , end = '' ) sys . stdout . flush ( )
10389	def calculate_average_scores_on_subgraphs ( subgraphs : Mapping [ H , BELGraph ] , key : Optional [ str ] = None , tag : Optional [ str ] = None , default_score : Optional [ float ] = None , runs : Optional [ int ] = None , use_tqdm : bool = False , tqdm_kwargs : Optional [ Mapping [ str , Any ] ] = None , ) -> Mapping [ H , Tuple [ float , float , float , float , int , int ] ] : results = { } log . info ( 'calculating results for %d candidate mechanisms using %d permutations' , len ( subgraphs ) , runs ) it = subgraphs . items ( ) if use_tqdm : _tqdm_kwargs = dict ( total = len ( subgraphs ) , desc = 'Candidate mechanisms' ) if tqdm_kwargs : _tqdm_kwargs . update ( tqdm_kwargs ) it = tqdm ( it , ** _tqdm_kwargs ) for node , subgraph in it : number_first_neighbors = subgraph . in_degree ( node ) number_first_neighbors = 0 if isinstance ( number_first_neighbors , dict ) else number_first_neighbors mechanism_size = subgraph . number_of_nodes ( ) runners = workflow ( subgraph , node , key = key , tag = tag , default_score = default_score , runs = runs ) scores = [ runner . get_final_score ( ) for runner in runners ] if 0 == len ( scores ) : results [ node ] = ( None , None , None , None , number_first_neighbors , mechanism_size , ) continue scores = np . array ( scores ) average_score = np . average ( scores ) score_std = np . std ( scores ) med_score = np . median ( scores ) chi_2_stat , norm_p = stats . normaltest ( scores ) results [ node ] = ( average_score , score_std , norm_p , med_score , number_first_neighbors , mechanism_size , ) return results
11995	def get_algorithms ( self ) : return { 'signature' : self . signature_algorithms , 'encryption' : self . encryption_algorithms , 'serialization' : self . serialization_algorithms , 'compression' : self . compression_algorithms , }
2899	def get_tasks_from_spec_name ( self , name ) : return [ task for task in self . get_tasks ( ) if task . task_spec . name == name ]
8146	def levels ( self ) : h = self . img . histogram ( ) r = h [ 0 : 255 ] g = h [ 256 : 511 ] b = h [ 512 : 767 ] a = h [ 768 : 1024 ] return r , g , b , a
11799	def prune ( self , var , value , removals ) : "Rule out var=value." self . curr_domains [ var ] . remove ( value ) if removals is not None : removals . append ( ( var , value ) )
7844	def set_type ( self , item_type ) : if not item_type : raise ValueError ( "Type is required in DiscoIdentity" ) item_type = unicode ( item_type ) self . xmlnode . setProp ( "type" , item_type . encode ( "utf-8" ) )
9968	def copy ( self , space = None , name = None ) : return Cells ( space = space , name = name , formula = self . formula )
9188	def admin_print_styles ( request ) : styles = [ ] with db_connect ( cursor_factory = DictCursor ) as db_conn : with db_conn . cursor ( ) as cursor : cursor . execute ( ) for row in cursor . fetchall ( ) : styles . append ( { 'print_style' : row [ 'print_style' ] , 'title' : row [ 'title' ] , 'type' : row [ 'type' ] , 'revised' : row [ 'revised' ] , 'tag' : row [ 'tag' ] , 'commit_id' : row [ 'commit_id' ] , 'number' : row [ 'count' ] , 'bad' : row [ 'bad' ] , 'link' : request . route_path ( 'admin-print-style-single' , style = row [ 'print_style' ] ) } ) return { 'styles' : styles }
1409	def to_table ( components , topo_info ) : inputs , outputs = defaultdict ( list ) , defaultdict ( list ) for ctype , component in components . items ( ) : if ctype == 'bolts' : for component_name , component_info in component . items ( ) : for input_stream in component_info [ 'inputs' ] : input_name = input_stream [ 'component_name' ] inputs [ component_name ] . append ( input_name ) outputs [ input_name ] . append ( component_name ) info = [ ] spouts_instance = topo_info [ 'physical_plan' ] [ 'spouts' ] bolts_instance = topo_info [ 'physical_plan' ] [ 'bolts' ] for ctype , component in components . items ( ) : if ctype == "stages" : continue for component_name , component_info in component . items ( ) : row = [ ctype [ : - 1 ] , component_name ] if ctype == 'spouts' : row . append ( len ( spouts_instance [ component_name ] ) ) else : row . append ( len ( bolts_instance [ component_name ] ) ) row . append ( ',' . join ( inputs . get ( component_name , [ '-' ] ) ) ) row . append ( ',' . join ( outputs . get ( component_name , [ '-' ] ) ) ) info . append ( row ) header = [ 'type' , 'name' , 'parallelism' , 'input' , 'output' ] return info , header
4482	def storages ( self ) : stores = self . _json ( self . _get ( self . _storages_url ) , 200 ) stores = stores [ 'data' ] for store in stores : yield Storage ( store , self . session )
2316	def create_graph_from_data ( self , data , ** kwargs ) : self . arguments [ '{CITEST}' ] = self . dir_CI_test [ self . CI_test ] self . arguments [ '{METHOD_INDEP}' ] = self . dir_method_indep [ self . method_indep ] self . arguments [ '{DIRECTED}' ] = 'TRUE' self . arguments [ '{ALPHA}' ] = str ( self . alpha ) self . arguments [ '{NJOBS}' ] = str ( self . nb_jobs ) self . arguments [ '{VERBOSE}' ] = str ( self . verbose ) . upper ( ) results = self . _run_pc ( data , verbose = self . verbose ) return nx . relabel_nodes ( nx . DiGraph ( results ) , { idx : i for idx , i in enumerate ( data . columns ) } )
13022	def query ( self , sql_string , * args , ** kwargs ) : commit = None columns = None if kwargs . get ( 'commit' ) is not None : commit = kwargs . pop ( 'commit' ) if kwargs . get ( 'columns' ) is not None : columns = kwargs . pop ( 'columns' ) query = self . _assemble_simple ( sql_string , * args , ** kwargs ) return self . _execute ( query , commit = commit , working_columns = columns )
6670	def task ( * args , ** kwargs ) : precursors = kwargs . pop ( 'precursors' , None ) post_callback = kwargs . pop ( 'post_callback' , False ) if args and callable ( args [ 0 ] ) : return _task ( * args ) def wrapper ( meth ) : if precursors : meth . deploy_before = list ( precursors ) if post_callback : meth . is_post_callback = True return _task ( meth ) return wrapper
11710	def request ( self , path , data = None , headers = None , method = None ) : if isinstance ( data , str ) : data = data . encode ( 'utf-8' ) response = urlopen ( self . _request ( path , data = data , headers = headers , method = method ) ) self . _set_session_cookie ( response ) return response
5656	def validate_day_start_ut ( conn ) : G = GTFS ( conn ) cur = conn . execute ( 'SELECT date, day_start_ut FROM days' ) for date , day_start_ut in cur : assert day_start_ut == G . get_day_start_ut ( date )
8763	def update_security_group_rule ( context , id , security_group_rule ) : LOG . info ( "update_security_group_rule for tenant %s" % ( context . tenant_id ) ) new_rule = security_group_rule [ "security_group_rule" ] new_rule = _filter_update_security_group_rule ( new_rule ) with context . session . begin ( ) : rule = db_api . security_group_rule_find ( context , id = id , scope = db_api . ONE ) if not rule : raise sg_ext . SecurityGroupRuleNotFound ( id = id ) db_rule = db_api . security_group_rule_update ( context , rule , ** new_rule ) group_id = db_rule . group_id group = db_api . security_group_find ( context , id = group_id , scope = db_api . ONE ) if not group : raise sg_ext . SecurityGroupNotFound ( id = group_id ) if group : _perform_async_update_rule ( context , group_id , group , rule . id , RULE_UPDATE ) return v . _make_security_group_rule_dict ( db_rule )
5505	def save ( url , * args , ** kwargs ) : device = heimdallDevice ( kwargs . get ( 'device' , None ) ) kwargs [ 'width' ] = kwargs . get ( 'width' , None ) or device . width kwargs [ 'height' ] = kwargs . get ( 'height' , None ) or device . height kwargs [ 'user_agent' ] = kwargs . get ( 'user_agent' , None ) or device . user_agent screenshot_image = screenshot ( url , ** kwargs ) if kwargs . get ( 'optimize' ) : image = Image . open ( screenshot_image . path ) image . save ( screenshot_image . path , optimize = True ) return screenshot_image
1855	def BSF ( cpu , dest , src ) : value = src . read ( ) flag = Operators . EXTRACT ( value , 0 , 1 ) == 1 res = 0 for pos in range ( 1 , src . size ) : res = Operators . ITEBV ( dest . size , flag , res , pos ) flag = Operators . OR ( flag , Operators . EXTRACT ( value , pos , 1 ) == 1 ) cpu . ZF = value == 0 dest . write ( Operators . ITEBV ( dest . size , cpu . ZF , dest . read ( ) , res ) )
6879	def _smartcast ( castee , caster , subval = None ) : try : return caster ( castee ) except Exception as e : if caster is float or caster is int : return nan elif caster is str : return '' else : return subval
12784	def get_xdg_dirs ( self ) : config_dirs = getenv ( 'XDG_CONFIG_DIRS' , '' ) if config_dirs : self . _log . debug ( 'XDG_CONFIG_DIRS is set to %r' , config_dirs ) output = [ ] for path in reversed ( config_dirs . split ( ':' ) ) : output . append ( join ( path , self . group_name , self . app_name ) ) return output return [ '/etc/xdg/%s/%s' % ( self . group_name , self . app_name ) ]
10921	def do_levmarq_all_particle_groups ( s , region_size = 40 , max_iter = 2 , damping = 1.0 , decrease_damp_factor = 10. , run_length = 4 , collect_stats = False , ** kwargs ) : lp = LMParticleGroupCollection ( s , region_size = region_size , damping = damping , run_length = run_length , decrease_damp_factor = decrease_damp_factor , get_cos = collect_stats , max_iter = max_iter , ** kwargs ) lp . do_run_2 ( ) if collect_stats : return lp . stats
3159	def get_metadata ( self ) : try : r = requests . get ( 'https://login.mailchimp.com/oauth2/metadata' , auth = self ) except requests . exceptions . RequestException as e : raise e else : r . raise_for_status ( ) output = r . json ( ) if 'error' in output : raise requests . exceptions . RequestException ( output [ 'error' ] ) return output
6454	def stem ( self , word ) : word = normalize ( 'NFC' , text_type ( word . lower ( ) ) ) word = word . translate ( self . _accents ) wlen = len ( word ) - 1 if wlen > 4 and word [ - 3 : ] == 'ern' : word = word [ : - 3 ] elif wlen > 3 and word [ - 2 : ] in { 'em' , 'en' , 'er' , 'es' } : word = word [ : - 2 ] elif wlen > 2 and ( word [ - 1 ] == 'e' or ( word [ - 1 ] == 's' and word [ - 2 ] in self . _st_ending ) ) : word = word [ : - 1 ] wlen = len ( word ) - 1 if wlen > 4 and word [ - 3 : ] == 'est' : word = word [ : - 3 ] elif wlen > 3 and ( word [ - 2 : ] in { 'er' , 'en' } or ( word [ - 2 : ] == 'st' and word [ - 3 ] in self . _st_ending ) ) : word = word [ : - 2 ] return word
11232	def run_excel_to_html ( ) : parser = argparse . ArgumentParser ( prog = 'excel_to_html' ) parser . add_argument ( '-p' , nargs = '?' , help = 'Path to an excel file for conversion.' ) parser . add_argument ( '-s' , nargs = '?' , help = 'The name of a sheet in our excel file. Defaults to "Sheet1".' , ) parser . add_argument ( '-css' , nargs = '?' , help = 'Space separated css classes to append to the table.' ) parser . add_argument ( '-m' , action = 'store_true' , help = 'Merge, attempt to combine merged cells.' ) parser . add_argument ( '-c' , nargs = '?' , help = 'Caption for creating an accessible table.' ) parser . add_argument ( '-d' , nargs = '?' , help = 'Two strings separated by a | character. The first string \ is for the html "summary" attribute and the second string is for the html "details" attribute. \ both values must be provided and nothing more.' , ) parser . add_argument ( '-r' , action = 'store_true' , help = 'Row headers. Does the table have row headers?' ) args = parser . parse_args ( ) inputs = { 'p' : args . p , 's' : args . s , 'css' : args . css , 'm' : args . m , 'c' : args . c , 'd' : args . d , 'r' : args . r , } p = inputs [ 'p' ] s = inputs [ 's' ] if inputs [ 's' ] else 'Sheet1' css = inputs [ 'css' ] if inputs [ 'css' ] else '' m = inputs [ 'm' ] if inputs [ 'm' ] else False c = inputs [ 'c' ] if inputs [ 'c' ] else '' d = inputs [ 'd' ] . split ( '|' ) if inputs [ 'd' ] else [ ] r = inputs [ 'r' ] if inputs [ 'r' ] else False html = fp . excel_to_html ( p , sheetname = s , css_classes = css , caption = c , details = d , row_headers = r , merge = m ) print ( html )
1018	def addSynapse ( self , srcCellCol , srcCellIdx , perm ) : self . syns . append ( [ int ( srcCellCol ) , int ( srcCellIdx ) , numpy . float32 ( perm ) ] )
1270	def _fire ( self , layers , the_plot ) : if the_plot . get ( 'last_marauder_shot' ) == the_plot . frame : return the_plot [ 'last_marauder_shot' ] = the_plot . frame col = np . random . choice ( np . nonzero ( layers [ 'X' ] . sum ( axis = 0 ) ) [ 0 ] ) row = np . nonzero ( layers [ 'X' ] [ : , col ] ) [ 0 ] [ - 1 ] + 1 self . _teleport ( ( row , col ) )
1030	def b32encode ( s ) : parts = [ ] quanta , leftover = divmod ( len ( s ) , 5 ) if leftover : s += ( '\0' * ( 5 - leftover ) ) quanta += 1 for i in range ( quanta ) : c1 , c2 , c3 = struct . unpack ( '!HHB' , s [ i * 5 : ( i + 1 ) * 5 ] ) c2 += ( c1 & 1 ) << 16 c3 += ( c2 & 3 ) << 8 parts . extend ( [ _b32tab [ c1 >> 11 ] , _b32tab [ ( c1 >> 6 ) & 0x1f ] , _b32tab [ ( c1 >> 1 ) & 0x1f ] , _b32tab [ c2 >> 12 ] , _b32tab [ ( c2 >> 7 ) & 0x1f ] , _b32tab [ ( c2 >> 2 ) & 0x1f ] , _b32tab [ c3 >> 5 ] , _b32tab [ c3 & 0x1f ] , ] ) encoded = EMPTYSTRING . join ( parts ) if leftover == 1 : return encoded [ : - 6 ] + '======' elif leftover == 2 : return encoded [ : - 4 ] + '====' elif leftover == 3 : return encoded [ : - 3 ] + '===' elif leftover == 4 : return encoded [ : - 1 ] + '=' return encoded
7743	def _prepare_io_handler_cb ( self , handler ) : self . _anything_done = True logger . debug ( "_prepar_io_handler_cb called for {0!r}" . format ( handler ) ) self . _configure_io_handler ( handler ) self . _prepare_sources . pop ( handler , None ) return False
6779	def get_deploy_funcs ( components , current_thumbprint , previous_thumbprint , preview = False ) : for component in components : funcs = manifest_deployers . get ( component , [ ] ) for func_name in funcs : if func_name . startswith ( 'burlap.' ) : print ( 'skipping %s' % func_name ) continue takes_diff = manifest_deployers_takes_diff . get ( func_name , False ) func = resolve_deployer ( func_name ) current = current_thumbprint . get ( component ) last = previous_thumbprint . get ( component ) if takes_diff : yield func_name , partial ( func , last = last , current = current ) else : yield func_name , partial ( func )
6640	def hasDependency ( self , name , target = None , test_dependencies = False ) : if name in self . description . get ( 'dependencies' , { } ) . keys ( ) : return True target_deps = self . description . get ( 'targetDependencies' , { } ) if target is not None : for conf_key , target_conf_deps in target_deps . items ( ) : if _truthyConfValue ( target . getConfigValue ( conf_key ) ) or conf_key in target . getSimilarTo_Deprecated ( ) : if name in target_conf_deps : return True if test_dependencies : if name in self . description . get ( 'testDependencies' , { } ) . keys ( ) : return True if target is not None : test_target_deps = self . description . get ( 'testTargetDependencies' , { } ) for conf_key , target_conf_deps in test_target_deps . items ( ) : if _truthyConfValue ( target . getConfigValue ( conf_key ) ) or conf_key in target . getSimilarTo_Deprecated ( ) : if name in target_conf_deps : return True return False
3192	def create ( self , list_id , data ) : self . list_id = list_id if 'status' not in data : raise KeyError ( 'The list member must have a status' ) if data [ 'status' ] not in [ 'subscribed' , 'unsubscribed' , 'cleaned' , 'pending' , 'transactional' ] : raise ValueError ( 'The list member status must be one of "subscribed", "unsubscribed", "cleaned", ' '"pending", or "transactional"' ) if 'email_address' not in data : raise KeyError ( 'The list member must have an email_address' ) check_email ( data [ 'email_address' ] ) response = self . _mc_client . _post ( url = self . _build_path ( list_id , 'members' ) , data = data ) if response is not None : self . subscriber_hash = response [ 'id' ] else : self . subscriber_hash = None return response
9625	def register ( self , cls ) : preview = cls ( site = self ) logger . debug ( 'Registering %r with %r' , preview , self ) index = self . __previews . setdefault ( preview . module , { } ) index [ cls . __name__ ] = preview
2862	def _i2c_write_bytes ( self , data ) : for byte in data : self . _command . append ( str ( bytearray ( ( 0x11 , 0x00 , 0x00 , byte ) ) ) ) self . _ft232h . output_pins ( { 0 : GPIO . LOW , 1 : GPIO . HIGH } , write = False ) self . _command . append ( self . _ft232h . mpsse_gpio ( ) * _REPEAT_DELAY ) self . _command . append ( '\x22\x00' ) self . _expected += len ( data )
4525	def get ( self , position = 0 ) : n = len ( self ) if n == 1 : return self [ 0 ] pos = position if self . length and self . autoscale : pos *= len ( self ) pos /= self . length pos *= self . scale pos += self . offset if not self . continuous : if not self . serpentine : return self [ int ( pos % n ) ] m = ( 2 * n ) - 2 pos %= m if pos < n : return self [ int ( pos ) ] else : return self [ int ( m - pos ) ] if self . serpentine : pos %= ( 2 * n ) if pos > n : pos = ( 2 * n ) - pos else : pos %= n pos *= n - 1 pos /= n index = int ( pos ) fade = pos - index if not fade : return self [ index ] r1 , g1 , b1 = self [ index ] r2 , g2 , b2 = self [ ( index + 1 ) % len ( self ) ] dr , dg , db = r2 - r1 , g2 - g1 , b2 - b1 return r1 + fade * dr , g1 + fade * dg , b1 + fade * db
4734	def round_data ( filter_data ) : for index , _ in enumerate ( filter_data ) : filter_data [ index ] [ 0 ] = round ( filter_data [ index ] [ 0 ] / 100.0 ) * 100.0 return filter_data
9433	def _load_savefile_header ( file_h ) : try : raw_savefile_header = file_h . read ( 24 ) except UnicodeDecodeError : print ( "\nMake sure the input file is opened in read binary, 'rb'\n" ) raise InvalidEncoding ( "Could not read file; it might not be opened in binary mode." ) if raw_savefile_header [ : 4 ] in [ struct . pack ( ">I" , _MAGIC_NUMBER ) , struct . pack ( ">I" , _MAGIC_NUMBER_NS ) ] : byte_order = b'big' unpacked = struct . unpack ( '>IhhIIII' , raw_savefile_header ) elif raw_savefile_header [ : 4 ] in [ struct . pack ( "<I" , _MAGIC_NUMBER ) , struct . pack ( "<I" , _MAGIC_NUMBER_NS ) ] : byte_order = b'little' unpacked = struct . unpack ( '<IhhIIII' , raw_savefile_header ) else : raise UnknownMagicNumber ( "No supported Magic Number found" ) ( magic , major , minor , tz_off , ts_acc , snaplen , ll_type ) = unpacked header = __pcap_header__ ( magic , major , minor , tz_off , ts_acc , snaplen , ll_type , ctypes . c_char_p ( byte_order ) , magic == _MAGIC_NUMBER_NS ) if not __validate_header__ ( header ) : raise InvalidHeader ( "Invalid Header" ) else : return header
12763	def load_attachments ( self , source , skeleton ) : self . targets = { } self . offsets = { } filename = source if isinstance ( source , str ) : source = open ( source ) else : filename = '(file-{})' . format ( id ( source ) ) for i , line in enumerate ( source ) : tokens = line . split ( '#' ) [ 0 ] . strip ( ) . split ( ) if not tokens : continue label = tokens . pop ( 0 ) if label not in self . channels : logging . info ( '%s:%d: unknown marker %s' , filename , i , label ) continue if not tokens : continue name = tokens . pop ( 0 ) bodies = [ b for b in skeleton . bodies if b . name == name ] if len ( bodies ) != 1 : logging . info ( '%s:%d: %d skeleton bodies match %s' , filename , i , len ( bodies ) , name ) continue b = self . targets [ label ] = bodies [ 0 ] o = self . offsets [ label ] = np . array ( list ( map ( float , tokens ) ) ) * b . dimensions / 2 logging . info ( '%s < , label , b . name , o )
695	def _loadDescriptionFile ( descriptionPyPath ) : global g_descriptionImportCount if not os . path . isfile ( descriptionPyPath ) : raise RuntimeError ( ( "Experiment description file %s does not exist or " + "is not a file" ) % ( descriptionPyPath , ) ) mod = imp . load_source ( "pf_description%d" % g_descriptionImportCount , descriptionPyPath ) g_descriptionImportCount += 1 if not hasattr ( mod , "descriptionInterface" ) : raise RuntimeError ( "Experiment description file %s does not define %s" % ( descriptionPyPath , "descriptionInterface" ) ) if not isinstance ( mod . descriptionInterface , exp_description_api . DescriptionIface ) : raise RuntimeError ( ( "Experiment description file %s defines %s but it " + "is not DescriptionIface-based" ) % ( descriptionPyPath , name ) ) return mod
12425	def reverse ( self ) : if not self . test_drive and self . bumps : map ( lambda b : b . reverse ( ) , self . bumpers )
12717	def angles ( self ) : return [ self . ode_obj . getAngle ( i ) for i in range ( self . ADOF ) ]
6591	def receive ( self ) : ret = [ ] while True : if self . runid_pkgidx_map : self . runid_to_return . extend ( self . dispatcher . poll ( ) ) ret . extend ( self . _collect_all_finished_pkgidx_result_pairs ( ) ) if not self . runid_pkgidx_map : break time . sleep ( self . sleep ) ret = sorted ( ret , key = itemgetter ( 0 ) ) return ret
9590	def switch_to_window ( self , window_name ) : data = { 'name' : window_name } self . _execute ( Command . SWITCH_TO_WINDOW , data )
4343	def silence ( self , location = 0 , silence_threshold = 0.1 , min_silence_duration = 0.1 , buffer_around_silence = False ) : if location not in [ - 1 , 0 , 1 ] : raise ValueError ( "location must be one of -1, 0, 1." ) if not is_number ( silence_threshold ) or silence_threshold < 0 : raise ValueError ( "silence_threshold must be a number between 0 and 100" ) elif silence_threshold >= 100 : raise ValueError ( "silence_threshold must be a number between 0 and 100" ) if not is_number ( min_silence_duration ) or min_silence_duration <= 0 : raise ValueError ( "min_silence_duration must be a positive number." ) if not isinstance ( buffer_around_silence , bool ) : raise ValueError ( "buffer_around_silence must be a boolean." ) effect_args = [ ] if location == - 1 : effect_args . append ( 'reverse' ) if buffer_around_silence : effect_args . extend ( [ 'silence' , '-l' ] ) else : effect_args . append ( 'silence' ) effect_args . extend ( [ '1' , '{:f}' . format ( min_silence_duration ) , '{:f}%' . format ( silence_threshold ) ] ) if location == 0 : effect_args . extend ( [ '-1' , '{:f}' . format ( min_silence_duration ) , '{:f}%' . format ( silence_threshold ) ] ) if location == - 1 : effect_args . append ( 'reverse' ) self . effects . extend ( effect_args ) self . effects_log . append ( 'silence' ) return self
6960	def read_model_table ( modelfile ) : infd = gzip . open ( modelfile ) model = np . genfromtxt ( infd , names = True ) infd . close ( ) return model
9479	def node ( self , node ) : if node == self . node1 : return self . node2 elif node == self . node2 : return self . node1 else : return None
9929	def authenticate ( username , password , service = 'login' , encoding = 'utf-8' , resetcred = True ) : if sys . version_info >= ( 3 , ) : if isinstance ( username , str ) : username = username . encode ( encoding ) if isinstance ( password , str ) : password = password . encode ( encoding ) if isinstance ( service , str ) : service = service . encode ( encoding ) @ conv_func def my_conv ( n_messages , messages , p_response , app_data ) : addr = calloc ( n_messages , sizeof ( PamResponse ) ) p_response [ 0 ] = cast ( addr , POINTER ( PamResponse ) ) for i in range ( n_messages ) : if messages [ i ] . contents . msg_style == PAM_PROMPT_ECHO_OFF : pw_copy = strdup ( password ) p_response . contents [ i ] . resp = cast ( pw_copy , c_char_p ) p_response . contents [ i ] . resp_retcode = 0 return 0 handle = PamHandle ( ) conv = PamConv ( my_conv , 0 ) retval = pam_start ( service , username , byref ( conv ) , byref ( handle ) ) if retval != 0 : return False retval = pam_authenticate ( handle , 0 ) auth_success = ( retval == 0 ) if auth_success and resetcred : retval = pam_setcred ( handle , PAM_REINITIALIZE_CRED ) pam_end ( handle , retval ) return auth_success
933	def _getModelCheckpointFilePath ( checkpointDir ) : path = os . path . join ( checkpointDir , "model.data" ) path = os . path . abspath ( path ) return path
143	def exterior_almost_equals ( self , other , max_distance = 1e-6 , points_per_edge = 8 ) : if isinstance ( other , list ) : other = Polygon ( np . float32 ( other ) ) elif ia . is_np_array ( other ) : other = Polygon ( other ) else : assert isinstance ( other , Polygon ) other = other return self . to_line_string ( closed = True ) . coords_almost_equals ( other . to_line_string ( closed = True ) , max_distance = max_distance , points_per_edge = points_per_edge )
3660	def _coeff_ind_from_T ( self , T ) : if self . n == 1 : return 0 for i in range ( self . n ) : if T <= self . Ts [ i + 1 ] : return i return self . n - 1
1898	def _getvalue ( self , expression ) : if not issymbolic ( expression ) : return expression assert isinstance ( expression , Variable ) if isinstance ( expression , Array ) : result = bytearray ( ) for c in expression : expression_str = translate_to_smtlib ( c ) self . _send ( '(get-value (%s))' % expression_str ) response = self . _recv ( ) result . append ( int ( '0x{:s}' . format ( response . split ( expression_str ) [ 1 ] [ 3 : - 2 ] ) , 16 ) ) return bytes ( result ) else : self . _send ( '(get-value (%s))' % expression . name ) ret = self . _recv ( ) assert ret . startswith ( '((' ) and ret . endswith ( '))' ) , ret if isinstance ( expression , Bool ) : return { 'true' : True , 'false' : False } [ ret [ 2 : - 2 ] . split ( ' ' ) [ 1 ] ] elif isinstance ( expression , BitVec ) : pattern , base = self . _get_value_fmt m = pattern . match ( ret ) expr , value = m . group ( 'expr' ) , m . group ( 'value' ) return int ( value , base ) raise NotImplementedError ( "_getvalue only implemented for Bool and BitVec" )
1331	def batch_predictions ( self , images , greedy = False , strict = True , return_details = False ) : if strict : in_bounds = self . in_bounds ( images ) assert in_bounds self . _total_prediction_calls += len ( images ) predictions = self . __model . batch_predictions ( images ) assert predictions . ndim == 2 assert predictions . shape [ 0 ] == images . shape [ 0 ] if return_details : assert greedy adversarials = [ ] for i in range ( len ( predictions ) ) : if strict : in_bounds_i = True else : in_bounds_i = self . in_bounds ( images [ i ] ) is_adversarial , is_best , distance = self . __is_adversarial ( images [ i ] , predictions [ i ] , in_bounds_i ) if is_adversarial and greedy : if return_details : return predictions , is_adversarial , i , is_best , distance else : return predictions , is_adversarial , i adversarials . append ( is_adversarial ) if greedy : if return_details : return predictions , False , None , False , None else : return predictions , False , None is_adversarial = np . array ( adversarials ) assert is_adversarial . ndim == 1 assert is_adversarial . shape [ 0 ] == images . shape [ 0 ] return predictions , is_adversarial
3602	def get_user ( self ) : token = self . authenticator . create_token ( self . extra ) user_id = self . extra . get ( 'id' ) return FirebaseUser ( self . email , token , self . provider , user_id )
9416	def to_value ( cls , instance ) : if not isinstance ( instance , OctaveUserClass ) or not instance . _attrs : return dict ( ) dtype = [ ] values = [ ] for attr in instance . _attrs : dtype . append ( ( str ( attr ) , object ) ) values . append ( getattr ( instance , attr ) ) struct = np . array ( [ tuple ( values ) ] , dtype ) return MatlabObject ( struct , instance . _name )
513	def _updateDutyCycles ( self , overlaps , activeColumns ) : overlapArray = numpy . zeros ( self . _numColumns , dtype = realDType ) activeArray = numpy . zeros ( self . _numColumns , dtype = realDType ) overlapArray [ overlaps > 0 ] = 1 activeArray [ activeColumns ] = 1 period = self . _dutyCyclePeriod if ( period > self . _iterationNum ) : period = self . _iterationNum self . _overlapDutyCycles = self . _updateDutyCyclesHelper ( self . _overlapDutyCycles , overlapArray , period ) self . _activeDutyCycles = self . _updateDutyCyclesHelper ( self . _activeDutyCycles , activeArray , period )
6874	def _pycompress_sqlitecurve ( sqlitecurve , force = False ) : outfile = '%s.gz' % sqlitecurve try : if os . path . exists ( outfile ) and not force : os . remove ( sqlitecurve ) return outfile else : with open ( sqlitecurve , 'rb' ) as infd : with gzip . open ( outfile , 'wb' ) as outfd : shutil . copyfileobj ( infd , outfd ) if os . path . exists ( outfile ) : os . remove ( sqlitecurve ) return outfile except Exception as e : return None
7125	def get_download_total ( rows ) : headers = rows . pop ( 0 ) index = headers . index ( 'download_count' ) total_downloads = sum ( int ( row [ index ] ) for row in rows ) rows . insert ( 0 , headers ) return total_downloads , index
1205	def target_optimizer_arguments ( self ) : variables = self . target_network . get_variables ( ) + [ variable for name in sorted ( self . target_distributions ) for variable in self . target_distributions [ name ] . get_variables ( ) ] source_variables = self . network . get_variables ( ) + [ variable for name in sorted ( self . distributions ) for variable in self . distributions [ name ] . get_variables ( ) ] arguments = dict ( time = self . global_timestep , variables = variables , source_variables = source_variables ) if self . global_model is not None : arguments [ 'global_variables' ] = self . global_model . target_network . get_variables ( ) + [ variable for name in sorted ( self . global_model . target_distributions ) for variable in self . global_model . target_distributions [ name ] . get_variables ( ) ] return arguments
10978	def leave ( group_id ) : group = Group . query . get_or_404 ( group_id ) if group . can_leave ( current_user ) : try : group . remove_member ( current_user ) except Exception as e : flash ( str ( e ) , "error" ) return redirect ( url_for ( '.index' ) ) flash ( _ ( 'You have successfully left %(group_name)s group.' , group_name = group . name ) , 'success' ) return redirect ( url_for ( '.index' ) ) flash ( _ ( 'You cannot leave the group %(group_name)s' , group_name = group . name ) , 'error' ) return redirect ( url_for ( '.index' ) )
438	def read_and_decode ( filename , is_train = None ) : filename_queue = tf . train . string_input_producer ( [ filename ] ) reader = tf . TFRecordReader ( ) _ , serialized_example = reader . read ( filename_queue ) features = tf . parse_single_example ( serialized_example , features = { 'label' : tf . FixedLenFeature ( [ ] , tf . int64 ) , 'img_raw' : tf . FixedLenFeature ( [ ] , tf . string ) , } ) img = tf . decode_raw ( features [ 'img_raw' ] , tf . float32 ) img = tf . reshape ( img , [ 32 , 32 , 3 ] ) if is_train == True : img = tf . random_crop ( img , [ 24 , 24 , 3 ] ) img = tf . image . random_flip_left_right ( img ) img = tf . image . random_brightness ( img , max_delta = 63 ) img = tf . image . random_contrast ( img , lower = 0.2 , upper = 1.8 ) img = tf . image . per_image_standardization ( img ) elif is_train == False : img = tf . image . resize_image_with_crop_or_pad ( img , 24 , 24 ) img = tf . image . per_image_standardization ( img ) elif is_train == None : img = img label = tf . cast ( features [ 'label' ] , tf . int32 ) return img , label
6209	def print_attrs ( data_file , node_name = '/' , which = 'user' , compress = False ) : node = data_file . get_node ( node_name ) print ( 'List of attributes for:\n %s\n' % node ) for attr in node . _v_attrs . _f_list ( ) : print ( '\t%s' % attr ) attr_content = repr ( node . _v_attrs [ attr ] ) if compress : attr_content = attr_content . split ( '\n' ) [ 0 ] print ( "\t %s" % attr_content )
6616	def receive ( self ) : pkgidx_result_pairs = self . receive_all ( ) if pkgidx_result_pairs is None : return results = [ r for _ , r in pkgidx_result_pairs ] return results
7983	def registration_error ( self , stanza ) : self . lock . acquire ( ) try : err = stanza . get_error ( ) ae = err . xpath_eval ( "e:*" , { "e" : "jabber:iq:auth:error" } ) if ae : ae = ae [ 0 ] . name else : ae = err . get_condition ( ) . name raise RegistrationError ( "Authentication error condition: %s" % ( ae , ) ) finally : self . lock . release ( )
10080	def _publish_edited ( self ) : record_pid , record = self . fetch_published ( ) if record . revision_id == self [ '_deposit' ] [ 'pid' ] [ 'revision_id' ] : data = dict ( self . dumps ( ) ) else : data = self . merge_with_published ( ) data [ '$schema' ] = self . record_schema data [ '_deposit' ] = self [ '_deposit' ] record = record . __class__ ( data , model = record . model ) return record
11152	def sha256file ( abspath , nbytes = 0 , chunk_size = DEFAULT_CHUNK_SIZE ) : return get_file_fingerprint ( abspath , hashlib . sha256 , nbytes = nbytes , chunk_size = chunk_size )
11313	def update_cnum ( self ) : if "ConferencePaper" not in self . collections : cnums = record_get_field_values ( self . record , '773' , code = "w" ) for cnum in cnums : cnum_subs = [ ( "9" , "INSPIRE-CNUM" ) , ( "a" , cnum ) ] record_add_field ( self . record , "035" , subfields = cnum_subs )
10550	def find_results ( project_id , ** kwargs ) : try : kwargs [ 'project_id' ] = project_id res = _pybossa_req ( 'get' , 'result' , params = kwargs ) if type ( res ) . __name__ == 'list' : return [ Result ( result ) for result in res ] else : return res except : raise
4399	def adsSyncSetTimeoutEx ( port , nMs ) : adsSyncSetTimeoutFct = _adsDLL . AdsSyncSetTimeoutEx cms = ctypes . c_long ( nMs ) err_code = adsSyncSetTimeoutFct ( port , cms ) if err_code : raise ADSError ( err_code )
4781	def is_between ( self , low , high ) : val_type = type ( self . val ) self . _validate_between_args ( val_type , low , high ) if self . val < low or self . val > high : if val_type is datetime . datetime : self . _err ( 'Expected <%s> to be between <%s> and <%s>, but was not.' % ( self . val . strftime ( '%Y-%m-%d %H:%M:%S' ) , low . strftime ( '%Y-%m-%d %H:%M:%S' ) , high . strftime ( '%Y-%m-%d %H:%M:%S' ) ) ) else : self . _err ( 'Expected <%s> to be between <%s> and <%s>, but was not.' % ( self . val , low , high ) ) return self
7326	def with_continuations ( ** c ) : if len ( c ) : keys , k = zip ( * c . items ( ) ) else : keys , k = tuple ( [ ] ) , tuple ( [ ] ) def d ( f ) : return C ( lambda kself , * conts : lambda * args : f ( * args , self = kself , ** dict ( zip ( keys , conts ) ) ) ) ( * k ) return d
9174	def bake ( binder , recipe_id , publisher , message , cursor ) : recipe = _get_recipe ( recipe_id , cursor ) includes = _formatter_callback_factory ( ) binder = collate_models ( binder , ruleset = recipe , includes = includes ) def flatten_filter ( model ) : return ( isinstance ( model , cnxepub . CompositeDocument ) or ( isinstance ( model , cnxepub . Binder ) and model . metadata . get ( 'type' ) == 'composite-chapter' ) ) def only_documents_filter ( model ) : return isinstance ( model , cnxepub . Document ) and not isinstance ( model , cnxepub . CompositeDocument ) for doc in cnxepub . flatten_to ( binder , flatten_filter ) : publish_composite_model ( cursor , doc , binder , publisher , message ) for doc in cnxepub . flatten_to ( binder , only_documents_filter ) : publish_collated_document ( cursor , doc , binder ) tree = cnxepub . model_to_tree ( binder ) publish_collated_tree ( cursor , tree ) return [ ]
10847	def new ( self , text , shorten = None , now = None , top = None , media = None , when = None ) : url = PATHS [ 'CREATE' ] post_data = "text=%s&" % text post_data += "profile_ids[]=%s&" % self . profile_id if shorten : post_data += "shorten=%s&" % shorten if now : post_data += "now=%s&" % now if top : post_data += "top=%s&" % top if when : post_data += "scheduled_at=%s&" % str ( when ) if media : media_format = "media[%s]=%s&" for media_type , media_item in media . iteritems ( ) : post_data += media_format % ( media_type , media_item ) response = self . api . post ( url = url , data = post_data ) new_update = Update ( api = self . api , raw_response = response [ 'updates' ] [ 0 ] ) self . append ( new_update ) return new_update
11074	def get_by_username ( self , username ) : res = filter ( lambda x : x . username == username , self . users . values ( ) ) if len ( res ) > 0 : return res [ 0 ] return None
7243	def geotiff ( self , ** kwargs ) : if 'proj' not in kwargs : kwargs [ 'proj' ] = self . proj return to_geotiff ( self , ** kwargs )
3217	def get_subnets ( vpc , ** conn ) : subnets = describe_subnets ( Filters = [ { "Name" : "vpc-id" , "Values" : [ vpc [ "id" ] ] } ] , ** conn ) s_ids = [ ] for s in subnets : s_ids . append ( s [ "SubnetId" ] ) return s_ids
13478	def _sentence_to_interstitial_spacing ( self ) : not_sentence_end_chars = [ ' ' ] abbreviations = [ 'i.e.' , 'e.g.' , ' v.' , ' w.' , ' wh.' ] titles = [ 'Prof.' , 'Mr.' , 'Mrs.' , 'Messrs.' , 'Mmes.' , 'Msgr.' , 'Ms.' , 'Fr.' , 'Rev.' , 'St.' , 'Dr.' , 'Lieut.' , 'Lt.' , 'Capt.' , 'Cptn.' , 'Sgt.' , 'Sjt.' , 'Gen.' , 'Hon.' , 'Cpl.' , 'L-Cpl.' , 'Pvt.' , 'Dvr.' , 'Gnr.' , 'Spr.' , 'Col.' , 'Lt-Col' , 'Lt-Gen.' , 'Mx.' ] for abbrev in abbreviations : for x in not_sentence_end_chars : self . _str_replacement ( abbrev + x , abbrev + '\ ' ) for title in titles : for x in not_sentence_end_chars : self . _str_replacement ( title + x , title + '~' )
8874	def _touch ( fname , mode = 0o666 , dir_fd = None , ** kwargs ) : flags = os . O_CREAT | os . O_APPEND with os . fdopen ( os . open ( fname , flags = flags , mode = mode , dir_fd = dir_fd ) ) as f : os . utime ( f . fileno ( ) if os . utime in os . supports_fd else fname , dir_fd = None if os . supports_fd else dir_fd , ** kwargs , )
8929	def pylint ( ctx , skip_tests = False , skip_root = False , reports = False ) : cfg = config . load ( ) add_dir2pypath ( cfg . project_root ) if not os . path . exists ( cfg . testjoin ( '__init__.py' ) ) : add_dir2pypath ( cfg . testjoin ( ) ) namelist = set ( ) for package in cfg . project . get ( 'packages' , [ ] ) : if '.' not in package : namelist . add ( cfg . srcjoin ( package ) ) for module in cfg . project . get ( 'py_modules' , [ ] ) : namelist . add ( module + '.py' ) if not skip_tests : test_py = antglob . FileSet ( cfg . testdir , '**/*.py' ) test_py = [ cfg . testjoin ( i ) for i in test_py ] if test_py : namelist |= set ( test_py ) if not skip_root : root_py = antglob . FileSet ( '.' , '*.py' ) if root_py : namelist |= set ( root_py ) namelist = set ( [ i [ len ( os . getcwd ( ) ) + 1 : ] if i . startswith ( os . getcwd ( ) + os . sep ) else i for i in namelist ] ) cmd = 'pylint' cmd += ' "{}"' . format ( '" "' . join ( sorted ( namelist ) ) ) cmd += ' --reports={0}' . format ( 'y' if reports else 'n' ) for cfgfile in ( '.pylintrc' , 'pylint.rc' , 'pylint.cfg' , 'project.d/pylint.cfg' ) : if os . path . exists ( cfgfile ) : cmd += ' --rcfile={0}' . format ( cfgfile ) break try : shell . run ( cmd , report_error = False , runner = ctx . run ) notify . info ( "OK - No problems found by pylint." ) except exceptions . Failure as exc : if exc . result . return_code & 32 : notify . error ( "Usage error, bad arguments in {}?!" . format ( repr ( cmd ) ) ) raise else : bits = { 1 : "fatal" , 2 : "error" , 4 : "warning" , 8 : "refactor" , 16 : "convention" , } notify . warning ( "Some messages of type {} issued by pylint." . format ( ", " . join ( [ text for bit , text in bits . items ( ) if exc . result . return_code & bit ] ) ) ) if exc . result . return_code & 3 : notify . error ( "Exiting due to fatal / error message." ) raise
364	def threading_data ( data = None , fn = None , thread_count = None , ** kwargs ) : def apply_fn ( results , i , data , kwargs ) : results [ i ] = fn ( data , ** kwargs ) if thread_count is None : results = [ None ] * len ( data ) threads = [ ] for i , d in enumerate ( data ) : t = threading . Thread ( name = 'threading_and_return' , target = apply_fn , args = ( results , i , d , kwargs ) ) t . start ( ) threads . append ( t ) else : divs = np . linspace ( 0 , len ( data ) , thread_count + 1 ) divs = np . round ( divs ) . astype ( int ) results = [ None ] * thread_count threads = [ ] for i in range ( thread_count ) : t = threading . Thread ( name = 'threading_and_return' , target = apply_fn , args = ( results , i , data [ divs [ i ] : divs [ i + 1 ] ] , kwargs ) ) t . start ( ) threads . append ( t ) for t in threads : t . join ( ) if thread_count is None : try : return np . asarray ( results ) except Exception : return results else : return np . concatenate ( results )
1371	def get_heron_dir ( ) : go_above_dirs = 9 path = "/" . join ( os . path . realpath ( __file__ ) . split ( '/' ) [ : - go_above_dirs ] ) return normalized_class_path ( path )
13629	def put ( self , metrics ) : if type ( metrics ) == list : for metric in metrics : self . c . put_metric_data ( ** metric ) else : self . c . put_metric_data ( ** metrics )
3878	async def _handle_conversation_delta ( self , conversation ) : conv_id = conversation . conversation_id . id conv = self . _conv_dict . get ( conv_id , None ) if conv is None : await self . _get_or_fetch_conversation ( conv_id ) else : conv . update_conversation ( conversation )
4455	def apply ( self , ** kwexpr ) : for alias , expr in kwexpr . items ( ) : self . _projections . append ( [ alias , expr ] ) return self
13800	def guid ( * args ) : t = float ( time . time ( ) * 1000 ) r = float ( random . random ( ) * 10000000000000 ) a = random . random ( ) * 10000000000000 data = str ( t ) + ' ' + str ( r ) + ' ' + str ( a ) + ' ' + str ( args ) data = hashlib . md5 ( data . encode ( ) ) . hexdigest ( ) [ : 10 ] return data
2417	def write_file ( spdx_file , out ) : out . write ( '# File\n\n' ) write_value ( 'FileName' , spdx_file . name , out ) write_value ( 'SPDXID' , spdx_file . spdx_id , out ) if spdx_file . has_optional_field ( 'type' ) : write_file_type ( spdx_file . type , out ) write_value ( 'FileChecksum' , spdx_file . chk_sum . to_tv ( ) , out ) if isinstance ( spdx_file . conc_lics , ( document . LicenseConjunction , document . LicenseDisjunction ) ) : write_value ( 'LicenseConcluded' , u'({0})' . format ( spdx_file . conc_lics ) , out ) else : write_value ( 'LicenseConcluded' , spdx_file . conc_lics , out ) for lics in sorted ( spdx_file . licenses_in_file ) : write_value ( 'LicenseInfoInFile' , lics , out ) if isinstance ( spdx_file . copyright , six . string_types ) : write_text_value ( 'FileCopyrightText' , spdx_file . copyright , out ) else : write_value ( 'FileCopyrightText' , spdx_file . copyright , out ) if spdx_file . has_optional_field ( 'license_comment' ) : write_text_value ( 'LicenseComments' , spdx_file . license_comment , out ) if spdx_file . has_optional_field ( 'comment' ) : write_text_value ( 'FileComment' , spdx_file . comment , out ) if spdx_file . has_optional_field ( 'notice' ) : write_text_value ( 'FileNotice' , spdx_file . notice , out ) for contributor in sorted ( spdx_file . contributors ) : write_value ( 'FileContributor' , contributor , out ) for dependency in sorted ( spdx_file . dependencies ) : write_value ( 'FileDependency' , dependency , out ) names = spdx_file . artifact_of_project_name homepages = spdx_file . artifact_of_project_home uris = spdx_file . artifact_of_project_uri for name , homepage , uri in sorted ( zip_longest ( names , homepages , uris ) ) : write_value ( 'ArtifactOfProjectName' , name , out ) if homepage is not None : write_value ( 'ArtifactOfProjectHomePage' , homepage , out ) if uri is not None : write_value ( 'ArtifactOfProjectURI' , uri , out )
5735	def _get_or_create_subscription ( self ) : topic_path = self . _get_topic_path ( ) subscription_name = '{}-{}-{}-worker' . format ( queue . PUBSUB_OBJECT_PREFIX , self . name , uuid4 ( ) . hex ) subscription_path = self . subscriber_client . subscription_path ( self . project , subscription_name ) try : self . subscriber_client . get_subscription ( subscription_path ) except google . cloud . exceptions . NotFound : logger . info ( "Creating worker subscription {}" . format ( subscription_name ) ) self . subscriber_client . create_subscription ( subscription_path , topic_path ) return subscription_path
11034	def get ( self , key : str , default : typing . Any = UNSET , type_ : typing . Type [ typing . Any ] = str , subtype : typing . Type [ typing . Any ] = str , mapper : typing . Optional [ typing . Callable [ [ object ] , object ] ] = None , ) -> typing . Any : value = self . environ . get ( key , UNSET ) if value is UNSET and default is UNSET : raise ConfigError ( "Unknown environment variable: {0}" . format ( key ) ) if value is UNSET : value = default else : value = self . parse ( typing . cast ( str , value ) , type_ , subtype ) if mapper : value = mapper ( value ) return value
5869	def _inactivate_organization_course_relationship ( relationship ) : relationship = internal . OrganizationCourse . objects . get ( id = relationship . id , active = True ) _inactivate_record ( relationship )
2792	def load ( self ) : data = self . get_data ( "certificates/%s" % self . id ) certificate = data [ "certificate" ] for attr in certificate . keys ( ) : setattr ( self , attr , certificate [ attr ] ) return self
1300	def WindowFromPoint ( x : int , y : int ) -> int : return ctypes . windll . user32 . WindowFromPoint ( ctypes . wintypes . POINT ( x , y ) )
8356	def _toUnicode ( self , data , encoding ) : if ( len ( data ) >= 4 ) and ( data [ : 2 ] == '\xfe\xff' ) and ( data [ 2 : 4 ] != '\x00\x00' ) : encoding = 'utf-16be' data = data [ 2 : ] elif ( len ( data ) >= 4 ) and ( data [ : 2 ] == '\xff\xfe' ) and ( data [ 2 : 4 ] != '\x00\x00' ) : encoding = 'utf-16le' data = data [ 2 : ] elif data [ : 3 ] == '\xef\xbb\xbf' : encoding = 'utf-8' data = data [ 3 : ] elif data [ : 4 ] == '\x00\x00\xfe\xff' : encoding = 'utf-32be' data = data [ 4 : ] elif data [ : 4 ] == '\xff\xfe\x00\x00' : encoding = 'utf-32le' data = data [ 4 : ] newdata = unicode ( data , encoding ) return newdata
10497	def doubleClickMouse ( self , coord ) : modFlags = 0 self . _queueMouseButton ( coord , Quartz . kCGMouseButtonLeft , modFlags ) self . _queueMouseButton ( coord , Quartz . kCGMouseButtonLeft , modFlags , clickCount = 2 ) self . _postQueuedEvents ( )
13098	def wait ( self ) : try : self . relay . wait ( ) self . responder . wait ( ) except KeyboardInterrupt : print_notification ( "Stopping" ) finally : self . terminate_processes ( )
3414	def model_to_dict ( model , sort = False ) : obj = OrderedDict ( ) obj [ "metabolites" ] = list ( map ( metabolite_to_dict , model . metabolites ) ) obj [ "reactions" ] = list ( map ( reaction_to_dict , model . reactions ) ) obj [ "genes" ] = list ( map ( gene_to_dict , model . genes ) ) obj [ "id" ] = model . id _update_optional ( model , obj , _OPTIONAL_MODEL_ATTRIBUTES , _ORDERED_OPTIONAL_MODEL_KEYS ) if sort : get_id = itemgetter ( "id" ) obj [ "metabolites" ] . sort ( key = get_id ) obj [ "reactions" ] . sort ( key = get_id ) obj [ "genes" ] . sort ( key = get_id ) return obj
6223	def look_at ( self , vec = None , pos = None ) : if pos is None : vec = Vector3 ( pos ) if vec is None : raise ValueError ( "vector or pos must be set" ) return self . _gl_look_at ( self . position , vec , self . _up )
7055	def register_lcformat ( formatkey , fileglob , timecols , magcols , errcols , readerfunc_module , readerfunc , readerfunc_kwargs = None , normfunc_module = None , normfunc = None , normfunc_kwargs = None , magsarefluxes = False , overwrite_existing = False , lcformat_dir = '~/.astrobase/lcformat-jsons' ) : LOGINFO ( 'adding %s to LC format registry...' % formatkey ) lcformat_dpath = os . path . abspath ( os . path . expanduser ( lcformat_dir ) ) if not os . path . exists ( lcformat_dpath ) : os . makedirs ( lcformat_dpath ) lcformat_jsonpath = os . path . join ( lcformat_dpath , '%s.json' % formatkey ) if os . path . exists ( lcformat_jsonpath ) and not overwrite_existing : LOGERROR ( 'There is an existing lcformat JSON: %s ' 'for this formatkey: %s and ' 'overwrite_existing = False, skipping...' % ( lcformat_jsonpath , formatkey ) ) return None readermodule = _check_extmodule ( readerfunc_module , formatkey ) if not readermodule : LOGERROR ( "could not import the required " "module: %s to read %s light curves" % ( readerfunc_module , formatkey ) ) return None try : getattr ( readermodule , readerfunc ) readerfunc_in = readerfunc except AttributeError : LOGEXCEPTION ( 'Could not get the specified reader ' 'function: %s for lcformat: %s ' 'from module: %s' % ( formatkey , readerfunc_module , readerfunc ) ) raise if normfunc_module : normmodule = _check_extmodule ( normfunc_module , formatkey ) if not normmodule : LOGERROR ( "could not import the required " "module: %s to normalize %s light curves" % ( normfunc_module , formatkey ) ) return None else : normmodule = None if normfunc_module and normfunc : try : getattr ( normmodule , normfunc ) normfunc_in = normfunc except AttributeError : LOGEXCEPTION ( 'Could not get the specified norm ' 'function: %s for lcformat: %s ' 'from module: %s' % ( normfunc , formatkey , normfunc_module ) ) raise else : normfunc_in = None formatdict = { 'fileglob' : fileglob , 'timecols' : timecols , 'magcols' : magcols , 'errcols' : errcols , 'magsarefluxes' : magsarefluxes , 'lcreader_module' : readerfunc_module , 'lcreader_func' : readerfunc_in , 'lcreader_kwargs' : readerfunc_kwargs , 'lcnorm_module' : normfunc_module , 'lcnorm_func' : normfunc_in , 'lcnorm_kwargs' : normfunc_kwargs } with open ( lcformat_jsonpath , 'w' ) as outfd : json . dump ( formatdict , outfd , indent = 4 ) return lcformat_jsonpath
6027	def voronoi_from_pixel_centers ( pixel_centers ) : return scipy . spatial . Voronoi ( np . asarray ( [ pixel_centers [ : , 1 ] , pixel_centers [ : , 0 ] ] ) . T , qhull_options = 'Qbb Qc Qx Qm' )
3789	def property_derivative_T ( self , T , P , zs , ws , order = 1 ) : r sorted_valid_methods = self . select_valid_methods ( T , P , zs , ws ) for method in sorted_valid_methods : try : return self . calculate_derivative_T ( T , P , zs , ws , method , order ) except : pass return None
6494	def set_mappings ( cls , index_name , doc_type , mappings ) : cache . set ( cls . get_cache_item_name ( index_name , doc_type ) , mappings )
7193	def histogram_match ( self , use_bands , blm_source = None , ** kwargs ) : assert has_rio , "To match image histograms please install rio_hist" data = self . _read ( self [ use_bands , ... ] , ** kwargs ) data = np . rollaxis ( data . astype ( np . float32 ) , 0 , 3 ) if 0 in data : data = np . ma . masked_values ( data , 0 ) bounds = self . _reproject ( box ( * self . bounds ) , from_proj = self . proj , to_proj = "EPSG:4326" ) . bounds if blm_source == 'browse' : from gbdxtools . images . browse_image import BrowseImage ref = BrowseImage ( self . cat_id , bbox = bounds ) . read ( ) else : from gbdxtools . images . tms_image import TmsImage tms = TmsImage ( zoom = self . _calc_tms_zoom ( self . affine [ 0 ] ) , bbox = bounds , ** kwargs ) ref = np . rollaxis ( tms . read ( ) , 0 , 3 ) out = np . dstack ( [ rio_match ( data [ : , : , idx ] , ref [ : , : , idx ] . astype ( np . double ) / 255.0 ) for idx in range ( data . shape [ - 1 ] ) ] ) if 'stretch' in kwargs or 'gamma' in kwargs : return self . _histogram_stretch ( out , ** kwargs ) else : return out
7359	def predict_subsequences ( self , sequence_dict , peptide_lengths = None ) : if isinstance ( sequence_dict , string_types ) : sequence_dict = { "seq" : sequence_dict } elif isinstance ( sequence_dict , ( list , tuple ) ) : sequence_dict = { seq : seq for seq in sequence_dict } peptide_lengths = self . _check_peptide_lengths ( peptide_lengths ) peptide_set = set ( [ ] ) peptide_to_name_offset_pairs = defaultdict ( list ) for name , sequence in sequence_dict . items ( ) : for peptide_length in peptide_lengths : for i in range ( len ( sequence ) - peptide_length + 1 ) : peptide = sequence [ i : i + peptide_length ] peptide_set . add ( peptide ) peptide_to_name_offset_pairs [ peptide ] . append ( ( name , i ) ) peptide_list = sorted ( peptide_set ) binding_predictions = self . predict_peptides ( peptide_list ) results = [ ] for binding_prediction in binding_predictions : for name , offset in peptide_to_name_offset_pairs [ binding_prediction . peptide ] : results . append ( binding_prediction . clone_with_updates ( source_sequence_name = name , offset = offset ) ) self . _check_results ( results , peptides = peptide_set , alleles = self . alleles ) return BindingPredictionCollection ( results )
8730	def get_nearest_year_for_day ( day ) : now = time . gmtime ( ) result = now . tm_year if day - now . tm_yday > 365 // 2 : result -= 1 if now . tm_yday - day > 365 // 2 : result += 1 return result
6925	def autocommit ( self ) : if len ( self . cursors . keys ( ) ) == 0 : self . connection . autocommit = True else : raise AttributeError ( 'database cursors are already active, ' 'cannot switch to autocommit now' )
13191	def json_struct_to_xml ( json_obj , root , custom_namespace = None ) : if isinstance ( root , ( str , unicode ) ) : if root . startswith ( '!' ) : root = etree . Element ( '{%s}%s' % ( NS_PROTECTED , root [ 1 : ] ) ) elif root . startswith ( '+' ) : if not custom_namespace : raise Exception ( "JSON fields starts with +, but no custom namespace provided" ) root = etree . Element ( '{%s}%s' % ( custom_namespace , root [ 1 : ] ) ) else : root = etree . Element ( root ) if root . tag in ( 'attachments' , 'grouped_events' , 'media_files' ) : for link in json_obj : root . append ( json_link_to_xml ( link ) ) elif isinstance ( json_obj , ( str , unicode ) ) : root . text = json_obj elif isinstance ( json_obj , ( int , float ) ) : root . text = unicode ( json_obj ) elif isinstance ( json_obj , dict ) : if frozenset ( json_obj . keys ( ) ) == frozenset ( ( 'type' , 'coordinates' ) ) : root . append ( geojson_to_gml ( json_obj ) ) else : for key , val in json_obj . items ( ) : if key == 'url' or key . endswith ( '_url' ) : el = json_link_to_xml ( val , json_link_key_to_xml_rel ( key ) ) else : el = json_struct_to_xml ( val , key , custom_namespace = custom_namespace ) if el is not None : root . append ( el ) elif isinstance ( json_obj , list ) : tag_name = root . tag if tag_name . endswith ( 'ies' ) : tag_name = tag_name [ : - 3 ] + 'y' elif tag_name . endswith ( 's' ) : tag_name = tag_name [ : - 1 ] for val in json_obj : el = json_struct_to_xml ( val , tag_name , custom_namespace = custom_namespace ) if el is not None : root . append ( el ) elif json_obj is None : return None else : raise NotImplementedError return root
12475	def remove_all ( filelist , folder = '' ) : if not folder : for f in filelist : os . remove ( f ) else : for f in filelist : os . remove ( op . join ( folder , f ) )
13157	def count ( cls , cur , table : str , where_keys : list = None ) : if where_keys : where_clause , values = cls . _get_where_clause_with_values ( where_keys ) query = cls . _count_query_where . format ( table , where_clause ) q , t = query , values else : query = cls . _count_query . format ( table ) q , t = query , ( ) yield from cur . execute ( q , t ) result = yield from cur . fetchone ( ) return int ( result [ 0 ] )
6528	def get_reports ( ) : if not hasattr ( get_reports , '_CACHE' ) : get_reports . _CACHE = dict ( ) for entry in pkg_resources . iter_entry_points ( 'tidypy.reports' ) : try : get_reports . _CACHE [ entry . name ] = entry . load ( ) except ImportError as exc : output_error ( 'Could not load report "%s" defined by "%s": %s' % ( entry , entry . dist , exc , ) , ) return get_reports . _CACHE
7522	def concat_vcf ( data , names , full ) : if not full : writer = open ( data . outfiles . vcf , 'w' ) else : writer = gzip . open ( data . outfiles . VCF , 'w' ) vcfheader ( data , names , writer ) writer . close ( ) vcfchunks = glob . glob ( data . outfiles . vcf + ".*" ) vcfchunks . sort ( key = lambda x : int ( x . rsplit ( "." ) [ - 1 ] ) ) if not full : writer = open ( data . outfiles . vcf , 'a' ) else : writer = gzip . open ( data . outfiles . VCF , 'a' ) if data . paramsdict [ "assembly_method" ] in [ "reference" , "denovo+reference" ] : cmd = [ "cat" ] + vcfchunks + [ " | sort -k 2,2 -n | sort -k 1,1 -s" ] cmd = " " . join ( cmd ) proc = sps . Popen ( cmd , shell = True , stderr = sps . STDOUT , stdout = writer , close_fds = True ) else : proc = sps . Popen ( [ "cat" ] + vcfchunks , stderr = sps . STDOUT , stdout = writer , close_fds = True ) err = proc . communicate ( ) [ 0 ] if proc . returncode : raise IPyradWarningExit ( "err in concat_vcf: %s" , err ) writer . close ( ) for chunk in vcfchunks : os . remove ( chunk )
1245	def import_experience ( self , experiences ) : if isinstance ( experiences , dict ) : if self . unique_state : experiences [ 'states' ] = dict ( state = experiences [ 'states' ] ) if self . unique_action : experiences [ 'actions' ] = dict ( action = experiences [ 'actions' ] ) self . model . import_experience ( ** experiences ) else : if self . unique_state : states = dict ( state = list ( ) ) else : states = { name : list ( ) for name in experiences [ 0 ] [ 'states' ] } internals = [ list ( ) for _ in experiences [ 0 ] [ 'internals' ] ] if self . unique_action : actions = dict ( action = list ( ) ) else : actions = { name : list ( ) for name in experiences [ 0 ] [ 'actions' ] } terminal = list ( ) reward = list ( ) for experience in experiences : if self . unique_state : states [ 'state' ] . append ( experience [ 'states' ] ) else : for name in sorted ( states ) : states [ name ] . append ( experience [ 'states' ] [ name ] ) for n , internal in enumerate ( internals ) : internal . append ( experience [ 'internals' ] [ n ] ) if self . unique_action : actions [ 'action' ] . append ( experience [ 'actions' ] ) else : for name in sorted ( actions ) : actions [ name ] . append ( experience [ 'actions' ] [ name ] ) terminal . append ( experience [ 'terminal' ] ) reward . append ( experience [ 'reward' ] ) self . model . import_experience ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward )
2620	def create_vpc ( self ) : try : vpc = self . ec2 . create_vpc ( CidrBlock = '10.0.0.0/16' , AmazonProvidedIpv6CidrBlock = False , ) except Exception as e : logger . error ( "{}\n" . format ( e ) ) raise e internet_gateway = self . ec2 . create_internet_gateway ( ) internet_gateway . attach_to_vpc ( VpcId = vpc . vpc_id ) self . internet_gateway = internet_gateway . id route_table = self . config_route_table ( vpc , internet_gateway ) self . route_table = route_table . id availability_zones = self . client . describe_availability_zones ( ) for num , zone in enumerate ( availability_zones [ 'AvailabilityZones' ] ) : if zone [ 'State' ] == "available" : subnet = vpc . create_subnet ( CidrBlock = '10.0.{}.0/20' . format ( 16 * num ) , AvailabilityZone = zone [ 'ZoneName' ] ) subnet . meta . client . modify_subnet_attribute ( SubnetId = subnet . id , MapPublicIpOnLaunch = { "Value" : True } ) route_table . associate_with_subnet ( SubnetId = subnet . id ) self . sn_ids . append ( subnet . id ) else : logger . info ( "{} unavailable" . format ( zone [ 'ZoneName' ] ) ) self . security_group ( vpc ) self . vpc_id = vpc . id return vpc
5618	def multipart_to_singleparts ( geom ) : if isinstance ( geom , base . BaseGeometry ) : if hasattr ( geom , "geoms" ) : for subgeom in geom : yield subgeom else : yield geom
7296	def get_widget ( model_field , disabled = False ) : attrs = get_attrs ( model_field , disabled ) if hasattr ( model_field , "max_length" ) and not model_field . max_length : return forms . Textarea ( attrs = attrs ) elif isinstance ( model_field , DateTimeField ) : return forms . DateTimeInput ( attrs = attrs ) elif isinstance ( model_field , BooleanField ) : return forms . CheckboxInput ( attrs = attrs ) elif isinstance ( model_field , ReferenceField ) or model_field . choices : return forms . Select ( attrs = attrs ) elif ( isinstance ( model_field , ListField ) or isinstance ( model_field , EmbeddedDocumentField ) or isinstance ( model_field , GeoPointField ) ) : return None else : return forms . TextInput ( attrs = attrs )
4444	def get_suggestions ( self , prefix , fuzzy = False , num = 10 , with_scores = False , with_payloads = False ) : args = [ AutoCompleter . SUGGET_COMMAND , self . key , prefix , 'MAX' , num ] if fuzzy : args . append ( AutoCompleter . FUZZY ) if with_scores : args . append ( AutoCompleter . WITHSCORES ) if with_payloads : args . append ( AutoCompleter . WITHPAYLOADS ) ret = self . redis . execute_command ( * args ) results = [ ] if not ret : return results parser = SuggestionParser ( with_scores , with_payloads , ret ) return [ s for s in parser ]
9182	def validate_model ( cursor , model ) : _validate_license ( model ) _validate_roles ( model ) required_metadata = ( 'title' , 'summary' , ) for metadata_key in required_metadata : if model . metadata . get ( metadata_key ) in [ None , '' , [ ] ] : raise exceptions . MissingRequiredMetadata ( metadata_key ) _validate_derived_from ( cursor , model ) _validate_subjects ( cursor , model )
13617	def publish ( ) : try : build_site ( dev_mode = False , clean = True ) click . echo ( 'Deploying the site...' ) call ( "rsync -avz -e ssh --progress %s/ %s" % ( BUILD_DIR , CONFIG [ "scp_target" ] , ) , shell = True ) if "cloudflare" in CONFIG and "purge" in CONFIG [ "cloudflare" ] and CONFIG [ "cloudflare" ] [ "purge" ] : do_purge ( ) except ( KeyboardInterrupt , SystemExit ) : raise sys . exit ( 1 )
1781	def AAM ( cpu , imm = None ) : if imm is None : imm = 10 else : imm = imm . read ( ) cpu . AH = Operators . UDIV ( cpu . AL , imm ) cpu . AL = Operators . UREM ( cpu . AL , imm ) cpu . _calculate_logic_flags ( 8 , cpu . AL )
3457	def find_bump ( target , tag ) : tmp = tag . split ( "." ) existing = [ intify ( basename ( f ) ) for f in glob ( join ( target , "[0-9]*.md" ) ) ] latest = max ( existing ) if int ( tmp [ 0 ] ) > latest [ 0 ] : return "major" elif int ( tmp [ 1 ] ) > latest [ 1 ] : return "minor" else : return "patch"
10791	def save_wisdom ( wisdomfile ) : if wisdomfile is None : return if wisdomfile : pickle . dump ( pyfftw . export_wisdom ( ) , open ( wisdomfile , 'wb' ) , protocol = 2 )
12242	def bohachevsky1 ( theta ) : x , y = theta obj = x ** 2 + 2 * y ** 2 - 0.3 * np . cos ( 3 * np . pi * x ) - 0.4 * np . cos ( 4 * np . pi * y ) + 0.7 grad = np . array ( [ 2 * x + 0.3 * np . sin ( 3 * np . pi * x ) * 3 * np . pi , 4 * y + 0.4 * np . sin ( 4 * np . pi * y ) * 4 * np . pi , ] ) return obj , grad
2505	def get_extr_license_text ( self , extr_lic ) : text_tripples = list ( self . graph . triples ( ( extr_lic , self . spdx_namespace [ 'extractedText' ] , None ) ) ) if not text_tripples : self . error = True msg = 'Extracted license must have extractedText property' self . logger . log ( msg ) return if len ( text_tripples ) > 1 : self . more_than_one_error ( 'extracted license text' ) return text_tripple = text_tripples [ 0 ] _s , _p , text = text_tripple return text
8392	def usable_class_name ( node ) : name = node . qname ( ) for prefix in [ "__builtin__." , "builtins." , "." ] : if name . startswith ( prefix ) : name = name [ len ( prefix ) : ] return name
12180	def get_author_and_version ( package ) : init_py = open ( os . path . join ( package , '__init__.py' ) ) . read ( ) author = re . search ( "__author__ = ['\"]([^'\"]+)['\"]" , init_py ) . group ( 1 ) version = re . search ( "__version__ = ['\"]([^'\"]+)['\"]" , init_py ) . group ( 1 ) return author , version
12346	def compress ( self , delete_tif = False , folder = None ) : return compress ( self . images , delete_tif , folder )
1411	def filter_spouts ( table , header ) : spouts_info = [ ] for row in table : if row [ 0 ] == 'spout' : spouts_info . append ( row ) return spouts_info , header
12393	def use ( ** kwargs ) : config = dict ( use . config ) use . config . update ( kwargs ) return config
12033	def averageSweep ( self , sweepFirst = 0 , sweepLast = None ) : if sweepLast is None : sweepLast = self . sweeps - 1 nSweeps = sweepLast - sweepFirst + 1 runningSum = np . zeros ( len ( self . sweepY ) ) self . log . debug ( "averaging sweep %d to %d" , sweepFirst , sweepLast ) for sweep in np . arange ( nSweeps ) + sweepFirst : self . setsweep ( sweep ) runningSum += self . sweepY . flatten ( ) average = runningSum / nSweeps return average
11705	def reproduce_asexually ( self , egg_word , sperm_word ) : egg = self . generate_gamete ( egg_word ) sperm = self . generate_gamete ( sperm_word ) self . genome = list ( set ( egg + sperm ) ) self . generation = 1 self . divinity = god
7247	def status ( self , workflow_id ) : self . logger . debug ( 'Get status of workflow: ' + workflow_id ) url = '%(wf_url)s/%(wf_id)s' % { 'wf_url' : self . workflows_url , 'wf_id' : workflow_id } r = self . gbdx_connection . get ( url ) r . raise_for_status ( ) return r . json ( ) [ 'state' ]
2118	def convert ( self , value , param , ctx ) : resource = tower_cli . get_resource ( self . resource_name ) if value is None : return None if isinstance ( value , int ) : return value if re . match ( r'^[\d]+$' , value ) : return int ( value ) if value == 'null' : return value try : debug . log ( 'The %s field is given as a name; ' 'looking it up.' % param . name , header = 'details' ) lookup_data = { resource . identity [ - 1 ] : value } rel = resource . get ( ** lookup_data ) except exc . MultipleResults : raise exc . MultipleRelatedError ( 'Cannot look up {0} exclusively by name, because multiple {0} ' 'objects exist with that name.\n' 'Please send an ID. You can get the ID for the {0} you want ' 'with:\n' ' tower-cli {0} list --name "{1}"' . format ( self . resource_name , value ) , ) except exc . TowerCLIError as ex : raise exc . RelatedError ( 'Could not get %s. %s' % ( self . resource_name , str ( ex ) ) ) return rel [ 'id' ]
4772	def does_not_contain ( self , * items ) : if len ( items ) == 0 : raise ValueError ( 'one or more args must be given' ) elif len ( items ) == 1 : if items [ 0 ] in self . val : self . _err ( 'Expected <%s> to not contain item <%s>, but did.' % ( self . val , items [ 0 ] ) ) else : found = [ ] for i in items : if i in self . val : found . append ( i ) if found : self . _err ( 'Expected <%s> to not contain items %s, but did contain %s.' % ( self . val , self . _fmt_items ( items ) , self . _fmt_items ( found ) ) ) return self
1758	def _raw_read ( self , where : int , size = 1 ) -> bytes : map = self . memory . map_containing ( where ) start = map . _get_offset ( where ) mapType = type ( map ) if mapType is FileMap : end = map . _get_offset ( where + size ) if end > map . _mapped_size : logger . warning ( f"Missing {end - map._mapped_size} bytes at the end of {map._filename}" ) raw_data = map . _data [ map . _get_offset ( where ) : min ( end , map . _mapped_size ) ] if len ( raw_data ) < end : raw_data += b'\x00' * ( end - len ( raw_data ) ) data = b'' for offset in sorted ( map . _overlay . keys ( ) ) : data += raw_data [ len ( data ) : offset ] data += map . _overlay [ offset ] data += raw_data [ len ( data ) : ] elif mapType is AnonMap : data = bytes ( map . _data [ start : start + size ] ) else : data = b'' . join ( self . memory [ where : where + size ] ) assert len ( data ) == size , 'Raw read resulted in wrong data read which should never happen' return data
1671	def ProcessConfigOverrides ( filename ) : abs_filename = os . path . abspath ( filename ) cfg_filters = [ ] keep_looking = True while keep_looking : abs_path , base_name = os . path . split ( abs_filename ) if not base_name : break cfg_file = os . path . join ( abs_path , "CPPLINT.cfg" ) abs_filename = abs_path if not os . path . isfile ( cfg_file ) : continue try : with open ( cfg_file ) as file_handle : for line in file_handle : line , _ , _ = line . partition ( '#' ) if not line . strip ( ) : continue name , _ , val = line . partition ( '=' ) name = name . strip ( ) val = val . strip ( ) if name == 'set noparent' : keep_looking = False elif name == 'filter' : cfg_filters . append ( val ) elif name == 'exclude_files' : if base_name : pattern = re . compile ( val ) if pattern . match ( base_name ) : _cpplint_state . PrintInfo ( 'Ignoring "%s": file excluded by ' '"%s". File path component "%s" matches pattern "%s"\n' % ( filename , cfg_file , base_name , val ) ) return False elif name == 'linelength' : global _line_length try : _line_length = int ( val ) except ValueError : _cpplint_state . PrintError ( 'Line length must be numeric.' ) elif name == 'extensions' : global _valid_extensions try : extensions = [ ext . strip ( ) for ext in val . split ( ',' ) ] _valid_extensions = set ( extensions ) except ValueError : sys . stderr . write ( 'Extensions should be a comma-separated list of values;' 'for example: extensions=hpp,cpp\n' 'This could not be parsed: "%s"' % ( val , ) ) elif name == 'headers' : global _header_extensions try : extensions = [ ext . strip ( ) for ext in val . split ( ',' ) ] _header_extensions = set ( extensions ) except ValueError : sys . stderr . write ( 'Extensions should be a comma-separated list of values;' 'for example: extensions=hpp,cpp\n' 'This could not be parsed: "%s"' % ( val , ) ) elif name == 'root' : global _root _root = val else : _cpplint_state . PrintError ( 'Invalid configuration option (%s) in file %s\n' % ( name , cfg_file ) ) except IOError : _cpplint_state . PrintError ( "Skipping config file '%s': Can't open for reading\n" % cfg_file ) keep_looking = False for cfg_filter in reversed ( cfg_filters ) : _AddFilters ( cfg_filter ) return True
8893	def calculate_uuid ( self ) : if self . uuid_input_fields is None : raise NotImplementedError ( ) if self . uuid_input_fields == "RANDOM" : return uuid . uuid4 ( ) . hex assert isinstance ( self . uuid_input_fields , tuple ) , "'uuid_input_fields' must either be a tuple or the string 'RANDOM'" hashable_input_vals = [ ] for field in self . uuid_input_fields : new_value = getattr ( self , field ) if new_value : hashable_input_vals . append ( str ( new_value ) ) hashable_input = ":" . join ( hashable_input_vals ) if not hashable_input : return uuid . uuid4 ( ) . hex return sha2_uuid ( hashable_input )
11808	def samples ( self , nwords ) : n = self . n nminus1gram = ( '' , ) * ( n - 1 ) output = [ ] for i in range ( nwords ) : if nminus1gram not in self . cond_prob : nminus1gram = ( '' , ) * ( n - 1 ) wn = self . cond_prob [ nminus1gram ] . sample ( ) output . append ( wn ) nminus1gram = nminus1gram [ 1 : ] + ( wn , ) return ' ' . join ( output )
8715	def file_print ( self , filename ) : log . info ( 'Printing ' + filename ) res = self . __exchange ( PRINT_FILE . format ( filename = filename ) ) log . info ( res ) return res
3384	def generate_fva_warmup ( self ) : self . n_warmup = 0 reactions = self . model . reactions self . warmup = np . zeros ( ( 2 * len ( reactions ) , len ( self . model . variables ) ) ) self . model . objective = Zero for sense in ( "min" , "max" ) : self . model . objective_direction = sense for i , r in enumerate ( reactions ) : variables = ( self . model . variables [ self . fwd_idx [ i ] ] , self . model . variables [ self . rev_idx [ i ] ] ) if r . upper_bound - r . lower_bound < self . bounds_tol : LOGGER . info ( "skipping fixed reaction %s" % r . id ) continue self . model . objective . set_linear_coefficients ( { variables [ 0 ] : 1 , variables [ 1 ] : - 1 } ) self . model . slim_optimize ( ) if not self . model . solver . status == OPTIMAL : LOGGER . info ( "can not maximize reaction %s, skipping it" % r . id ) continue primals = self . model . solver . primal_values sol = [ primals [ v . name ] for v in self . model . variables ] self . warmup [ self . n_warmup , ] = sol self . n_warmup += 1 self . model . objective . set_linear_coefficients ( { variables [ 0 ] : 0 , variables [ 1 ] : 0 } ) self . warmup = self . warmup [ 0 : self . n_warmup , : ] keep = np . logical_not ( self . _is_redundant ( self . warmup ) ) self . warmup = self . warmup [ keep , : ] self . n_warmup = self . warmup . shape [ 0 ] if len ( self . warmup . shape ) == 1 or self . warmup . shape [ 0 ] == 1 : raise ValueError ( "Your flux cone consists only of a single point!" ) elif self . n_warmup == 2 : if not self . problem . homogeneous : raise ValueError ( "Can not sample from an inhomogenous problem" " with only 2 search directions :(" ) LOGGER . info ( "All search directions on a line, adding another one." ) newdir = self . warmup . T . dot ( [ 0.25 , 0.25 ] ) self . warmup = np . vstack ( [ self . warmup , newdir ] ) self . n_warmup += 1 self . warmup = shared_np_array ( ( self . n_warmup , len ( self . model . variables ) ) , self . warmup )
13029	def exploit_single ( self , ip , operating_system ) : result = None if "Windows Server 2008" in operating_system or "Windows 7" in operating_system : result = subprocess . run ( [ 'python2' , os . path . join ( self . datadir , 'MS17-010' , 'eternalblue_exploit7.py' ) , str ( ip ) , os . path . join ( self . datadir , 'final_combined.bin' ) , "12" ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) elif "Windows Server 2012" in operating_system or "Windows 10" in operating_system or "Windows 8.1" in operating_system : result = subprocess . run ( [ 'python2' , os . path . join ( self . datadir , 'MS17-010' , 'eternalblue_exploit8.py' ) , str ( ip ) , os . path . join ( self . datadir , 'final_combined.bin' ) , "12" ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) else : return [ "System target could not be automatically identified" ] return result . stdout . decode ( 'utf-8' ) . split ( '\n' )
2024	def GT ( self , a , b ) : return Operators . ITEBV ( 256 , Operators . UGT ( a , b ) , 1 , 0 )
641	def dict ( cls ) : if cls . _properties is None : cls . _readStdConfigFiles ( ) result = dict ( cls . _properties ) keys = os . environ . keys ( ) replaceKeys = filter ( lambda x : x . startswith ( cls . envPropPrefix ) , keys ) for envKey in replaceKeys : key = envKey [ len ( cls . envPropPrefix ) : ] key = key . replace ( '_' , '.' ) result [ key ] = os . environ [ envKey ] return result
7949	def send_stream_tail ( self ) : with self . lock : if not self . _socket or self . _hup : logger . debug ( u"Cannot send stream closing tag: already closed" ) return data = self . _serializer . emit_tail ( ) try : self . _write ( data . encode ( "utf-8" ) ) except ( IOError , SystemError , socket . error ) , err : logger . debug ( u"Sending stream closing tag failed: {0}" . format ( err ) ) self . _serializer = None self . _hup = True if self . _tls_state is None : try : self . _socket . shutdown ( socket . SHUT_WR ) except socket . error : pass self . _set_state ( "closing" ) self . _write_queue . clear ( ) self . _write_queue_cond . notify ( )
2346	def predict_proba ( self , a , b , device = None ) : device = SETTINGS . get_default ( device = device ) if self . model is None : print ( 'Model has to be trained before doing any predictions' ) raise ValueError if len ( np . array ( a ) . shape ) == 1 : a = np . array ( a ) . reshape ( ( - 1 , 1 ) ) b = np . array ( b ) . reshape ( ( - 1 , 1 ) ) m = np . hstack ( ( a , b ) ) m = scale ( m ) m = m . astype ( 'float32' ) m = th . from_numpy ( m ) . t ( ) . unsqueeze ( 0 ) if th . cuda . is_available ( ) : m = m . cuda ( ) return ( self . model ( m ) . data . cpu ( ) . numpy ( ) - .5 ) * 2
5887	def extract ( self , url = None , raw_html = None ) : crawl_candidate = CrawlCandidate ( self . config , url , raw_html ) return self . __crawl ( crawl_candidate )
763	def createRecordSensor ( network , name , dataSource ) : regionType = "py.RecordSensor" regionParams = json . dumps ( { "verbosity" : _VERBOSITY } ) network . addRegion ( name , regionType , regionParams ) sensorRegion = network . regions [ name ] . getSelf ( ) sensorRegion . encoder = createEncoder ( ) network . regions [ name ] . setParameter ( "predictedField" , "consumption" ) sensorRegion . dataSource = dataSource return sensorRegion
6624	def availableVersions ( self ) : r = [ ] for t in self . _getTags ( ) : logger . debug ( "available version tag: %s" , t ) if not len ( t [ 0 ] . strip ( ) ) : continue try : r . append ( GithubComponentVersion ( t [ 0 ] , t [ 0 ] , url = t [ 1 ] , name = self . name , cache_key = None ) ) except ValueError : logger . debug ( 'invalid version tag: %s' , t ) return r
3351	def get_by_any ( self , iterable ) : def get_item ( item ) : if isinstance ( item , int ) : return self [ item ] elif isinstance ( item , string_types ) : return self . get_by_id ( item ) elif item in self : return item else : raise TypeError ( "item in iterable cannot be '%s'" % type ( item ) ) if not isinstance ( iterable , list ) : iterable = [ iterable ] return [ get_item ( item ) for item in iterable ]
12221	def dispatch ( self , func ) : self . callees . append ( self . _make_dispatch ( func ) ) return self . _make_wrapper ( func )
223	def build_environ ( scope : Scope , body : bytes ) -> dict : environ = { "REQUEST_METHOD" : scope [ "method" ] , "SCRIPT_NAME" : scope . get ( "root_path" , "" ) , "PATH_INFO" : scope [ "path" ] , "QUERY_STRING" : scope [ "query_string" ] . decode ( "ascii" ) , "SERVER_PROTOCOL" : f"HTTP/{scope['http_version']}" , "wsgi.version" : ( 1 , 0 ) , "wsgi.url_scheme" : scope . get ( "scheme" , "http" ) , "wsgi.input" : io . BytesIO ( body ) , "wsgi.errors" : sys . stdout , "wsgi.multithread" : True , "wsgi.multiprocess" : True , "wsgi.run_once" : False , } server = scope . get ( "server" ) or ( "localhost" , 80 ) environ [ "SERVER_NAME" ] = server [ 0 ] environ [ "SERVER_PORT" ] = server [ 1 ] if scope . get ( "client" ) : environ [ "REMOTE_ADDR" ] = scope [ "client" ] [ 0 ] for name , value in scope . get ( "headers" , [ ] ) : name = name . decode ( "latin1" ) if name == "content-length" : corrected_name = "CONTENT_LENGTH" elif name == "content-type" : corrected_name = "CONTENT_TYPE" else : corrected_name = f"HTTP_{name}" . upper ( ) . replace ( "-" , "_" ) value = value . decode ( "latin1" ) if corrected_name in environ : value = environ [ corrected_name ] + "," + value environ [ corrected_name ] = value return environ
431	def save_image ( image , image_path = '_temp.png' ) : try : imageio . imwrite ( image_path , image ) except Exception : imageio . imwrite ( image_path , image [ : , : , 0 ] )
11433	def _shift_field_positions_global ( record , start , delta = 1 ) : if not delta : return for tag , fields in record . items ( ) : newfields = [ ] for field in fields : if field [ 4 ] < start : newfields . append ( field ) else : newfields . append ( tuple ( list ( field [ : 4 ] ) + [ field [ 4 ] + delta ] ) ) record [ tag ] = newfields
13223	def dinner ( self , message = "Dinner is served" , shout : bool = False ) : return self . helper . output ( message , shout )
6712	def tunnel ( self , local_port , remote_port ) : r = self . local_renderer r . env . tunnel_local_port = local_port r . env . tunnel_remote_port = remote_port r . local ( ' ssh -i {key_filename} -L {tunnel_local_port}:localhost:{tunnel_remote_port} {user}@{host_string} -N' )
5703	def get_vehicle_hours_by_type ( gtfs , route_type ) : day = gtfs . get_suitable_date_for_daily_extract ( ) query = ( " SELECT * , SUM(end_time_ds - start_time_ds)/3600 as vehicle_hours_type" " FROM" " (SELECT * FROM day_trips as q1" " INNER JOIN" " (SELECT route_I, type FROM routes) as q2" " ON q1.route_I = q2.route_I" " WHERE type = {route_type}" " AND date = '{day}')" . format ( day = day , route_type = route_type ) ) df = gtfs . execute_custom_query_pandas ( query ) return df [ 'vehicle_hours_type' ] . item ( )
7265	def run ( self , ctx ) : if ctx . reverse : self . engine . reverse ( ) if self . engine . empty : raise AssertionError ( 'grappa: no assertions to run' ) try : return self . run_assertions ( ctx ) except Exception as _err : if getattr ( _err , '__legit__' , False ) : raise _err return self . render_error ( ctx , _err )
4011	def get_dusty_containers ( services , include_exited = False ) : client = get_docker_client ( ) if services : containers = [ get_container_for_app_or_service ( service , include_exited = include_exited ) for service in services ] return [ container for container in containers if container ] else : return [ container for container in client . containers ( all = include_exited ) if any ( name . startswith ( '/dusty' ) for name in container . get ( 'Names' , [ ] ) ) ]
1043	def generic_visit ( self , node ) : for field_name in node . _fields : setattr ( node , field_name , self . visit ( getattr ( node , field_name ) ) ) return node
2286	def graph_evaluation ( data , adj_matrix , gpu = None , gpu_id = 0 , ** kwargs ) : gpu = SETTINGS . get_default ( gpu = gpu ) device = 'cuda:{}' . format ( gpu_id ) if gpu else 'cpu' obs = th . FloatTensor ( data ) . to ( device ) cgnn = CGNN_model ( adj_matrix , data . shape [ 0 ] , gpu_id = gpu_id , ** kwargs ) . to ( device ) cgnn . reset_parameters ( ) return cgnn . run ( obs , ** kwargs )
551	def __checkMaturity ( self ) : if self . _currentRecordIndex + 1 < self . _MIN_RECORDS_TO_BE_BEST : return if self . _isMature : return metric = self . _getMetrics ( ) [ self . _optimizedMetricLabel ] self . _metricRegression . addPoint ( x = self . _currentRecordIndex , y = metric ) pctChange , absPctChange = self . _metricRegression . getPctChanges ( ) if pctChange is not None and absPctChange <= self . _MATURITY_MAX_CHANGE : self . _jobsDAO . modelSetFields ( self . _modelID , { 'engMatured' : True } ) self . _cmpReason = ClientJobsDAO . CMPL_REASON_STOPPED self . _isMature = True self . _logger . info ( "Model %d has matured (pctChange=%s, n=%d). \n" "Scores = %s\n" "Stopping execution" , self . _modelID , pctChange , self . _MATURITY_NUM_POINTS , self . _metricRegression . _window )
3836	async def set_conversation_notification_level ( self , set_conversation_notification_level_request ) : response = hangouts_pb2 . SetConversationNotificationLevelResponse ( ) await self . _pb_request ( 'conversations/setconversationnotificationlevel' , set_conversation_notification_level_request , response ) return response
3792	def solve_T ( self , P , V , quick = True ) : r self . Tc = sum ( self . Tcs ) / self . N return super ( type ( self ) . __mro__ [ - 3 ] , self ) . solve_T ( P = P , V = V , quick = quick )
3102	def run_flow ( flow , storage , flags = None , http = None ) : if flags is None : flags = argparser . parse_args ( ) logging . getLogger ( ) . setLevel ( getattr ( logging , flags . logging_level ) ) if not flags . noauth_local_webserver : success = False port_number = 0 for port in flags . auth_host_port : port_number = port try : httpd = ClientRedirectServer ( ( flags . auth_host_name , port ) , ClientRedirectHandler ) except socket . error : pass else : success = True break flags . noauth_local_webserver = not success if not success : print ( _FAILED_START_MESSAGE ) if not flags . noauth_local_webserver : oauth_callback = 'http://{host}:{port}/' . format ( host = flags . auth_host_name , port = port_number ) else : oauth_callback = client . OOB_CALLBACK_URN flow . redirect_uri = oauth_callback authorize_url = flow . step1_get_authorize_url ( ) if not flags . noauth_local_webserver : import webbrowser webbrowser . open ( authorize_url , new = 1 , autoraise = True ) print ( _BROWSER_OPENED_MESSAGE . format ( address = authorize_url ) ) else : print ( _GO_TO_LINK_MESSAGE . format ( address = authorize_url ) ) code = None if not flags . noauth_local_webserver : httpd . handle_request ( ) if 'error' in httpd . query_params : sys . exit ( 'Authentication request was rejected.' ) if 'code' in httpd . query_params : code = httpd . query_params [ 'code' ] else : print ( 'Failed to find "code" in the query parameters ' 'of the redirect.' ) sys . exit ( 'Try running with --noauth_local_webserver.' ) else : code = input ( 'Enter verification code: ' ) . strip ( ) try : credential = flow . step2_exchange ( code , http = http ) except client . FlowExchangeError as e : sys . exit ( 'Authentication has failed: {0}' . format ( e ) ) storage . put ( credential ) credential . set_store ( storage ) print ( 'Authentication successful.' ) return credential
13381	def env_to_dict ( env , pathsep = os . pathsep ) : out_dict = { } for k , v in env . iteritems ( ) : if pathsep in v : out_dict [ k ] = v . split ( pathsep ) else : out_dict [ k ] = v return out_dict
6817	def create_local_renderer ( self ) : r = super ( ApacheSatchel , self ) . create_local_renderer ( ) os_version = self . os_version apache_specifics = r . env . specifics [ os_version . type ] [ os_version . distro ] r . env . update ( apache_specifics ) return r
4877	def validate_course_run_id ( self , value ) : enterprise_customer = self . context . get ( 'enterprise_customer' ) if not enterprise_customer . catalog_contains_course ( value ) : raise serializers . ValidationError ( 'The course run id {course_run_id} is not in the catalog ' 'for Enterprise Customer {enterprise_customer}' . format ( course_run_id = value , enterprise_customer = enterprise_customer . name , ) ) return value
8503	def as_call ( self ) : default = self . _default ( ) default = ', ' + default if default else '' return "pyconfig.%s(%r%s)" % ( self . method , self . get_key ( ) , default )
8849	def setup_actions ( self ) : self . actionOpen . triggered . connect ( self . on_open ) self . actionNew . triggered . connect ( self . on_new ) self . actionSave . triggered . connect ( self . on_save ) self . actionSave_as . triggered . connect ( self . on_save_as ) self . actionQuit . triggered . connect ( QtWidgets . QApplication . instance ( ) . quit ) self . tabWidget . current_changed . connect ( self . on_current_tab_changed ) self . tabWidget . last_tab_closed . connect ( self . on_last_tab_closed ) self . actionAbout . triggered . connect ( self . on_about ) self . actionRun . triggered . connect ( self . on_run ) self . interactiveConsole . process_finished . connect ( self . on_process_finished ) self . actionConfigure_run . triggered . connect ( self . on_configure_run )
12341	def _set_path ( self , path ) : "Set self.path, self.dirname and self.basename." import os . path self . path = os . path . abspath ( path ) self . dirname = os . path . dirname ( path ) self . basename = os . path . basename ( path )
3270	def md_entry ( node ) : key = None value = None if 'key' in node . attrib : key = node . attrib [ 'key' ] else : key = None if key in [ 'time' , 'elevation' ] or key . startswith ( 'custom_dimension' ) : value = md_dimension_info ( key , node . find ( "dimensionInfo" ) ) elif key == 'DynamicDefaultValues' : value = md_dynamic_default_values_info ( key , node . find ( "DynamicDefaultValues" ) ) elif key == 'JDBC_VIRTUAL_TABLE' : value = md_jdbc_virtual_table ( key , node . find ( "virtualTable" ) ) else : value = node . text if None in [ key , value ] : return None else : return ( key , value )
8436	def map ( cls , x , palette , limits , na_value = None ) : n = len ( limits ) pal = palette ( n ) [ match ( x , limits ) ] try : pal [ pd . isnull ( x ) ] = na_value except TypeError : pal = [ v if not pd . isnull ( v ) else na_value for v in pal ] return pal
12830	def set_data ( self , data = { } , datetime_fields = [ ] ) : if datetime_fields : for field in datetime_fields : if field in data : data [ field ] = self . _parse_datetime ( data [ field ] ) super ( CampfireEntity , self ) . set_data ( data )
3847	def parse_watermark_notification ( p ) : return WatermarkNotification ( conv_id = p . conversation_id . id , user_id = from_participantid ( p . sender_id ) , read_timestamp = from_timestamp ( p . latest_read_timestamp ) , )
6465	def size ( self ) : for fd in range ( 3 ) : cr = self . _ioctl_GWINSZ ( fd ) if cr : break if not cr : try : fd = os . open ( os . ctermid ( ) , os . O_RDONLY ) cr = self . _ioctl_GWINSZ ( fd ) os . close ( fd ) except Exception : pass if not cr : env = os . environ cr = ( env . get ( 'LINES' , 25 ) , env . get ( 'COLUMNS' , 80 ) ) return int ( cr [ 1 ] ) , int ( cr [ 0 ] )
2506	def get_extr_lic_name ( self , extr_lic ) : extr_name_list = list ( self . graph . triples ( ( extr_lic , self . spdx_namespace [ 'licenseName' ] , None ) ) ) if len ( extr_name_list ) > 1 : self . more_than_one_error ( 'extracted license name' ) return elif len ( extr_name_list ) == 0 : return return self . to_special_value ( extr_name_list [ 0 ] [ 2 ] )
10205	def extract_date ( self , date ) : if isinstance ( date , six . string_types ) : try : date = dateutil . parser . parse ( date ) except ValueError : raise ValueError ( 'Invalid date format for statistic {}.' ) . format ( self . query_name ) if not isinstance ( date , datetime ) : raise TypeError ( 'Invalid date type for statistic {}.' ) . format ( self . query_name ) return date
12844	def execute_undo ( self , message ) : info ( "undoing message: {message}" ) with self . world . _unlock_temporarily ( ) : message . _undo ( self . world ) self . world . _react_to_undo_response ( message ) for actor in self . actors : actor . _react_to_undo_response ( message )
13517	def resistance ( self ) : self . total_resistance_coef = frictional_resistance_coef ( self . length , self . speed ) + residual_resistance_coef ( self . slenderness_coefficient , self . prismatic_coefficient , froude_number ( self . speed , self . length ) ) RT = 1 / 2 * self . total_resistance_coef * 1025 * self . surface_area * self . speed ** 2 return RT
8714	def file_format ( self ) : log . info ( 'Formating, can take minutes depending on flash size...' ) res = self . __exchange ( 'file.format()' , timeout = 300 ) if 'format done' not in res : log . error ( res ) else : log . info ( res ) return res
11081	def webhook ( * args , ** kwargs ) : def wrapper ( func ) : func . is_webhook = True func . route = args [ 0 ] func . form_params = kwargs . get ( 'form_params' , [ ] ) func . method = kwargs . get ( 'method' , 'POST' ) return func return wrapper
4549	def draw_round_rect ( setter , x , y , w , h , r , color = None , aa = False ) : _draw_fast_hline ( setter , x + r , y , w - 2 * r , color , aa ) _draw_fast_hline ( setter , x + r , y + h - 1 , w - 2 * r , color , aa ) _draw_fast_vline ( setter , x , y + r , h - 2 * r , color , aa ) _draw_fast_vline ( setter , x + w - 1 , y + r , h - 2 * r , color , aa ) _draw_circle_helper ( setter , x + r , y + r , r , 1 , color , aa ) _draw_circle_helper ( setter , x + w - r - 1 , y + r , r , 2 , color , aa ) _draw_circle_helper ( setter , x + w - r - 1 , y + h - r - 1 , r , 4 , color , aa ) _draw_circle_helper ( setter , x + r , y + h - r - 1 , r , 8 , color , aa )
1864	def SARX ( cpu , dest , src , count ) : OperandSize = dest . size count = count . read ( ) countMask = { 8 : 0x1f , 16 : 0x1f , 32 : 0x1f , 64 : 0x3f } [ OperandSize ] tempCount = count & countMask tempDest = value = src . read ( ) sign = value & ( 1 << ( OperandSize - 1 ) ) while tempCount != 0 : cpu . CF = ( value & 0x1 ) != 0 value = ( value >> 1 ) | sign tempCount = tempCount - 1 res = dest . write ( value )
5009	def create_course_completion ( self , user_id , payload ) : url = self . enterprise_configuration . sapsf_base_url + self . global_sap_config . completion_status_api_path return self . _call_post_with_user_override ( user_id , url , payload )
4939	def get_link_by_email ( self , user_email ) : try : user = User . objects . get ( email = user_email ) try : return self . get ( user_id = user . id ) except EnterpriseCustomerUser . DoesNotExist : pass except User . DoesNotExist : pass try : return PendingEnterpriseCustomerUser . objects . get ( user_email = user_email ) except PendingEnterpriseCustomerUser . DoesNotExist : pass return None
141	def to_line_string ( self , closed = True ) : from imgaug . augmentables . lines import LineString if not closed or len ( self . exterior ) <= 1 : return LineString ( self . exterior , label = self . label ) return LineString ( np . concatenate ( [ self . exterior , self . exterior [ 0 : 1 , : ] ] , axis = 0 ) , label = self . label )
2328	def orient_directed_graph ( self , data , graph ) : warnings . warn ( "The algorithm is ran on the skeleton of the given graph." ) return self . orient_undirected_graph ( data , nx . Graph ( graph ) )
6470	def consume_line ( self , line ) : data = RE_VALUE_KEY . split ( line . strip ( ) , 1 ) if len ( data ) == 1 : return float ( data [ 0 ] ) , None else : return float ( data [ 0 ] ) , data [ 1 ] . strip ( )
6534	def merge_list ( list1 , list2 ) : merged = list ( list1 ) for value in list2 : if value not in merged : merged . append ( value ) return merged
5041	def enroll_users_in_program ( cls , enterprise_customer , program_details , course_mode , emails , cohort = None ) : existing_users , unregistered_emails = cls . get_users_by_email ( emails ) course_ids = get_course_runs_from_program ( program_details ) successes = [ ] pending = [ ] failures = [ ] for user in existing_users : succeeded = cls . enroll_user ( enterprise_customer , user , course_mode , * course_ids ) if succeeded : successes . append ( user ) else : failures . append ( user ) for email in unregistered_emails : pending_user = enterprise_customer . enroll_user_pending_registration ( email , course_mode , * course_ids , cohort = cohort ) pending . append ( pending_user ) return successes , pending , failures
9101	def write_directory ( self , directory : str ) -> bool : current_md5_hash = self . get_namespace_hash ( ) md5_hash_path = os . path . join ( directory , f'{self.module_name}.belns.md5' ) if not os . path . exists ( md5_hash_path ) : old_md5_hash = None else : with open ( md5_hash_path ) as file : old_md5_hash = file . read ( ) . strip ( ) if old_md5_hash == current_md5_hash : return False with open ( os . path . join ( directory , f'{self.module_name}.belns' ) , 'w' ) as file : self . write_bel_namespace ( file , use_names = False ) with open ( md5_hash_path , 'w' ) as file : print ( current_md5_hash , file = file ) if self . has_names : with open ( os . path . join ( directory , f'{self.module_name}-names.belns' ) , 'w' ) as file : self . write_bel_namespace ( file , use_names = True ) with open ( os . path . join ( directory , f'{self.module_name}.belns.mapping' ) , 'w' ) as file : self . write_bel_namespace_mappings ( file , desc = 'writing mapping' ) return True
8415	def min_max ( x , na_rm = False , finite = True ) : if not hasattr ( x , 'dtype' ) : x = np . asarray ( x ) if na_rm and finite : x = x [ np . isfinite ( x ) ] elif not na_rm and np . any ( np . isnan ( x ) ) : return np . nan , np . nan elif na_rm : x = x [ ~ np . isnan ( x ) ] elif finite : x = x [ ~ np . isinf ( x ) ] if ( len ( x ) ) : return np . min ( x ) , np . max ( x ) else : return float ( '-inf' ) , float ( 'inf' )
9366	def account_number ( ) : account = [ random . randint ( 1 , 9 ) for _ in range ( 20 ) ] return "" . join ( map ( str , account ) )
2125	def disassociate_always_node ( self , parent , child ) : return self . _disassoc ( self . _forward_rel_name ( 'always' ) , parent , child )
786	def jobGetCancellingJobs ( self , ) : with ConnectionFactory . get ( ) as conn : query = 'SELECT job_id ' 'FROM %s ' 'WHERE (status<>%%s AND cancel is TRUE)' % ( self . jobsTableName , ) conn . cursor . execute ( query , [ self . STATUS_COMPLETED ] ) rows = conn . cursor . fetchall ( ) return tuple ( r [ 0 ] for r in rows )
12063	def getAvgBySweep ( abf , feature , T0 = None , T1 = None ) : if T1 is None : T1 = abf . sweepLength if T0 is None : T0 = 0 data = [ np . empty ( ( 0 ) ) ] * abf . sweeps for AP in cm . dictFlat ( cm . matrixToDicts ( abf . APs ) ) : if T0 < AP [ 'sweepT' ] < T1 : val = AP [ feature ] data [ int ( AP [ 'sweep' ] ) ] = np . concatenate ( ( data [ int ( AP [ 'sweep' ] ) ] , [ val ] ) ) for sweep in range ( abf . sweeps ) : if len ( data [ sweep ] ) > 1 and np . any ( data [ sweep ] ) : data [ sweep ] = np . nanmean ( data [ sweep ] ) elif len ( data [ sweep ] ) == 1 : data [ sweep ] = data [ sweep ] [ 0 ] else : data [ sweep ] = np . nan return data
13253	async def download_metadata_yaml ( session , github_url ) : metadata_yaml_url = _build_metadata_yaml_url ( github_url ) async with session . get ( metadata_yaml_url ) as response : response . raise_for_status ( ) yaml_data = await response . text ( ) return yaml . safe_load ( yaml_data )
11651	def transform ( self , X ) : self . _check_fitted ( ) M = self . smoothness dim = self . dim_ inds = self . inds_ do_check = self . do_bounds_check X = as_features ( X ) if X . dim != dim : msg = "model fit for dimension {} but got dim {}" raise ValueError ( msg . format ( dim , X . dim ) ) Xt = np . empty ( ( len ( X ) , self . inds_ . shape [ 0 ] ) ) Xt . fill ( np . nan ) if self . basis == 'cosine' : coefs = ( np . pi * np . arange ( M + 1 ) ) [ ... , : ] for i , bag in enumerate ( X ) : if do_check : if np . min ( bag ) < 0 or np . max ( bag ) > 1 : raise ValueError ( "Bag {} not in [0, 1]" . format ( i ) ) phi = coefs * bag [ ... , np . newaxis ] np . cos ( phi , out = phi ) phi [ : , : , 1 : ] *= np . sqrt ( 2 ) B = reduce ( op . mul , ( phi [ : , i , inds [ : , i ] ] for i in xrange ( dim ) ) ) Xt [ i , : ] = np . mean ( B , axis = 0 ) else : raise ValueError ( "unknown basis '{}'" . format ( self . basis ) ) return Xt
9261	def filter_wo_labels ( self , all_issues ) : issues_wo_labels = [ ] if not self . options . add_issues_wo_labels : for issue in all_issues : if not issue [ 'labels' ] : issues_wo_labels . append ( issue ) return issues_wo_labels
2839	def pullup ( self , pin , enabled ) : self . _validate_pin ( pin ) if enabled : self . gppu [ int ( pin / 8 ) ] |= 1 << ( int ( pin % 8 ) ) else : self . gppu [ int ( pin / 8 ) ] &= ~ ( 1 << ( int ( pin % 8 ) ) ) self . write_gppu ( )
1261	def get_component ( self , component_name ) : mapping = self . get_components ( ) return mapping [ component_name ] if component_name in mapping else None
13187	def image_path ( instance , filename ) : filename , ext = os . path . splitext ( filename . lower ( ) ) instance_id_hash = hashlib . md5 ( str ( instance . id ) ) . hexdigest ( ) filename_hash = '' . join ( random . sample ( hashlib . md5 ( filename . encode ( 'utf-8' ) ) . hexdigest ( ) , 8 ) ) return '{}/{}{}' . format ( instance_id_hash , filename_hash , ext )
5516	def append ( self , data , start ) : if self . _limit is not None and self . _limit > 0 : if self . _start is None : self . _start = start if start - self . _start > self . reset_rate : self . _sum -= round ( ( start - self . _start ) * self . _limit ) self . _start = start self . _sum += len ( data )
11816	def score ( self , code ) : text = permutation_decode ( self . ciphertext , code ) logP = ( sum ( [ log ( self . Pwords [ word ] ) for word in words ( text ) ] ) + sum ( [ log ( self . P1 [ c ] ) for c in text ] ) + sum ( [ log ( self . P2 [ b ] ) for b in bigrams ( text ) ] ) ) return exp ( logP )
7010	def skyview_stamp ( ra , decl , survey = 'DSS2 Red' , scaling = 'Linear' , flip = True , convolvewith = None , forcefetch = False , cachedir = '~/.astrobase/stamp-cache' , timeout = 10.0 , retry_failed = False , savewcsheader = True , verbose = False ) : stampdict = get_stamp ( ra , decl , survey = survey , scaling = scaling , forcefetch = forcefetch , cachedir = cachedir , timeout = timeout , retry_failed = retry_failed , verbose = verbose ) if stampdict : stampfits = pyfits . open ( stampdict [ 'fitsfile' ] ) header = stampfits [ 0 ] . header frame = stampfits [ 0 ] . data stampfits . close ( ) if flip : frame = np . flipud ( frame ) if verbose : LOGINFO ( 'fetched stamp successfully for (%.3f, %.3f)' % ( ra , decl ) ) if convolvewith : convolved = aconv . convolve ( frame , convolvewith ) if savewcsheader : return convolved , header else : return convolved else : if savewcsheader : return frame , header else : return frame else : LOGERROR ( 'could not fetch the requested stamp for ' 'coords: (%.3f, %.3f) from survey: %s and scaling: %s' % ( ra , decl , survey , scaling ) ) return None
1223	def from_spec ( spec , kwargs = None ) : if isinstance ( spec , dict ) : spec = [ spec ] stack = PreprocessorStack ( ) for preprocessor_spec in spec : preprocessor_kwargs = copy . deepcopy ( kwargs ) preprocessor = util . get_object ( obj = preprocessor_spec , predefined_objects = tensorforce . core . preprocessors . preprocessors , kwargs = preprocessor_kwargs ) assert isinstance ( preprocessor , Preprocessor ) stack . preprocessors . append ( preprocessor ) return stack
13411	def removeLogbooks ( self , type = None , logs = [ ] ) : if type is not None and type in self . logList : if len ( logs ) == 0 or logs == "All" : del self . logList [ type ] else : for logbook in logs : if logbook in self . logList [ type ] : self . logList [ type ] . remove ( logbook ) self . changeLogType ( )
10139	def encrypt_files ( selected_host , only_link , file_name ) : if ENCRYPTION_DISABLED : print ( 'For encryption please install gpg' ) exit ( ) passphrase = '%030x' % random . randrange ( 16 ** 30 ) source_filename = file_name cmd = 'gpg --batch --symmetric --cipher-algo AES256 --passphrase-fd 0 ' '--output - {}' . format ( source_filename ) encrypted_output = Popen ( shlex . split ( cmd ) , stdout = PIPE , stdin = PIPE , stderr = PIPE ) encrypted_data = encrypted_output . communicate ( passphrase . encode ( ) ) [ 0 ] return upload_files ( encrypted_data , selected_host , only_link , file_name ) + '#' + passphrase
756	def createExperimentInferenceDir ( cls , experimentDir ) : path = cls . getExperimentInferenceDirPath ( experimentDir ) cls . makeDirectory ( path ) return path
1819	def SETO ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . OF , 1 , 0 ) )
3751	def TWA ( CASRN , AvailableMethods = False , Method = None ) : def list_methods ( ) : methods = [ ] if CASRN in _OntarioExposureLimits and ( _OntarioExposureLimits [ CASRN ] [ "TWA (ppm)" ] or _OntarioExposureLimits [ CASRN ] [ "TWA (mg/m^3)" ] ) : methods . append ( ONTARIO ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == ONTARIO : if _OntarioExposureLimits [ CASRN ] [ "TWA (ppm)" ] : _TWA = ( _OntarioExposureLimits [ CASRN ] [ "TWA (ppm)" ] , 'ppm' ) elif _OntarioExposureLimits [ CASRN ] [ "TWA (mg/m^3)" ] : _TWA = ( _OntarioExposureLimits [ CASRN ] [ "TWA (mg/m^3)" ] , 'mg/m^3' ) elif Method == NONE : _TWA = None else : raise Exception ( 'Failure in in function' ) return _TWA
13859	def contents ( self , f , text ) : text += self . _read ( f . abs_path ) + "\r\n" return text
6697	def install ( packages , update = False , options = None , version = None ) : manager = MANAGER if update : update_index ( ) if options is None : options = [ ] if version is None : version = '' if version and not isinstance ( packages , list ) : version = '=' + version if not isinstance ( packages , six . string_types ) : packages = " " . join ( packages ) options . append ( "--quiet" ) options . append ( "--assume-yes" ) options = " " . join ( options ) cmd = '%(manager)s install %(options)s %(packages)s%(version)s' % locals ( ) run_as_root ( cmd , pty = False )
213	def from_0to1 ( arr_0to1 , shape , min_value = 0.0 , max_value = 1.0 ) : heatmaps = HeatmapsOnImage ( arr_0to1 , shape , min_value = 0.0 , max_value = 1.0 ) heatmaps . min_value = min_value heatmaps . max_value = max_value return heatmaps
10099	def snippets ( self , timeout = None ) : return self . _api_request ( self . SNIPPETS_ENDPOINT , self . HTTP_GET , timeout = timeout )
1200	def from_spec ( spec , kwargs = None ) : baseline = util . get_object ( obj = spec , predefined_objects = tensorforce . core . baselines . baselines , kwargs = kwargs ) assert isinstance ( baseline , Baseline ) return baseline
7024	def _read_checkplot_picklefile ( checkplotpickle ) : if checkplotpickle . endswith ( '.gz' ) : try : with gzip . open ( checkplotpickle , 'rb' ) as infd : cpdict = pickle . load ( infd ) except UnicodeDecodeError : with gzip . open ( checkplotpickle , 'rb' ) as infd : cpdict = pickle . load ( infd , encoding = 'latin1' ) else : try : with open ( checkplotpickle , 'rb' ) as infd : cpdict = pickle . load ( infd ) except UnicodeDecodeError : with open ( checkplotpickle , 'rb' ) as infd : cpdict = pickle . load ( infd , encoding = 'latin1' ) return cpdict
8830	def segment_allocation_find ( context , lock_mode = False , ** filters ) : range_ids = filters . pop ( "segment_allocation_range_ids" , None ) query = context . session . query ( models . SegmentAllocation ) if lock_mode : query = query . with_lockmode ( "update" ) query = query . filter_by ( ** filters ) if range_ids : query . filter ( models . SegmentAllocation . segment_allocation_range_id . in_ ( range_ids ) ) return query
4553	def set_colors ( self , colors , pos ) : self . _colors = colors self . _pos = pos end = self . _pos + self . numLEDs if end > len ( self . _colors ) : raise ValueError ( 'Needed %d colors but found %d' % ( end , len ( self . _colors ) ) )
7539	def get_binom ( base1 , base2 , estE , estH ) : prior_homo = ( 1. - estH ) / 2. prior_hete = estH bsum = base1 + base2 hetprob = scipy . misc . comb ( bsum , base1 ) / ( 2. ** ( bsum ) ) homoa = scipy . stats . binom . pmf ( base2 , bsum , estE ) homob = scipy . stats . binom . pmf ( base1 , bsum , estE ) hetprob *= prior_hete homoa *= prior_homo homob *= prior_homo probabilities = [ homoa , homob , hetprob ] bestprob = max ( probabilities ) / float ( sum ( probabilities ) ) if hetprob > homoa : return True , bestprob else : return False , bestprob
7779	def __from_rfc2426 ( self , data ) : data = from_utf8 ( data ) lines = data . split ( "\n" ) started = 0 current = None for l in lines : if not l : continue if l [ - 1 ] == "\r" : l = l [ : - 1 ] if not l : continue if l [ 0 ] in " \t" : if current is None : continue current += l [ 1 : ] continue if not started and current and current . upper ( ) . strip ( ) == "BEGIN:VCARD" : started = 1 elif started and current . upper ( ) . strip ( ) == "END:VCARD" : current = None break elif current and started : self . _process_rfc2425_record ( current ) current = l if started and current : self . _process_rfc2425_record ( current )
6945	def jhk_to_rmag ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , RJHK , RJH , RJK , RHK , RJ , RH , RK )
2092	def last_job_data ( self , pk = None , ** kwargs ) : ujt = self . get ( pk , include_debug_header = True , ** kwargs ) if 'current_update' in ujt [ 'related' ] : debug . log ( 'A current job; retrieving it.' , header = 'details' ) return client . get ( ujt [ 'related' ] [ 'current_update' ] [ 7 : ] ) . json ( ) elif ujt [ 'related' ] . get ( 'last_update' , None ) : debug . log ( 'No current job or update exists; retrieving the most recent.' , header = 'details' ) return client . get ( ujt [ 'related' ] [ 'last_update' ] [ 7 : ] ) . json ( ) else : raise exc . NotFound ( 'No related jobs or updates exist.' )
4801	def is_named ( self , filename ) : self . is_file ( ) if not isinstance ( filename , str_types ) : raise TypeError ( 'given filename arg must be a path' ) val_filename = os . path . basename ( os . path . abspath ( self . val ) ) if val_filename != filename : self . _err ( 'Expected filename <%s> to be equal to <%s>, but was not.' % ( val_filename , filename ) ) return self
11798	def suppose ( self , var , value ) : "Start accumulating inferences from assuming var=value." self . support_pruning ( ) removals = [ ( var , a ) for a in self . curr_domains [ var ] if a != value ] self . curr_domains [ var ] = [ value ] return removals
5261	def parse_10qk ( self , response ) : loader = ReportItemLoader ( response = response ) item = loader . load_item ( ) if 'doc_type' in item : doc_type = item [ 'doc_type' ] if doc_type in ( '10-Q' , '10-K' ) : return item return None
4585	def animated_gif_to_colorlists ( image , container = list ) : deprecated . deprecated ( 'util.gif.animated_gif_to_colorlists' ) from PIL import ImageSequence it = ImageSequence . Iterator ( image ) return [ image_to_colorlist ( i , container ) for i in it ]
13056	def _plugin_endpoint_rename ( fn_name , instance ) : if instance and instance . namespaced : fn_name = "r_{0}_{1}" . format ( instance . name , fn_name [ 2 : ] ) return fn_name
6065	def density_between_circular_annuli_in_angular_units ( self , inner_annuli_radius , outer_annuli_radius ) : annuli_area = ( np . pi * outer_annuli_radius ** 2.0 ) - ( np . pi * inner_annuli_radius ** 2.0 ) return ( self . mass_within_circle_in_units ( radius = outer_annuli_radius ) - self . mass_within_circle_in_units ( radius = inner_annuli_radius ) ) / annuli_area
2982	def cmd_add ( opts ) : config = load_config ( opts . config ) b = get_blockade ( config , opts ) b . add_container ( opts . containers )
2027	def SHA3 ( self , start , size ) : data = self . try_simplify_to_constant ( self . read_buffer ( start , size ) ) if issymbolic ( data ) : known_sha3 = { } self . _publish ( 'on_symbolic_sha3' , data , known_sha3 ) value = 0 known_hashes_cond = False for key , hsh in known_sha3 . items ( ) : assert not issymbolic ( key ) , "Saved sha3 data,hash pairs should be concrete" cond = key == data known_hashes_cond = Operators . OR ( cond , known_hashes_cond ) value = Operators . ITEBV ( 256 , cond , hsh , value ) return value value = sha3 . keccak_256 ( data ) . hexdigest ( ) value = int ( value , 16 ) self . _publish ( 'on_concrete_sha3' , data , value ) logger . info ( "Found a concrete SHA3 example %r -> %x" , data , value ) return value
6405	def cmp_features ( feat1 , feat2 ) : if feat1 < 0 or feat2 < 0 : return - 1.0 if feat1 == feat2 : return 1.0 magnitude = len ( _FEATURE_MASK ) featxor = feat1 ^ feat2 diffbits = 0 while featxor : if featxor & 0b1 : diffbits += 1 featxor >>= 1 return 1 - ( diffbits / ( 2 * magnitude ) )
2813	def convert_unsqueeze ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting unsqueeze ...' ) if names == 'short' : tf_name = 'UNSQ' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) def target_layer ( x ) : import keras return keras . backend . expand_dims ( x ) lambda_layer = keras . layers . Lambda ( target_layer , name = tf_name + 'E' ) layers [ scope_name ] = lambda_layer ( layers [ inputs [ 0 ] ] )
13886	def ReplaceInFile ( filename , old , new , encoding = None ) : contents = GetFileContents ( filename , encoding = encoding ) contents = contents . replace ( old , new ) CreateFile ( filename , contents , encoding = encoding ) return contents
13016	def hook ( name ) : def hookTarget ( wrapped ) : if not hasattr ( wrapped , '__hook__' ) : wrapped . __hook__ = [ name ] else : wrapped . __hook__ . append ( name ) return wrapped return hookTarget
1595	def format_mode ( sres ) : mode = sres . st_mode root = ( mode & 0o700 ) >> 6 group = ( mode & 0o070 ) >> 3 user = ( mode & 0o7 ) def stat_type ( md ) : if stat . S_ISDIR ( md ) : return 'd' elif stat . S_ISSOCK ( md ) : return 's' else : return '-' def triple ( md ) : return '%c%c%c' % ( 'r' if md & 0b100 else '-' , 'w' if md & 0b010 else '-' , 'x' if md & 0b001 else '-' ) return '' . join ( [ stat_type ( mode ) , triple ( root ) , triple ( group ) , triple ( user ) ] )
4513	def drawCircle ( self , x0 , y0 , r , color = None ) : md . draw_circle ( self . set , x0 , y0 , r , color )
841	def getPattern ( self , idx , sparseBinaryForm = False , cat = None ) : if cat is not None : assert idx is None idx = self . _categoryList . index ( cat ) if not self . useSparseMemory : pattern = self . _Memory [ idx ] if sparseBinaryForm : pattern = pattern . nonzero ( ) [ 0 ] else : ( nz , values ) = self . _Memory . rowNonZeros ( idx ) if not sparseBinaryForm : pattern = numpy . zeros ( self . _Memory . nCols ( ) ) numpy . put ( pattern , nz , 1 ) else : pattern = nz return pattern
2760	def get_load_balancer ( self , id ) : return LoadBalancer . get_object ( api_token = self . token , id = id )
4687	def decrypt ( self , message ) : if not message : return None try : memo_wif = self . blockchain . wallet . getPrivateKeyForPublicKey ( message [ "to" ] ) pubkey = message [ "from" ] except KeyNotFound : try : memo_wif = self . blockchain . wallet . getPrivateKeyForPublicKey ( message [ "from" ] ) pubkey = message [ "to" ] except KeyNotFound : raise MissingKeyError ( "None of the required memo keys are installed!" "Need any of {}" . format ( [ message [ "to" ] , message [ "from" ] ] ) ) if not hasattr ( self , "chain_prefix" ) : self . chain_prefix = self . blockchain . prefix return memo . decode_memo ( self . privatekey_class ( memo_wif ) , self . publickey_class ( pubkey , prefix = self . chain_prefix ) , message . get ( "nonce" ) , message . get ( "message" ) , )
8030	def groupByContent ( paths ) : handles , results = [ ] , [ ] hList = [ ] for path in paths : try : hList . append ( ( path , open ( path , 'rb' ) , '' ) ) except IOError : pass handles . append ( hList ) while handles : more , done = compareChunks ( handles . pop ( 0 ) ) handles . extend ( more ) results . extend ( done ) return dict ( ( x [ 0 ] , x ) for x in results )
8979	def _path ( self , path ) : mode , encoding = self . _mode_and_encoding_for_open ( ) with open ( path , mode , encoding = encoding ) as file : self . __dump_to_file ( file )
8480	def get ( name , default = None , allow_default = True ) : return Config ( ) . get ( name , default , allow_default = allow_default )
3056	def _create_file_if_needed ( filename ) : if os . path . exists ( filename ) : return False else : open ( filename , 'a+b' ) . close ( ) logger . info ( 'Credential file {0} created' . format ( filename ) ) return True
2961	def __write ( self , containers , initialize = True ) : path = self . _state_file self . _assure_dir ( ) try : flags = os . O_WRONLY | os . O_CREAT if initialize : flags |= os . O_EXCL with os . fdopen ( os . open ( path , flags ) , "w" ) as f : yaml . safe_dump ( self . __base_state ( containers ) , f ) except OSError as err : if err . errno == errno . EEXIST : raise AlreadyInitializedError ( "Path %s exists. " "You may need to destroy a previous blockade." % path ) raise except Exception : self . _state_delete ( ) raise
5599	def open ( self , tile , process , ** kwargs ) : return InputTile ( tile , process , kwargs . get ( "resampling" , None ) )
2645	def App ( apptype , data_flow_kernel = None , walltime = 60 , cache = False , executors = 'all' ) : from parsl . app . python import PythonApp from parsl . app . bash import BashApp logger . warning ( "The 'App' decorator will be deprecated in Parsl 0.8. Please use 'python_app' or 'bash_app' instead." ) if apptype == 'python' : app_class = PythonApp elif apptype == 'bash' : app_class = BashApp else : raise InvalidAppTypeError ( "Invalid apptype requested {}; must be 'python' or 'bash'" . format ( apptype ) ) def wrapper ( f ) : return app_class ( f , data_flow_kernel = data_flow_kernel , walltime = walltime , cache = cache , executors = executors ) return wrapper
922	def _filterRecord ( filterList , record ) : for ( fieldIdx , fp , params ) in filterList : x = dict ( ) x [ 'value' ] = record [ fieldIdx ] x [ 'acceptValues' ] = params [ 'acceptValues' ] x [ 'min' ] = params [ 'min' ] x [ 'max' ] = params [ 'max' ] if not fp ( x ) : return False return True
11670	def _get_Ks ( self ) : "Ks as an array and type-checked." Ks = as_integer_type ( self . Ks ) if Ks . ndim != 1 : raise TypeError ( "Ks should be 1-dim, got shape {}" . format ( Ks . shape ) ) if Ks . min ( ) < 1 : raise ValueError ( "Ks should be positive; got {}" . format ( Ks . min ( ) ) ) return Ks
9628	def detail_view ( self , request ) : context = { 'preview' : self , } kwargs = { } if self . form_class : if request . GET : form = self . form_class ( data = request . GET ) else : form = self . form_class ( ) context [ 'form' ] = form if not form . is_bound or not form . is_valid ( ) : return render ( request , 'mailviews/previews/detail.html' , context ) kwargs . update ( form . get_message_view_kwargs ( ) ) message_view = self . get_message_view ( request , ** kwargs ) message = message_view . render_to_message ( ) raw = message . message ( ) headers = OrderedDict ( ( header , maybe_decode_header ( raw [ header ] ) ) for header in self . headers ) context . update ( { 'message' : message , 'subject' : message . subject , 'body' : message . body , 'headers' : headers , 'raw' : raw . as_string ( ) , } ) alternatives = getattr ( message , 'alternatives' , [ ] ) try : html = next ( alternative [ 0 ] for alternative in alternatives if alternative [ 1 ] == 'text/html' ) context . update ( { 'html' : html , 'escaped_html' : b64encode ( html . encode ( 'utf-8' ) ) , } ) except StopIteration : pass return render ( request , self . template_name , context )
787	def partitionAtIntervals ( data , intervals ) : assert sum ( intervals ) <= len ( data ) start = 0 for interval in intervals : end = start + interval yield data [ start : end ] start = end raise StopIteration
286	def plot_perf_stats ( returns , factor_returns , ax = None ) : if ax is None : ax = plt . gca ( ) bootstrap_values = timeseries . perf_stats_bootstrap ( returns , factor_returns , return_stats = False ) bootstrap_values = bootstrap_values . drop ( 'Kurtosis' , axis = 'columns' ) sns . boxplot ( data = bootstrap_values , orient = 'h' , ax = ax ) return ax
5411	def build_machine ( network = None , machine_type = None , preemptible = None , service_account = None , boot_disk_size_gb = None , disks = None , accelerators = None , labels = None , cpu_platform = None , nvidia_driver_version = None ) : return { 'network' : network , 'machineType' : machine_type , 'preemptible' : preemptible , 'serviceAccount' : service_account , 'bootDiskSizeGb' : boot_disk_size_gb , 'disks' : disks , 'accelerators' : accelerators , 'labels' : labels , 'cpuPlatform' : cpu_platform , 'nvidiaDriverVersion' : nvidia_driver_version , }
10808	def delete ( self ) : with db . session . begin_nested ( ) : Membership . query_by_group ( self ) . delete ( ) GroupAdmin . query_by_group ( self ) . delete ( ) GroupAdmin . query_by_admin ( self ) . delete ( ) db . session . delete ( self )
11024	def _get_networking_mode ( app ) : networks = app . get ( 'networks' ) if networks : return networks [ - 1 ] . get ( 'mode' , 'container' ) container = app . get ( 'container' ) if container is not None and 'docker' in container : docker_network = container [ 'docker' ] . get ( 'network' ) if docker_network == 'USER' : return 'container' elif docker_network == 'BRIDGE' : return 'container/bridge' return 'container' if _is_legacy_ip_per_task ( app ) else 'host'
13135	def autocomplete ( query , country = None , hurricanes = False , cities = True , timeout = 5 ) : data = { } data [ 'query' ] = quote ( query ) data [ 'country' ] = country or '' data [ 'hurricanes' ] = 1 if hurricanes else 0 data [ 'cities' ] = 1 if cities else 0 data [ 'format' ] = 'JSON' r = requests . get ( AUTOCOMPLETE_URL . format ( ** data ) , timeout = timeout ) results = json . loads ( r . content ) [ 'RESULTS' ] return results
11460	def add_systemnumber ( self , source , recid = None ) : if not recid : recid = self . get_recid ( ) if not self . hidden and recid : record_add_field ( self . record , tag = '035' , subfields = [ ( '9' , source ) , ( 'a' , recid ) ] )
4124	def data_cosine ( N = 1024 , A = 0.1 , sampling = 1024. , freq = 200 ) : r t = arange ( 0 , float ( N ) / sampling , 1. / sampling ) x = cos ( 2. * pi * t * freq ) + A * randn ( t . size ) return x
1243	def sample_minibatch ( self , batch_size ) : pool_size = len ( self ) if pool_size == 0 : return [ ] delta_p = self . _memory [ 0 ] / batch_size chosen_idx = [ ] if abs ( self . _memory [ 0 ] ) < util . epsilon : chosen_idx = np . random . randint ( self . _capacity - 1 , self . _capacity - 1 + len ( self ) , size = batch_size ) . tolist ( ) else : for i in xrange ( batch_size ) : lower = max ( i * delta_p , 0 ) upper = min ( ( i + 1 ) * delta_p , self . _memory [ 0 ] ) p = random . uniform ( lower , upper ) chosen_idx . append ( self . _sample_with_priority ( p ) ) return [ ( i , self . _memory [ i ] ) for i in chosen_idx ]
8779	def _try_allocate ( self , context , segment_id , network_id ) : LOG . info ( "Attempting to allocate segment for network %s " "segment_id %s segment_type %s" % ( network_id , segment_id , self . segment_type ) ) filter_dict = { "segment_id" : segment_id , "segment_type" : self . segment_type , "do_not_use" : False } available_ranges = db_api . segment_allocation_range_find ( context , scope = db_api . ALL , ** filter_dict ) available_range_ids = [ r [ "id" ] for r in available_ranges ] try : with context . session . begin ( subtransactions = True ) : filter_dict = { "deallocated" : True , "segment_id" : segment_id , "segment_type" : self . segment_type , "segment_allocation_range_ids" : available_range_ids } allocations = db_api . segment_allocation_find ( context , lock_mode = True , ** filter_dict ) . limit ( 100 ) . all ( ) if allocations : allocation = random . choice ( allocations ) update_dict = { "deallocated" : False , "deallocated_at" : None , "network_id" : network_id } allocation = db_api . segment_allocation_update ( context , allocation , ** update_dict ) LOG . info ( "Allocated segment %s for network %s " "segment_id %s segment_type %s" % ( allocation [ "id" ] , network_id , segment_id , self . segment_type ) ) return allocation except Exception : LOG . exception ( "Error in segment reallocation." ) LOG . info ( "Cannot find reallocatable segment for network %s " "segment_id %s segment_type %s" % ( network_id , segment_id , self . segment_type ) )
13635	def _splitHeaders ( headers ) : return [ cgi . parse_header ( value ) for value in chain . from_iterable ( s . split ( ',' ) for s in headers if s ) ]
1169	def rlecode_hqx ( s ) : if not s : return '' result = [ ] prev = s [ 0 ] count = 1 if s [ - 1 ] == '!' : s = s [ 1 : ] + '?' else : s = s [ 1 : ] + '!' for c in s : if c == prev and count < 255 : count += 1 else : if count == 1 : if prev != '\x90' : result . append ( prev ) else : result += [ '\x90' , '\x00' ] elif count < 4 : if prev != '\x90' : result += [ prev ] * count else : result += [ '\x90' , '\x00' ] * count else : if prev != '\x90' : result += [ prev , '\x90' , chr ( count ) ] else : result += [ '\x90' , '\x00' , '\x90' , chr ( count ) ] count = 1 prev = c return '' . join ( result )
13038	def main ( ) : cred_search = CredentialSearch ( ) arg = argparse . ArgumentParser ( parents = [ cred_search . argparser ] , conflict_handler = 'resolve' ) arg . add_argument ( '-c' , '--count' , help = "Only show the number of results" , action = "store_true" ) arguments = arg . parse_args ( ) if arguments . count : print_line ( "Number of credentials: {}" . format ( cred_search . argument_count ( ) ) ) else : response = cred_search . get_credentials ( ) for hit in response : print_json ( hit . to_dict ( include_meta = True ) )
8733	def parse_timedelta ( str ) : deltas = ( _parse_timedelta_part ( part . strip ( ) ) for part in str . split ( ',' ) ) return sum ( deltas , datetime . timedelta ( ) )
11787	def add ( self , o ) : "Add an observation o to the distribution." self . smooth_for ( o ) self . dictionary [ o ] += 1 self . n_obs += 1 self . sampler = None
9443	def call ( self , call_params ) : path = '/' + self . api_version + '/Call/' method = 'POST' return self . request ( path , method , call_params )
8669	def unlock_key ( key_name , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase ) try : click . echo ( 'Unlocking key...' ) stash . unlock ( key_name = key_name ) click . echo ( 'Key unlocked successfully' ) except GhostError as ex : sys . exit ( ex )
8671	def delete_key ( key_name , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase ) for key in key_name : try : click . echo ( 'Deleting key {0}...' . format ( key ) ) stash . delete ( key_name = key ) except GhostError as ex : sys . exit ( ex ) click . echo ( 'Keys deleted successfully' )
11130	def start ( self ) : with self . _status_lock : if self . _running : raise RuntimeError ( "Already running" ) self . _running = True self . _observer = Observer ( ) self . _observer . schedule ( self . _event_handler , self . _directory_location , recursive = True ) self . _observer . start ( ) self . _origin_mapped_data = self . _load_all_in_directory ( )
62	def is_fully_within_image ( self , image ) : shape = normalize_shape ( image ) height , width = shape [ 0 : 2 ] return self . x1 >= 0 and self . x2 < width and self . y1 >= 0 and self . y2 < height
11597	def _rc_dbsize ( self ) : "Returns the number of keys in the current database" result = 0 for alias , redisent in iteritems ( self . redises ) : if alias . find ( '_slave' ) == - 1 : continue result += redisent . dbsize ( ) return result
6314	def load ( self ) : self . create_effect_classes ( ) self . _add_resource_descriptions_to_pools ( self . create_external_resources ( ) ) self . _add_resource_descriptions_to_pools ( self . create_resources ( ) ) for meta , resource in resources . textures . load_pool ( ) : self . _textures [ meta . label ] = resource for meta , resource in resources . programs . load_pool ( ) : self . _programs [ meta . label ] = resource for meta , resource in resources . scenes . load_pool ( ) : self . _scenes [ meta . label ] = resource for meta , resource in resources . data . load_pool ( ) : self . _data [ meta . label ] = resource self . create_effect_instances ( ) self . post_load ( )
7079	def tic_xmatch ( ra , decl , radius_arcsec = 5.0 , apiversion = 'v0' , forcefetch = False , cachedir = '~/.astrobase/mast-cache' , verbose = True , timeout = 90.0 , refresh = 5.0 , maxtimeout = 180.0 , maxtries = 3 , jitter = 5.0 , raiseonfail = False ) : service = 'Mast.Tic.Crossmatch' xmatch_input = { 'fields' : [ { 'name' : 'ra' , 'type' : 'float' } , { 'name' : 'dec' , 'type' : 'float' } ] } xmatch_input [ 'data' ] = [ { 'ra' : x , 'dec' : y } for ( x , y ) in zip ( ra , decl ) ] params = { 'raColumn' : 'ra' , 'decColumn' : 'dec' , 'radius' : radius_arcsec / 3600.0 } return mast_query ( service , params , data = xmatch_input , jitter = jitter , apiversion = apiversion , forcefetch = forcefetch , cachedir = cachedir , verbose = verbose , timeout = timeout , refresh = refresh , maxtimeout = maxtimeout , maxtries = maxtries , raiseonfail = raiseonfail )
8418	def nearest_int ( x ) : if x == 0 : return np . int64 ( 0 ) elif x > 0 : return np . int64 ( x + 0.5 ) else : return np . int64 ( x - 0.5 )
5927	def configuration ( self ) : configuration = { 'configfilename' : self . filename , 'logfilename' : self . getpath ( 'Logging' , 'logfilename' ) , 'loglevel_console' : self . getLogLevel ( 'Logging' , 'loglevel_console' ) , 'loglevel_file' : self . getLogLevel ( 'Logging' , 'loglevel_file' ) , 'configdir' : self . getpath ( 'DEFAULT' , 'configdir' ) , 'qscriptdir' : self . getpath ( 'DEFAULT' , 'qscriptdir' ) , 'templatesdir' : self . getpath ( 'DEFAULT' , 'templatesdir' ) , } configuration [ 'path' ] = [ os . path . curdir , configuration [ 'qscriptdir' ] , configuration [ 'templatesdir' ] ] return configuration
13120	def argument_count ( self ) : arguments , _ = self . argparser . parse_known_args ( ) return self . count ( ** vars ( arguments ) )
11280	def get_item_creator ( item_type ) : if item_type not in Pipe . pipe_item_types : for registered_type in Pipe . pipe_item_types : if issubclass ( item_type , registered_type ) : return Pipe . pipe_item_types [ registered_type ] return None else : return Pipe . pipe_item_types [ item_type ]
13355	def profil_annuel ( df , func = 'mean' ) : func = _get_funky ( func ) res = df . groupby ( lambda x : x . month ) . aggregate ( func ) res . index = [ cal . month_name [ i ] for i in range ( 1 , 13 ) ] return res
5751	def already_downloaded ( filename ) : cur_file = os . path . join ( c . bview_dir , filename ) old_file = os . path . join ( c . bview_dir , 'old' , filename ) if not os . path . exists ( cur_file ) and not os . path . exists ( old_file ) : return False return True
13378	def preprocess_dict ( d ) : out_env = { } for k , v in d . items ( ) : if not type ( v ) in PREPROCESSORS : raise KeyError ( 'Invalid type in dict: {}' . format ( type ( v ) ) ) out_env [ k ] = PREPROCESSORS [ type ( v ) ] ( v ) return out_env
12294	def annotate_metadata_platform ( repo ) : print ( "Added platform information" ) package = repo . package mgr = plugins_get_mgr ( ) repomgr = mgr . get ( what = 'instrumentation' , name = 'platform' ) package [ 'platform' ] = repomgr . get_metadata ( )
5553	def _validate_zooms ( zooms ) : if isinstance ( zooms , dict ) : if any ( [ a not in zooms for a in [ "min" , "max" ] ] ) : raise MapcheteConfigError ( "min and max zoom required" ) zmin = _validate_zoom ( zooms [ "min" ] ) zmax = _validate_zoom ( zooms [ "max" ] ) if zmin > zmax : raise MapcheteConfigError ( "max zoom must not be smaller than min zoom" ) return list ( range ( zmin , zmax + 1 ) ) elif isinstance ( zooms , list ) : if len ( zooms ) == 1 : return zooms elif len ( zooms ) == 2 : zmin , zmax = sorted ( [ _validate_zoom ( z ) for z in zooms ] ) return list ( range ( zmin , zmax + 1 ) ) else : return zooms else : return [ _validate_zoom ( zooms ) ]
9018	def new_pattern ( self , id_ , name , rows = None ) : if rows is None : rows = self . new_row_collection ( ) return self . _spec . new_pattern ( id_ , name , rows , self )
2224	def _convert_to_hashable ( data , types = True ) : r if data is None : hashable = b'NONE' prefix = b'NULL' elif isinstance ( data , six . binary_type ) : hashable = data prefix = b'TXT' elif isinstance ( data , six . text_type ) : hashable = data . encode ( 'utf-8' ) prefix = b'TXT' elif isinstance ( data , _intlike ) : hashable = _int_to_bytes ( data ) prefix = b'INT' elif isinstance ( data , float ) : a , b = float ( data ) . as_integer_ratio ( ) hashable = _int_to_bytes ( a ) + b'/' + _int_to_bytes ( b ) prefix = b'FLT' else : hash_func = _HASHABLE_EXTENSIONS . lookup ( data ) prefix , hashable = hash_func ( data ) if types : return prefix , hashable else : return b'' , hashable
12709	def world_to_body ( self , position ) : return np . array ( self . ode_body . getPosRelPoint ( tuple ( position ) ) )
5587	def extract_subset ( self , input_data_tiles = None , out_tile = None ) : if self . METADATA [ "data_type" ] == "raster" : mosaic = create_mosaic ( input_data_tiles ) return extract_from_array ( in_raster = prepare_array ( mosaic . data , nodata = self . nodata , dtype = self . output_params [ "dtype" ] ) , in_affine = mosaic . affine , out_tile = out_tile ) elif self . METADATA [ "data_type" ] == "vector" : return [ feature for feature in list ( chain . from_iterable ( [ features for _ , features in input_data_tiles ] ) ) if shape ( feature [ "geometry" ] ) . intersects ( out_tile . bbox ) ]
9658	def get_sinks ( G ) : sinks = [ ] for node in G : if not len ( list ( G . successors ( node ) ) ) : sinks . append ( node ) return sinks
13562	def save ( self , * args , ** kwargs ) : rerank = kwargs . pop ( 'rerank' , True ) if rerank : if not self . id : self . _process_new_rank_obj ( ) elif self . rank == self . _rank_at_load : pass else : self . _process_moved_rank_obj ( ) super ( RankedModel , self ) . save ( * args , ** kwargs )
1317	def GetChildren ( self ) -> list : children = [ ] child = self . GetFirstChildControl ( ) while child : children . append ( child ) child = child . GetNextSiblingControl ( ) return children
5586	def output_cleaned ( self , process_data ) : if self . METADATA [ "data_type" ] == "raster" : if is_numpy_or_masked_array ( process_data ) : return process_data elif is_numpy_or_masked_array_with_tags ( process_data ) : data , tags = process_data return self . output_cleaned ( data ) , tags elif self . METADATA [ "data_type" ] == "vector" : return list ( process_data )
4995	def handle_user_post_save ( sender , ** kwargs ) : created = kwargs . get ( "created" , False ) user_instance = kwargs . get ( "instance" , None ) if user_instance is None : return try : pending_ecu = PendingEnterpriseCustomerUser . objects . get ( user_email = user_instance . email ) except PendingEnterpriseCustomerUser . DoesNotExist : return if not created : try : existing_record = EnterpriseCustomerUser . objects . get ( user_id = user_instance . id ) message_template = "User {user} have changed email to match pending Enterprise Customer link, " "but was already linked to Enterprise Customer {enterprise_customer} - " "deleting pending link record" logger . info ( message_template . format ( user = user_instance , enterprise_customer = existing_record . enterprise_customer ) ) pending_ecu . delete ( ) return except EnterpriseCustomerUser . DoesNotExist : pass enterprise_customer_user = EnterpriseCustomerUser . objects . create ( enterprise_customer = pending_ecu . enterprise_customer , user_id = user_instance . id ) pending_enrollments = list ( pending_ecu . pendingenrollment_set . all ( ) ) if pending_enrollments : def _complete_user_enrollment ( ) : for enrollment in pending_enrollments : enterprise_customer_user . enroll ( enrollment . course_id , enrollment . course_mode , cohort = enrollment . cohort_name ) track_enrollment ( 'pending-admin-enrollment' , user_instance . id , enrollment . course_id ) pending_ecu . delete ( ) transaction . on_commit ( _complete_user_enrollment ) else : pending_ecu . delete ( )
10465	def _getRunningApps ( cls ) : def runLoopAndExit ( ) : AppHelper . stopEventLoop ( ) AppHelper . callLater ( 1 , runLoopAndExit ) AppHelper . runConsoleEventLoop ( ) ws = AppKit . NSWorkspace . sharedWorkspace ( ) apps = ws . runningApplications ( ) return apps
334	def compute_bayes_cone ( preds , starting_value = 1. ) : def scoreatpercentile ( cum_preds , p ) : return [ stats . scoreatpercentile ( c , p ) for c in cum_preds . T ] cum_preds = np . cumprod ( preds + 1 , 1 ) * starting_value perc = { p : scoreatpercentile ( cum_preds , p ) for p in ( 5 , 25 , 75 , 95 ) } return perc
3424	def get_solution ( model , reactions = None , metabolites = None , raise_error = False ) : check_solver_status ( model . solver . status , raise_error = raise_error ) if reactions is None : reactions = model . reactions if metabolites is None : metabolites = model . metabolites rxn_index = list ( ) fluxes = empty ( len ( reactions ) ) reduced = empty ( len ( reactions ) ) var_primals = model . solver . primal_values shadow = empty ( len ( metabolites ) ) if model . solver . is_integer : reduced . fill ( nan ) shadow . fill ( nan ) for ( i , rxn ) in enumerate ( reactions ) : rxn_index . append ( rxn . id ) fluxes [ i ] = var_primals [ rxn . id ] - var_primals [ rxn . reverse_id ] met_index = [ met . id for met in metabolites ] else : var_duals = model . solver . reduced_costs for ( i , rxn ) in enumerate ( reactions ) : forward = rxn . id reverse = rxn . reverse_id rxn_index . append ( forward ) fluxes [ i ] = var_primals [ forward ] - var_primals [ reverse ] reduced [ i ] = var_duals [ forward ] - var_duals [ reverse ] met_index = list ( ) constr_duals = model . solver . shadow_prices for ( i , met ) in enumerate ( metabolites ) : met_index . append ( met . id ) shadow [ i ] = constr_duals [ met . id ] return Solution ( model . solver . objective . value , model . solver . status , Series ( index = rxn_index , data = fluxes , name = "fluxes" ) , Series ( index = rxn_index , data = reduced , name = "reduced_costs" ) , Series ( index = met_index , data = shadow , name = "shadow_prices" ) )
310	def var_cov_var_normal ( P , c , mu = 0 , sigma = 1 ) : alpha = sp . stats . norm . ppf ( 1 - c , mu , sigma ) return P - P * ( alpha + 1 )
7758	def send ( self , stanza ) : if self . uplink : self . uplink . send ( stanza ) else : raise NoRouteError ( "No route for stanza" )
12053	def getIDsFromFiles ( files ) : if type ( files ) is str : files = glob . glob ( files + "/*.*" ) IDs = [ ] for fname in files : if fname [ - 4 : ] . lower ( ) == '.abf' : ext = fname . split ( '.' ) [ - 1 ] IDs . append ( os . path . basename ( fname ) . replace ( '.' + ext , '' ) ) return sorted ( IDs )
7290	def make_key ( * args , ** kwargs ) : sep = kwargs . get ( 'sep' , u"_" ) exclude_last_string = kwargs . get ( 'exclude_last_string' , False ) string_array = [ ] for arg in args : if isinstance ( arg , list ) : string_array . append ( six . text_type ( sep . join ( arg ) ) ) else : if exclude_last_string : new_key_array = arg . split ( sep ) [ : - 1 ] if len ( new_key_array ) > 0 : string_array . append ( make_key ( new_key_array ) ) else : string_array . append ( six . text_type ( arg ) ) return sep . join ( string_array )
5977	def mask_circular_annular_from_shape_pixel_scale_and_radii ( shape , pixel_scale , inner_radius_arcsec , outer_radius_arcsec , centre = ( 0.0 , 0.0 ) ) : mask = np . full ( shape , True ) centres_arcsec = mask_centres_from_shape_pixel_scale_and_centre ( shape = mask . shape , pixel_scale = pixel_scale , centre = centre ) for y in range ( mask . shape [ 0 ] ) : for x in range ( mask . shape [ 1 ] ) : y_arcsec = ( y - centres_arcsec [ 0 ] ) * pixel_scale x_arcsec = ( x - centres_arcsec [ 1 ] ) * pixel_scale r_arcsec = np . sqrt ( x_arcsec ** 2 + y_arcsec ** 2 ) if outer_radius_arcsec >= r_arcsec >= inner_radius_arcsec : mask [ y , x ] = False return mask
10365	def has_translocation_increases_activity ( data : Dict ) -> bool : return part_has_modifier ( data , SUBJECT , TRANSLOCATION ) and part_has_modifier ( data , OBJECT , ACTIVITY )
11788	def smooth_for ( self , o ) : if o not in self . dictionary : self . dictionary [ o ] = self . default self . n_obs += self . default self . sampler = None
937	def save ( self , saveModelDir ) : logger = self . _getLogger ( ) logger . debug ( "(%s) Creating local checkpoint in %r..." , self , saveModelDir ) modelPickleFilePath = self . _getModelPickleFilePath ( saveModelDir ) if os . path . exists ( saveModelDir ) : if not os . path . isdir ( saveModelDir ) : raise Exception ( ( "Existing filesystem entry <%s> is not a model" " checkpoint -- refusing to delete (not a directory)" ) % saveModelDir ) if not os . path . isfile ( modelPickleFilePath ) : raise Exception ( ( "Existing filesystem entry <%s> is not a model" " checkpoint -- refusing to delete" " (%s missing or not a file)" ) % ( saveModelDir , modelPickleFilePath ) ) shutil . rmtree ( saveModelDir ) self . __makeDirectoryFromAbsolutePath ( saveModelDir ) with open ( modelPickleFilePath , 'wb' ) as modelPickleFile : logger . debug ( "(%s) Pickling Model instance..." , self ) pickle . dump ( self , modelPickleFile , protocol = pickle . HIGHEST_PROTOCOL ) logger . debug ( "(%s) Finished pickling Model instance" , self ) self . _serializeExtraData ( extraDataDir = self . _getModelExtraDataDir ( saveModelDir ) ) logger . debug ( "(%s) Finished creating local checkpoint" , self ) return
7295	def create_document_dictionary ( self , document , document_key = None , owner_document = None ) : doc_dict = self . create_doc_dict ( document , document_key , owner_document ) for doc_key , doc_field in doc_dict . items ( ) : if doc_key . startswith ( "_" ) : continue if isinstance ( doc_field , ListField ) : doc_dict [ doc_key ] = self . create_list_dict ( document , doc_field , doc_key ) elif isinstance ( doc_field , EmbeddedDocumentField ) : doc_dict [ doc_key ] = self . create_document_dictionary ( doc_dict [ doc_key ] . document_type_obj , doc_key ) else : doc_dict [ doc_key ] = { "_document" : document , "_key" : doc_key , "_document_field" : doc_field , "_widget" : get_widget ( doc_dict [ doc_key ] , getattr ( doc_field , 'disabled' , False ) ) } return doc_dict
1774	def invalidate_cache ( cpu , address , size ) : cache = cpu . instruction_cache for offset in range ( size ) : if address + offset in cache : del cache [ address + offset ]
3552	def _state_changed ( self , state ) : logger . debug ( 'Adapter state change: {0}' . format ( state ) ) if state == 5 : self . _powered_off . clear ( ) self . _powered_on . set ( ) elif state == 4 : self . _powered_on . clear ( ) self . _powered_off . set ( )
13748	def get_counter ( self , name , start = 0 ) : item = self . get_item ( hash_key = name , start = start ) counter = Counter ( dynamo_item = item , pool = self ) return counter
10439	def stopprocessmonitor ( self , process_name ) : if process_name in self . _process_stats : self . _process_stats [ process_name ] . stop ( ) return 1
4752	def extract_hook_names ( ent ) : hnames = [ ] for hook in ent [ "hooks" ] [ "enter" ] + ent [ "hooks" ] [ "exit" ] : hname = os . path . basename ( hook [ "fpath_orig" ] ) hname = os . path . splitext ( hname ) [ 0 ] hname = hname . strip ( ) hname = hname . replace ( "_enter" , "" ) hname = hname . replace ( "_exit" , "" ) if hname in hnames : continue hnames . append ( hname ) hnames . sort ( ) return hnames
6575	def from_json_list ( cls , api_client , data ) : return [ cls . from_json ( api_client , item ) for item in data ]
7277	def set_video_pos ( self , x1 , y1 , x2 , y2 ) : position = "%s %s %s %s" % ( str ( x1 ) , str ( y1 ) , str ( x2 ) , str ( y2 ) ) self . _player_interface . VideoPos ( ObjectPath ( '/not/used' ) , String ( position ) )
11540	def pin_type ( self , pin ) : if type ( pin ) is list : return [ self . pin_type ( p ) for p in pin ] pin_id = self . _pin_mapping . get ( pin , None ) if pin_id : return self . _pin_type ( pin_id ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
8764	def get_public_net_id ( self ) : for id , net_params in self . strategy . iteritems ( ) : if id == CONF . QUARK . public_net_id : return id return None
9375	def get_run_time_period ( run_steps ) : init_ts_start = get_standardized_timestamp ( 'now' , None ) ts_start = init_ts_start ts_end = '0' for run_step in run_steps : if run_step . ts_start and run_step . ts_end : if run_step . ts_start < ts_start : ts_start = run_step . ts_start if run_step . ts_end > ts_end : ts_end = run_step . ts_end if ts_end == '0' : ts_end = None if ts_start == init_ts_start : ts_start = None logger . info ( 'get_run_time_period range returned ' + str ( ts_start ) + ' to ' + str ( ts_end ) ) return ts_start , ts_end
10792	def tile_overlap ( inner , outer , norm = False ) : div = 1.0 / inner . volume if norm else 1.0 return div * ( inner . volume - util . Tile . intersection ( inner , outer ) . volume )
13826	def FromJsonString ( self , value ) : self . Clear ( ) for path in value . split ( ',' ) : self . paths . append ( path )
1569	def invoke_hook_bolt_fail ( self , heron_tuple , fail_latency_ns ) : if len ( self . task_hooks ) > 0 : bolt_fail_info = BoltFailInfo ( heron_tuple = heron_tuple , failing_task_id = self . get_task_id ( ) , fail_latency_ms = fail_latency_ns * system_constants . NS_TO_MS ) for task_hook in self . task_hooks : task_hook . bolt_fail ( bolt_fail_info )
13737	def get_context ( request , model = None ) : param_values = get_param_values ( request , model = model ) context = param_values . pop ( 'orb_context' , { } ) if isinstance ( context , ( unicode , str ) ) : context = projex . rest . unjsonify ( context ) has_limit = 'limit' in context or 'limit' in param_values orb_context = orb . Context ( ** context ) used = set ( ) query_context = { } for key in orb . Context . Defaults : if key in param_values : used . add ( key ) query_context [ key ] = param_values . get ( key ) schema_values = { } if model : for key , value in request . matchdict . items ( ) : if model . schema ( ) . column ( key , raise_ = False ) : schema_values [ key ] = value for key , value in param_values . items ( ) : root_key = key . split ( '.' ) [ 0 ] schema_object = model . schema ( ) . column ( root_key , raise_ = False ) or model . schema ( ) . collector ( root_key ) if schema_object : value = param_values . pop ( key ) if isinstance ( schema_object , orb . Collector ) and type ( value ) not in ( tuple , list ) : value = [ value ] schema_values [ key ] = value query_context [ 'scope' ] = { 'request' : request } try : default_context = request . orb_default_context except AttributeError : try : query_context [ 'scope' ] . update ( request . orb_scope ) except AttributeError : pass else : if 'scope' in default_context : query_context [ 'scope' ] . update ( default_context . pop ( 'scope' ) ) for k , v in default_context . items ( ) : query_context . setdefault ( k , v ) orb_context . update ( query_context ) return schema_values , orb_context
4536	def fillHSV ( self , hsv , start = 0 , end = - 1 ) : self . fill ( conversions . hsv2rgb ( hsv ) , start , end )
3308	def _run_flup ( app , config , mode ) : if mode == "flup-fcgi" : from flup . server . fcgi import WSGIServer , __version__ as flupver elif mode == "flup-fcgi-fork" : from flup . server . fcgi_fork import WSGIServer , __version__ as flupver else : raise ValueError _logger . info ( "Running WsgiDAV/{} {}/{}..." . format ( __version__ , WSGIServer . __module__ , flupver ) ) server = WSGIServer ( app , bindAddress = ( config [ "host" ] , config [ "port" ] ) , ) try : server . run ( ) except KeyboardInterrupt : _logger . warning ( "Caught Ctrl-C, shutting down..." ) return
10723	def xformers ( sig ) : return [ ( _wrapper ( f ) , l ) for ( f , l ) in _XFORMER . PARSER . parseString ( sig , parseAll = True ) ]
12210	def cache_result ( func ) : def cache_set ( key , value ) : cache . set ( key , value , AVATAR_CACHE_TIMEOUT ) return value def cached_func ( user , size ) : prefix = func . __name__ cached_funcs . add ( prefix ) key = get_cache_key ( user , size , prefix = prefix ) return cache . get ( key ) or cache_set ( key , func ( user , size ) ) return cached_func
11946	def _prepare_messages ( self , messages ) : for message in messages : if not self . backend . can_handle ( message ) : message . _prepare ( )
13567	def linspacestep ( start , stop , step = 1 ) : numsteps = _np . int ( ( stop - start ) / step ) return _np . linspace ( start , start + step * numsteps , numsteps + 1 )
11011	def get_collection_endpoint ( cls ) : return cls . Meta . collection_endpoint if cls . Meta . collection_endpoint is not None else cls . __name__ . lower ( ) + "s/"
8275	def recombine ( self , other , d = 0.7 ) : a , b = self , other d1 = max ( 0 , min ( d , 1 ) ) d2 = d1 c = ColorTheme ( name = a . name [ : int ( len ( a . name ) * d1 ) ] + b . name [ int ( len ( b . name ) * d2 ) : ] , ranges = a . ranges [ : int ( len ( a . ranges ) * d1 ) ] + b . ranges [ int ( len ( b . ranges ) * d2 ) : ] , top = a . top , cache = os . path . join ( DEFAULT_CACHE , "recombined" ) , blue = a . blue , length = a . length * d1 + b . length * d2 ) c . tags = a . tags [ : int ( len ( a . tags ) * d1 ) ] c . tags += b . tags [ int ( len ( b . tags ) * d2 ) : ] return c
12694	def is_disjoint ( set1 , set2 , warn ) : for elem in set2 : if elem in set1 : raise ValueError ( warn ) return True
13874	def _CopyFileLocal ( source_filename , target_filename , copy_symlink = True ) : import shutil try : dir_name = os . path . dirname ( target_filename ) if dir_name and not os . path . isdir ( dir_name ) : os . makedirs ( dir_name ) if copy_symlink and IsLink ( source_filename ) : if os . path . isfile ( target_filename ) or IsLink ( target_filename ) : DeleteFile ( target_filename ) source_filename = ReadLink ( source_filename ) CreateLink ( source_filename , target_filename ) else : if sys . platform == 'win32' : while IsLink ( source_filename ) : link = ReadLink ( source_filename ) if os . path . isabs ( link ) : source_filename = link else : source_filename = os . path . join ( os . path . dirname ( source_filename ) , link ) shutil . copyfile ( source_filename , target_filename ) shutil . copymode ( source_filename , target_filename ) except Exception as e : reraise ( e , 'While executiong _filesystem._CopyFileLocal(%s, %s)' % ( source_filename , target_filename ) )
10306	def calculate_tanimoto_set_distances ( dict_of_sets : Mapping [ X , Set ] ) -> Mapping [ X , Mapping [ X , float ] ] : result : Dict [ X , Dict [ X , float ] ] = defaultdict ( dict ) for x , y in itt . combinations ( dict_of_sets , 2 ) : result [ x ] [ y ] = result [ y ] [ x ] = tanimoto_set_similarity ( dict_of_sets [ x ] , dict_of_sets [ y ] ) for x in dict_of_sets : result [ x ] [ x ] = 1.0 return dict ( result )
7192	def histogram_equalize ( self , use_bands , ** kwargs ) : data = self . _read ( self [ use_bands , ... ] , ** kwargs ) data = np . rollaxis ( data . astype ( np . float32 ) , 0 , 3 ) flattened = data . flatten ( ) if 0 in data : masked = np . ma . masked_values ( data , 0 ) . compressed ( ) image_histogram , bin_edges = np . histogram ( masked , 256 ) else : image_histogram , bin_edges = np . histogram ( flattened , 256 ) bins = ( bin_edges [ : - 1 ] + bin_edges [ 1 : ] ) / 2.0 cdf = image_histogram . cumsum ( ) cdf = cdf / float ( cdf [ - 1 ] ) image_equalized = np . interp ( flattened , bins , cdf ) . reshape ( data . shape ) if 'stretch' in kwargs or 'gamma' in kwargs : return self . _histogram_stretch ( image_equalized , ** kwargs ) else : return image_equalized
4980	def get ( self , request , enterprise_uuid , course_id ) : enrollment_course_mode = request . GET . get ( 'course_mode' ) enterprise_catalog_uuid = request . GET . get ( 'catalog' ) if not enrollment_course_mode : return redirect ( LMS_DASHBOARD_URL ) enrollment_api_client = EnrollmentApiClient ( ) course_modes = enrollment_api_client . get_course_modes ( course_id ) enterprise_customer = get_enterprise_customer_or_404 ( enterprise_uuid ) enterprise_customer_user = get_enterprise_customer_user ( request . user . id , enterprise_customer . uuid ) if not course_modes : context_data = get_global_context ( request , enterprise_customer ) error_code = 'ENTHCE000' log_message = ( 'No course_modes for course_id {course_id} for enterprise_catalog_uuid ' '{enterprise_catalog_uuid}.' 'The following error was presented to ' 'user {userid}: {error_code}' . format ( userid = request . user . id , enterprise_catalog_uuid = enterprise_catalog_uuid , course_id = course_id , error_code = error_code ) ) return render_page_with_error_code_message ( request , context_data , error_code , log_message ) selected_course_mode = None for course_mode in course_modes : if course_mode [ 'slug' ] == enrollment_course_mode : selected_course_mode = course_mode break if not selected_course_mode : return redirect ( LMS_DASHBOARD_URL ) __ , created = EnterpriseCourseEnrollment . objects . get_or_create ( enterprise_customer_user = enterprise_customer_user , course_id = course_id , ) if created : track_enrollment ( 'course-landing-page-enrollment' , request . user . id , course_id , request . get_full_path ( ) ) DataSharingConsent . objects . update_or_create ( username = enterprise_customer_user . username , course_id = course_id , enterprise_customer = enterprise_customer_user . enterprise_customer , defaults = { 'granted' : True } , ) audit_modes = getattr ( settings , 'ENTERPRISE_COURSE_ENROLLMENT_AUDIT_MODES' , [ 'audit' , 'honor' ] ) if selected_course_mode [ 'slug' ] in audit_modes : enrollment_api_client . enroll_user_in_course ( request . user . username , course_id , selected_course_mode [ 'slug' ] ) return redirect ( LMS_COURSEWARE_URL . format ( course_id = course_id ) ) premium_flow = LMS_START_PREMIUM_COURSE_FLOW_URL . format ( course_id = course_id ) if enterprise_catalog_uuid : premium_flow += '?catalog={catalog_uuid}' . format ( catalog_uuid = enterprise_catalog_uuid ) return redirect ( premium_flow )
1529	def establish_ssh_tunnel ( self ) : localportlist = [ ] for ( host , port ) in self . hostportlist : localport = self . pick_unused_port ( ) self . tunnel . append ( subprocess . Popen ( ( 'ssh' , self . tunnelhost , '-NL127.0.0.1:%d:%s:%d' % ( localport , host , port ) ) ) ) localportlist . append ( ( '127.0.0.1' , localport ) ) return localportlist
8179	def clear ( self ) : dict . clear ( self ) self . nodes = [ ] self . edges = [ ] self . root = None self . layout . i = 0 self . alpha = 0
9052	def covariance ( self ) : r from numpy_sugar . linalg import ddot , sum2diag Q0 = self . _QS [ 0 ] [ 0 ] S0 = self . _QS [ 1 ] return sum2diag ( dot ( ddot ( Q0 , self . v0 * S0 ) , Q0 . T ) , self . v1 )
6844	def create_supervisor_services ( self , site ) : self . vprint ( 'create_supervisor_services:' , site ) self . set_site_specifics ( site = site ) r = self . local_renderer if self . verbose : print ( 'r.env:' ) pprint ( r . env , indent = 4 ) self . vprint ( 'r.env.has_worker:' , r . env . has_worker ) if not r . env . has_worker : self . vprint ( 'skipping: no celery worker' ) return if self . name . lower ( ) not in self . genv . services : self . vprint ( 'skipping: celery not enabled' ) return hostname = self . current_hostname target_sites = self . genv . available_sites_by_host . get ( hostname , None ) if target_sites and site not in target_sites : self . vprint ( 'skipping: site not supported on this server' ) return self . render_paths ( ) conf_name = 'celery_%s.conf' % site ret = r . render_to_string ( 'celery/celery_supervisor.template.conf' ) return conf_name , ret
7943	def _start_connect ( self ) : family , addr = self . _dst_addrs . pop ( 0 ) self . _socket = socket . socket ( family , socket . SOCK_STREAM ) self . _socket . setblocking ( False ) self . _dst_addr = addr self . _family = family try : self . _socket . connect ( addr ) except socket . error , err : logger . debug ( "Connect error: {0}" . format ( err ) ) if err . args [ 0 ] in BLOCKING_ERRORS : self . _set_state ( "connecting" ) self . _write_queue . append ( ContinueConnect ( ) ) self . _write_queue_cond . notify ( ) self . event ( ConnectingEvent ( addr ) ) return elif self . _dst_addrs : self . _set_state ( "connect" ) return elif self . _dst_nameports : self . _set_state ( "resolve-hostname" ) return else : self . _socket . close ( ) self . _socket = None self . _set_state ( "aborted" ) self . _write_queue . clear ( ) self . _write_queue_cond . notify ( ) raise self . _connected ( )
7617	def coerce_annotation ( ann , namespace ) : ann = convert ( ann , namespace ) ann . validate ( strict = True ) return ann
9828	def write ( self , file , optstring = "" , quote = False ) : classid = str ( self . id ) if quote : classid = '"' + classid + '"' file . write ( 'object ' + classid + ' class ' + str ( self . name ) + ' ' + optstring + '\n' )
13544	def formatter ( color , s ) : if no_coloring : return s return "{begin}{s}{reset}" . format ( begin = color , s = s , reset = Colors . RESET )
4030	def load ( self ) : con = sqlite3 . connect ( self . tmp_cookie_file ) cur = con . cursor ( ) try : cur . execute ( 'SELECT host_key, path, secure, expires_utc, name, value, encrypted_value ' 'FROM cookies WHERE host_key like "%{}%";' . format ( self . domain_name ) ) except sqlite3 . OperationalError : cur . execute ( 'SELECT host_key, path, is_secure, expires_utc, name, value, encrypted_value ' 'FROM cookies WHERE host_key like "%{}%";' . format ( self . domain_name ) ) cj = http . cookiejar . CookieJar ( ) for item in cur . fetchall ( ) : host , path , secure , expires , name = item [ : 5 ] value = self . _decrypt ( item [ 5 ] , item [ 6 ] ) c = create_cookie ( host , path , secure , expires , name , value ) cj . set_cookie ( c ) con . close ( ) return cj
9253	def get_string_for_issue ( self , issue ) : encapsulated_title = self . encapsulate_string ( issue [ 'title' ] ) try : title_with_number = u"{0} [\\#{1}]({2})" . format ( encapsulated_title , issue [ "number" ] , issue [ "html_url" ] ) except UnicodeEncodeError : title_with_number = "ERROR ERROR ERROR: #{0} {1}" . format ( issue [ "number" ] , issue [ 'title' ] ) print ( title_with_number , '\n' , issue [ "html_url" ] ) return self . issue_line_with_user ( title_with_number , issue )
458	def alphas ( shape , alpha_value , name = None ) : with ops . name_scope ( name , "alphas" , [ shape ] ) as name : alpha_tensor = convert_to_tensor ( alpha_value ) alpha_dtype = dtypes . as_dtype ( alpha_tensor . dtype ) . base_dtype if not isinstance ( shape , ops . Tensor ) : try : shape = constant_op . _tensor_shape_tensor_conversion_function ( tensor_shape . TensorShape ( shape ) ) except ( TypeError , ValueError ) : shape = ops . convert_to_tensor ( shape , dtype = dtypes . int32 ) if not shape . _shape_tuple ( ) : shape = reshape ( shape , [ - 1 ] ) try : output = constant ( alpha_value , shape = shape , dtype = alpha_dtype , name = name ) except ( TypeError , ValueError ) : output = fill ( shape , constant ( alpha_value , dtype = alpha_dtype ) , name = name ) if output . dtype . base_dtype != alpha_dtype : raise AssertionError ( "Dtypes do not corresponds: %s and %s" % ( output . dtype . base_dtype , alpha_dtype ) ) return output
1773	def pop ( cpu , size ) : assert size in ( 16 , cpu . address_bit_size ) base , _ , _ = cpu . get_descriptor ( cpu . SS ) address = cpu . STACK + base value = cpu . read_int ( address , size ) cpu . STACK = cpu . STACK + size // 8 return value
687	def getAllEncodings ( self ) : numEncodings = self . fields [ 0 ] . numEncodings assert ( all ( field . numEncodings == numEncodings for field in self . fields ) ) encodings = [ self . getEncoding ( index ) for index in range ( numEncodings ) ] return encodings
5303	def parse_json_color_file ( path ) : with open ( path , "r" ) as color_file : color_list = json . load ( color_file ) color_dict = { c [ "name" ] : c [ "hex" ] for c in color_list } return color_dict
13000	def calculate_diagram_ranges ( data ) : data = round_arr_teff_luminosity ( data ) temps = data [ 'temp' ] x_range = [ 1.05 * np . amax ( temps ) , .95 * np . amin ( temps ) ] lums = data [ 'lum' ] y_range = [ .50 * np . amin ( lums ) , 2 * np . amax ( lums ) ] return ( x_range , y_range )
8659	def filter_by ( zips = _zips , ** kwargs ) : return [ z for z in zips if all ( [ k in z and z [ k ] == v for k , v in kwargs . items ( ) ] ) ]
7003	def collect_nonperiodic_features ( featuresdir , magcol , outfile , pklglob = 'varfeatures-*.pkl' , featurestouse = NONPERIODIC_FEATURES_TO_COLLECT , maxobjects = None , labeldict = None , labeltype = 'binary' , ) : pklist = glob . glob ( os . path . join ( featuresdir , pklglob ) ) if maxobjects : pklist = pklist [ : maxobjects ] if TQDM : listiterator = tqdm ( pklist ) else : listiterator = pklist feature_dict = { 'objectids' : [ ] , 'magcol' : magcol , 'availablefeatures' : [ ] } LOGINFO ( 'collecting features for magcol: %s' % magcol ) for pkl in listiterator : with open ( pkl , 'rb' ) as infd : varf = pickle . load ( infd ) objectid = varf [ 'objectid' ] if objectid not in feature_dict [ 'objectids' ] : feature_dict [ 'objectids' ] . append ( objectid ) thisfeatures = varf [ magcol ] if featurestouse and len ( featurestouse ) > 0 : featurestoget = featurestouse else : featurestoget = NONPERIODIC_FEATURES_TO_COLLECT for feature in featurestoget : if ( ( feature not in feature_dict [ 'availablefeatures' ] ) and ( feature in thisfeatures ) ) : feature_dict [ 'availablefeatures' ] . append ( feature ) feature_dict [ feature ] = [ ] if feature in thisfeatures : feature_dict [ feature ] . append ( thisfeatures [ feature ] ) for feat in feature_dict [ 'availablefeatures' ] : feature_dict [ feat ] = np . array ( feature_dict [ feat ] ) feature_dict [ 'objectids' ] = np . array ( feature_dict [ 'objectids' ] ) feature_array = np . column_stack ( [ feature_dict [ feat ] for feat in feature_dict [ 'availablefeatures' ] ] ) feature_dict [ 'features_array' ] = feature_array if isinstance ( labeldict , dict ) : labelarray = np . zeros ( feature_dict [ 'objectids' ] . size , dtype = np . int64 ) for ind , objectid in enumerate ( feature_dict [ 'objectids' ] ) : if objectid in labeldict : if labeltype == 'binary' : if labeldict [ objectid ] : labelarray [ ind ] = 1 elif labeltype == 'classes' : labelarray [ ind ] = labeldict [ objectid ] feature_dict [ 'labels_array' ] = labelarray feature_dict [ 'kwargs' ] = { 'pklglob' : pklglob , 'featurestouse' : featurestouse , 'maxobjects' : maxobjects , 'labeltype' : labeltype } with open ( outfile , 'wb' ) as outfd : pickle . dump ( feature_dict , outfd , pickle . HIGHEST_PROTOCOL ) return feature_dict
2577	def _gather_all_deps ( self , args , kwargs ) : depends = [ ] count = 0 for dep in args : if isinstance ( dep , Future ) : if self . tasks [ dep . tid ] [ 'status' ] not in FINAL_STATES : count += 1 depends . extend ( [ dep ] ) for key in kwargs : dep = kwargs [ key ] if isinstance ( dep , Future ) : if self . tasks [ dep . tid ] [ 'status' ] not in FINAL_STATES : count += 1 depends . extend ( [ dep ] ) for dep in kwargs . get ( 'inputs' , [ ] ) : if isinstance ( dep , Future ) : if self . tasks [ dep . tid ] [ 'status' ] not in FINAL_STATES : count += 1 depends . extend ( [ dep ] ) return count , depends
718	def emit ( self , modelInfo ) : if self . __csvFileObj is None : self . __openAndInitCSVFile ( modelInfo ) csv = self . __csvFileObj print >> csv , "%s, " % ( self . __searchJobID ) , print >> csv , "%s, " % ( modelInfo . getModelID ( ) ) , print >> csv , "%s, " % ( modelInfo . statusAsString ( ) ) , if modelInfo . isFinished ( ) : print >> csv , "%s, " % ( modelInfo . getCompletionReason ( ) ) , else : print >> csv , "NA, " , if not modelInfo . isWaitingToStart ( ) : print >> csv , "%s, " % ( modelInfo . getStartTime ( ) ) , else : print >> csv , "NA, " , if modelInfo . isFinished ( ) : dateFormat = "%Y-%m-%d %H:%M:%S" startTime = modelInfo . getStartTime ( ) endTime = modelInfo . getEndTime ( ) print >> csv , "%s, " % endTime , st = datetime . strptime ( startTime , dateFormat ) et = datetime . strptime ( endTime , dateFormat ) print >> csv , "%s, " % ( str ( ( et - st ) . seconds ) ) , else : print >> csv , "NA, " , print >> csv , "NA, " , print >> csv , "%s, " % str ( modelInfo . getModelDescription ( ) ) , print >> csv , "%s, " % str ( modelInfo . getNumRecords ( ) ) , paramLabelsDict = modelInfo . getParamLabels ( ) for key in self . __sortedVariableNames : if key in paramLabelsDict : print >> csv , "%s, " % ( paramLabelsDict [ key ] ) , else : print >> csv , "None, " , metrics = modelInfo . getReportMetrics ( ) for key in self . __sortedMetricsKeys : value = metrics . get ( key , "NA" ) value = str ( value ) value = value . replace ( "\n" , " " ) print >> csv , "%s, " % ( value ) , print >> csv
5557	def _strip_zoom ( input_string , strip_string ) : try : return int ( input_string . strip ( strip_string ) ) except Exception as e : raise MapcheteConfigError ( "zoom level could not be determined: %s" % e )
9857	def get_devices ( self ) : retn = [ ] api_devices = self . api_call ( 'devices' ) self . log ( 'DEVICES:' ) self . log ( api_devices ) for device in api_devices : retn . append ( AmbientWeatherStation ( self , device ) ) self . log ( 'DEVICE INSTANCE LIST:' ) self . log ( retn ) return retn
9821	def create ( ctx , name , description , tags , private , init ) : try : tags = tags . split ( ',' ) if tags else None project_dict = dict ( name = name , description = description , is_public = not private , tags = tags ) project_config = ProjectConfig . from_dict ( project_dict ) except ValidationError : Printer . print_error ( 'Project name should contain only alpha numerical, "-", and "_".' ) sys . exit ( 1 ) try : _project = PolyaxonClient ( ) . project . create_project ( project_config ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not create project `{}`.' . format ( name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Project `{}` was created successfully." . format ( _project . name ) ) if init : ctx . obj = { } ctx . invoke ( init_project , project = name )
4498	def guid ( self , guid ) : return self . _json ( self . _get ( self . _build_url ( 'guids' , guid ) ) , 200 ) [ 'data' ] [ 'type' ]
7250	def launch_batch_workflow ( self , batch_workflow ) : url = '%(base_url)s/batch_workflows' % { 'base_url' : self . base_url } try : r = self . gbdx_connection . post ( url , json = batch_workflow ) batch_workflow_id = r . json ( ) [ 'batch_workflow_id' ] return batch_workflow_id except TypeError as e : self . logger . debug ( 'Batch Workflow not launched, reason: {0}' . format ( e ) )
12164	def add_listener ( self , event , listener ) : self . emit ( 'new_listener' , event , listener ) self . _listeners [ event ] . append ( listener ) self . _check_limit ( event ) return self
1287	def process_docstring ( app , what , name , obj , options , lines ) : markdown = "\n" . join ( lines ) rest = m2r ( markdown ) rest . replace ( "\r\n" , "\n" ) del lines [ : ] lines . extend ( rest . split ( "\n" ) )
353	def assign_params ( sess , params , network ) : ops = [ ] for idx , param in enumerate ( params ) : ops . append ( network . all_params [ idx ] . assign ( param ) ) if sess is not None : sess . run ( ops ) return ops
6333	def dist_abs ( self , src , tar ) : return self . _lev . dist_abs ( src , tar , mode = 'lev' , cost = ( 1 , 1 , 9999 , 9999 ) )
10743	def declaration ( function ) : function , name = _strip_function ( function ) if not function . __code__ . co_code in [ empty_function . __code__ . co_code , doc_string_only_function . __code__ . co_code ] : raise ValueError ( 'Declaration requires empty function definition' ) def not_implemented_function ( * args , ** kwargs ) : raise ValueError ( 'Argument \'{}\' did not specify how \'{}\' should act on it' . format ( args [ 0 ] , name ) ) not_implemented_function . __qualname__ = not_implemented_function . __name__ return default ( not_implemented_function , name = name )
6693	def get_or_create_bucket ( self , name ) : from boto . s3 import connection if self . dryrun : print ( 'boto.connect_s3().create_bucket(%s)' % repr ( name ) ) else : conn = connection . S3Connection ( self . genv . aws_access_key_id , self . genv . aws_secret_access_key ) bucket = conn . create_bucket ( name ) return bucket
1936	def constructor_abi ( self ) -> Dict [ str , Any ] : item = self . _constructor_abi_item if item : return dict ( item ) return { 'inputs' : [ ] , 'payable' : False , 'stateMutability' : 'nonpayable' , 'type' : 'constructor' }
10473	def _sendKey ( self , keychr , modFlags = 0 , globally = False ) : escapedChrs = { '\n' : AXKeyCodeConstants . RETURN , '\r' : AXKeyCodeConstants . RETURN , '\t' : AXKeyCodeConstants . TAB , } if keychr in escapedChrs : keychr = escapedChrs [ keychr ] self . _addKeyToQueue ( keychr , modFlags , globally = globally ) self . _postQueuedEvents ( )
1533	def get_scheduler_location ( self , topologyName , callback = None ) : if callback : self . scheduler_location_watchers [ topologyName ] . append ( callback ) else : scheduler_location_path = self . get_scheduler_location_path ( topologyName ) with open ( scheduler_location_path ) as f : data = f . read ( ) scheduler_location = SchedulerLocation ( ) scheduler_location . ParseFromString ( data ) return scheduler_location
1027	def unhex ( s ) : bits = 0 for c in s : if '0' <= c <= '9' : i = ord ( '0' ) elif 'a' <= c <= 'f' : i = ord ( 'a' ) - 10 elif 'A' <= c <= 'F' : i = ord ( 'A' ) - 10 else : break bits = bits * 16 + ( ord ( c ) - i ) return bits
9934	def get_finder ( import_path ) : Finder = import_string ( import_path ) if not issubclass ( Finder , BaseFinder ) : raise ImproperlyConfigured ( 'Finder "%s" is not a subclass of "%s"' % ( Finder , BaseFinder ) ) return Finder ( )
5203	def delete_connection ( ) : if _CON_SYM_ in globals ( ) : con = globals ( ) . pop ( _CON_SYM_ ) if not getattr ( con , '_session' ) . start ( ) : con . stop ( )
6946	def jhk_to_imag ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , IJHK , IJH , IJK , IHK , IJ , IH , IK )
5278	def preprocess_constraints ( ml , cl , n ) : "Create a graph of constraints for both must- and cannot-links" ml_graph , cl_graph = { } , { } for i in range ( n ) : ml_graph [ i ] = set ( ) cl_graph [ i ] = set ( ) def add_both ( d , i , j ) : d [ i ] . add ( j ) d [ j ] . add ( i ) for ( i , j ) in ml : ml_graph [ i ] . add ( j ) ml_graph [ j ] . add ( i ) for ( i , j ) in cl : cl_graph [ i ] . add ( j ) cl_graph [ j ] . add ( i ) def dfs ( i , graph , visited , component ) : visited [ i ] = True for j in graph [ i ] : if not visited [ j ] : dfs ( j , graph , visited , component ) component . append ( i ) visited = [ False ] * n neighborhoods = [ ] for i in range ( n ) : if not visited [ i ] and ml_graph [ i ] : component = [ ] dfs ( i , ml_graph , visited , component ) for x1 in component : for x2 in component : if x1 != x2 : ml_graph [ x1 ] . add ( x2 ) neighborhoods . append ( component ) for ( i , j ) in cl : for x in ml_graph [ i ] : add_both ( cl_graph , x , j ) for y in ml_graph [ j ] : add_both ( cl_graph , i , y ) for x in ml_graph [ i ] : for y in ml_graph [ j ] : add_both ( cl_graph , x , y ) for i in ml_graph : for j in ml_graph [ i ] : if j != i and j in cl_graph [ i ] : raise InconsistentConstraintsException ( 'Inconsistent constraints between {} and {}' . format ( i , j ) ) return ml_graph , cl_graph , neighborhoods
4127	def plot ( self , ** kargs ) : from pylab import plot , linspace , xlabel , ylabel , grid time = linspace ( 1 * self . dt , self . N * self . dt , self . N ) plot ( time , self . data , ** kargs ) xlabel ( 'Time' ) ylabel ( 'Amplitude' ) grid ( True )
6358	def lcsstr ( self , src , tar ) : lengths = np_zeros ( ( len ( src ) + 1 , len ( tar ) + 1 ) , dtype = np_int ) longest , i_longest = 0 , 0 for i in range ( 1 , len ( src ) + 1 ) : for j in range ( 1 , len ( tar ) + 1 ) : if src [ i - 1 ] == tar [ j - 1 ] : lengths [ i , j ] = lengths [ i - 1 , j - 1 ] + 1 if lengths [ i , j ] > longest : longest = lengths [ i , j ] i_longest = i else : lengths [ i , j ] = 0 return src [ i_longest - longest : i_longest ]
1732	def is_empty_object ( n , last ) : if n . strip ( ) : return False last = last . strip ( ) markers = { ')' , ';' , } if not last or last [ - 1 ] in markers : return False return True
5976	def total_regular_pixels_from_mask ( mask ) : total_regular_pixels = 0 for y in range ( mask . shape [ 0 ] ) : for x in range ( mask . shape [ 1 ] ) : if not mask [ y , x ] : total_regular_pixels += 1 return total_regular_pixels
8301	def dispatch ( self , message , source = None ) : msgtype = "" try : if type ( message [ 0 ] ) == str : address = message [ 0 ] self . callbacks [ address ] ( message ) elif type ( message [ 0 ] ) == list : for msg in message : self . dispatch ( msg ) except KeyError , key : print 'address %s not found, %s: %s' % ( address , key , message ) pprint . pprint ( message ) except IndexError , e : print '%s: %s' % ( e , message ) pass except None , e : print "Exception in" , address , "callback :" , e return
7679	def event ( annotation , ** kwargs ) : times , values = annotation . to_interval_values ( ) if any ( values ) : labels = values else : labels = None return mir_eval . display . events ( times , labels = labels , ** kwargs )
9793	def _matches_patterns ( path , patterns ) : for glob in patterns : try : if PurePath ( path ) . match ( glob ) : return True except TypeError : pass return False
12075	def analyze ( fname = False , save = True , show = None ) : if fname and os . path . exists ( fname . replace ( ".abf" , ".rst" ) ) : print ( "SKIPPING DUE TO RST FILE" ) return swhlab . plotting . core . IMAGE_SAVE = save if show is None : if cm . isIpython ( ) : swhlab . plotting . core . IMAGE_SHOW = True else : swhlab . plotting . core . IMAGE_SHOW = False abf = ABF ( fname ) print ( ">>>>> PROTOCOL >>>>>" , abf . protocomment ) runFunction = "proto_unknown" if "proto_" + abf . protocomment in globals ( ) : runFunction = "proto_" + abf . protocomment abf . log . debug ( "running %s()" % ( runFunction ) ) plt . close ( 'all' ) globals ( ) [ runFunction ] ( abf ) try : globals ( ) [ runFunction ] ( abf ) except : abf . log . error ( "EXCEPTION DURING PROTOCOL FUNCTION" ) abf . log . error ( sys . exc_info ( ) [ 0 ] ) return "ERROR" plt . close ( 'all' ) return "SUCCESS"
7161	def go_back ( self , n = 1 ) : if not self . can_go_back : return N = max ( len ( self . answers ) - abs ( n ) , 0 ) self . answers = OrderedDict ( islice ( self . answers . items ( ) , N ) )
12879	def sep1 ( parser , separator ) : first = [ parser ( ) ] def inner ( ) : separator ( ) return parser ( ) return first + many ( tri ( inner ) )
1196	def _make_future_features ( node ) : assert isinstance ( node , ast . ImportFrom ) assert node . module == '__future__' features = FutureFeatures ( ) for alias in node . names : name = alias . name if name in _FUTURE_FEATURES : if name not in _IMPLEMENTED_FUTURE_FEATURES : msg = 'future feature {} not yet implemented by grumpy' . format ( name ) raise util . ParseError ( node , msg ) setattr ( features , name , True ) elif name == 'braces' : raise util . ParseError ( node , 'not a chance' ) elif name not in _REDUNDANT_FUTURE_FEATURES : msg = 'future feature {} is not defined' . format ( name ) raise util . ParseError ( node , msg ) return features
3603	def _build_endpoint_url ( self , url , name = None ) : if not url . endswith ( self . URL_SEPERATOR ) : url = url + self . URL_SEPERATOR if name is None : name = '' return '%s%s%s' % ( urlparse . urljoin ( self . dsn , url ) , name , self . NAME_EXTENSION )
12893	def get_power ( self ) : power = ( yield from self . handle_int ( self . API . get ( 'power' ) ) ) return bool ( power )
9523	def scaffolds_to_contigs ( infile , outfile , number_contigs = False ) : seq_reader = sequences . file_reader ( infile ) fout = utils . open_file_write ( outfile ) for seq in seq_reader : contigs = seq . contig_coords ( ) counter = 1 for contig in contigs : if number_contigs : name = seq . id + '.' + str ( counter ) counter += 1 else : name = '.' . join ( [ seq . id , str ( contig . start + 1 ) , str ( contig . end + 1 ) ] ) print ( sequences . Fasta ( name , seq [ contig . start : contig . end + 1 ] ) , file = fout ) utils . close ( fout )
7524	def _collapse_outgroup ( tree , taxdicts ) : outg = taxdicts [ 0 ] [ "p4" ] if not all ( [ i [ "p4" ] == outg for i in taxdicts ] ) : raise Exception ( "no good" ) tre = ete . Tree ( tree . write ( format = 1 ) ) alltax = [ i for i in tre . get_leaf_names ( ) if i not in outg ] alltax += [ outg [ 0 ] ] tre . prune ( alltax ) tre . search_nodes ( name = outg [ 0 ] ) [ 0 ] . name = "outgroup" tre . ladderize ( ) taxd = copy . deepcopy ( taxdicts ) newtaxdicts = [ ] for test in taxd : test [ "p4" ] = [ "outgroup" ] newtaxdicts . append ( test ) return tre , newtaxdicts
12192	def _instruction_list ( self , filters ) : return '\n\n' . join ( [ self . INSTRUCTIONS . strip ( ) , '*Supported methods:*' , 'If you send "@{}: help" to me I reply with these ' 'instructions.' . format ( self . user ) , 'If you send "@{}: version" to me I reply with my current ' 'version.' . format ( self . user ) , ] + [ filter . description ( ) for filter in filters ] )
13314	def remove ( self ) : self . run_hook ( 'preremove' ) utils . rmtree ( self . path ) self . run_hook ( 'postremove' )
13317	def remove ( name_or_path ) : r = resolve ( name_or_path ) r . resolved [ 0 ] . remove ( ) EnvironmentCache . discard ( r . resolved [ 0 ] ) EnvironmentCache . save ( )
2408	def extract_features_and_generate_model ( essays , algorithm = util_functions . AlgorithmTypes . regression ) : f = feature_extractor . FeatureExtractor ( ) f . initialize_dictionaries ( essays ) train_feats = f . gen_feats ( essays ) set_score = numpy . asarray ( essays . _score , dtype = numpy . int ) if len ( util_functions . f7 ( list ( set_score ) ) ) > 5 : algorithm = util_functions . AlgorithmTypes . regression else : algorithm = util_functions . AlgorithmTypes . classification clf , clf2 = get_algorithms ( algorithm ) cv_error_results = get_cv_error ( clf2 , train_feats , essays . _score ) try : clf . fit ( train_feats , set_score ) except ValueError : log . exception ( "Not enough classes (0,1,etc) in sample." ) set_score [ 0 ] = 1 set_score [ 1 ] = 0 clf . fit ( train_feats , set_score ) return f , clf , cv_error_results
8495	def _get_module_filename ( module ) : module = module . split ( '.' ) package = '.' . join ( module [ : - 1 ] ) module = module [ - 1 ] try : if not package : module = __import__ ( module ) else : package = __import__ ( package , fromlist = [ module ] ) module = getattr ( package , module , None ) filename = getattr ( module , '__file__' , None ) if not filename : return Unparseable ( ) if filename . endswith ( '.pyc' ) : filename = filename [ : - 1 ] if not os . path . exists ( filename ) and os . path . isfile ( filename ) : return Unparseable ( ) if filename . endswith ( '__init__.py' ) : filename = filename [ : - 11 ] return filename except ImportError : return
375	def adjust_hue ( im , hout = 0.66 , is_offset = True , is_clip = True , is_random = False ) : hsv = rgb_to_hsv ( im ) if is_random : hout = np . random . uniform ( - hout , hout ) if is_offset : hsv [ ... , 0 ] += hout else : hsv [ ... , 0 ] = hout if is_clip : hsv [ ... , 0 ] = np . clip ( hsv [ ... , 0 ] , 0 , np . inf ) rgb = hsv_to_rgb ( hsv ) return rgb
456	def ternary_operation ( x ) : g = tf . get_default_graph ( ) with g . gradient_override_map ( { "Sign" : "Identity" } ) : threshold = _compute_threshold ( x ) x = tf . sign ( tf . add ( tf . sign ( tf . add ( x , threshold ) ) , tf . sign ( tf . add ( x , - threshold ) ) ) ) return x
10089	def files ( self ) : files_ = super ( Deposit , self ) . files if files_ : sort_by_ = files_ . sort_by def sort_by ( * args , ** kwargs ) : if 'draft' != self . status : raise PIDInvalidAction ( ) return sort_by_ ( * args , ** kwargs ) files_ . sort_by = sort_by return files_
11086	def whoami ( self , msg , args ) : output = [ "Hello %s" % msg . user ] if hasattr ( self . _bot . dispatcher , 'auth_manager' ) and msg . user . is_admin is True : output . append ( "You are a *bot admin*." ) output . append ( "Bot version: %s-%s" % ( self . _bot . version , self . _bot . commit ) ) return '\n' . join ( output )
13109	def r_annotations ( self ) : target = request . args . get ( "target" , None ) wildcard = request . args . get ( "wildcard" , "." , type = str ) include = request . args . get ( "include" ) exclude = request . args . get ( "exclude" ) limit = request . args . get ( "limit" , None , type = int ) start = request . args . get ( "start" , 1 , type = int ) expand = request . args . get ( "expand" , False , type = bool ) if target : try : urn = MyCapytain . common . reference . URN ( target ) except ValueError : return "invalid urn" , 400 count , annotations = self . __queryinterface__ . getAnnotations ( urn , wildcard = wildcard , include = include , exclude = exclude , limit = limit , start = start , expand = expand ) else : count , annotations = self . __queryinterface__ . getAnnotations ( None , limit = limit , start = start , expand = expand ) mapped = [ ] response = { "@context" : type ( self ) . JSONLD_CONTEXT , "id" : url_for ( ".r_annotations" , start = start , limit = limit ) , "type" : "AnnotationCollection" , "startIndex" : start , "items" : [ ] , "total" : count } for a in annotations : mapped . append ( { "id" : url_for ( ".r_annotation" , sha = a . sha ) , "body" : url_for ( ".r_annotation_body" , sha = a . sha ) , "type" : "Annotation" , "target" : a . target . to_json ( ) , "dc:type" : a . type_uri , "owl:sameAs" : [ a . uri ] , "nemo:slug" : a . slug } ) response [ "items" ] = mapped response = jsonify ( response ) return response
12467	def run_hook ( hook , config , quiet = False ) : if not hook : return True if not quiet : print_message ( '== Step 3. Run post-bootstrap hook ==' ) result = not run_cmd ( prepare_args ( hook , config ) , echo = not quiet , fail_silently = True , shell = True ) if not quiet : print_message ( ) return result
8125	def draw_cornu_bezier ( x0 , y0 , t0 , t1 , s0 , c0 , flip , cs , ss , cmd , scale , rot ) : s = None for j in range ( 0 , 5 ) : t = j * .2 t2 = t + .2 curvetime = t0 + t * ( t1 - t0 ) curvetime2 = t0 + t2 * ( t1 - t0 ) Dt = ( curvetime2 - curvetime ) * scale if not s : s , c = eval_cornu ( curvetime ) s *= flip s -= s0 c -= c0 dx1 = cos ( pow ( curvetime , 2 ) + ( flip * rot ) ) dy1 = flip * sin ( pow ( curvetime , 2 ) + ( flip * rot ) ) x = ( ( c * cs - s * ss ) + x0 ) y = ( ( s * cs + c * ss ) + y0 ) s2 , c2 = eval_cornu ( curvetime2 ) s2 *= flip s2 -= s0 c2 -= c0 dx2 = cos ( pow ( curvetime2 , 2 ) + ( flip * rot ) ) dy2 = flip * sin ( pow ( curvetime2 , 2 ) + ( flip * rot ) ) x3 = ( ( c2 * cs - s2 * ss ) + x0 ) y3 = ( ( s2 * cs + c2 * ss ) + y0 ) x1 = ( x + ( ( Dt / 3.0 ) * dx1 ) ) y1 = ( y + ( ( Dt / 3.0 ) * dy1 ) ) x2 = ( x3 - ( ( Dt / 3.0 ) * dx2 ) ) y2 = ( y3 - ( ( Dt / 3.0 ) * dy2 ) ) if cmd == 'moveto' : print_pt ( x , y , cmd ) cmd = 'curveto' print_crv ( x1 , y1 , x2 , y2 , x3 , y3 ) dx1 , dy1 = dx2 , dy2 x , y = x3 , y3 return cmd
9962	def get_interfaces ( impls ) : if impls is None : return None elif isinstance ( impls , OrderMixin ) : result = OrderedDict ( ) for name in impls . order : result [ name ] = impls [ name ] . interface return result elif isinstance ( impls , Mapping ) : return { name : impls [ name ] . interface for name in impls } elif isinstance ( impls , Sequence ) : return [ impl . interface for impl in impls ] else : return impls . interface
11353	def make_user_agent ( component = None ) : packageinfo = pkg_resources . require ( "harvestingkit" ) [ 0 ] useragent = "{0}/{1}" . format ( packageinfo . project_name , packageinfo . version ) if component is not None : useragent += " {0}" . format ( component ) return useragent
10158	def get_viewset_transition_action_mixin ( model , ** kwargs ) : instance = model ( ) class Mixin ( object ) : save_after_transition = True transitions = instance . get_all_status_transitions ( ) transition_names = set ( x . name for x in transitions ) for transition_name in transition_names : setattr ( Mixin , transition_name , get_transition_viewset_method ( transition_name , ** kwargs ) ) return Mixin
4465	def save ( filename_audio , filename_jam , jam , strict = True , fmt = 'auto' , ** kwargs ) : y = jam . sandbox . muda . _audio [ 'y' ] sr = jam . sandbox . muda . _audio [ 'sr' ] psf . write ( filename_audio , y , sr , ** kwargs ) jam . save ( filename_jam , strict = strict , fmt = fmt )
9323	def refresh_information ( self , accept = MEDIA_TYPE_TAXII_V20 ) : response = self . __raw = self . _conn . get ( self . url , headers = { "Accept" : accept } ) self . _populate_fields ( ** response ) self . _loaded_information = True
10137	def _assert_version ( self , version ) : if self . nearest_version < version : if self . _version_given : raise ValueError ( 'Data type requires version %s' % version ) else : self . _version = version
13777	def AddEnumDescriptor ( self , enum_desc ) : if not isinstance ( enum_desc , descriptor . EnumDescriptor ) : raise TypeError ( 'Expected instance of descriptor.EnumDescriptor.' ) self . _enum_descriptors [ enum_desc . full_name ] = enum_desc self . AddFileDescriptor ( enum_desc . file )
11381	def do_autodiscover ( parser , token ) : args = token . split_contents ( ) if len ( args ) != 2 : raise template . TemplateSyntaxError ( '%s takes an object as its parameter.' % args [ 0 ] ) else : obj = args [ 1 ] return OEmbedAutodiscoverNode ( obj )
1853	def SHR ( cpu , dest , src ) : OperandSize = dest . size count = Operators . ZEXTEND ( src . read ( ) & ( OperandSize - 1 ) , OperandSize ) value = dest . read ( ) res = dest . write ( value >> count ) MASK = ( 1 << OperandSize ) - 1 SIGN_MASK = 1 << ( OperandSize - 1 ) if issymbolic ( count ) : cpu . CF = Operators . ITE ( count != 0 , ( ( value >> Operators . ZEXTEND ( count - 1 , OperandSize ) ) & 1 ) != 0 , cpu . CF ) else : if count != 0 : cpu . CF = Operators . EXTRACT ( value , count - 1 , 1 ) != 0 cpu . ZF = Operators . ITE ( count != 0 , res == 0 , cpu . ZF ) cpu . SF = Operators . ITE ( count != 0 , ( res & SIGN_MASK ) != 0 , cpu . SF ) cpu . OF = Operators . ITE ( count != 0 , ( ( value >> ( OperandSize - 1 ) ) & 0x1 ) == 1 , cpu . OF ) cpu . PF = Operators . ITE ( count != 0 , cpu . _calculate_parity_flag ( res ) , cpu . PF )
8595	def create_group ( self , group ) : data = json . dumps ( self . _create_group_dict ( group ) ) response = self . _perform_request ( url = '/um/groups' , method = 'POST' , data = data ) return response
1827	def CALL ( cpu , op0 ) : proc = op0 . read ( ) cpu . push ( cpu . PC , cpu . address_bit_size ) cpu . PC = proc
12189	async def from_api_token ( cls , token = None , api_cls = SlackBotApi ) : api = api_cls . from_env ( ) if token is None else api_cls ( api_token = token ) data = await api . execute_method ( cls . API_AUTH_ENDPOINT ) return cls ( data [ 'user_id' ] , data [ 'user' ] , api )
1166	def join ( self , timeout = None ) : if not self . __initialized : raise RuntimeError ( "Thread.__init__() not called" ) if not self . __started . is_set ( ) : raise RuntimeError ( "cannot join thread before it is started" ) if self is current_thread ( ) : raise RuntimeError ( "cannot join current thread" ) if __debug__ : if not self . __stopped : self . _note ( "%s.join(): waiting until thread stops" , self ) self . __block . acquire ( ) try : if timeout is None : while not self . __stopped : self . __block . wait ( ) if __debug__ : self . _note ( "%s.join(): thread stopped" , self ) else : deadline = _time ( ) + timeout while not self . __stopped : delay = deadline - _time ( ) if delay <= 0 : if __debug__ : self . _note ( "%s.join(): timed out" , self ) break self . __block . wait ( delay ) else : if __debug__ : self . _note ( "%s.join(): thread stopped" , self ) finally : self . __block . release ( )
2549	def include ( f ) : fl = open ( f , 'r' ) data = fl . read ( ) fl . close ( ) return raw ( data )
13718	def _camelcase_to_underscore ( url ) : def upper2underscore ( text ) : for char in text : if char . islower ( ) : yield char else : yield '_' if char . isalpha ( ) : yield char . lower ( ) return '' . join ( upper2underscore ( url ) )
10216	def to_jupyter ( graph : BELGraph , chart : Optional [ str ] = None ) -> Javascript : with open ( os . path . join ( HERE , 'render_with_javascript.js' ) , 'rt' ) as f : js_template = Template ( f . read ( ) ) return Javascript ( js_template . render ( ** _get_context ( graph , chart = chart ) ) )
11706	def reproduce_sexually ( self , egg_donor , sperm_donor ) : egg_word = random . choice ( egg_donor . genome ) egg = self . generate_gamete ( egg_word ) sperm_word = random . choice ( sperm_donor . genome ) sperm = self . generate_gamete ( sperm_word ) self . genome = list ( set ( egg + sperm ) ) self . parents = [ egg_donor . name , sperm_donor . name ] self . generation = max ( egg_donor . generation , sperm_donor . generation ) + 1 sum_ = egg_donor . divinity + sperm_donor . divinity self . divinity = int ( npchoice ( divinities , 1 , p = p_divinity [ sum_ ] ) [ 0 ] )
12494	def check_X_y ( X , y , accept_sparse = None , dtype = None , order = None , copy = False , force_all_finite = True , ensure_2d = True , allow_nd = False , multi_output = False ) : X = check_array ( X , accept_sparse , dtype , order , copy , force_all_finite , ensure_2d , allow_nd ) if multi_output : y = check_array ( y , 'csr' , force_all_finite = True , ensure_2d = False ) else : y = column_or_1d ( y , warn = True ) _assert_all_finite ( y ) check_consistent_length ( X , y ) return X , y
10186	def _aggregations_config ( self ) : result = { } for ep in iter_entry_points ( group = self . entry_point_group_aggs ) : for cfg in ep . load ( ) ( ) : if cfg [ 'aggregation_name' ] not in self . enabled_aggregations : continue elif cfg [ 'aggregation_name' ] in result : raise DuplicateAggregationError ( 'Duplicate aggregation {0} in entry point ' '{1}' . format ( cfg [ 'event_type' ] , ep . name ) ) cfg . update ( self . enabled_aggregations [ cfg [ 'aggregation_name' ] ] or { } ) result [ cfg [ 'aggregation_name' ] ] = cfg return result
7662	def slice ( self , start_time , end_time , strict = False ) : sliced_ann = self . trim ( start_time , end_time , strict = strict ) raw_data = sliced_ann . pop_data ( ) for obs in raw_data : new_time = max ( 0 , obs . time - start_time ) sliced_ann . append ( time = new_time , duration = obs . duration , value = obs . value , confidence = obs . confidence ) ref_time = sliced_ann . time slice_start = ref_time slice_end = ref_time + sliced_ann . duration if 'slice' not in sliced_ann . sandbox . keys ( ) : sliced_ann . sandbox . update ( slice = [ { 'start_time' : start_time , 'end_time' : end_time , 'slice_start' : slice_start , 'slice_end' : slice_end } ] ) else : sliced_ann . sandbox . slice . append ( { 'start_time' : start_time , 'end_time' : end_time , 'slice_start' : slice_start , 'slice_end' : slice_end } ) sliced_ann . time = max ( 0 , ref_time - start_time ) return sliced_ann
12120	def generate_colormap ( self , colormap = None , reverse = False ) : if colormap is None : colormap = pylab . cm . Dark2 self . cm = colormap self . colormap = [ ] for i in range ( self . sweeps ) : self . colormap . append ( colormap ( i / self . sweeps ) ) if reverse : self . colormap . reverse ( )
4787	def matches ( self , pattern ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a string' ) if not isinstance ( pattern , str_types ) : raise TypeError ( 'given pattern arg must be a string' ) if len ( pattern ) == 0 : raise ValueError ( 'given pattern arg must not be empty' ) if re . search ( pattern , self . val ) is None : self . _err ( 'Expected <%s> to match pattern <%s>, but did not.' % ( self . val , pattern ) ) return self
3628	def pad_cells ( table ) : col_sizes = [ max ( map ( len , col ) ) for col in zip ( * table ) ] for row in table : for cell_num , cell in enumerate ( row ) : row [ cell_num ] = pad_to ( cell , col_sizes [ cell_num ] ) return table
6974	def rfepd_magseries ( times , mags , errs , externalparam_arrs , magsarefluxes = False , epdsmooth = True , epdsmooth_sigclip = 3.0 , epdsmooth_windowsize = 21 , epdsmooth_func = smooth_magseries_savgol , epdsmooth_extraparams = None , rf_subsample = 1.0 , rf_ntrees = 300 , rf_extraparams = { 'criterion' : 'mse' , 'oob_score' : False , 'n_jobs' : - 1 } ) : finind = np . isfinite ( times ) & np . isfinite ( mags ) & np . isfinite ( errs ) ftimes , fmags , ferrs = times [ : : ] [ finind ] , mags [ : : ] [ finind ] , errs [ : : ] [ finind ] finalparam_arrs = [ ] for ep in externalparam_arrs : finalparam_arrs . append ( ep [ : : ] [ finind ] ) stimes , smags , serrs , eparams = sigclip_magseries_with_extparams ( times , mags , errs , externalparam_arrs , sigclip = epdsmooth_sigclip , magsarefluxes = magsarefluxes ) if epdsmooth : if isinstance ( epdsmooth_extraparams , dict ) : smoothedmags = epdsmooth_func ( smags , epdsmooth_windowsize , ** epdsmooth_extraparams ) else : smoothedmags = epdsmooth_func ( smags , epdsmooth_windowsize ) else : smoothedmags = smags if isinstance ( rf_extraparams , dict ) : RFR = RandomForestRegressor ( n_estimators = rf_ntrees , ** rf_extraparams ) else : RFR = RandomForestRegressor ( n_estimators = rf_ntrees ) features = np . column_stack ( eparams ) if rf_subsample < 1.0 : featureindices = np . arange ( smoothedmags . size ) training_indices = np . sort ( npr . choice ( featureindices , size = int ( rf_subsample * smoothedmags . size ) , replace = False ) ) else : training_indices = np . arange ( smoothedmags . size ) RFR . fit ( features [ training_indices , : ] , smoothedmags [ training_indices ] ) flux_corrections = RFR . predict ( np . column_stack ( finalparam_arrs ) ) corrected_fmags = npmedian ( fmags ) + fmags - flux_corrections retdict = { 'times' : ftimes , 'mags' : corrected_fmags , 'errs' : ferrs , 'feature_importances' : RFR . feature_importances_ , 'regressor' : RFR , 'mags_median' : npmedian ( corrected_fmags ) , 'mags_mad' : npmedian ( npabs ( corrected_fmags - npmedian ( corrected_fmags ) ) ) } return retdict
2979	def cmd_join ( opts ) : config = load_config ( opts . config ) b = get_blockade ( config , opts ) b . join ( )
8651	def get_jobs ( session , job_ids , seo_details , lang ) : get_jobs_data = { 'jobs[]' : job_ids , 'seo_details' : seo_details , 'lang' : lang , } response = make_get_request ( session , 'jobs' , params_data = get_jobs_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise JobsNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
10474	def _isSingleCharacter ( keychr ) : if not keychr : return False if len ( keychr ) == 1 : return True return keychr . count ( '<' ) == 1 and keychr . count ( '>' ) == 1 and keychr [ 0 ] == '<' and keychr [ - 1 ] == '>'
7719	def xpath_eval ( self , expr ) : ctxt = common_doc . xpathNewContext ( ) ctxt . setContextNode ( self . xmlnode ) ctxt . xpathRegisterNs ( "muc" , self . ns . getContent ( ) ) ret = ctxt . xpathEval ( to_utf8 ( expr ) ) ctxt . xpathFreeContext ( ) return ret
1524	def dereference_symlinks ( src ) : while os . path . islink ( src ) : src = os . path . join ( os . path . dirname ( src ) , os . readlink ( src ) ) return src
426	def check_unfinished_task ( self , task_name = None , ** kwargs ) : if not isinstance ( task_name , str ) : raise Exception ( "task_name should be string" ) self . _fill_project_info ( kwargs ) kwargs . update ( { '$or' : [ { 'status' : 'pending' } , { 'status' : 'running' } ] } ) task = self . db . Task . find ( kwargs ) task_id_list = task . distinct ( '_id' ) n_task = len ( task_id_list ) if n_task == 0 : logging . info ( "[Database] No unfinished task - task_name: {}" . format ( task_name ) ) return False else : logging . info ( "[Database] Find {} unfinished task - task_name: {}" . format ( n_task , task_name ) ) return True
9389	def check_sla ( self , sla , diff_metric ) : try : if sla . display is '%' : diff_val = float ( diff_metric [ 'percent_diff' ] ) else : diff_val = float ( diff_metric [ 'absolute_diff' ] ) except ValueError : return False if not ( sla . check_sla_passed ( diff_val ) ) : self . sla_failures += 1 self . sla_failure_list . append ( DiffSLAFailure ( sla , diff_metric ) ) return True
4679	def getAccountFromPrivateKey ( self , wif ) : pub = self . publickey_from_wif ( wif ) return self . getAccountFromPublicKey ( pub )
6033	def from_shape_pixel_scale_and_sub_grid_size ( cls , shape , pixel_scale , sub_grid_size = 2 ) : regular_grid = RegularGrid . from_shape_and_pixel_scale ( shape = shape , pixel_scale = pixel_scale ) sub_grid = SubGrid . from_shape_pixel_scale_and_sub_grid_size ( shape = shape , pixel_scale = pixel_scale , sub_grid_size = sub_grid_size ) blurring_grid = np . array ( [ [ 0.0 , 0.0 ] ] ) return GridStack ( regular_grid , sub_grid , blurring_grid )
4683	def getAccounts ( self ) : pubkeys = self . getPublicKeys ( ) accounts = [ ] for pubkey in pubkeys : if pubkey [ : len ( self . prefix ) ] == self . prefix : accounts . extend ( self . getAccountsFromPublicKey ( pubkey ) ) return accounts
10539	def update_category ( category ) : try : res = _pybossa_req ( 'put' , 'category' , category . id , payload = category . data ) if res . get ( 'id' ) : return Category ( res ) else : return res except : raise
9752	def build_swig ( ) : print ( "Looking for FANN libs..." ) find_fann ( ) print ( "running SWIG..." ) swig_bin = find_swig ( ) swig_cmd = [ swig_bin , '-c++' , '-python' , 'fann2/fann2.i' ] subprocess . Popen ( swig_cmd ) . wait ( )
13141	def build_index_and_mapping ( triples ) : ents = bidict ( ) rels = bidict ( ) ent_id = 0 rel_id = 0 collected = [ ] for t in triples : for e in ( t . head , t . tail ) : if e not in ents : ents [ e ] = ent_id ent_id += 1 if t . relation not in rels : rels [ t . relation ] = rel_id rel_id += 1 collected . append ( kgedata . TripleIndex ( ents [ t . head ] , rels [ t . relation ] , ents [ t . tail ] ) ) return collected , ents , rels
11491	def download ( server_path , local_path = '.' ) : session . token = verify_credentials ( ) is_item , resource_id = _find_resource_id_from_path ( server_path ) if resource_id == - 1 : print ( 'Unable to locate {0}' . format ( server_path ) ) else : if is_item : _download_item ( resource_id , local_path ) else : _download_folder_recursive ( resource_id , local_path )
125	def Positive ( other_param , mode = "invert" , reroll_count_max = 2 ) : return ForceSign ( other_param = other_param , positive = True , mode = mode , reroll_count_max = reroll_count_max )
10604	def create_entity ( self , name , gl_structure , description = None ) : new_entity = Entity ( name , gl_structure , description = description ) self . entities . append ( new_entity ) return new_entity
3615	def get_raw_record ( self , instance , update_fields = None ) : tmp = { 'objectID' : self . objectID ( instance ) } if update_fields : if isinstance ( update_fields , str ) : update_fields = ( update_fields , ) for elt in update_fields : key = self . __translate_fields . get ( elt , None ) if key : tmp [ key ] = self . __named_fields [ key ] ( instance ) else : for key , value in self . __named_fields . items ( ) : tmp [ key ] = value ( instance ) if self . geo_field : loc = self . geo_field ( instance ) if isinstance ( loc , tuple ) : tmp [ '_geoloc' ] = { 'lat' : loc [ 0 ] , 'lng' : loc [ 1 ] } elif isinstance ( loc , dict ) : self . _validate_geolocation ( loc ) tmp [ '_geoloc' ] = loc elif isinstance ( loc , list ) : [ self . _validate_geolocation ( geo ) for geo in loc ] tmp [ '_geoloc' ] = loc if self . tags : if callable ( self . tags ) : tmp [ '_tags' ] = self . tags ( instance ) if not isinstance ( tmp [ '_tags' ] , list ) : tmp [ '_tags' ] = list ( tmp [ '_tags' ] ) logger . debug ( 'BUILD %s FROM %s' , tmp [ 'objectID' ] , self . model ) return tmp
10711	def models_preparing ( app ) : def wrapper ( resource , parent ) : if isinstance ( resource , DeclarativeMeta ) : resource = ListResource ( resource ) if not getattr ( resource , '__parent__' , None ) : resource . __parent__ = parent return resource resources_preparing_factory ( app , wrapper )
10449	def getallstates ( self , window_name , object_name ) : object_handle = self . _get_object_handle ( window_name , object_name ) _obj_states = [ ] if object_handle . AXEnabled : _obj_states . append ( "enabled" ) if object_handle . AXFocused : _obj_states . append ( "focused" ) else : try : if object_handle . AXFocused : _obj_states . append ( "focusable" ) except : pass if re . match ( "AXCheckBox" , object_handle . AXRole , re . M | re . U | re . L ) or re . match ( "AXRadioButton" , object_handle . AXRole , re . M | re . U | re . L ) : if object_handle . AXValue : _obj_states . append ( "checked" ) return _obj_states
925	def _aggr_mode ( inList ) : valueCounts = dict ( ) nonNone = 0 for elem in inList : if elem == SENTINEL_VALUE_FOR_MISSING_DATA : continue nonNone += 1 if elem in valueCounts : valueCounts [ elem ] += 1 else : valueCounts [ elem ] = 1 if nonNone == 0 : return None sortedCounts = valueCounts . items ( ) sortedCounts . sort ( cmp = lambda x , y : x [ 1 ] - y [ 1 ] , reverse = True ) return sortedCounts [ 0 ] [ 0 ]
10889	def kvectors ( self , norm = False , form = 'broadcast' , real = False , shift = False ) : if norm is False : norm = 1 if norm is True : norm = np . array ( self . shape ) norm = aN ( norm , self . dim , dtype = 'float' ) v = list ( np . fft . fftfreq ( self . shape [ i ] ) / norm [ i ] for i in range ( self . dim ) ) if shift : v = list ( np . fft . fftshift ( t ) for t in v ) if real : v [ - 1 ] = v [ - 1 ] [ : ( self . shape [ - 1 ] + 1 ) // 2 ] return self . _format_vector ( v , form = form )
10326	def canonical_averages ( ps , microcanonical_averages_arrays ) : num_sites = microcanonical_averages_arrays [ 'N' ] num_edges = microcanonical_averages_arrays [ 'M' ] spanning_cluster = ( 'spanning_cluster' in microcanonical_averages_arrays ) ret = dict ( ) ret [ 'ps' ] = ps ret [ 'N' ] = num_sites ret [ 'M' ] = num_edges ret [ 'max_cluster_size' ] = np . empty ( ps . size ) ret [ 'max_cluster_size_ci' ] = np . empty ( ( ps . size , 2 ) ) if spanning_cluster : ret [ 'spanning_cluster' ] = np . empty ( ps . size ) ret [ 'spanning_cluster_ci' ] = np . empty ( ( ps . size , 2 ) ) ret [ 'moments' ] = np . empty ( ( 5 , ps . size ) ) ret [ 'moments_ci' ] = np . empty ( ( 5 , ps . size , 2 ) ) for p_index , p in enumerate ( ps ) : binomials = _binomial_pmf ( n = num_edges , p = p ) for key , value in microcanonical_averages_arrays . items ( ) : if len ( key ) <= 1 : continue if key in [ 'max_cluster_size' , 'spanning_cluster' ] : ret [ key ] [ p_index ] = np . sum ( binomials * value ) elif key in [ 'max_cluster_size_ci' , 'spanning_cluster_ci' ] : ret [ key ] [ p_index ] = np . sum ( np . tile ( binomials , ( 2 , 1 ) ) . T * value , axis = 0 ) elif key == 'moments' : ret [ key ] [ : , p_index ] = np . sum ( np . tile ( binomials , ( 5 , 1 ) ) * value , axis = 1 ) elif key == 'moments_ci' : ret [ key ] [ : , p_index ] = np . sum ( np . rollaxis ( np . tile ( binomials , ( 5 , 2 , 1 ) ) , 2 , 1 ) * value , axis = 1 ) else : raise NotImplementedError ( '{}-dimensional array' . format ( value . ndim ) ) return ret
12656	def dictify ( a_named_tuple ) : return dict ( ( s , getattr ( a_named_tuple , s ) ) for s in a_named_tuple . _fields )
13176	def get_observations ( self ) : if self . empty : return [ ] rows = list ( self . tbody ) observations = [ ] for row_observation , row_details in zip ( rows [ : : 2 ] , rows [ 1 : : 2 ] ) : data = { } cells = OBSERVATION_XPATH ( row_observation ) data [ 'name' ] = _clean_cell ( cells [ 0 ] ) data [ 'date' ] = _clean_cell ( cells [ 1 ] ) data [ 'magnitude' ] = _clean_cell ( cells [ 3 ] ) data [ 'obscode' ] = _clean_cell ( cells [ 6 ] ) cells = DETAILS_XPATH ( row_details ) data [ 'comp1' ] = _clean_cell ( cells [ 0 ] ) data [ 'chart' ] = _clean_cell ( cells [ 3 ] ) . replace ( 'None' , '' ) data [ 'comment_code' ] = _clean_cell ( cells [ 4 ] ) data [ 'notes' ] = _clean_cell ( cells [ 5 ] ) observations . append ( data ) return observations
10598	def clear ( self ) : self . solid_density = 1.0 self . H2O_mass = 0.0 self . size_class_masses = self . size_class_masses * 0.0
1092	def sub ( pattern , repl , string , count = 0 , flags = 0 ) : return _compile ( pattern , flags ) . sub ( repl , string , count )
13836	def _MergeMessageField ( self , tokenizer , message , field ) : is_map_entry = _IsMapEntry ( field ) if tokenizer . TryConsume ( '<' ) : end_token = '>' else : tokenizer . Consume ( '{' ) end_token = '}' if field . label == descriptor . FieldDescriptor . LABEL_REPEATED : if field . is_extension : sub_message = message . Extensions [ field ] . add ( ) elif is_map_entry : sub_message = field . message_type . _concrete_class ( ) else : sub_message = getattr ( message , field . name ) . add ( ) else : if field . is_extension : sub_message = message . Extensions [ field ] else : sub_message = getattr ( message , field . name ) sub_message . SetInParent ( ) while not tokenizer . TryConsume ( end_token ) : if tokenizer . AtEnd ( ) : raise tokenizer . ParseErrorPreviousToken ( 'Expected "%s".' % ( end_token , ) ) self . _MergeField ( tokenizer , sub_message ) if is_map_entry : value_cpptype = field . message_type . fields_by_name [ 'value' ] . cpp_type if value_cpptype == descriptor . FieldDescriptor . CPPTYPE_MESSAGE : value = getattr ( message , field . name ) [ sub_message . key ] value . MergeFrom ( sub_message . value ) else : getattr ( message , field . name ) [ sub_message . key ] = sub_message . value
3172	def get ( self , store_id , customer_id , ** queryparams ) : self . store_id = store_id self . customer_id = customer_id return self . _mc_client . _get ( url = self . _build_path ( store_id , 'customers' , customer_id ) , ** queryparams )
160	def height ( self ) : if len ( self . coords ) <= 1 : return 0 return np . max ( self . yy ) - np . min ( self . yy )
547	def __flushPredictionCache ( self ) : if not self . __predictionCache : return if self . _predictionLogger is None : self . _createPredictionLogger ( ) startTime = time . time ( ) self . _predictionLogger . writeRecords ( self . __predictionCache , progressCB = self . __writeRecordsCallback ) self . _logger . info ( "Flushed prediction cache; numrows=%s; elapsed=%s sec." , len ( self . __predictionCache ) , time . time ( ) - startTime ) self . __predictionCache . clear ( )
10514	def windowuptime ( self , window_name ) : tmp_time = self . _remote_windowuptime ( window_name ) if tmp_time : tmp_time = tmp_time . split ( '-' ) start_time = tmp_time [ 0 ] . split ( ' ' ) end_time = tmp_time [ 1 ] . split ( ' ' ) _start_time = datetime . datetime ( int ( start_time [ 0 ] ) , int ( start_time [ 1 ] ) , int ( start_time [ 2 ] ) , int ( start_time [ 3 ] ) , int ( start_time [ 4 ] ) , int ( start_time [ 5 ] ) ) _end_time = datetime . datetime ( int ( end_time [ 0 ] ) , int ( end_time [ 1 ] ) , int ( end_time [ 2 ] ) , int ( end_time [ 3 ] ) , int ( end_time [ 4 ] ) , int ( end_time [ 5 ] ) ) return _start_time , _end_time return None
9742	def send_command ( self , command , callback = True , command_type = QRTPacketType . PacketCommand ) : if self . transport is not None : cmd_length = len ( command ) LOG . debug ( "S: %s" , command ) self . transport . write ( struct . pack ( RTCommand % cmd_length , RTheader . size + cmd_length + 1 , command_type . value , command . encode ( ) , b"\0" , ) ) future = self . loop . create_future ( ) if callback : self . request_queue . append ( future ) else : future . set_result ( None ) return future raise QRTCommandException ( "Not connected!" )
8011	def from_request ( cls , request , webhook_id = PAYPAL_WEBHOOK_ID ) : headers = fix_django_headers ( request . META ) assert headers try : body = request . body . decode ( request . encoding or "utf-8" ) except Exception : body = "(error decoding body)" ip = request . META [ "REMOTE_ADDR" ] obj = cls . objects . create ( headers = headers , body = body , remote_ip = ip ) try : obj . valid = obj . verify ( PAYPAL_WEBHOOK_ID ) if obj . valid : obj . process ( save = False ) except Exception as e : max_length = WebhookEventTrigger . _meta . get_field ( "exception" ) . max_length obj . exception = str ( e ) [ : max_length ] obj . traceback = format_exc ( ) finally : obj . save ( ) return obj
15	def q_retrace ( R , D , q_i , v , rho_i , nenvs , nsteps , gamma ) : rho_bar = batch_to_seq ( tf . minimum ( 1.0 , rho_i ) , nenvs , nsteps , True ) rs = batch_to_seq ( R , nenvs , nsteps , True ) ds = batch_to_seq ( D , nenvs , nsteps , True ) q_is = batch_to_seq ( q_i , nenvs , nsteps , True ) vs = batch_to_seq ( v , nenvs , nsteps + 1 , True ) v_final = vs [ - 1 ] qret = v_final qrets = [ ] for i in range ( nsteps - 1 , - 1 , - 1 ) : check_shape ( [ qret , ds [ i ] , rs [ i ] , rho_bar [ i ] , q_is [ i ] , vs [ i ] ] , [ [ nenvs ] ] * 6 ) qret = rs [ i ] + gamma * qret * ( 1.0 - ds [ i ] ) qrets . append ( qret ) qret = ( rho_bar [ i ] * ( qret - q_is [ i ] ) ) + vs [ i ] qrets = qrets [ : : - 1 ] qret = seq_to_batch ( qrets , flat = True ) return qret
442	def get_all_params ( self , session = None ) : _params = [ ] for p in self . all_params : if session is None : _params . append ( p . eval ( ) ) else : _params . append ( session . run ( p ) ) return _params
6485	def do_search ( request , course_id = None ) : SearchInitializer . set_search_enviroment ( request = request , course_id = course_id ) results = { "error" : _ ( "Nothing to search" ) } status_code = 500 search_term = request . POST . get ( "search_string" , None ) try : if not search_term : raise ValueError ( _ ( 'No search term provided for search' ) ) size , from_ , page = _process_pagination_values ( request ) track . emit ( 'edx.course.search.initiated' , { "search_term" : search_term , "page_size" : size , "page_number" : page , } ) results = perform_search ( search_term , user = request . user , size = size , from_ = from_ , course_id = course_id ) status_code = 200 track . emit ( 'edx.course.search.results_displayed' , { "search_term" : search_term , "page_size" : size , "page_number" : page , "results_count" : results [ "total" ] , } ) except ValueError as invalid_err : results = { "error" : six . text_type ( invalid_err ) } log . debug ( six . text_type ( invalid_err ) ) except QueryParseError : results = { "error" : _ ( 'Your query seems malformed. Check for unmatched quotes.' ) } except Exception as err : results = { "error" : _ ( 'An error occurred when searching for "{search_string}"' ) . format ( search_string = search_term ) } log . exception ( 'Search view exception when searching for %s for user %s: %r' , search_term , request . user . id , err ) return JsonResponse ( results , status = status_code )
8856	def on_current_tab_changed ( self ) : self . menuEdit . clear ( ) self . menuModes . clear ( ) self . menuPanels . clear ( ) editor = self . tabWidget . current_widget ( ) self . menuEdit . setEnabled ( editor is not None ) self . menuModes . setEnabled ( editor is not None ) self . menuPanels . setEnabled ( editor is not None ) self . actionSave . setEnabled ( editor is not None ) self . actionSave_as . setEnabled ( editor is not None ) self . actionConfigure_run . setEnabled ( editor is not None ) self . actionRun . setEnabled ( editor is not None ) if editor is not None : self . setup_mnu_edit ( editor ) self . setup_mnu_modes ( editor ) self . setup_mnu_panels ( editor ) self . widgetOutline . set_editor ( editor ) self . _update_status_bar ( editor )
8440	def setup ( template , version = None ) : temple . check . is_git_ssh_path ( template ) temple . check . not_in_git_repo ( ) repo_path = temple . utils . get_repo_path ( template ) msg = ( 'You will be prompted for the parameters of your new project.' ' Please read the docs at https://github.com/{} before entering parameters.' ) . format ( repo_path ) print ( msg ) cc_repo_dir , config = temple . utils . get_cookiecutter_config ( template , version = version ) if not version : with temple . utils . cd ( cc_repo_dir ) : ret = temple . utils . shell ( 'git rev-parse HEAD' , stdout = subprocess . PIPE ) version = ret . stdout . decode ( 'utf-8' ) . strip ( ) _generate_files ( repo_dir = cc_repo_dir , config = config , template = template , version = version )
10809	def update ( self , name = None , description = None , privacy_policy = None , subscription_policy = None , is_managed = None ) : with db . session . begin_nested ( ) : if name is not None : self . name = name if description is not None : self . description = description if ( privacy_policy is not None and PrivacyPolicy . validate ( privacy_policy ) ) : self . privacy_policy = privacy_policy if ( subscription_policy is not None and SubscriptionPolicy . validate ( subscription_policy ) ) : self . subscription_policy = subscription_policy if is_managed is not None : self . is_managed = is_managed db . session . merge ( self ) return self
10846	def reorder ( self , updates_ids , offset = None , utc = None ) : url = PATHS [ 'REORDER' ] % self . profile_id order_format = "order[]=%s&" post_data = '' if offset : post_data += 'offset=%s&' % offset if utc : post_data += 'utc=%s&' % utc for update in updates_ids : post_data += order_format % update return self . api . post ( url = url , data = post_data )
8099	def copy ( self , graph ) : s = styles ( graph ) s . guide = self . guide . copy ( graph ) dict . __init__ ( s , [ ( v . name , v . copy ( ) ) for v in self . values ( ) ] ) return s
11859	def sum_out ( var , factors , bn ) : "Eliminate var from all factors by summing over its values." result , var_factors = [ ] , [ ] for f in factors : ( var_factors if var in f . vars else result ) . append ( f ) result . append ( pointwise_product ( var_factors , bn ) . sum_out ( var , bn ) ) return result
8115	def distance ( x0 , y0 , x1 , y1 ) : return sqrt ( pow ( x1 - x0 , 2 ) + pow ( y1 - y0 , 2 ) )
13843	def close ( self ) : try : self . conn . close ( ) self . logger . debug ( "Close connect succeed." ) except pymssql . Error as e : self . unknown ( "Close connect error: %s" % e )
3024	def _in_gce_environment ( ) : if SETTINGS . env_name is not None : return SETTINGS . env_name == 'GCE_PRODUCTION' if NO_GCE_CHECK != 'True' and _detect_gce_environment ( ) : SETTINGS . env_name = 'GCE_PRODUCTION' return True return False
11880	def scanAllProcessesForCwd ( searchPortion , isExactMatch = False ) : pids = getAllRunningPids ( ) cwdResults = [ scanProcessForCwd ( pid , searchPortion , isExactMatch ) for pid in pids ] ret = { } for i in range ( len ( pids ) ) : if cwdResults [ i ] is not None : ret [ pids [ i ] ] = cwdResults [ i ] return ret
1809	def SETE ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . ZF , 1 , 0 ) )
8644	def update_track ( session , track_id , latitude , longitude , stop_tracking = False ) : tracking_data = { 'track_point' : { 'latitude' : latitude , 'longitude' : longitude , } , 'stop_tracking' : stop_tracking } response = make_put_request ( session , 'tracks/{}' . format ( track_id ) , json_data = tracking_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise TrackNotUpdatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
12564	def _partition_data ( datavol , roivol , roivalue , maskvol = None , zeroe = True ) : if maskvol is not None : indices = ( roivol == roivalue ) * ( maskvol > 0 ) else : indices = roivol == roivalue if datavol . ndim == 4 : ts = datavol [ indices , : ] else : ts = datavol [ indices ] if zeroe : if datavol . ndim == 4 : ts = ts [ ts . sum ( axis = 1 ) != 0 , : ] return ts
6102	def intensities_from_grid_radii ( self , grid_radii ) : return np . multiply ( np . multiply ( self . intensity_prime , np . power ( np . add ( 1 , np . power ( np . divide ( self . radius_break , grid_radii ) , self . alpha ) ) , ( self . gamma / self . alpha ) ) ) , np . exp ( np . multiply ( - self . sersic_constant , ( np . power ( np . divide ( np . add ( np . power ( grid_radii , self . alpha ) , ( self . radius_break ** self . alpha ) ) , ( self . effective_radius ** self . alpha ) ) , ( 1.0 / ( self . alpha * self . sersic_index ) ) ) ) ) ) )
8665	def _prettify_list ( items ) : assert isinstance ( items , list ) keys_list = 'Available Keys:' for item in items : keys_list += '\n - {0}' . format ( item ) return keys_list
6827	def add_remote ( self , path , name , remote_url , use_sudo = False , user = None , fetch = True ) : if path is None : raise ValueError ( "Path to the working copy is needed to add a remote" ) if fetch : cmd = 'git remote add -f %s %s' % ( name , remote_url ) else : cmd = 'git remote add %s %s' % ( name , remote_url ) with cd ( path ) : if use_sudo and user is None : run_as_root ( cmd ) elif use_sudo : sudo ( cmd , user = user ) else : run ( cmd )
1751	def scan_mem ( self , data_to_find ) : if isinstance ( data_to_find , bytes ) : data_to_find = [ bytes ( [ c ] ) for c in data_to_find ] for mapping in sorted ( self . maps ) : for ptr in mapping : if ptr + len ( data_to_find ) >= mapping . end : break candidate = mapping [ ptr : ptr + len ( data_to_find ) ] if issymbolic ( candidate [ 0 ] ) : break if candidate == data_to_find : yield ptr
3466	def gene_name_reaction_rule ( self ) : names = { i . id : i . name for i in self . _genes } ast = parse_gpr ( self . _gene_reaction_rule ) [ 0 ] return ast2str ( ast , names = names )
1363	def get_argument_offset ( self ) : try : offset = self . get_argument ( constants . PARAM_OFFSET ) return offset except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
3213	def get_access_details ( self , key = None ) : if key in self . _CACHE_STATS : return self . _CACHE_STATS [ 'access_stats' ] [ key ] else : return self . _CACHE_STATS [ 'access_stats' ]
8594	def get_group ( self , group_id , depth = 1 ) : response = self . _perform_request ( '/um/groups/%s?depth=%s' % ( group_id , str ( depth ) ) ) return response
10162	def setup ( app ) : lexer = MarkdownLexer ( ) for alias in lexer . aliases : app . add_lexer ( alias , lexer ) return dict ( version = __version__ )
3008	def _credentials_from_request ( request ) : if ( oauth2_settings . storage_model is None or request . user . is_authenticated ( ) ) : return get_storage ( request ) . get ( ) else : return None
5371	def load_file ( file_path , credentials = None ) : if file_path . startswith ( 'gs://' ) : return _load_file_from_gcs ( file_path , credentials ) else : return open ( file_path , 'r' )
13459	def create_ical ( request , slug ) : event = get_object_or_404 ( Event , slug = slug ) start = event . start_date start = datetime . datetime ( start . year , start . month , start . day ) if event . end_date : end = event . end_date end = datetime . datetime ( end . year , end . month , end . day ) else : end = start cal = card_me . iCalendar ( ) cal . add ( 'method' ) . value = 'PUBLISH' vevent = cal . add ( 'vevent' ) vevent . add ( 'dtstart' ) . value = start vevent . add ( 'dtend' ) . value = end vevent . add ( 'dtstamp' ) . value = datetime . datetime . now ( ) vevent . add ( 'summary' ) . value = event . name response = HttpResponse ( cal . serialize ( ) , content_type = 'text/calendar' ) response [ 'Filename' ] = 'filename.ics' response [ 'Content-Disposition' ] = 'attachment; filename=filename.ics' return response
5242	def market_exact ( self , session , start_time : str , end_time : str ) -> Session : if session not in self . exch : return SessNA ss = self . exch [ session ] same_day = ss [ 0 ] < ss [ - 1 ] if not start_time : s_time = ss [ 0 ] else : s_time = param . to_hour ( start_time ) if same_day : s_time = max ( s_time , ss [ 0 ] ) if not end_time : e_time = ss [ - 1 ] else : e_time = param . to_hour ( end_time ) if same_day : e_time = min ( e_time , ss [ - 1 ] ) if same_day and ( s_time > e_time ) : return SessNA return Session ( start_time = s_time , end_time = e_time )
6168	def from_bin ( bin_array ) : width = len ( bin_array ) bin_wgts = 2 ** np . arange ( width - 1 , - 1 , - 1 ) return int ( np . dot ( bin_array , bin_wgts ) )
10108	def normalize_name ( s ) : s = s . replace ( '-' , '_' ) . replace ( '.' , '_' ) . replace ( ' ' , '_' ) if s in keyword . kwlist : return s + '_' s = '_' . join ( slug ( ss , lowercase = False ) for ss in s . split ( '_' ) ) if not s : s = '_' if s [ 0 ] not in string . ascii_letters + '_' : s = '_' + s return s
3454	def add_SBO ( model ) : for r in model . reactions : if r . annotation . get ( "sbo" ) : continue if len ( r . metabolites ) != 1 : continue met_id = list ( r . _metabolites ) [ 0 ] . id if r . id . startswith ( "EX_" ) and r . id == "EX_" + met_id : r . annotation [ "sbo" ] = "SBO:0000627" elif r . id . startswith ( "DM_" ) and r . id == "DM_" + met_id : r . annotation [ "sbo" ] = "SBO:0000628"
9148	def actions ( connection ) : session = _make_session ( connection = connection ) for action in Action . ls ( session = session ) : click . echo ( f'{action.created} {action.action} {action.resource}' )
2339	def weighted_mean_and_std ( values , weights ) : average = np . average ( values , weights = weights , axis = 0 ) variance = np . dot ( weights , ( values - average ) ** 2 ) / weights . sum ( ) return ( average , np . sqrt ( variance ) )
7160	def next_question ( self ) : for key , questions in self . questions . items ( ) : if key in self . answers : continue for question in questions : if self . check_condition ( question . _condition ) : return question return None
8314	def parse ( self , light = False ) : markup = self . markup self . disambiguation = self . parse_disambiguation ( markup ) self . categories = self . parse_categories ( markup ) self . links = self . parse_links ( markup ) if not light : markup = self . convert_pre ( markup ) markup = self . convert_li ( markup ) markup = self . convert_table ( markup ) markup = replace_entities ( markup ) markup = markup . replace ( "{{Cite" , "{{cite" ) markup = re . sub ( "\{\{ {1,2}cite" , "{{cite" , markup ) self . references , markup = self . parse_references ( markup ) markup = re . sub ( "\n+(\{\{legend)" , "\\1" , markup ) self . images , markup = self . parse_images ( markup ) self . images . extend ( self . parse_gallery_images ( markup ) ) self . paragraphs = self . parse_paragraphs ( markup ) self . tables = self . parse_tables ( markup ) self . translations = self . parse_translations ( markup ) self . important = self . parse_important ( markup )
4617	def formatTime ( t ) : if isinstance ( t , float ) : return datetime . utcfromtimestamp ( t ) . strftime ( timeFormat ) if isinstance ( t , datetime ) : return t . strftime ( timeFormat )
10127	def draw ( self ) : if self . enabled : self . _vertex_list . colors = self . _gl_colors self . _vertex_list . vertices = self . _gl_vertices self . _vertex_list . draw ( pyglet . gl . GL_TRIANGLES )
8702	def prepare ( self ) : log . info ( 'Preparing esp for transfer.' ) for func in LUA_FUNCTIONS : detected = self . __exchange ( 'print({0})' . format ( func ) ) if detected . find ( 'function:' ) == - 1 : break else : log . info ( 'Preparation already done. Not adding functions again.' ) return True functions = RECV_LUA + '\n' + SEND_LUA data = functions . format ( baud = self . _port . baudrate ) lines = data . replace ( '\r' , '' ) . split ( '\n' ) for line in lines : line = line . strip ( ) . replace ( ', ' , ',' ) . replace ( ' = ' , '=' ) if len ( line ) == 0 : continue resp = self . __exchange ( line ) if ( 'unexpected' in resp ) or ( 'stdin' in resp ) or len ( resp ) > len ( functions ) + 10 : log . error ( 'error when preparing "%s"' , resp ) return False return True
1907	def all_events ( cls ) : all_evts = set ( ) for cls , evts in cls . __all_events__ . items ( ) : all_evts . update ( evts ) return all_evts
7871	def set_payload ( self , payload ) : if isinstance ( payload , ElementClass ) : self . _payload = [ XMLPayload ( payload ) ] elif isinstance ( payload , StanzaPayload ) : self . _payload = [ payload ] else : raise TypeError ( "Bad payload type" ) self . _dirty = True
10988	def _translate_particles ( s , max_mem = 1e9 , desc = '' , min_rad = 'calc' , max_rad = 'calc' , invert = 'guess' , rz_order = 0 , do_polish = True ) : if desc is not None : desc_trans = desc + 'translate-particles' desc_burn = desc + 'addsub_burn' desc_polish = desc + 'addsub_polish' else : desc_trans , desc_burn , desc_polish = [ None ] * 3 RLOG . info ( 'Translate Particles:' ) opt . burn ( s , mode = 'do-particles' , n_loop = 4 , fractol = 0.1 , desc = desc_trans , max_mem = max_mem , include_rad = False , dowarn = False ) opt . burn ( s , mode = 'do-particles' , n_loop = 4 , fractol = 0.05 , desc = desc_trans , max_mem = max_mem , include_rad = True , dowarn = False ) RLOG . info ( 'Start add-subtract' ) addsub . add_subtract ( s , tries = 30 , min_rad = min_rad , max_rad = max_rad , invert = invert ) if desc is not None : states . save ( s , desc = desc + 'translate-addsub' ) if do_polish : RLOG . info ( 'Final Burn:' ) opt . burn ( s , mode = 'burn' , n_loop = 3 , fractol = 3e-4 , desc = desc_burn , max_mem = max_mem , rz_order = rz_order , dowarn = False ) RLOG . info ( 'Final Polish:' ) d = opt . burn ( s , mode = 'polish' , n_loop = 4 , fractol = 3e-4 , desc = desc_polish , max_mem = max_mem , rz_order = rz_order , dowarn = False ) if not d [ 'converged' ] : RLOG . warn ( 'Optimization did not converge; consider re-running' )
4999	def enterprise_customer_required ( view ) : @ wraps ( view ) def wrapper ( request , * args , ** kwargs ) : user = request . user enterprise_customer = get_enterprise_customer_for_user ( user ) if enterprise_customer : args = args + ( enterprise_customer , ) return view ( request , * args , ** kwargs ) else : raise PermissionDenied ( 'User {username} is not associated with an EnterpriseCustomer.' . format ( username = user . username ) ) return wrapper
13350	def add_file ( self , file , ** kwargs ) : if os . access ( file , os . F_OK ) : if file in self . f_repository : raise DuplicationError ( "file already added." ) self . f_repository . append ( file ) else : raise IOError ( "file not found." )
1652	def _GetTextInside ( text , start_pattern ) : r matching_punctuation = { '(' : ')' , '{' : '}' , '[' : ']' } closing_punctuation = set ( itervalues ( matching_punctuation ) ) match = re . search ( start_pattern , text , re . M ) if not match : return None start_position = match . end ( 0 ) assert start_position > 0 , ( 'start_pattern must ends with an opening punctuation.' ) assert text [ start_position - 1 ] in matching_punctuation , ( 'start_pattern must ends with an opening punctuation.' ) punctuation_stack = [ matching_punctuation [ text [ start_position - 1 ] ] ] position = start_position while punctuation_stack and position < len ( text ) : if text [ position ] == punctuation_stack [ - 1 ] : punctuation_stack . pop ( ) elif text [ position ] in closing_punctuation : return None elif text [ position ] in matching_punctuation : punctuation_stack . append ( matching_punctuation [ text [ position ] ] ) position += 1 if punctuation_stack : return None return text [ start_position : position - 1 ]
4833	def traverse_pagination ( response , endpoint , content_filter_query , query_params ) : results = response . get ( 'results' , [ ] ) page = 1 while response . get ( 'next' ) : page += 1 response = endpoint ( ) . post ( content_filter_query , ** dict ( query_params , page = page ) ) results += response . get ( 'results' , [ ] ) return results
9442	def reload_cache_config ( self , call_params ) : path = '/' + self . api_version + '/ReloadCacheConfig/' method = 'POST' return self . request ( path , method , call_params )
12268	def evaluate ( self , repo , spec , args ) : status = [ ] if len ( spec [ 'files' ] ) == 0 : return status with cd ( repo . rootdir ) : rules = None if 'rules-files' in spec and len ( spec [ 'rules-files' ] ) > 0 : rulesfiles = spec [ 'rules-files' ] rules = { } for f in rulesfiles : d = json . loads ( open ( f ) . read ( ) ) rules . update ( d ) elif 'rules' in spec : rules = { 'inline' : spec [ 'rules' ] } if rules is None or len ( rules ) == 0 : print ( "Regression quality validation has been enabled but no rules file has been specified" ) print ( "Example: { 'min-r2': 0.25 }. Put this either in file or in dgit.json" ) raise InvalidParameters ( "Regression quality checking rules missing" ) files = dict ( [ ( f , open ( f ) . read ( ) ) for f in spec [ 'files' ] ] ) for r in rules : if 'min-r2' not in rules [ r ] : continue minr2 = float ( rules [ r ] [ 'min-r2' ] ) for f in files : match = re . search ( r"R-squared:\s+(\d.\d+)" , files [ f ] ) if match is None : status . append ( { 'target' : f , 'validator' : self . name , 'description' : self . description , 'rules' : r , 'status' : "ERROR" , 'message' : "Invalid model output" } ) else : r2 = match . group ( 1 ) r2 = float ( r2 ) if r2 > minr2 : status . append ( { 'target' : f , 'validator' : self . name , 'description' : self . description , 'rules' : r , 'status' : "OK" , 'message' : "Acceptable R2" } ) else : status . append ( { 'target' : f , 'validator' : self . name , 'description' : self . description , 'rules' : r , 'status' : "ERROR" , 'message' : "R2 is too low" } ) return status
13306	def correlation ( a , b ) : diff1 = a - a . mean ( ) diff2 = b - b . mean ( ) return ( diff1 * diff2 ) . mean ( ) / ( np . sqrt ( np . square ( diff1 ) . mean ( ) * np . square ( diff2 ) . mean ( ) ) )
6512	def _set ( self , name , gender , country_values ) : if '+' in name : for replacement in [ '' , ' ' , '-' ] : self . _set ( name . replace ( '+' , replacement ) , gender , country_values ) else : if name not in self . names : self . names [ name ] = { } self . names [ name ] [ gender ] = country_values
875	def copyVarStatesFrom ( self , particleState , varNames ) : allowedToMove = True for varName in particleState [ 'varStates' ] : if varName in varNames : if varName not in self . permuteVars : continue state = copy . deepcopy ( particleState [ 'varStates' ] [ varName ] ) state [ '_position' ] = state [ 'position' ] state [ 'bestPosition' ] = state [ 'position' ] if not allowedToMove : state [ 'velocity' ] = 0 self . permuteVars [ varName ] . setState ( state ) if allowedToMove : self . permuteVars [ varName ] . resetVelocity ( self . _rng )
5654	def makedirs ( path ) : if not os . path . isdir ( path ) : os . makedirs ( path ) return path
10605	def remove_entity ( self , name ) : entity_to_remove = None for e in self . entities : if e . name == name : entity_to_remove = e if entity_to_remove is not None : self . entities . remove ( entity_to_remove )
2723	def take_snapshot ( self , snapshot_name , return_dict = True , power_off = False ) : if power_off is True and self . status != "off" : action = self . power_off ( return_dict = False ) action . wait ( ) self . load ( ) return self . _perform_action ( { "type" : "snapshot" , "name" : snapshot_name } , return_dict )
8590	def reboot_server ( self , datacenter_id , server_id ) : response = self . _perform_request ( url = '/datacenters/%s/servers/%s/reboot' % ( datacenter_id , server_id ) , method = 'POST-ACTION' ) return response
7748	def _process_iq_response ( self , stanza ) : stanza_id = stanza . stanza_id from_jid = stanza . from_jid if from_jid : ufrom = from_jid . as_unicode ( ) else : ufrom = None res_handler = err_handler = None try : res_handler , err_handler = self . _iq_response_handlers . pop ( ( stanza_id , ufrom ) ) except KeyError : logger . debug ( "No response handler for id={0!r} from={1!r}" . format ( stanza_id , ufrom ) ) logger . debug ( " from_jid: {0!r} peer: {1!r} me: {2!r}" . format ( from_jid , self . peer , self . me ) ) if ( ( from_jid == self . peer or from_jid == self . me or self . me and from_jid == self . me . bare ( ) ) ) : try : logger . debug ( " trying id={0!r} from=None" . format ( stanza_id ) ) res_handler , err_handler = self . _iq_response_handlers . pop ( ( stanza_id , None ) ) except KeyError : pass if stanza . stanza_type == "result" : if res_handler : response = res_handler ( stanza ) else : return False else : if err_handler : response = err_handler ( stanza ) else : return False self . _process_handler_result ( response ) return True
6083	def deflections_of_galaxies_from_sub_grid ( sub_grid , galaxies ) : if galaxies : return sum ( map ( lambda galaxy : galaxy . deflections_from_grid ( sub_grid ) , galaxies ) ) else : return np . full ( ( sub_grid . shape [ 0 ] , 2 ) , 0.0 )
9858	def create_url ( self , path , params = { } , opts = { } ) : if opts : warnings . warn ( '`opts` has been deprecated. Use `params` instead.' , DeprecationWarning , stacklevel = 2 ) params = params or opts if self . _shard_strategy == SHARD_STRATEGY_CRC : crc = zlib . crc32 ( path . encode ( 'utf-8' ) ) & 0xffffffff index = crc % len ( self . _domains ) domain = self . _domains [ index ] elif self . _shard_strategy == SHARD_STRATEGY_CYCLE : domain = self . _domains [ self . _shard_next_index ] self . _shard_next_index = ( self . _shard_next_index + 1 ) % len ( self . _domains ) else : domain = self . _domains [ 0 ] scheme = "https" if self . _use_https else "http" url_obj = UrlHelper ( domain , path , scheme , sign_key = self . _sign_key , include_library_param = self . _include_library_param , params = params ) return str ( url_obj )
10934	def check_terminate ( self ) : if not self . _has_run : return False else : terminate = self . check_completion ( ) terminate |= ( self . _num_iter >= self . max_iter ) return terminate
7864	def main ( ) : parser = argparse . ArgumentParser ( description = 'XMPP version checker' , parents = [ XMPPSettings . get_arg_parser ( ) ] ) parser . add_argument ( 'source' , metavar = 'SOURCE' , help = 'Source JID' ) parser . add_argument ( 'target' , metavar = 'TARGET' , nargs = '?' , help = 'Target JID (default: domain of SOURCE)' ) parser . add_argument ( '--debug' , action = 'store_const' , dest = 'log_level' , const = logging . DEBUG , default = logging . INFO , help = 'Print debug messages' ) parser . add_argument ( '--quiet' , const = logging . ERROR , action = 'store_const' , dest = 'log_level' , help = 'Print only error messages' ) args = parser . parse_args ( ) settings = XMPPSettings ( ) settings . load_arguments ( args ) if settings . get ( "password" ) is None : password = getpass ( "{0!r} password: " . format ( args . source ) ) if sys . version_info . major < 3 : password = password . decode ( "utf-8" ) settings [ "password" ] = password if sys . version_info . major < 3 : args . source = args . source . decode ( "utf-8" ) source = JID ( args . source ) if args . target : if sys . version_info . major < 3 : args . target = args . target . decode ( "utf-8" ) target = JID ( args . target ) else : target = JID ( source . domain ) logging . basicConfig ( level = args . log_level ) checker = VersionChecker ( source , target , settings ) try : checker . run ( ) except KeyboardInterrupt : checker . disconnect ( )
10491	def dragMouseButtonLeft ( self , coord , dest_coord , interval = 0.5 ) : modFlags = 0 self . _queueMouseButton ( coord , Quartz . kCGMouseButtonLeft , modFlags , dest_coord = dest_coord ) self . _postQueuedEvents ( interval = interval )
13726	def set_connection ( host = None , database = None , user = None , password = None ) : c . CONNECTION [ 'HOST' ] = host c . CONNECTION [ 'DATABASE' ] = database c . CONNECTION [ 'USER' ] = user c . CONNECTION [ 'PASSWORD' ] = password
11803	def nconflicts ( self , var , val , assignment ) : n = len ( self . vars ) c = self . rows [ val ] + self . downs [ var + val ] + self . ups [ var - val + n - 1 ] if assignment . get ( var , None ) == val : c -= 3 return c
8148	def _load_namespace ( self , namespace , filename = None ) : from shoebot import data for name in dir ( data ) : namespace [ name ] = getattr ( data , name ) for name in dir ( self ) : if name [ 0 ] != '_' : namespace [ name ] = getattr ( self , name ) namespace [ '_ctx' ] = self namespace [ '__file__' ] = filename
1969	def awake ( self , procid ) : logger . debug ( f"Remove procid:{procid} from waitlists and reestablish it in the running list" ) for wait_list in self . rwait : if procid in wait_list : wait_list . remove ( procid ) for wait_list in self . twait : if procid in wait_list : wait_list . remove ( procid ) self . timers [ procid ] = None self . running . append ( procid ) if self . _current is None : self . _current = procid
11046	def init_logging ( log_level ) : log_level_filter = LogLevelFilterPredicate ( LogLevel . levelWithName ( log_level ) ) log_level_filter . setLogLevelForNamespace ( 'twisted.web.client._HTTP11ClientFactory' , LogLevel . warn ) log_observer = FilteringLogObserver ( textFileLogObserver ( sys . stdout ) , [ log_level_filter ] ) globalLogPublisher . addObserver ( log_observer )
4113	def rc2is ( k ) : assert numpy . isrealobj ( k ) , 'Inverse sine parameters not defined for complex reflection coefficients.' if max ( numpy . abs ( k ) ) >= 1 : raise ValueError ( 'All reflection coefficients should have magnitude less than unity.' ) return ( 2 / numpy . pi ) * numpy . arcsin ( k )
10357	def random_by_edges ( graph : BELGraph , percentage : Optional [ float ] = None ) -> BELGraph : percentage = percentage or 0.9 assert 0 < percentage <= 1 edges = graph . edges ( keys = True ) n = int ( graph . number_of_edges ( ) * percentage ) subedges = random . sample ( edges , n ) rv = graph . fresh_copy ( ) for u , v , k in subedges : safe_add_edge ( rv , u , v , k , graph [ u ] [ v ] [ k ] ) update_node_helper ( graph , rv ) return rv
4375	def encode_payload ( self , messages ) : if not messages or messages [ 0 ] is None : return '' if len ( messages ) == 1 : return messages [ 0 ] . encode ( 'utf-8' ) payload = u'' . join ( [ ( u'\ufffd%d\ufffd%s' % ( len ( p ) , p ) ) for p in messages if p is not None ] ) return payload . encode ( 'utf-8' )
822	def next ( self , newValue ) : newAverage , self . slidingWindow , self . total = self . compute ( self . slidingWindow , self . total , newValue , self . windowSize ) return newAverage
8384	def draw ( self ) : if len ( self . q ) > 0 : self . update ( ) if self . delay == 0 : p , h = self . textpath ( self . i ) f = self . fontsize self . _ctx . fill ( self . background ) self . _ctx . rect ( self . node . x + f * 1.0 , self . node . y + f * 0.5 , self . _w + f , h + f * 1.5 , roundness = 0.2 ) alpha = 1.0 if self . fi < 5 : alpha = 0.2 * self . fi if self . fn - self . fi < 5 : alpha = 0.2 * ( self . fn - self . fi ) self . _ctx . fill ( self . text . r , self . text . g , self . text . b , self . text . a * alpha ) self . _ctx . translate ( self . node . x + f * 2.0 , self . node . y + f * 2.5 ) self . _ctx . drawpath ( p )
10517	def setmax ( self , window_name , object_name ) : object_handle = self . _get_object_handle ( window_name , object_name ) object_handle . AXValue = 1 return 1
2009	def concretized_args ( ** policies ) : def concretizer ( func ) : @ wraps ( func ) def wrapper ( * args , ** kwargs ) : spec = inspect . getfullargspec ( func ) for arg , policy in policies . items ( ) : assert arg in spec . args , "Concretizer argument not found in wrapped function." index = spec . args . index ( arg ) if not issymbolic ( args [ index ] ) : continue if not policy : policy = 'SAMPLED' if policy == "ACCOUNTS" : value = args [ index ] world = args [ 0 ] . world cond = world . _constraint_to_accounts ( value , ty = 'both' , include_zero = True ) world . constraints . add ( cond ) policy = 'ALL' raise ConcretizeArgument ( index , policy = policy ) return func ( * args , ** kwargs ) wrapper . __signature__ = inspect . signature ( func ) return wrapper return concretizer
13829	def remove ( self , collection , ** kwargs ) : callback = kwargs . pop ( 'callback' ) yield Op ( self . db [ collection ] . remove , kwargs ) callback ( )
8692	def init ( self ) : self . es . indices . create ( index = self . params [ 'index' ] , ignore = 400 )
5446	def _parse_local_mount_uri ( self , raw_uri ) : raw_uri = directory_fmt ( raw_uri ) _ , docker_path = _local_uri_rewriter ( raw_uri ) local_path = docker_path [ len ( 'file' ) : ] docker_uri = os . path . join ( self . _relative_path , docker_path ) return local_path , docker_uri
11640	def json_get_data ( filename ) : with open ( filename ) as fp : json_data = json . load ( fp ) return json_data return False
3392	def prune_unused_reactions ( cobra_model ) : output_model = cobra_model . copy ( ) reactions_to_prune = [ r for r in output_model . reactions if len ( r . metabolites ) == 0 ] output_model . remove_reactions ( reactions_to_prune ) return output_model , reactions_to_prune
3168	def replicate ( self , campaign_id ) : self . campaign_id = campaign_id return self . _mc_client . _post ( url = self . _build_path ( campaign_id , 'actions/replicate' ) )
3285	def end_write ( self , with_errors ) : if not with_errors : commands . add ( self . provider . ui , self . provider . repo , self . localHgPath )
13417	def syncdb ( args ) : cmd = args and 'syncdb %s' % ' ' . join ( options . args ) or 'syncdb --noinput' call_manage ( cmd ) for fixture in options . paved . django . syncdb . fixtures : call_manage ( "loaddata %s" % fixture )
2193	def isatty ( self ) : return ( self . redirect is not None and hasattr ( self . redirect , 'isatty' ) and self . redirect . isatty ( ) )
1890	def minmax ( self , constraints , x , iters = 10000 ) : if issymbolic ( x ) : m = self . min ( constraints , x , iters ) M = self . max ( constraints , x , iters ) return m , M else : return x , x
13440	def unlock_file ( filename ) : lockfile = "%s.lock" % filename if isfile ( lockfile ) : os . remove ( lockfile ) return True else : return False
11498	def get_community_by_name ( self , name , token = None ) : parameters = dict ( ) parameters [ 'name' ] = name if token : parameters [ 'token' ] = token response = self . request ( 'midas.community.get' , parameters ) return response
9254	def issue_line_with_user ( self , line , issue ) : if not issue . get ( "pull_request" ) or not self . options . author : return line if not issue . get ( "user" ) : line += u" (Null user)" elif self . options . username_as_tag : line += u" (@{0})" . format ( issue [ "user" ] [ "login" ] ) else : line += u" ([{0}]({1}))" . format ( issue [ "user" ] [ "login" ] , issue [ "user" ] [ "html_url" ] ) return line
7516	def init_arrays ( data ) : co5 = h5py . File ( data . clust_database , 'r' ) io5 = h5py . File ( data . database , 'w' ) maxlen = data . _hackersonly [ "max_fragment_length" ] + 20 chunks = co5 [ "seqs" ] . attrs [ "chunksize" ] [ 0 ] nloci = co5 [ "seqs" ] . shape [ 0 ] snps = io5 . create_dataset ( "snps" , ( nloci , maxlen , 2 ) , dtype = np . bool , chunks = ( chunks , maxlen , 2 ) , compression = 'gzip' ) snps . attrs [ "chunksize" ] = chunks snps . attrs [ "names" ] = [ "-" , "*" ] filters = io5 . create_dataset ( "filters" , ( nloci , 6 ) , dtype = np . bool ) filters . attrs [ "filters" ] = [ "duplicates" , "max_indels" , "max_snps" , "max_shared_hets" , "min_samps" , "max_alleles" ] edges = io5 . create_dataset ( "edges" , ( nloci , 5 ) , dtype = np . uint16 , chunks = ( chunks , 5 ) , compression = "gzip" ) edges . attrs [ "chunksize" ] = chunks edges . attrs [ "names" ] = [ "R1_L" , "R1_R" , "R2_L" , "R2_R" , "sep" ] edges [ : , 4 ] = co5 [ "splits" ] [ : ] filters [ : , 0 ] = co5 [ "duplicates" ] [ : ] io5 . close ( ) co5 . close ( )
2372	def variables ( self ) : for table in self . tables : if isinstance ( table , VariableTable ) : for statement in table . rows : if statement [ 0 ] != "" : yield statement
8502	def as_live ( self ) : key = self . get_key ( ) default = pyconfig . get ( key ) if default : default = repr ( default ) else : default = self . _default ( ) or NotSet ( ) return "%s = %s" % ( key , default )
9037	def walk_connections ( self , mapping = identity ) : for start in self . walk_instructions ( ) : for stop_instruction in start . instruction . consuming_instructions : if stop_instruction is None : continue stop = self . _walk . instruction_in_grid ( stop_instruction ) connection = Connection ( start , stop ) if connection . is_visible ( ) : yield mapping ( connection )
578	def dictDiffAndReport ( da , db ) : differences = dictDiff ( da , db ) if not differences : return differences if differences [ 'inAButNotInB' ] : print ">>> inAButNotInB: %s" % differences [ 'inAButNotInB' ] if differences [ 'inBButNotInA' ] : print ">>> inBButNotInA: %s" % differences [ 'inBButNotInA' ] for key in differences [ 'differentValues' ] : print ">>> da[%s] != db[%s]" % ( key , key ) print "da[%s] = %r" % ( key , da [ key ] ) print "db[%s] = %r" % ( key , db [ key ] ) return differences
13370	def is_redirecting ( path ) : candidate = unipath ( path , '.cpenv' ) return os . path . exists ( candidate ) and os . path . isfile ( candidate )
7715	def update_item ( self , jid , name = NO_CHANGE , groups = NO_CHANGE , callback = None , error_callback = None ) : item = self . roster [ jid ] if name is NO_CHANGE and groups is NO_CHANGE : return if name is NO_CHANGE : name = item . name if groups is NO_CHANGE : groups = item . groups item = RosterItem ( jid , name , groups ) self . _roster_set ( item , callback , error_callback )
4271	def get_exif_tags ( data , datetime_format = '%c' ) : logger = logging . getLogger ( __name__ ) simple = { } for tag in ( 'Model' , 'Make' , 'LensModel' ) : if tag in data : if isinstance ( data [ tag ] , tuple ) : simple [ tag ] = data [ tag ] [ 0 ] . strip ( ) else : simple [ tag ] = data [ tag ] . strip ( ) if 'FNumber' in data : fnumber = data [ 'FNumber' ] try : simple [ 'fstop' ] = float ( fnumber [ 0 ] ) / fnumber [ 1 ] except Exception : logger . debug ( 'Skipped invalid FNumber: %r' , fnumber , exc_info = True ) if 'FocalLength' in data : focal = data [ 'FocalLength' ] try : simple [ 'focal' ] = round ( float ( focal [ 0 ] ) / focal [ 1 ] ) except Exception : logger . debug ( 'Skipped invalid FocalLength: %r' , focal , exc_info = True ) if 'ExposureTime' in data : exptime = data [ 'ExposureTime' ] if isinstance ( exptime , tuple ) : try : simple [ 'exposure' ] = str ( fractions . Fraction ( exptime [ 0 ] , exptime [ 1 ] ) ) except ZeroDivisionError : logger . info ( 'Invalid ExposureTime: %r' , exptime ) elif isinstance ( exptime , int ) : simple [ 'exposure' ] = str ( exptime ) else : logger . info ( 'Unknown format for ExposureTime: %r' , exptime ) if data . get ( 'ISOSpeedRatings' ) : simple [ 'iso' ] = data [ 'ISOSpeedRatings' ] if 'DateTimeOriginal' in data : date = data [ 'DateTimeOriginal' ] . rsplit ( '\x00' ) [ 0 ] try : simple [ 'dateobj' ] = datetime . strptime ( date , '%Y:%m:%d %H:%M:%S' ) simple [ 'datetime' ] = simple [ 'dateobj' ] . strftime ( datetime_format ) except ( ValueError , TypeError ) as e : logger . info ( 'Could not parse DateTimeOriginal: %s' , e ) if 'GPSInfo' in data : info = data [ 'GPSInfo' ] lat_info = info . get ( 'GPSLatitude' ) lon_info = info . get ( 'GPSLongitude' ) lat_ref_info = info . get ( 'GPSLatitudeRef' ) lon_ref_info = info . get ( 'GPSLongitudeRef' ) if lat_info and lon_info and lat_ref_info and lon_ref_info : try : lat = dms_to_degrees ( lat_info ) lon = dms_to_degrees ( lon_info ) except ( ZeroDivisionError , ValueError , TypeError ) : logger . info ( 'Failed to read GPS info' ) else : simple [ 'gps' ] = { 'lat' : - lat if lat_ref_info != 'N' else lat , 'lon' : - lon if lon_ref_info != 'E' else lon , } return simple
9022	def add_row ( self , id_ ) : row = self . _parser . new_row ( id_ ) self . _rows . append ( row ) return row
8867	def _check_word_cursor ( self , tc = None ) : if not tc : tc = TextHelper ( self . editor ) . word_under_cursor ( ) request_data = { 'code' : self . editor . toPlainText ( ) , 'line' : tc . blockNumber ( ) , 'column' : tc . columnNumber ( ) , 'path' : self . editor . file . path , 'encoding' : self . editor . file . encoding } try : self . editor . backend . send_request ( workers . goto_assignments , request_data , on_receive = self . _on_results_available ) except NotRunning : pass
8462	def set_cmd_env_var ( value ) : def func_decorator ( function ) : @ functools . wraps ( function ) def wrapper ( * args , ** kwargs ) : previous_cmd_env_var = os . getenv ( temple . constants . TEMPLE_ENV_VAR ) os . environ [ temple . constants . TEMPLE_ENV_VAR ] = value try : ret_val = function ( * args , ** kwargs ) finally : if previous_cmd_env_var is None : del os . environ [ temple . constants . TEMPLE_ENV_VAR ] else : os . environ [ temple . constants . TEMPLE_ENV_VAR ] = previous_cmd_env_var return ret_val return wrapper return func_decorator
13593	def check_environment ( target , label ) : if not git . exists ( ) : click . secho ( 'You must have git installed to use yld.' , fg = 'red' ) sys . exit ( 1 ) if not os . path . isdir ( '.git' ) : click . secho ( 'You must cd into a git repository to use yld.' , fg = 'red' ) sys . exit ( 1 ) if not git . is_committed ( ) : click . secho ( 'You must commit or stash your work before proceeding.' , fg = 'red' ) sys . exit ( 1 ) if target is None and label is None : click . secho ( 'You must specify either a target or a label.' , fg = 'red' ) sys . exit ( 1 )
2653	def push_file ( self , local_source , remote_dir ) : remote_dest = remote_dir + '/' + os . path . basename ( local_source ) try : self . makedirs ( remote_dir , exist_ok = True ) except IOError as e : logger . exception ( "Pushing {0} to {1} failed" . format ( local_source , remote_dir ) ) if e . errno == 2 : raise BadScriptPath ( e , self . hostname ) elif e . errno == 13 : raise BadPermsScriptPath ( e , self . hostname ) else : logger . exception ( "File push failed due to SFTP client failure" ) raise FileCopyException ( e , self . hostname ) try : self . sftp_client . put ( local_source , remote_dest , confirm = True ) self . sftp_client . chmod ( remote_dest , 0o777 ) except Exception as e : logger . exception ( "File push from local source {} to remote destination {} failed" . format ( local_source , remote_dest ) ) raise FileCopyException ( e , self . hostname ) return remote_dest
9133	def get_data_dir ( module_name : str ) -> str : module_name = module_name . lower ( ) data_dir = os . path . join ( BIO2BEL_DIR , module_name ) os . makedirs ( data_dir , exist_ok = True ) return data_dir
4711	def env ( ) : if cij . ssh . env ( ) : cij . err ( "cij.block.env: invalid SSH environment" ) return 1 block = cij . env_to_dict ( PREFIX , REQUIRED ) block [ "DEV_PATH" ] = "/dev/%s" % block [ "DEV_NAME" ] cij . env_export ( PREFIX , EXPORTED , block ) return 0
588	def compute ( self ) : result = self . _constructClassificationRecord ( ) if result . ROWID >= self . _autoDetectWaitRecords : self . _updateState ( result ) self . saved_states . append ( result ) if len ( self . saved_states ) > self . _history_length : self . saved_states . pop ( 0 ) return result
581	def _setRandomEncoderResolution ( minResolution = 0.001 ) : encoder = ( model_params . MODEL_PARAMS [ "modelParams" ] [ "sensorParams" ] [ "encoders" ] [ "value" ] ) if encoder [ "type" ] == "RandomDistributedScalarEncoder" : rangePadding = abs ( _INPUT_MAX - _INPUT_MIN ) * 0.2 minValue = _INPUT_MIN - rangePadding maxValue = _INPUT_MAX + rangePadding resolution = max ( minResolution , ( maxValue - minValue ) / encoder . pop ( "numBuckets" ) ) encoder [ "resolution" ] = resolution
8782	def select_ipam_strategy ( self , network_id , network_strategy , ** kwargs ) : LOG . info ( "Selecting IPAM strategy for network_id:%s " "network_strategy:%s" % ( network_id , network_strategy ) ) net_type = "tenant" if STRATEGY . is_provider_network ( network_id ) : net_type = "provider" strategy = self . _ipam_strategies . get ( net_type , { } ) default = strategy . get ( "default" ) overrides = strategy . get ( "overrides" , { } ) if network_strategy in overrides : LOG . info ( "Selected overridden IPAM strategy: %s" % ( overrides [ network_strategy ] ) ) return overrides [ network_strategy ] if default : LOG . info ( "Selected default IPAM strategy for tenant " "network: %s" % ( default ) ) return default LOG . info ( "Selected network strategy for tenant " "network: %s" % ( network_strategy ) ) return network_strategy
7652	def match_query ( string , query ) : if six . callable ( query ) : return query ( string ) elif ( isinstance ( query , six . string_types ) and isinstance ( string , six . string_types ) ) : return re . match ( query , string ) is not None else : return query == string
8190	def nodes_by_betweenness ( self , treshold = 0.0 ) : nodes = [ ( n . betweenness , n ) for n in self . nodes if n . betweenness > treshold ] nodes . sort ( ) nodes . reverse ( ) return [ n for w , n in nodes ]
11521	def add_condor_dag ( self , token , batchmaketaskid , dagfilename , dagmanoutfilename ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'batchmaketaskid' ] = batchmaketaskid parameters [ 'dagfilename' ] = dagfilename parameters [ 'outfilename' ] = dagmanoutfilename response = self . request ( 'midas.batchmake.add.condor.dag' , parameters ) return response
2567	def check_tracking_enabled ( self ) : track = True test = False testvar = str ( os . environ . get ( "PARSL_TESTING" , 'None' ) ) . lower ( ) if testvar == 'true' : test = True if not self . config . usage_tracking : track = False envvar = str ( os . environ . get ( "PARSL_TRACKING" , True ) ) . lower ( ) if envvar == "false" : track = False return test , track
6271	def sphere ( radius = 0.5 , sectors = 32 , rings = 16 ) -> VAO : R = 1.0 / ( rings - 1 ) S = 1.0 / ( sectors - 1 ) vertices = [ 0 ] * ( rings * sectors * 3 ) normals = [ 0 ] * ( rings * sectors * 3 ) uvs = [ 0 ] * ( rings * sectors * 2 ) v , n , t = 0 , 0 , 0 for r in range ( rings ) : for s in range ( sectors ) : y = math . sin ( - math . pi / 2 + math . pi * r * R ) x = math . cos ( 2 * math . pi * s * S ) * math . sin ( math . pi * r * R ) z = math . sin ( 2 * math . pi * s * S ) * math . sin ( math . pi * r * R ) uvs [ t ] = s * S uvs [ t + 1 ] = r * R vertices [ v ] = x * radius vertices [ v + 1 ] = y * radius vertices [ v + 2 ] = z * radius normals [ n ] = x normals [ n + 1 ] = y normals [ n + 2 ] = z t += 2 v += 3 n += 3 indices = [ 0 ] * rings * sectors * 6 i = 0 for r in range ( rings - 1 ) : for s in range ( sectors - 1 ) : indices [ i ] = r * sectors + s indices [ i + 1 ] = ( r + 1 ) * sectors + ( s + 1 ) indices [ i + 2 ] = r * sectors + ( s + 1 ) indices [ i + 3 ] = r * sectors + s indices [ i + 4 ] = ( r + 1 ) * sectors + s indices [ i + 5 ] = ( r + 1 ) * sectors + ( s + 1 ) i += 6 vbo_vertices = numpy . array ( vertices , dtype = numpy . float32 ) vbo_normals = numpy . array ( normals , dtype = numpy . float32 ) vbo_uvs = numpy . array ( uvs , dtype = numpy . float32 ) vbo_elements = numpy . array ( indices , dtype = numpy . uint32 ) vao = VAO ( "sphere" , mode = mlg . TRIANGLES ) vao . buffer ( vbo_vertices , '3f' , [ 'in_position' ] ) vao . buffer ( vbo_normals , '3f' , [ 'in_normal' ] ) vao . buffer ( vbo_uvs , '2f' , [ 'in_uv' ] ) vao . index_buffer ( vbo_elements , index_element_size = 4 ) return vao
4451	def search ( self , query ) : args , query = self . _mk_query_args ( query ) st = time . time ( ) res = self . redis . execute_command ( self . SEARCH_CMD , * args ) return Result ( res , not query . _no_content , duration = ( time . time ( ) - st ) * 1000.0 , has_payload = query . _with_payloads )
6636	def publish ( self , registry = None ) : if ( registry is None ) or ( registry == registry_access . Registry_Base_URL ) : if 'private' in self . description and self . description [ 'private' ] : return "this %s is private and cannot be published" % ( self . description_filename . split ( '.' ) [ 0 ] ) upload_archive = os . path . join ( self . path , 'upload.tar.gz' ) fsutils . rmF ( upload_archive ) fd = os . open ( upload_archive , os . O_CREAT | os . O_EXCL | os . O_RDWR | getattr ( os , "O_BINARY" , 0 ) ) with os . fdopen ( fd , 'rb+' ) as tar_file : tar_file . truncate ( ) self . generateTarball ( tar_file ) logger . debug ( 'generated tar file of length %s' , tar_file . tell ( ) ) tar_file . seek ( 0 ) shasum = hashlib . sha256 ( ) while True : chunk = tar_file . read ( 1000 ) if not chunk : break shasum . update ( chunk ) logger . debug ( 'generated tar file has hash %s' , shasum . hexdigest ( ) ) tar_file . seek ( 0 ) with self . findAndOpenReadme ( ) as readme_file_wrapper : if not readme_file_wrapper : logger . warning ( "no readme.md file detected" ) with open ( self . getDescriptionFile ( ) , 'r' ) as description_file : return registry_access . publish ( self . getRegistryNamespace ( ) , self . getName ( ) , self . getVersion ( ) , description_file , tar_file , readme_file_wrapper . file , readme_file_wrapper . extension ( ) . lower ( ) , registry = registry )
12295	def annotate_metadata_dependencies ( repo ) : options = repo . options if 'dependencies' not in options : print ( "No dependencies" ) return [ ] repos = [ ] dependent_repos = options [ 'dependencies' ] for d in dependent_repos : if "/" not in d : print ( "Invalid dependency specification" ) ( username , reponame ) = d . split ( "/" ) try : repos . append ( repo . manager . lookup ( username , reponame ) ) except : print ( "Repository does not exist. Please create one" , d ) package = repo . package package [ 'dependencies' ] = [ ] for r in repos : package [ 'dependencies' ] . append ( { 'username' : r . username , 'reponame' : r . reponame , } )
3869	async def update_read_timestamp ( self , read_timestamp = None ) : if read_timestamp is None : read_timestamp = ( self . events [ - 1 ] . timestamp if self . events else datetime . datetime . now ( datetime . timezone . utc ) ) if read_timestamp > self . latest_read_timestamp : logger . info ( 'Setting {} latest_read_timestamp from {} to {}' . format ( self . id_ , self . latest_read_timestamp , read_timestamp ) ) state = self . _conversation . self_conversation_state state . self_read_state . latest_read_timestamp = ( parsers . to_timestamp ( read_timestamp ) ) try : await self . _client . update_watermark ( hangouts_pb2 . UpdateWatermarkRequest ( request_header = self . _client . get_request_header ( ) , conversation_id = hangouts_pb2 . ConversationId ( id = self . id_ ) , last_read_timestamp = parsers . to_timestamp ( read_timestamp ) , ) ) except exceptions . NetworkError as e : logger . warning ( 'Failed to update read timestamp: {}' . format ( e ) ) raise
11844	def run ( self , steps = 1000 ) : "Run the Environment for given number of time steps." for step in range ( steps ) : if self . is_done ( ) : return self . step ( )
2743	def load ( self ) : identifier = None if self . id : identifier = self . id elif self . fingerprint is not None : identifier = self . fingerprint data = self . get_data ( "account/keys/%s" % identifier , type = GET ) ssh_key = data [ 'ssh_key' ] for attr in ssh_key . keys ( ) : setattr ( self , attr , ssh_key [ attr ] ) self . id = ssh_key [ 'id' ]
9057	def economic_qs_zeros ( n ) : Q0 = empty ( ( n , 0 ) ) Q1 = eye ( n ) S0 = empty ( 0 ) return ( ( Q0 , Q1 ) , S0 )
1387	def set_execution_state ( self , execution_state ) : if not execution_state : self . execution_state = None self . cluster = None self . environ = None else : self . execution_state = execution_state cluster , environ = self . get_execution_state_dc_environ ( execution_state ) self . cluster = cluster self . environ = environ self . zone = cluster self . trigger_watches ( )
7400	def up ( self ) : self . swap ( self . get_ordering_queryset ( ) . filter ( order__lt = self . order ) . order_by ( '-order' ) )
13723	def log_file ( self , url = None ) : if url is None : url = self . url f = re . sub ( "file://" , "" , url ) try : with open ( f , "a" ) as of : of . write ( str ( self . store . get_json_tuples ( True ) ) ) except IOError as e : print ( e ) print ( "Could not write the content to the file.." )
6113	def zoomed_scaled_array_around_mask ( self , mask , buffer = 1 ) : return self . new_with_array ( array = array_util . extracted_array_2d_from_array_2d_and_coordinates ( array_2d = self , y0 = mask . zoom_region [ 0 ] - buffer , y1 = mask . zoom_region [ 1 ] + buffer , x0 = mask . zoom_region [ 2 ] - buffer , x1 = mask . zoom_region [ 3 ] + buffer ) )
4505	def get_and_run_edits ( self ) : if self . empty ( ) : return edits = [ ] while True : try : edits . append ( self . get_nowait ( ) ) except queue . Empty : break for e in edits : try : e ( ) except : log . error ( 'Error on edit %s' , e ) traceback . print_exc ( )
6232	def calc_scene_bbox ( self ) : bbox_min , bbox_max = None , None for node in self . root_nodes : bbox_min , bbox_max = node . calc_global_bbox ( matrix44 . create_identity ( ) , bbox_min , bbox_max ) self . bbox_min = bbox_min self . bbox_max = bbox_max self . diagonal_size = vector3 . length ( self . bbox_max - self . bbox_min )
4147	def DaniellPeriodogram ( data , P , NFFT = None , detrend = 'mean' , sampling = 1. , scale_by_freq = True , window = 'hamming' ) : r psd = speriodogram ( data , NFFT = NFFT , detrend = detrend , sampling = sampling , scale_by_freq = scale_by_freq , window = window ) if len ( psd ) % 2 == 1 : datatype = 'real' else : datatype = 'complex' N = len ( psd ) _slice = 2 * P + 1 if datatype == 'real' : newN = np . ceil ( psd . size / float ( _slice ) ) if newN % 2 == 0 : newN = psd . size / _slice else : newN = np . ceil ( psd . size / float ( _slice ) ) if newN % 2 == 1 : newN = psd . size / _slice newpsd = np . zeros ( int ( newN ) ) for i in range ( 0 , newpsd . size ) : count = 0 for n in range ( i * _slice - P , i * _slice + P + 1 ) : if n > 0 and n < N : count += 1 newpsd [ i ] += psd [ n ] newpsd [ i ] /= float ( count ) if datatype == 'complex' : freq = np . linspace ( 0 , sampling , len ( newpsd ) ) else : df = 1. / sampling freq = np . linspace ( 0 , sampling / 2. , len ( newpsd ) ) return newpsd , freq
8862	def quick_doc ( request_data ) : code = request_data [ 'code' ] line = request_data [ 'line' ] + 1 column = request_data [ 'column' ] path = request_data [ 'path' ] encoding = 'utf-8' script = jedi . Script ( code , line , column , path , encoding ) try : definitions = script . goto_definitions ( ) except jedi . NotFoundError : return [ ] else : ret_val = [ d . docstring ( ) for d in definitions ] return ret_val
10736	def split_list ( l , N ) : npmode = isinstance ( l , np . ndarray ) if npmode : l = list ( l ) g = np . concatenate ( ( np . array ( [ 0 ] ) , np . cumsum ( split_integer ( len ( l ) , length = N ) ) ) ) s = [ l [ g [ i ] : g [ i + 1 ] ] for i in range ( N ) ] if npmode : s = [ np . array ( sl ) for sl in s ] return s
7520	def write_usnps ( data , sidx , pnames ) : tmparrs = os . path . join ( data . dirs . outfiles , "tmp-{}.h5" . format ( data . name ) ) with h5py . File ( tmparrs , 'r' ) as io5 : bisarr = io5 [ "bisarr" ] end = np . where ( np . all ( bisarr [ : ] == "" , axis = 0 ) ) [ 0 ] if np . any ( end ) : end = end . min ( ) else : end = bisarr . shape [ 1 ] with open ( data . outfiles . usnpsphy , 'w' ) as out : out . write ( "{} {}\n" . format ( bisarr . shape [ 0 ] , end ) ) for idx , name in enumerate ( pnames ) : out . write ( "{}{}\n" . format ( name , "" . join ( bisarr [ idx , : end ] ) ) )
7606	def search_clans ( self , ** params : clansearch ) : url = self . api . CLAN return self . _get_model ( url , PartialClan , ** params )
8967	def step ( self , key , chain ) : if chain == "sending" : self . __previous_sending_chain_length = self . sending_chain_length self . __sending_chain = self . __SendingChain ( key ) if chain == "receiving" : self . __receiving_chain = self . __ReceivingChain ( key )
11197	def find_probable_year_index ( self , tokens ) : for index , token in enumerate ( self ) : potential_year_tokens = _ymd . find_potential_year_tokens ( token , tokens ) if len ( potential_year_tokens ) == 1 and len ( potential_year_tokens [ 0 ] ) > 2 : return index
498	def _addRecordToKNN ( self , record ) : knn = self . _knnclassifier . _knn prototype_idx = self . _knnclassifier . getParameter ( 'categoryRecencyList' ) category = self . _labelListToCategoryNumber ( record . anomalyLabel ) if record . ROWID in prototype_idx : knn . prototypeSetCategory ( record . ROWID , category ) return pattern = self . _getStateAnomalyVector ( record ) rowID = record . ROWID knn . learn ( pattern , category , rowID = rowID )
10920	def do_levmarq_particles ( s , particles , damping = 1.0 , decrease_damp_factor = 10. , run_length = 4 , collect_stats = False , max_iter = 2 , ** kwargs ) : lp = LMParticles ( s , particles , damping = damping , run_length = run_length , decrease_damp_factor = decrease_damp_factor , max_iter = max_iter , ** kwargs ) lp . do_run_2 ( ) if collect_stats : return lp . get_termination_stats ( )
11126	def dump_copy ( self , path , relativePath , name = None , description = None , replace = False , verbose = False ) : relativePath = os . path . normpath ( relativePath ) if relativePath == '.' : relativePath = '' if name is None : _ , name = os . path . split ( path ) self . add_directory ( relativePath ) realPath = os . path . join ( self . __path , relativePath ) dirInfoDict , errorMessage = self . get_directory_info ( relativePath ) assert dirInfoDict is not None , errorMessage if name in dict . __getitem__ ( dirInfoDict , "files" ) : if not replace : if verbose : warnings . warn ( "a file with the name '%s' is already defined in repository dictionary info. Set replace flag to True if you want to replace the existing file" % ( name ) ) return dump = "raise Exception(\"dump is ambiguous for copied file '$FILE_PATH' \")" pull = "raise Exception(\"pull is ambiguous for copied file '$FILE_PATH' \")" try : shutil . copyfile ( path , os . path . join ( realPath , name ) ) except Exception as e : if verbose : warnings . warn ( e ) return klass = None dict . __getitem__ ( dirInfoDict , "files" ) [ name ] = { "dump" : dump , "pull" : pull , "timestamp" : datetime . utcnow ( ) , "id" : str ( uuid . uuid1 ( ) ) , "class" : klass , "description" : description } self . save ( )
4943	def get_data_sharing_consent ( username , enterprise_customer_uuid , course_id = None , program_uuid = None ) : EnterpriseCustomer = apps . get_model ( 'enterprise' , 'EnterpriseCustomer' ) try : if course_id : return get_course_data_sharing_consent ( username , course_id , enterprise_customer_uuid ) return get_program_data_sharing_consent ( username , program_uuid , enterprise_customer_uuid ) except EnterpriseCustomer . DoesNotExist : return None
12312	def create_ingest_point ( self , privateStreamName , publicStreamName ) : return self . protocol . execute ( 'createIngestPoint' , privateStreamName = privateStreamName , publicStreamName = publicStreamName )
12388	def set ( self , target , value ) : if not self . _set : return if self . path is None : self . set = lambda * a : None return None if self . _segments [ target . __class__ ] : self . get ( target ) if self . _segments [ target . __class__ ] : return parent_getter = compose ( * self . _getters [ target . __class__ ] [ : - 1 ] ) target = parent_getter ( target ) func = self . _make_setter ( self . path . split ( '.' ) [ - 1 ] , target . __class__ ) func ( target , value ) def setter ( target , value ) : func ( parent_getter ( target ) , value ) self . set = setter
475	def sentence_to_token_ids ( sentence , vocabulary , tokenizer = None , normalize_digits = True , UNK_ID = 3 , _DIGIT_RE = re . compile ( br"\d" ) ) : if tokenizer : words = tokenizer ( sentence ) else : words = basic_tokenizer ( sentence ) if not normalize_digits : return [ vocabulary . get ( w , UNK_ID ) for w in words ] return [ vocabulary . get ( re . sub ( _DIGIT_RE , b"0" , w ) , UNK_ID ) for w in words ]
813	def read ( cls , proto ) : tm = super ( TemporalMemoryMonitorMixin , cls ) . read ( proto ) tm . mmName = None tm . _mmTraces = None tm . _mmData = None tm . mmClearHistory ( ) tm . _mmResetActive = True return tm
6893	def parallel_starfeatures ( lclist , outdir , lc_catalog_pickle , neighbor_radius_arcsec , maxobjects = None , deredden = True , custom_bandpasses = None , lcformat = 'hat-sql' , lcformatdir = None , nworkers = NCPUS ) : try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( dfileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None if not os . path . exists ( outdir ) : os . makedirs ( outdir ) if maxobjects : lclist = lclist [ : maxobjects ] with open ( lc_catalog_pickle , 'rb' ) as infd : kdt_dict = pickle . load ( infd ) kdt = kdt_dict [ 'kdtree' ] objlist = kdt_dict [ 'objects' ] [ 'objectid' ] objlcfl = kdt_dict [ 'objects' ] [ 'lcfname' ] tasks = [ ( x , outdir , kdt , objlist , objlcfl , neighbor_radius_arcsec , deredden , custom_bandpasses , lcformat ) for x in lclist ] with ProcessPoolExecutor ( max_workers = nworkers ) as executor : resultfutures = executor . map ( _starfeatures_worker , tasks ) results = [ x for x in resultfutures ] resdict = { os . path . basename ( x ) : y for ( x , y ) in zip ( lclist , results ) } return resdict
6295	def index_buffer ( self , buffer , index_element_size = 4 ) : if not type ( buffer ) in [ moderngl . Buffer , numpy . ndarray , bytes ] : raise VAOError ( "buffer parameter must be a moderngl.Buffer, numpy.ndarray or bytes instance" ) if isinstance ( buffer , numpy . ndarray ) : buffer = self . ctx . buffer ( buffer . tobytes ( ) ) if isinstance ( buffer , bytes ) : buffer = self . ctx . buffer ( data = buffer ) self . _index_buffer = buffer self . _index_element_size = index_element_size
7392	def adjust_angles ( self , start_node , start_angle , end_node , end_angle ) : start_group = self . find_node_group_membership ( start_node ) end_group = self . find_node_group_membership ( end_node ) if start_group == 0 and end_group == len ( self . nodes . keys ( ) ) - 1 : if self . has_edge_within_group ( start_group ) : start_angle = correct_negative_angle ( start_angle - self . minor_angle ) if self . has_edge_within_group ( end_group ) : end_angle = correct_negative_angle ( end_angle + self . minor_angle ) elif start_group == len ( self . nodes . keys ( ) ) - 1 and end_group == 0 : if self . has_edge_within_group ( start_group ) : start_angle = correct_negative_angle ( start_angle + self . minor_angle ) if self . has_edge_within_group ( end_group ) : end_angle = correct_negative_angle ( end_angle - self . minor_angle ) elif start_group < end_group : if self . has_edge_within_group ( end_group ) : end_angle = correct_negative_angle ( end_angle - self . minor_angle ) if self . has_edge_within_group ( start_group ) : start_angle = correct_negative_angle ( start_angle + self . minor_angle ) elif end_group < start_group : if self . has_edge_within_group ( start_group ) : start_angle = correct_negative_angle ( start_angle - self . minor_angle ) if self . has_edge_within_group ( end_group ) : end_angle = correct_negative_angle ( end_angle + self . minor_angle ) return start_angle , end_angle
6793	def load_django_settings ( self ) : r = self . local_renderer _env = { } save_vars = [ 'ALLOW_CELERY' , 'DJANGO_SETTINGS_MODULE' ] for var_name in save_vars : _env [ var_name ] = os . environ . get ( var_name ) try : if r . env . local_project_dir : sys . path . insert ( 0 , r . env . local_project_dir ) os . environ [ 'ALLOW_CELERY' ] = '0' os . environ [ 'DJANGO_SETTINGS_MODULE' ] = r . format ( r . env . settings_module ) try : import django django . setup ( ) except AttributeError : pass settings = self . get_settings ( ) try : from django . contrib import staticfiles from django . conf import settings as _settings if settings is not None : for k , v in settings . __dict__ . items ( ) : setattr ( _settings , k , v ) else : raise ImportError except ( ImportError , RuntimeError ) : print ( 'Unable to load settings.' ) traceback . print_exc ( ) finally : for var_name , var_value in _env . items ( ) : if var_value is None : del os . environ [ var_name ] else : os . environ [ var_name ] = var_value return settings
85	def ContrastNormalization ( alpha = 1.0 , per_channel = False , name = None , deterministic = False , random_state = None ) : from . import contrast as contrast_lib return contrast_lib . LinearContrast ( alpha = alpha , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )
3151	def update ( self , list_id , webhook_id , data ) : self . list_id = list_id self . webhook_id = webhook_id return self . _mc_client . _patch ( url = self . _build_path ( list_id , 'webhooks' , webhook_id ) , data = data )
4835	def get_paginated_catalog_courses ( self , catalog_id , querystring = None ) : return self . _load_data ( self . CATALOGS_COURSES_ENDPOINT . format ( catalog_id ) , default = [ ] , querystring = querystring , traverse_pagination = False , many = False , )
10052	def get ( self , pid , record ) : return self . make_response ( obj = record . files , pid = pid , record = record )
13181	def _get_printable_columns ( columns , row ) : if not columns : return row return tuple ( row [ c ] for c in columns )
4808	def generate_best_dataset ( best_path , output_path = 'cleaned_data' , create_val = False ) : if not os . path . isdir ( output_path ) : os . mkdir ( output_path ) if not os . path . isdir ( os . path . join ( output_path , 'train' ) ) : os . makedirs ( os . path . join ( output_path , 'train' ) ) if not os . path . isdir ( os . path . join ( output_path , 'test' ) ) : os . makedirs ( os . path . join ( output_path , 'test' ) ) if not os . path . isdir ( os . path . join ( output_path , 'val' ) ) and create_val : os . makedirs ( os . path . join ( output_path , 'val' ) ) for article_type in article_types : files = glob ( os . path . join ( best_path , article_type , '*.txt' ) ) files_train , files_test = train_test_split ( files , random_state = 0 , test_size = 0.1 ) if create_val : files_train , files_val = train_test_split ( files_train , random_state = 0 , test_size = 0.1 ) val_words = generate_words ( files_val ) val_df = create_char_dataframe ( val_words ) val_df . to_csv ( os . path . join ( output_path , 'val' , 'df_best_{}_val.csv' . format ( article_type ) ) , index = False ) train_words = generate_words ( files_train ) test_words = generate_words ( files_test ) train_df = create_char_dataframe ( train_words ) test_df = create_char_dataframe ( test_words ) train_df . to_csv ( os . path . join ( output_path , 'train' , 'df_best_{}_train.csv' . format ( article_type ) ) , index = False ) test_df . to_csv ( os . path . join ( output_path , 'test' , 'df_best_{}_test.csv' . format ( article_type ) ) , index = False ) print ( "Save {} to CSV file" . format ( article_type ) )
4881	def delete_switch ( apps , schema_editor ) : Switch = apps . get_model ( 'waffle' , 'Switch' ) Switch . objects . filter ( name = ENTERPRISE_ROLE_BASED_ACCESS_CONTROL_SWITCH ) . delete ( )
6000	def pix_to_regular ( self ) : pix_to_regular = [ [ ] for _ in range ( self . pixels ) ] for regular_pixel , pix_pixel in enumerate ( self . regular_to_pix ) : pix_to_regular [ pix_pixel ] . append ( regular_pixel ) return pix_to_regular
13599	def push ( self , k ) : if not self . _first : self . _first = self . _last = node = DLL . Node ( k ) elif self . _first . value == k : return else : try : self . delete ( k ) except KeyError : pass self . _first = node = self . _first . insert_before ( k ) self . _index [ k ] = node self . _size += 1
842	def getPartitionId ( self , i ) : if ( i < 0 ) or ( i >= self . _numPatterns ) : raise RuntimeError ( "index out of bounds" ) partitionId = self . _partitionIdList [ i ] if partitionId == numpy . inf : return None else : return partitionId
1598	def read_chunk ( filename , offset = - 1 , length = - 1 , escape_data = False ) : try : length = int ( length ) offset = int ( offset ) except ValueError : return { } if not os . path . isfile ( filename ) : return { } try : fstat = os . stat ( filename ) except Exception : return { } if offset == - 1 : offset = fstat . st_size if length == - 1 : length = fstat . st_size - offset with open ( filename , "r" ) as fp : fp . seek ( offset ) try : data = fp . read ( length ) except IOError : return { } if data : data = _escape_data ( data ) if escape_data else data return dict ( offset = offset , length = len ( data ) , data = data ) return dict ( offset = offset , length = 0 )
6518	def is_excluded ( self , path ) : relpath = path . relative_to ( self . base_path ) . as_posix ( ) return matches_masks ( relpath , self . excludes )
9886	def _call_multi_fortran_z ( self , names , data_types , rec_nums , dim_sizes , input_type_code , func , epoch = False , data_offset = None , epoch16 = False ) : idx , = np . where ( data_types == input_type_code ) if len ( idx ) > 0 : max_rec = rec_nums [ idx ] . max ( ) sub_names = np . array ( names ) [ idx ] sub_sizes = dim_sizes [ idx ] status , data = func ( self . fname , sub_names . tolist ( ) , sub_sizes , sub_sizes . sum ( ) , max_rec , len ( sub_names ) ) if status == 0 : if data_offset is not None : data = data . astype ( int ) idx , idy , = np . where ( data < 0 ) data [ idx , idy ] += data_offset if epoch : data -= 62167219200000 data = data . astype ( '<M8[ms]' ) if epoch16 : data [ 0 : : 2 , : ] -= 62167219200 data = data [ 0 : : 2 , : ] * 1E9 + data [ 1 : : 2 , : ] / 1.E3 data = data . astype ( 'datetime64[ns]' ) sub_sizes /= 2 self . _process_return_multi_z ( data , sub_names , sub_sizes ) else : raise IOError ( fortran_cdf . statusreporter ( status ) )
10671	def _finalise_result_ ( compound , value , mass ) : result = value / 3.6E6 result = result / compound . molar_mass result = result * mass return result
7131	def setup_paths ( source , destination , name , add_to_global , force ) : if source [ - 1 ] == "/" : source = source [ : - 1 ] if not name : name = os . path . split ( source ) [ - 1 ] elif name . endswith ( ".docset" ) : name = name . replace ( ".docset" , "" ) if add_to_global : destination = DEFAULT_DOCSET_PATH dest = os . path . join ( destination or "" , name + ".docset" ) dst_exists = os . path . lexists ( dest ) if dst_exists and force : shutil . rmtree ( dest ) elif dst_exists : log . error ( 'Destination path "{}" already exists.' . format ( click . format_filename ( dest ) ) ) raise SystemExit ( errno . EEXIST ) return source , dest , name
678	def generateRecords ( self , records ) : if self . verbosity > 0 : print 'Generating' , len ( records ) , 'records...' for record in records : self . generateRecord ( record )
13872	def StandardizePath ( path , strip = False ) : path = path . replace ( SEPARATOR_WINDOWS , SEPARATOR_UNIX ) if strip : path = path . rstrip ( SEPARATOR_UNIX ) return path
9657	def get_direct_ancestors ( G , list_of_nodes ) : parents = [ ] for item in list_of_nodes : anc = G . predecessors ( item ) for one in anc : parents . append ( one ) return parents
12389	def parse ( specifiers ) : specifiers = "" . join ( specifiers . split ( ) ) for specifier in specifiers . split ( ',' ) : if len ( specifier ) == 0 : raise ValueError ( "Range: Invalid syntax; missing specifier." ) count = specifier . count ( '-' ) if ( count and specifier [ 0 ] == '-' ) or not count : yield int ( specifier ) , int ( specifier ) continue specifier = list ( map ( int , specifier . split ( '-' ) ) ) if len ( specifier ) == 2 : if specifier [ 0 ] < 0 or specifier [ 1 ] < 0 : raise ValueError ( "Range: Invalid syntax; negative indexing " "not supported in a range specifier." ) if specifier [ 1 ] < specifier [ 0 ] : raise ValueError ( "Range: Invalid syntax; stop is less than start." ) yield tuple ( specifier ) continue raise ValueError ( "Range: Invalid syntax." )
3044	def _generate_refresh_request_body ( self ) : body = urllib . parse . urlencode ( { 'grant_type' : 'refresh_token' , 'client_id' : self . client_id , 'client_secret' : self . client_secret , 'refresh_token' : self . refresh_token , } ) return body
6269	def on_resize ( self , width , height ) : self . width , self . height = width , height self . buffer_width , self . buffer_height = width , height self . resize ( width , height )
6145	def DSP_callback_toc ( self ) : if self . Tcapture > 0 : self . DSP_toc . append ( time . time ( ) - self . start_time )
8193	def crown ( self , depth = 2 ) : nodes = [ ] for node in self . leaves : nodes += node . flatten ( depth - 1 ) return cluster . unique ( nodes )
7732	def get_join_info ( self ) : x = self . get_muc_child ( ) if not x : return None if not isinstance ( x , MucX ) : return None return x
536	def readFromProto ( cls , proto ) : instance = cls ( ) instance . implementation = proto . implementation instance . steps = proto . steps instance . stepsList = [ int ( i ) for i in proto . steps . split ( "," ) ] instance . alpha = proto . alpha instance . verbosity = proto . verbosity instance . maxCategoryCount = proto . maxCategoryCount instance . _sdrClassifier = SDRClassifierFactory . read ( proto ) instance . learningMode = proto . learningMode instance . inferenceMode = proto . inferenceMode instance . recordNum = proto . recordNum return instance
1455	def add_ckpt_state ( self , ckpt_id , ckpt_state ) : self . _flush_remaining ( ) msg = ckptmgr_pb2 . StoreInstanceStateCheckpoint ( ) istate = ckptmgr_pb2 . InstanceStateCheckpoint ( ) istate . checkpoint_id = ckpt_id istate . state = ckpt_state msg . state . CopyFrom ( istate ) self . _push_tuple_to_stream ( msg )
4162	def _parse_dict_recursive ( dict_str ) : dict_out = dict ( ) pos_last = 0 pos = dict_str . find ( ':' ) while pos >= 0 : key = dict_str [ pos_last : pos ] if dict_str [ pos + 1 ] == '[' : pos_tmp = dict_str . find ( ']' , pos + 1 ) if pos_tmp < 0 : raise RuntimeError ( 'error when parsing dict' ) value = dict_str [ pos + 2 : pos_tmp ] . split ( ',' ) for i in range ( len ( value ) ) : try : value [ i ] = int ( value [ i ] ) except ValueError : pass elif dict_str [ pos + 1 ] == '{' : subdict_str = _select_block ( dict_str [ pos : ] , '{' , '}' ) value = _parse_dict_recursive ( subdict_str ) pos_tmp = pos + len ( subdict_str ) else : raise ValueError ( 'error when parsing dict: unknown elem' ) key = key . strip ( '"' ) if len ( key ) > 0 : dict_out [ key ] = value pos_last = dict_str . find ( ',' , pos_tmp ) if pos_last < 0 : break pos_last += 1 pos = dict_str . find ( ':' , pos_last ) return dict_out
7411	def sample_loci ( self ) : idxs = np . random . choice ( self . idxs , self . ntests ) with open ( self . data ) as indata : liter = ( indata . read ( ) . strip ( ) . split ( "|\n" ) ) seqdata = { i : "" for i in self . samples } for idx , loc in enumerate ( liter ) : if idx in idxs : lines = loc . split ( "\n" ) [ : - 1 ] names = [ i . split ( ) [ 0 ] for i in lines ] seqs = [ i . split ( ) [ 1 ] for i in lines ] dd = { i : j for i , j in zip ( names , seqs ) } for name in seqdata : if name in names : seqdata [ name ] += dd [ name ] else : seqdata [ name ] += "N" * len ( seqs [ 0 ] ) return seqdata
10410	def finalize_canonical_averages ( number_of_nodes , ps , canonical_averages , alpha , ) : spanning_cluster = ( ( 'percolation_probability_mean' in canonical_averages . dtype . names ) and 'percolation_probability_m2' in canonical_averages . dtype . names ) ret = np . empty_like ( canonical_averages , dtype = finalized_canonical_averages_dtype ( spanning_cluster = spanning_cluster ) , ) n = canonical_averages [ 'number_of_runs' ] sqrt_n = np . sqrt ( canonical_averages [ 'number_of_runs' ] ) ret [ 'number_of_runs' ] = n ret [ 'p' ] = ps ret [ 'alpha' ] = alpha def _transform ( original_key , final_key = None , normalize = False , transpose = False , ) : if final_key is None : final_key = original_key keys_mean = [ '{}_mean' . format ( key ) for key in [ original_key , final_key ] ] keys_std = [ '{}_m2' . format ( original_key ) , '{}_std' . format ( final_key ) , ] key_ci = '{}_ci' . format ( final_key ) ret [ keys_mean [ 1 ] ] = canonical_averages [ keys_mean [ 0 ] ] if normalize : ret [ keys_mean [ 1 ] ] /= number_of_nodes array = canonical_averages [ keys_std [ 0 ] ] result = np . sqrt ( ( array . T if transpose else array ) / ( n - 1 ) ) ret [ keys_std [ 1 ] ] = ( result . T if transpose else result ) if normalize : ret [ keys_std [ 1 ] ] /= number_of_nodes array = ret [ keys_std [ 1 ] ] scale = ( array . T if transpose else array ) / sqrt_n array = ret [ keys_mean [ 1 ] ] mean = ( array . T if transpose else array ) result = scipy . stats . t . interval ( 1 - alpha , df = n - 1 , loc = mean , scale = scale , ) ( ret [ key_ci ] [ ... , 0 ] , ret [ key_ci ] [ ... , 1 ] ) = ( [ my_array . T for my_array in result ] if transpose else result ) if spanning_cluster : _transform ( 'percolation_probability' ) _transform ( 'max_cluster_size' , 'percolation_strength' , normalize = True ) _transform ( 'moments' , normalize = True , transpose = True ) return ret
4622	def _new_masterpassword ( self , password ) : if self . config_key in self . config and self . config [ self . config_key ] : raise Exception ( "Storage already has a masterpassword!" ) self . decrypted_master = hexlify ( os . urandom ( 32 ) ) . decode ( "ascii" ) self . password = password self . _save_encrypted_masterpassword ( ) return self . masterkey
6413	def ghmean ( nums ) : m_g = gmean ( nums ) m_h = hmean ( nums ) if math . isnan ( m_g ) or math . isnan ( m_h ) : return float ( 'nan' ) while round ( m_h , 12 ) != round ( m_g , 12 ) : m_g , m_h = ( m_g * m_h ) ** ( 1 / 2 ) , ( 2 * m_g * m_h ) / ( m_g + m_h ) return m_g
10798	def _weight ( self , rsq , sigma = None ) : sigma = sigma or self . filter_size if not self . clip : o = np . exp ( - rsq / ( 2 * sigma ** 2 ) ) else : o = np . zeros ( rsq . shape , dtype = 'float' ) m = ( rsq < self . clipsize ** 2 ) o [ m ] = np . exp ( - rsq [ m ] / ( 2 * sigma ** 2 ) ) return o
736	def _sortChunk ( records , key , chunkIndex , fields ) : title ( additional = '(key=%s, chunkIndex=%d)' % ( str ( key ) , chunkIndex ) ) assert len ( records ) > 0 records . sort ( key = itemgetter ( * key ) ) if chunkIndex is not None : filename = 'chunk_%d.csv' % chunkIndex with FileRecordStream ( filename , write = True , fields = fields ) as o : for r in records : o . appendRecord ( r ) assert os . path . getsize ( filename ) > 0 return records
7688	def pitch_contour ( annotation , sr = 22050 , length = None , ** kwargs ) : times = defaultdict ( list ) freqs = defaultdict ( list ) for obs in annotation : times [ obs . value [ 'index' ] ] . append ( obs . time ) freqs [ obs . value [ 'index' ] ] . append ( obs . value [ 'frequency' ] * ( - 1 ) ** ( ~ obs . value [ 'voiced' ] ) ) y_out = 0.0 for ix in times : y_out = y_out + filter_kwargs ( mir_eval . sonify . pitch_contour , np . asarray ( times [ ix ] ) , np . asarray ( freqs [ ix ] ) , fs = sr , length = length , ** kwargs ) if length is None : length = len ( y_out ) return y_out
12226	def on_pref_update ( * args , ** kwargs ) : Preference . update_prefs ( * args , ** kwargs ) Preference . read_prefs ( get_prefs ( ) )
686	def getEncoding ( self , n ) : assert ( all ( field . numEncodings > n for field in self . fields ) ) encoding = np . concatenate ( [ field . encodings [ n ] for field in self . fields ] ) return encoding
12668	def matrix_to_4dvolume ( arr , mask , order = 'C' ) : if mask . dtype != np . bool : raise ValueError ( "mask must be a boolean array" ) if arr . ndim != 2 : raise ValueError ( "X must be a 2-dimensional array" ) if mask . sum ( ) != arr . shape [ 0 ] : raise ValueError ( 'Expected arr of shape ({}, samples). Got {}.' . format ( mask . sum ( ) , arr . shape ) ) data = np . zeros ( mask . shape + ( arr . shape [ 1 ] , ) , dtype = arr . dtype , order = order ) data [ mask , : ] = arr return data
890	def _growSynapses ( cls , connections , random , segment , nDesiredNewSynapes , prevWinnerCells , initialPermanence , maxSynapsesPerSegment ) : candidates = list ( prevWinnerCells ) for synapse in connections . synapsesForSegment ( segment ) : i = binSearch ( candidates , synapse . presynapticCell ) if i != - 1 : del candidates [ i ] nActual = min ( nDesiredNewSynapes , len ( candidates ) ) overrun = connections . numSynapses ( segment ) + nActual - maxSynapsesPerSegment if overrun > 0 : cls . _destroyMinPermanenceSynapses ( connections , random , segment , overrun , prevWinnerCells ) nActual = min ( nActual , maxSynapsesPerSegment - connections . numSynapses ( segment ) ) for _ in range ( nActual ) : i = random . getUInt32 ( len ( candidates ) ) connections . createSynapse ( segment , candidates [ i ] , initialPermanence ) del candidates [ i ]
8825	def update_sg ( self , context , sg , rule_id , action ) : db_sg = db_api . security_group_find ( context , id = sg , scope = db_api . ONE ) if not db_sg : return None with context . session . begin ( ) : job_body = dict ( action = "%s sg rule %s" % ( action , rule_id ) , resource_id = rule_id , tenant_id = db_sg [ 'tenant_id' ] ) job_body = dict ( job = job_body ) job = job_api . create_job ( context . elevated ( ) , job_body ) rpc_client = QuarkSGAsyncProducerClient ( ) try : rpc_client . populate_subtasks ( context , sg , job [ 'id' ] ) except om_exc . MessagingTimeout : LOG . error ( "Failed to create subtasks. Rabbit running?" ) return None return { "job_id" : job [ 'id' ] }
12481	def get_rcfile_variable_value ( var_name , app_name , section_name = None ) : cfg = get_rcfile_section ( app_name , section_name ) if var_name in cfg : raise KeyError ( 'Option {} not found in {} ' 'section.' . format ( var_name , section_name ) ) return cfg [ var_name ]
1134	def updatecache ( filename , module_globals = None ) : if filename in cache : del cache [ filename ] if not filename or ( filename . startswith ( '<' ) and filename . endswith ( '>' ) ) : return [ ] fullname = filename try : stat = os . stat ( fullname ) except OSError : basename = filename if module_globals and '__loader__' in module_globals : name = module_globals . get ( '__name__' ) loader = module_globals [ '__loader__' ] get_source = getattr ( loader , 'get_source' , None ) if name and get_source : try : data = get_source ( name ) except ( ImportError , IOError ) : pass else : if data is None : return [ ] cache [ filename ] = ( len ( data ) , None , [ line + '\n' for line in data . splitlines ( ) ] , fullname ) return cache [ filename ] [ 2 ] if os . path . isabs ( filename ) : return [ ] for dirname in sys . path : try : fullname = os . path . join ( dirname , basename ) except ( TypeError , AttributeError ) : continue try : stat = os . stat ( fullname ) break except os . error : pass else : return [ ] try : with open ( fullname , 'rU' ) as fp : lines = fp . readlines ( ) except IOError : return [ ] if lines and not lines [ - 1 ] . endswith ( '\n' ) : lines [ - 1 ] += '\n' size , mtime = stat . st_size , stat . st_mtime cache [ filename ] = size , mtime , lines , fullname return lines
5532	def batch_process ( self , zoom = None , tile = None , multi = cpu_count ( ) , max_chunksize = 1 ) : list ( self . batch_processor ( zoom , tile , multi , max_chunksize ) )
4218	def _get_env ( self , env_var ) : value = os . environ . get ( env_var ) if not value : raise ValueError ( 'Missing environment variable:%s' % env_var ) return value
7742	def _prepare_pending ( self ) : if not self . _unprepared_pending : return for handler in list ( self . _unprepared_pending ) : self . _configure_io_handler ( handler ) self . check_events ( )
2476	def set_lic_comment ( self , doc , comment ) : if self . has_extr_lic ( doc ) : if not self . extr_lic_comment_set : self . extr_lic_comment_set = True if validations . validate_is_free_form_text ( comment ) : self . extr_lic ( doc ) . comment = str_from_text ( comment ) return True else : raise SPDXValueError ( 'ExtractedLicense::comment' ) else : raise CardinalityError ( 'ExtractedLicense::comment' ) else : raise OrderError ( 'ExtractedLicense::comment' )
5427	def _wait_for_any_job ( provider , job_ids , poll_interval ) : if not job_ids : return while True : tasks = provider . lookup_job_tasks ( { '*' } , job_ids = job_ids ) running_jobs = set ( ) failed_jobs = set ( ) for t in tasks : status = t . get_field ( 'task-status' ) job_id = t . get_field ( 'job-id' ) if status in [ 'FAILURE' , 'CANCELED' ] : failed_jobs . add ( job_id ) if status == 'RUNNING' : running_jobs . add ( job_id ) remaining_jobs = running_jobs . difference ( failed_jobs ) if failed_jobs or len ( remaining_jobs ) != len ( job_ids ) : return remaining_jobs SLEEP_FUNCTION ( poll_interval )
1254	def setup_saver ( self ) : if self . execution_type == "single" : global_variables = self . get_variables ( include_submodules = True , include_nontrainable = True ) else : global_variables = self . global_model . get_variables ( include_submodules = True , include_nontrainable = True ) for c in self . get_savable_components ( ) : c . register_saver_ops ( ) self . saver = tf . train . Saver ( var_list = global_variables , reshape = False , sharded = False , max_to_keep = 5 , keep_checkpoint_every_n_hours = 10000.0 , name = None , restore_sequentially = False , saver_def = None , builder = None , defer_build = False , allow_empty = True , write_version = tf . train . SaverDef . V2 , pad_step_number = False , save_relative_paths = True )
13147	def shrink_indexes_in_place ( self , triples ) : _ent_roots = self . UnionFind ( self . _ent_id ) _rel_roots = self . UnionFind ( self . _rel_id ) for t in triples : _ent_roots . add ( t . head ) _ent_roots . add ( t . tail ) _rel_roots . add ( t . relation ) for i , t in enumerate ( triples ) : h = _ent_roots . find ( t . head ) r = _rel_roots . find ( t . relation ) t = _ent_roots . find ( t . tail ) triples [ i ] = kgedata . TripleIndex ( h , r , t ) ents = bidict ( ) available_ent_idx = 0 for previous_idx , ent_exist in enumerate ( _ent_roots . roots ( ) ) : if not ent_exist : self . _ents . inverse . pop ( previous_idx ) else : ents [ self . _ents . inverse [ previous_idx ] ] = available_ent_idx available_ent_idx += 1 rels = bidict ( ) available_rel_idx = 0 for previous_idx , rel_exist in enumerate ( _rel_roots . roots ( ) ) : if not rel_exist : self . _rels . inverse . pop ( previous_idx ) else : rels [ self . _rels . inverse [ previous_idx ] ] = available_rel_idx available_rel_idx += 1 self . _ents = ents self . _rels = rels self . _ent_id = available_ent_idx self . _rel_id = available_rel_idx
2560	def heartbeat ( self ) : heartbeat = ( HEARTBEAT_CODE ) . to_bytes ( 4 , "little" ) r = self . task_incoming . send ( heartbeat ) logger . debug ( "Return from heartbeat : {}" . format ( r ) )
7550	def _debug_off ( ) : if _os . path . exists ( __debugflag__ ) : _os . remove ( __debugflag__ ) __loglevel__ = "ERROR" _LOGGER . info ( "debugging turned off" ) _set_debug_dict ( __loglevel__ )
10638	def get_element_mfr ( self , element ) : result = 0.0 for compound in self . material . compounds : formula = compound . split ( '[' ) [ 0 ] result += self . get_compound_mfr ( compound ) * stoich . element_mass_fraction ( formula , element ) return result
12137	def _load_expansion ( self , key , root , pattern ) : path_pattern = os . path . join ( root , pattern ) expanded_paths = self . _expand_pattern ( path_pattern ) specs = [ ] for ( path , tags ) in expanded_paths : filelist = [ os . path . join ( path , f ) for f in os . listdir ( path ) ] if os . path . isdir ( path ) else [ path ] for filepath in filelist : specs . append ( dict ( tags , ** { key : os . path . abspath ( filepath ) } ) ) return sorted ( specs , key = lambda s : s [ key ] )
11767	def weighted_sample_with_replacement ( seq , weights , n ) : sample = weighted_sampler ( seq , weights ) return [ sample ( ) for s in range ( n ) ]
9134	def get_module_config_cls ( module_name : str ) -> Type [ _AbstractModuleConfig ] : class ModuleConfig ( _AbstractModuleConfig ) : NAME = f'bio2bel:{module_name}' FILES = DEFAULT_CONFIG_PATHS + [ os . path . join ( DEFAULT_CONFIG_DIRECTORY , module_name , 'config.ini' ) ] return ModuleConfig
6778	def get_component_order ( component_names ) : assert isinstance ( component_names , ( tuple , list ) ) component_dependences = { } for _name in component_names : deps = set ( manifest_deployers_befores . get ( _name , [ ] ) ) deps = deps . intersection ( component_names ) component_dependences [ _name ] = deps component_order = list ( topological_sort ( component_dependences . items ( ) ) ) return component_order
13428	def get_site ( self , site_id ) : url = "/2/sites/%s" % site_id return self . site_from_json ( self . _get_resource ( url ) [ "site" ] )
12769	def load_markers ( self , filename , attachments , max_frames = 1e100 ) : self . markers = Markers ( self ) fn = filename . lower ( ) if fn . endswith ( '.c3d' ) : self . markers . load_c3d ( filename , max_frames = max_frames ) elif fn . endswith ( '.csv' ) or fn . endswith ( '.csv.gz' ) : self . markers . load_csv ( filename , max_frames = max_frames ) else : logging . fatal ( '%s: not sure how to load markers!' , filename ) self . markers . load_attachments ( attachments , self . skeleton )
4227	def _check_old_config_root ( ) : globals ( ) [ '_check_old_config_root' ] = lambda : None config_file_new = os . path . join ( _config_root_Linux ( ) , 'keyringrc.cfg' ) config_file_old = os . path . join ( _data_root_Linux ( ) , 'keyringrc.cfg' ) if os . path . isfile ( config_file_old ) and not os . path . isfile ( config_file_new ) : msg = ( "Keyring config exists only in the old location " "{config_file_old} and should be moved to {config_file_new} " "to work with this version of keyring." ) raise RuntimeError ( msg . format ( ** locals ( ) ) )
7684	def mkclick ( freq , sr = 22050 , duration = 0.1 ) : times = np . arange ( int ( sr * duration ) ) click = np . sin ( 2 * np . pi * times * freq / float ( sr ) ) click *= np . exp ( - times / ( 1e-2 * sr ) ) return click
6664	def list_expiration_dates ( self , base = 'roles/all/ssl' ) : max_fn_len = 0 max_date_len = 0 data = [ ] for fn in os . listdir ( base ) : fqfn = os . path . join ( base , fn ) if not os . path . isfile ( fqfn ) : continue if not fn . endswith ( '.crt' ) : continue expiration_date = self . get_expiration_date ( fqfn ) max_fn_len = max ( max_fn_len , len ( fn ) ) max_date_len = max ( max_date_len , len ( str ( expiration_date ) ) ) data . append ( ( fn , expiration_date ) ) print ( '%s %s %s' % ( 'Filename' . ljust ( max_fn_len ) , 'Expiration Date' . ljust ( max_date_len ) , 'Expired' ) ) now = datetime . now ( ) . replace ( tzinfo = pytz . UTC ) for fn , dt in sorted ( data ) : if dt is None : expired = '?' elif dt < now : expired = 'YES' else : expired = 'NO' print ( '%s %s %s' % ( fn . ljust ( max_fn_len ) , str ( dt ) . ljust ( max_date_len ) , expired ) )
10813	def search ( cls , query , q ) : return query . filter ( Group . name . like ( '%{0}%' . format ( q ) ) )
5702	def route_frequencies ( gtfs , results_by_mode = False ) : day = gtfs . get_suitable_date_for_daily_extract ( ) query = ( " SELECT f.route_I, type, frequency FROM routes as r" " JOIN" " (SELECT route_I, COUNT(route_I) as frequency" " FROM" " (SELECT date, route_I, trip_I" " FROM day_stop_times" " WHERE date = '{day}'" " GROUP by route_I, trip_I)" " GROUP BY route_I) as f" " ON f.route_I = r.route_I" " ORDER BY frequency DESC" . format ( day = day ) ) return pd . DataFrame ( gtfs . execute_custom_query_pandas ( query ) )
5742	def service_start ( service = None , param = None ) : if service is not None : to_run = [ "python" , service ] if param is not None : to_run += param return subprocess . Popen ( to_run ) return False
9865	def currency ( self ) : try : current_subscription = self . info [ "viewer" ] [ "home" ] [ "currentSubscription" ] return current_subscription [ "priceInfo" ] [ "current" ] [ "currency" ] except ( KeyError , TypeError , IndexError ) : _LOGGER . error ( "Could not find currency." ) return ""
6127	def contained_in ( filename , directory ) : filename = os . path . normcase ( os . path . abspath ( filename ) ) directory = os . path . normcase ( os . path . abspath ( directory ) ) return os . path . commonprefix ( [ filename , directory ] ) == directory
3934	def _get_session_cookies ( session , access_token ) : headers = { 'Authorization' : 'Bearer {}' . format ( access_token ) } try : r = session . get ( ( 'https://accounts.google.com/accounts/OAuthLogin' '?source=hangups&issueuberauth=1' ) , headers = headers ) r . raise_for_status ( ) except requests . RequestException as e : raise GoogleAuthError ( 'OAuthLogin request failed: {}' . format ( e ) ) uberauth = r . text try : r = session . get ( ( 'https://accounts.google.com/MergeSession?' 'service=mail&' 'continue=http://www.google.com&uberauth={}' ) . format ( uberauth ) , headers = headers ) r . raise_for_status ( ) except requests . RequestException as e : raise GoogleAuthError ( 'MergeSession request failed: {}' . format ( e ) ) cookies = session . cookies . get_dict ( domain = '.google.com' ) if cookies == { } : raise GoogleAuthError ( 'Failed to find session cookies' ) return cookies
13125	def id_to_object ( self , line ) : user = User . get ( line , ignore = 404 ) if not user : user = User ( username = line ) user . save ( ) return user
4796	def contains_entry ( self , * args , ** kwargs ) : self . _check_dict_like ( self . val , check_values = False ) entries = list ( args ) + [ { k : v } for k , v in kwargs . items ( ) ] if len ( entries ) == 0 : raise ValueError ( 'one or more entry args must be given' ) missing = [ ] for e in entries : if type ( e ) is not dict : raise TypeError ( 'given entry arg must be a dict' ) if len ( e ) != 1 : raise ValueError ( 'given entry args must contain exactly one key-value pair' ) k = next ( iter ( e ) ) if k not in self . val : missing . append ( e ) elif self . val [ k ] != e [ k ] : missing . append ( e ) if missing : self . _err ( 'Expected <%s> to contain entries %s, but did not contain %s.' % ( self . val , self . _fmt_items ( entries ) , self . _fmt_items ( missing ) ) ) return self
5565	def baselevels ( self ) : if "baselevels" not in self . _raw : return { } baselevels = self . _raw [ "baselevels" ] minmax = { k : v for k , v in baselevels . items ( ) if k in [ "min" , "max" ] } if not minmax : raise MapcheteConfigError ( "no min and max values given for baselevels" ) for v in minmax . values ( ) : if not isinstance ( v , int ) or v < 0 : raise MapcheteConfigError ( "invalid baselevel zoom parameter given: %s" % minmax . values ( ) ) zooms = list ( range ( minmax . get ( "min" , min ( self . zoom_levels ) ) , minmax . get ( "max" , max ( self . zoom_levels ) ) + 1 ) ) if not set ( self . zoom_levels ) . difference ( set ( zooms ) ) : raise MapcheteConfigError ( "baselevels zooms fully cover process zooms" ) return dict ( zooms = zooms , lower = baselevels . get ( "lower" , "nearest" ) , higher = baselevels . get ( "higher" , "nearest" ) , tile_pyramid = BufferedTilePyramid ( self . output_pyramid . grid , pixelbuffer = self . output_pyramid . pixelbuffer , metatiling = self . process_pyramid . metatiling ) )
9604	def raise_for_status ( self ) : if not self . status : return error = find_exception_by_code ( self . status ) message = None screen = None stacktrace = None if isinstance ( self . value , str ) : message = self . value elif isinstance ( self . value , dict ) : message = self . value . get ( 'message' , None ) screen = self . value . get ( 'screen' , None ) stacktrace = self . value . get ( 'stacktrace' , None ) raise WebDriverException ( error , message , screen , stacktrace )
8718	def file_remove ( self , path ) : log . info ( 'Remove ' + path ) cmd = 'file.remove("%s")' % path res = self . __exchange ( cmd ) log . info ( res ) return res
13432	def admin_link_move_down ( obj , link_text = 'down' ) : if obj . rank == obj . grouped_filter ( ) . count ( ) : return '' content_type = ContentType . objects . get_for_model ( obj ) link = reverse ( 'awl-rankedmodel-move' , args = ( content_type . id , obj . id , obj . rank + 1 ) ) return '<a href="%s">%s</a>' % ( link , link_text )
1600	def str_cmd ( cmd , cwd , env ) : process = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . PIPE , cwd = cwd , env = env ) stdout_builder , stderr_builder = proc . async_stdout_stderr_builder ( process ) process . wait ( ) stdout , stderr = stdout_builder . result ( ) , stderr_builder . result ( ) return { 'command' : ' ' . join ( cmd ) , 'stderr' : stderr , 'stdout' : stdout }
5766	def _setup_evp_encrypt_decrypt ( cipher , data ) : evp_cipher = { 'aes128' : libcrypto . EVP_aes_128_cbc , 'aes192' : libcrypto . EVP_aes_192_cbc , 'aes256' : libcrypto . EVP_aes_256_cbc , 'rc2' : libcrypto . EVP_rc2_cbc , 'rc4' : libcrypto . EVP_rc4 , 'des' : libcrypto . EVP_des_cbc , 'tripledes_2key' : libcrypto . EVP_des_ede_cbc , 'tripledes_3key' : libcrypto . EVP_des_ede3_cbc , } [ cipher ] ( ) if cipher == 'rc4' : buffer_size = len ( data ) else : block_size = { 'aes128' : 16 , 'aes192' : 16 , 'aes256' : 16 , 'rc2' : 8 , 'des' : 8 , 'tripledes_2key' : 8 , 'tripledes_3key' : 8 , } [ cipher ] buffer_size = block_size * int ( math . ceil ( len ( data ) / block_size ) ) return ( evp_cipher , buffer_size )
1725	def eval ( self , expression , use_compilation_plan = False ) : code = 'PyJsEvalResult = eval(%s)' % json . dumps ( expression ) self . execute ( code , use_compilation_plan = use_compilation_plan ) return self [ 'PyJsEvalResult' ]
9386	def parse ( self ) : for infile in self . infile_list : logger . info ( 'Processing : %s' , infile ) status = True file_status = naarad . utils . is_valid_file ( infile ) if not file_status : return False with open ( infile ) as fh : for line in fh : words = line . split ( ) if not words : continue if re . match ( '^\d\d\d\d-\d\d-\d\d$' , line ) : self . ts_date = words [ 0 ] continue prefix_word = words [ 0 ] . strip ( ) if prefix_word == 'top' : self . process_top_line ( words ) self . saw_pid = False elif self . ts_valid_lines : if prefix_word == 'Tasks:' : self . process_tasks_line ( words ) elif prefix_word == 'Cpu(s):' : self . process_cpu_line ( words ) elif prefix_word == 'Mem:' : self . process_mem_line ( words ) elif prefix_word == 'Swap:' : self . process_swap_line ( words ) elif prefix_word == 'PID' : self . saw_pid = True self . process_headers = words else : if self . saw_pid and len ( words ) >= len ( self . process_headers ) : self . process_individual_command ( words ) for out_csv in self . data . keys ( ) : self . csv_files . append ( out_csv ) with open ( out_csv , 'w' ) as fh : fh . write ( '\n' . join ( self . data [ out_csv ] ) ) gc . collect ( ) return status
9467	def conference_play ( self , call_params ) : path = '/' + self . api_version + '/ConferencePlay/' method = 'POST' return self . request ( path , method , call_params )
9743	async def reboot ( ip_address ) : _ , protocol = await asyncio . get_event_loop ( ) . create_datagram_endpoint ( QRebootProtocol , local_addr = ( ip_address , 0 ) , allow_broadcast = True , reuse_address = True , ) LOG . info ( "Sending reboot on %s" , ip_address ) protocol . send_reboot ( )
12583	def _safe_cache ( memory , func , ** kwargs ) : cachedir = memory . cachedir if cachedir is None or cachedir in __CACHE_CHECKED : return memory . cache ( func , ** kwargs ) version_file = os . path . join ( cachedir , 'module_versions.json' ) versions = dict ( ) if os . path . exists ( version_file ) : with open ( version_file , 'r' ) as _version_file : versions = json . load ( _version_file ) modules = ( nibabel , ) my_versions = dict ( ( m . __name__ , LooseVersion ( m . __version__ ) . version [ : 2 ] ) for m in modules ) commons = set ( versions . keys ( ) ) . intersection ( set ( my_versions . keys ( ) ) ) collisions = [ m for m in commons if versions [ m ] != my_versions [ m ] ] if len ( collisions ) > 0 : if nilearn . CHECK_CACHE_VERSION : warnings . warn ( "Incompatible cache in %s: " "different version of nibabel. Deleting " "the cache. Put nilearn.CHECK_CACHE_VERSION " "to false to avoid this behavior." % cachedir ) try : tmp_dir = ( os . path . split ( cachedir ) [ : - 1 ] + ( 'old_%i' % os . getpid ( ) , ) ) tmp_dir = os . path . join ( * tmp_dir ) os . rename ( cachedir , tmp_dir ) shutil . rmtree ( tmp_dir ) except OSError : pass try : os . makedirs ( cachedir ) except OSError : pass else : warnings . warn ( "Incompatible cache in %s: " "old version of nibabel." % cachedir ) if versions != my_versions : with open ( version_file , 'w' ) as _version_file : json . dump ( my_versions , _version_file ) __CACHE_CHECKED [ cachedir ] = True return memory . cache ( func , ** kwargs )
2056	def CBZ ( cpu , op , dest ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , op . read ( ) , cpu . PC , dest . read ( ) )
8543	def _get_password ( self , password , use_config = True , config_filename = None , use_keyring = HAS_KEYRING ) : if not password and use_config : if self . _config is None : self . _read_config ( config_filename ) password = self . _config . get ( "credentials" , "password" , fallback = None ) if not password and use_keyring : logger = logging . getLogger ( __name__ ) question = ( "Please enter your password for {} on {}: " . format ( self . username , self . host_base ) ) if HAS_KEYRING : password = keyring . get_password ( self . keyring_identificator , self . username ) if password is None : password = getpass . getpass ( question ) try : keyring . set_password ( self . keyring_identificator , self . username , password ) except keyring . errors . PasswordSetError as error : logger . warning ( "Storing password in keyring '%s' failed: %s" , self . keyring_identificator , error ) else : logger . warning ( "Install the 'keyring' Python module to store your password " "securely in your keyring!" ) password = self . _config . get ( "credentials" , "password" , fallback = None ) if password is None : password = getpass . getpass ( question ) store_plaintext_passwords = self . _config . get ( "preferences" , "store-plaintext-passwords" , fallback = None ) if store_plaintext_passwords != "no" : question = ( "Do you want to store your password in plain text in " + self . _config_filename ( ) ) answer = ask ( question , [ "yes" , "no" , "never" ] , "no" ) if answer == "yes" : self . _config . set ( "credentials" , "password" , password ) self . _save_config ( ) elif answer == "never" : if "preferences" not in self . _config : self . _config . add_section ( "preferences" ) self . _config . set ( "preferences" , "store-plaintext-passwords" , "no" ) self . _save_config ( ) return password
3	def make_vec_env ( env_id , env_type , num_env , seed , wrapper_kwargs = None , start_index = 0 , reward_scale = 1.0 , flatten_dict_observations = True , gamestate = None ) : wrapper_kwargs = wrapper_kwargs or { } mpi_rank = MPI . COMM_WORLD . Get_rank ( ) if MPI else 0 seed = seed + 10000 * mpi_rank if seed is not None else None logger_dir = logger . get_dir ( ) def make_thunk ( rank ) : return lambda : make_env ( env_id = env_id , env_type = env_type , mpi_rank = mpi_rank , subrank = rank , seed = seed , reward_scale = reward_scale , gamestate = gamestate , flatten_dict_observations = flatten_dict_observations , wrapper_kwargs = wrapper_kwargs , logger_dir = logger_dir ) set_global_seeds ( seed ) if num_env > 1 : return SubprocVecEnv ( [ make_thunk ( i + start_index ) for i in range ( num_env ) ] ) else : return DummyVecEnv ( [ make_thunk ( start_index ) ] )
1442	def next_tuple ( self , latency_in_ns ) : self . update_reduced_metric ( self . NEXT_TUPLE_LATENCY , latency_in_ns ) self . update_count ( self . NEXT_TUPLE_COUNT )
11237	def sendreturn ( gen , value ) : try : gen . send ( value ) except StopIteration as e : return stopiter_value ( e ) else : raise RuntimeError ( 'generator did not return as expected' )
9735	def get_3d_markers ( self , component_info = None , data = None , component_position = None ) : return self . _get_3d_markers ( RT3DMarkerPosition , component_info , data , component_position )
1788	def DAS ( cpu ) : oldAL = cpu . AL oldCF = cpu . CF cpu . AF = Operators . OR ( ( cpu . AL & 0x0f ) > 9 , cpu . AF ) cpu . AL = Operators . ITEBV ( 8 , cpu . AF , cpu . AL - 6 , cpu . AL ) cpu . CF = Operators . ITE ( cpu . AF , Operators . OR ( oldCF , cpu . AL > oldAL ) , cpu . CF ) cpu . CF = Operators . ITE ( Operators . OR ( oldAL > 0x99 , oldCF ) , True , cpu . CF ) cpu . AL = Operators . ITEBV ( 8 , Operators . OR ( oldAL > 0x99 , oldCF ) , cpu . AL - 0x60 , cpu . AL ) cpu . ZF = cpu . AL == 0 cpu . SF = ( cpu . AL & 0x80 ) != 0 cpu . PF = cpu . _calculate_parity_flag ( cpu . AL )
7607	def search_tournaments ( self , name : str , ** params : keys ) : url = self . api . TOURNAMENT params [ 'name' ] = name return self . _get_model ( url , PartialTournament , ** params )
10868	def j2 ( x ) : to_return = 2. / ( x + 1e-15 ) * j1 ( x ) - j0 ( x ) to_return [ x == 0 ] = 0 return to_return
3211	def insert ( self , key , obj , future_expiration_minutes = 15 ) : expiration_time = self . _calculate_expiration ( future_expiration_minutes ) self . _CACHE [ key ] = ( expiration_time , obj ) return True
12550	def write_meta_header ( filename , meta_dict ) : header = '' for tag in MHD_TAGS : if tag in meta_dict . keys ( ) : header += '{} = {}\n' . format ( tag , meta_dict [ tag ] ) with open ( filename , 'w' ) as f : f . write ( header )
6310	def load ( self ) : self . _open_image ( ) components , data = image_data ( self . image ) texture = self . ctx . texture ( self . image . size , components , data , ) texture . extra = { 'meta' : self . meta } if self . meta . mipmap : texture . build_mipmaps ( ) self . _close_image ( ) return texture
10484	def _matchOther ( self , obj , ** kwargs ) : if obj is not None : if self . _findFirstR ( ** kwargs ) : return obj . _match ( ** kwargs ) return False
8359	def scale_context_and_center ( self , cr ) : bot_width , bot_height = self . bot_size if self . width != bot_width or self . height != bot_height : if self . width < self . height : scale_x = float ( self . width ) / float ( bot_width ) scale_y = scale_x cr . translate ( 0 , ( self . height - ( bot_height * scale_y ) ) / 2.0 ) elif self . width > self . height : scale_y = float ( self . height ) / float ( bot_height ) scale_x = scale_y cr . translate ( ( self . width - ( bot_width * scale_x ) ) / 2.0 , 0 ) else : scale_x = 1.0 scale_y = 1.0 cr . scale ( scale_x , scale_y ) self . input_device . scale_x = scale_y self . input_device . scale_y = scale_y
92	def _quokka_normalize_extract ( extract ) : from imgaug . augmentables . bbs import BoundingBox , BoundingBoxesOnImage if extract == "square" : bb = BoundingBox ( x1 = 0 , y1 = 0 , x2 = 643 , y2 = 643 ) elif isinstance ( extract , tuple ) and len ( extract ) == 4 : bb = BoundingBox ( x1 = extract [ 0 ] , y1 = extract [ 1 ] , x2 = extract [ 2 ] , y2 = extract [ 3 ] ) elif isinstance ( extract , BoundingBox ) : bb = extract elif isinstance ( extract , BoundingBoxesOnImage ) : do_assert ( len ( extract . bounding_boxes ) == 1 ) do_assert ( extract . shape [ 0 : 2 ] == ( 643 , 960 ) ) bb = extract . bounding_boxes [ 0 ] else : raise Exception ( "Expected 'square' or tuple of four entries or BoundingBox or BoundingBoxesOnImage " + "for parameter 'extract', got %s." % ( type ( extract ) , ) ) return bb
5593	def tiles_from_geom ( self , geometry , zoom ) : for tile in self . tile_pyramid . tiles_from_geom ( geometry , zoom ) : yield self . tile ( * tile . id )
434	def CNN2d ( CNN = None , second = 10 , saveable = True , name = 'cnn' , fig_idx = 3119362 ) : import matplotlib . pyplot as plt n_mask = CNN . shape [ 3 ] n_row = CNN . shape [ 0 ] n_col = CNN . shape [ 1 ] n_color = CNN . shape [ 2 ] row = int ( np . sqrt ( n_mask ) ) col = int ( np . ceil ( n_mask / row ) ) plt . ion ( ) fig = plt . figure ( fig_idx ) count = 1 for _ir in range ( 1 , row + 1 ) : for _ic in range ( 1 , col + 1 ) : if count > n_mask : break fig . add_subplot ( col , row , count ) if n_color == 1 : plt . imshow ( np . reshape ( CNN [ : , : , : , count - 1 ] , ( n_row , n_col ) ) , cmap = 'gray' , interpolation = "nearest" ) elif n_color == 3 : plt . imshow ( np . reshape ( CNN [ : , : , : , count - 1 ] , ( n_row , n_col , n_color ) ) , cmap = 'gray' , interpolation = "nearest" ) else : raise Exception ( "Unknown n_color" ) plt . gca ( ) . xaxis . set_major_locator ( plt . NullLocator ( ) ) plt . gca ( ) . yaxis . set_major_locator ( plt . NullLocator ( ) ) count = count + 1 if saveable : plt . savefig ( name + '.pdf' , format = 'pdf' ) else : plt . draw ( ) plt . pause ( second )
7894	def send_message ( self , body ) : m = Message ( to_jid = self . room_jid . bare ( ) , stanza_type = "groupchat" , body = body ) self . manager . stream . send ( m )
6994	def shutdown_check_handler ( ) : url = 'http://169.254.169.254/latest/meta-data/spot/instance-action' try : resp = requests . get ( url , timeout = 1.0 ) resp . raise_for_status ( ) stopinfo = resp . json ( ) if 'action' in stopinfo and stopinfo [ 'action' ] in ( 'stop' , 'terminate' , 'hibernate' ) : stoptime = stopinfo [ 'time' ] LOGWARNING ( 'instance is going to %s at %s' % ( stopinfo [ 'action' ] , stoptime ) ) resp . close ( ) return True else : resp . close ( ) return False except HTTPError as e : resp . close ( ) return False except Exception as e : resp . close ( ) return False
5739	def enqueue_task ( self , task ) : data = dumps ( task ) if self . _async : self . publisher_client . publish ( self . topic_path , data = data ) logger . info ( 'Task {} queued.' . format ( task . id ) ) else : unpickled_task = unpickle ( data ) logger . info ( 'Executing task {} synchronously.' . format ( unpickled_task . id ) ) with measure_time ( ) as summary , self . queue_context ( ) : unpickled_task . execute ( queue = self ) summary ( unpickled_task . summary ( ) ) return TaskResult ( task . id , self )
9326	def refresh ( self ) : response = self . __raw = self . _conn . get ( self . url ) self . _populate_fields ( ** response ) self . _loaded = True
3322	def clear ( self ) : self . _lock . acquire_write ( ) try : was_closed = self . _dict is None if was_closed : self . open ( ) if len ( self . _dict ) : self . _dict . clear ( ) self . _dict . sync ( ) if was_closed : self . close ( ) finally : self . _lock . release ( )
3459	def _multi_deletion ( model , entity , element_lists , method = "fba" , solution = None , processes = None , ** kwargs ) : solver = sutil . interface_to_str ( model . problem . __name__ ) if method == "moma" and solver not in sutil . qp_solvers : raise RuntimeError ( "Cannot use MOMA since '{}' is not QP-capable." "Please choose a different solver or use FBA only." . format ( solver ) ) if processes is None : processes = CONFIGURATION . processes with model : if "moma" in method : add_moma ( model , solution = solution , linear = "linear" in method ) elif "room" in method : add_room ( model , solution = solution , linear = "linear" in method , ** kwargs ) args = set ( [ frozenset ( comb ) for comb in product ( * element_lists ) ] ) processes = min ( processes , len ( args ) ) def extract_knockout_results ( result_iter ) : result = pd . DataFrame ( [ ( frozenset ( ids ) , growth , status ) for ( ids , growth , status ) in result_iter ] , columns = [ 'ids' , 'growth' , 'status' ] ) result . set_index ( 'ids' , inplace = True ) return result if processes > 1 : worker = dict ( gene = _gene_deletion_worker , reaction = _reaction_deletion_worker ) [ entity ] chunk_size = len ( args ) // processes pool = multiprocessing . Pool ( processes , initializer = _init_worker , initargs = ( model , ) ) results = extract_knockout_results ( pool . imap_unordered ( worker , args , chunksize = chunk_size ) ) pool . close ( ) pool . join ( ) else : worker = dict ( gene = _gene_deletion , reaction = _reaction_deletion ) [ entity ] results = extract_knockout_results ( map ( partial ( worker , model ) , args ) ) return results
2370	def dump ( self ) : for table in self . tables : print ( "*** %s ***" % table . name ) table . dump ( )
5185	def catalog ( self , node ) : catalogs = self . catalogs ( path = node ) return next ( x for x in catalogs )
503	def _labelListToCategoryNumber ( self , labelList ) : categoryNumber = 0 for label in labelList : categoryNumber += self . _labelToCategoryNumber ( label ) return categoryNumber
13129	def initialize_indices ( ) : Host . init ( ) Range . init ( ) Service . init ( ) User . init ( ) Credential . init ( ) Log . init ( )
5014	def filter_queryset ( self , request , queryset , view ) : if not request . user . is_staff : filter_kwargs = { view . USER_ID_FILTER : request . user . id } queryset = queryset . filter ( ** filter_kwargs ) return queryset
7820	def flush ( self , dispatch = True ) : if dispatch : while True : event = self . dispatch ( False ) if event in ( None , QUIT ) : return event else : while True : try : self . queue . get ( False ) except Queue . Empty : return None
2250	def highlight_code ( text , lexer_name = 'python' , ** kwargs ) : lexer_name = { 'py' : 'python' , 'h' : 'cpp' , 'cpp' : 'cpp' , 'cxx' : 'cpp' , 'c' : 'cpp' , } . get ( lexer_name . replace ( '.' , '' ) , lexer_name ) try : import pygments import pygments . lexers import pygments . formatters import pygments . formatters . terminal if sys . platform . startswith ( 'win32' ) : import colorama colorama . init ( ) formater = pygments . formatters . terminal . TerminalFormatter ( bg = 'dark' ) lexer = pygments . lexers . get_lexer_by_name ( lexer_name , ** kwargs ) new_text = pygments . highlight ( text , lexer , formater ) except ImportError : import warnings warnings . warn ( 'pygments is not installed, code will not be highlighted' ) new_text = text return new_text
6904	def hms_to_decimal ( hours , minutes , seconds , returndeg = True ) : if hours > 24 : return None else : dec_hours = fabs ( hours ) + fabs ( minutes ) / 60.0 + fabs ( seconds ) / 3600.0 if returndeg : dec_deg = dec_hours * 15.0 if dec_deg < 0 : dec_deg = dec_deg + 360.0 dec_deg = dec_deg % 360.0 return dec_deg else : return dec_hours
5476	def get_zones ( input_list ) : if not input_list : return [ ] output_list = [ ] for zone in input_list : if zone . endswith ( '*' ) : prefix = zone [ : - 1 ] output_list . extend ( [ z for z in _ZONES if z . startswith ( prefix ) ] ) else : output_list . append ( zone ) return output_list
2331	def normal_noise ( points ) : return np . random . rand ( 1 ) * np . random . randn ( points , 1 ) + random . sample ( [ 2 , - 2 ] , 1 )
1355	def get_argument_role ( self ) : try : return self . get_argument ( constants . PARAM_ROLE , default = None ) except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
3670	def Wilson ( xs , params ) : r gammas = [ ] cmps = range ( len ( xs ) ) for i in cmps : tot1 = log ( sum ( [ params [ i ] [ j ] * xs [ j ] for j in cmps ] ) ) tot2 = 0. for j in cmps : tot2 += params [ j ] [ i ] * xs [ j ] / sum ( [ params [ j ] [ k ] * xs [ k ] for k in cmps ] ) gamma = exp ( 1. - tot1 - tot2 ) gammas . append ( gamma ) return gammas
8643	def post_track ( session , user_id , project_id , latitude , longitude ) : tracking_data = { 'user_id' : user_id , 'project_id' : project_id , 'track_point' : { 'latitude' : latitude , 'longitude' : longitude } } response = make_post_request ( session , 'tracks' , json_data = tracking_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise TrackNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
5462	def _emit_search_criteria ( user_ids , job_ids , task_ids , labels ) : print ( 'Delete running jobs:' ) print ( ' user:' ) print ( ' %s\n' % user_ids ) print ( ' job-id:' ) print ( ' %s\n' % job_ids ) if task_ids : print ( ' task-id:' ) print ( ' %s\n' % task_ids ) if labels : print ( ' labels:' ) print ( ' %s\n' % repr ( labels ) )
7163	def answer_display ( self , s = '' ) : padding = len ( max ( self . questions . keys ( ) , key = len ) ) + 5 for key in list ( self . answers . keys ( ) ) : s += '{:>{}} : {}\n' . format ( key , padding , self . answers [ key ] ) return s
13300	def upgrade ( self , package ) : logger . debug ( 'Upgrading ' + package ) shell . run ( self . pip_path , 'install' , '--upgrade' , '--no-deps' , package ) shell . run ( self . pip_path , 'install' , package )
10375	def calculate_concordance_helper ( graph : BELGraph , key : str , cutoff : Optional [ float ] = None , ) -> Tuple [ int , int , int , int ] : scores = defaultdict ( int ) for u , v , k , d in graph . edges ( keys = True , data = True ) : c = edge_concords ( graph , u , v , k , d , key , cutoff = cutoff ) scores [ c ] += 1 return ( scores [ Concordance . correct ] , scores [ Concordance . incorrect ] , scores [ Concordance . ambiguous ] , scores [ Concordance . unassigned ] , )
8780	def delete_locks ( context , network_ids , addresses ) : addresses_no_longer_null_routed = _find_addresses_to_be_unlocked ( context , network_ids , addresses ) LOG . info ( "Deleting %s lock holders on IPAddress with ids: %s" , len ( addresses_no_longer_null_routed ) , [ addr . id for addr in addresses_no_longer_null_routed ] ) for address in addresses_no_longer_null_routed : lock_holder = None try : lock_holder = db_api . lock_holder_find ( context , lock_id = address . lock_id , name = LOCK_NAME , scope = db_api . ONE ) if lock_holder : db_api . lock_holder_delete ( context , address , lock_holder ) except Exception : LOG . exception ( "Failed to delete lock holder %s" , lock_holder ) continue context . session . flush ( )
10115	def parse ( grid_str , mode = MODE_ZINC , charset = 'utf-8' ) : if isinstance ( grid_str , six . binary_type ) : grid_str = grid_str . decode ( encoding = charset ) _parse = functools . partial ( parse_grid , mode = mode , charset = charset ) if mode == MODE_JSON : if isinstance ( grid_str , six . string_types ) : grid_data = json . loads ( grid_str ) else : grid_data = grid_str if isinstance ( grid_data , dict ) : return _parse ( grid_data ) else : return list ( map ( _parse , grid_data ) ) else : return list ( map ( _parse , GRID_SEP . split ( grid_str . rstrip ( ) ) ) )
11413	def record_replace_field ( rec , tag , new_field , field_position_global = None , field_position_local = None ) : if field_position_global is None and field_position_local is None : raise InvenioBibRecordFieldError ( "A field position is required to " "complete this operation." ) elif field_position_global is not None and field_position_local is not None : raise InvenioBibRecordFieldError ( "Only one field position is required " "to complete this operation." ) elif field_position_global : if tag not in rec : raise InvenioBibRecordFieldError ( "No tag '%s' in record." % tag ) replaced = False for position , field in enumerate ( rec [ tag ] ) : if field [ 4 ] == field_position_global : rec [ tag ] [ position ] = new_field replaced = True if not replaced : raise InvenioBibRecordFieldError ( "No field has the tag '%s' and " "the global field position '%d'." % ( tag , field_position_global ) ) else : try : rec [ tag ] [ field_position_local ] = new_field except KeyError : raise InvenioBibRecordFieldError ( "No tag '%s' in record." % tag ) except IndexError : raise InvenioBibRecordFieldError ( "No field has the tag '%s' and " "the local field position '%d'." % ( tag , field_position_local ) )
12602	def col_values ( df , col_name ) : _check_cols ( df , [ col_name ] ) if 'O' in df [ col_name ] or pd . np . issubdtype ( df [ col_name ] . dtype , str ) : return [ nom . lower ( ) for nom in df [ pd . notnull ( df ) ] [ col_name ] if not pd . isnull ( nom ) ] else : return [ nom for nom in df [ pd . notnull ( df ) ] [ col_name ] if not pd . isnull ( nom ) ]
4691	def decode_memo ( priv , pub , nonce , message ) : shared_secret = get_shared_secret ( priv , pub ) aes = init_aes ( shared_secret , nonce ) " Encryption " raw = bytes ( message , "ascii" ) cleartext = aes . decrypt ( unhexlify ( raw ) ) " Checksum " checksum = cleartext [ 0 : 4 ] message = cleartext [ 4 : ] message = _unpad ( message , 16 ) " Verify checksum " check = hashlib . sha256 ( message ) . digest ( ) [ 0 : 4 ] if check != checksum : raise ValueError ( "checksum verification failure" ) return message . decode ( "utf8" )
2669	def cleanup_old_versions ( src , keep_last_versions , config_file = 'config.yaml' , profile_name = None , ) : if keep_last_versions <= 0 : print ( "Won't delete all versions. Please do this manually" ) else : path_to_config_file = os . path . join ( src , config_file ) cfg = read_cfg ( path_to_config_file , profile_name ) profile_name = cfg . get ( 'profile' ) aws_access_key_id = cfg . get ( 'aws_access_key_id' ) aws_secret_access_key = cfg . get ( 'aws_secret_access_key' ) client = get_client ( 'lambda' , profile_name , aws_access_key_id , aws_secret_access_key , cfg . get ( 'region' ) , ) response = client . list_versions_by_function ( FunctionName = cfg . get ( 'function_name' ) , ) versions = response . get ( 'Versions' ) if len ( response . get ( 'Versions' ) ) < keep_last_versions : print ( 'Nothing to delete. (Too few versions published)' ) else : version_numbers = [ elem . get ( 'Version' ) for elem in versions [ 1 : - keep_last_versions ] ] for version_number in version_numbers : try : client . delete_function ( FunctionName = cfg . get ( 'function_name' ) , Qualifier = version_number , ) except botocore . exceptions . ClientError as e : print ( 'Skipping Version {}: {}' . format ( version_number , e . message ) )
8934	def auto_detect ( workdir ) : if os . path . isdir ( os . path . join ( workdir , '.git' ) ) and os . path . isfile ( os . path . join ( workdir , '.git' , 'HEAD' ) ) : return 'git' return 'unknown'
13388	def ttl ( self , response ) : if response . code != 200 : return 0 if not self . request . method in [ 'GET' , 'HEAD' , 'OPTIONS' ] : return 0 try : pragma = self . request . headers [ 'pragma' ] if pragma == 'no-cache' : return 0 except KeyError : pass try : cache_control = self . request . headers [ 'cache-control' ] for option in [ 'private' , 'no-cache' , 'no-store' , 'must-revalidate' , 'proxy-revalidate' ] : if cache_control . find ( option ) : return 0 options = parse_cache_control ( cache_control ) try : return int ( options [ 's-maxage' ] ) except KeyError : pass try : return int ( options [ 'max-age' ] ) except KeyError : pass if 's-maxage' in options : max_age = options [ 's-maxage' ] if max_age < ttl : ttl = max_age if 'max-age' in options : max_age = options [ 'max-age' ] if max_age < ttl : ttl = max_age return ttl except KeyError : pass try : expires = self . request . headers [ 'expires' ] return time . mktime ( time . strptime ( expires , '%a, %d %b %Y %H:%M:%S' ) ) - time . time ( ) except KeyError : pass
9620	def main ( ) : import time print ( 'Testing controller in position 1:' ) print ( 'Running 3 x 3 seconds tests' ) con = rController ( 1 ) for i in range ( 3 ) : print ( 'Waiting...' ) time . sleep ( 2.5 ) print ( 'State: ' , con . gamepad ) print ( 'Buttons: ' , con . buttons ) time . sleep ( 0.5 ) print ( 'Done!' )
2727	def get_actions ( self ) : answer = self . get_data ( "droplets/%s/actions/" % self . id , type = GET ) actions = [ ] for action_dict in answer [ 'actions' ] : action = Action ( ** action_dict ) action . token = self . token action . droplet_id = self . id action . load ( ) actions . append ( action ) return actions
2364	def append ( self , linenumber , raw_text , cells ) : self . rows . append ( Row ( linenumber , raw_text , cells ) )
3899	def main ( ) : parser = argparse . ArgumentParser ( ) parser . add_argument ( 'protofilepath' ) args = parser . parse_args ( ) out_file = compile_protofile ( args . protofilepath ) with open ( out_file , 'rb' ) as proto_file : file_descriptor_set = descriptor_pb2 . FileDescriptorSet . FromString ( proto_file . read ( ) ) for file_descriptor in file_descriptor_set . file : locations = { } for location in file_descriptor . source_code_info . location : locations [ tuple ( location . path ) ] = location print ( make_comment ( 'This file was automatically generated from {} and ' 'should not be edited directly.' . format ( args . protofilepath ) ) ) for index , message_desc in enumerate ( file_descriptor . message_type ) : generate_message_doc ( message_desc , locations , ( 4 , index ) ) for index , enum_desc in enumerate ( file_descriptor . enum_type ) : generate_enum_doc ( enum_desc , locations , ( 5 , index ) )
8925	def get_egg_info ( cfg , verbose = False ) : result = Bunch ( ) setup_py = cfg . rootjoin ( 'setup.py' ) if not os . path . exists ( setup_py ) : return result egg_info = shell . capture ( "python {} egg_info" . format ( setup_py ) , echo = True if verbose else None ) for info_line in egg_info . splitlines ( ) : if info_line . endswith ( 'PKG-INFO' ) : pkg_info_file = info_line . split ( None , 1 ) [ 1 ] result [ '__file__' ] = pkg_info_file with io . open ( pkg_info_file , encoding = 'utf-8' ) as handle : lastkey = None for line in handle : if line . lstrip ( ) != line : assert lastkey , "Bad continuation in PKG-INFO file '{}': {}" . format ( pkg_info_file , line ) result [ lastkey ] += '\n' + line else : lastkey , value = line . split ( ':' , 1 ) lastkey = lastkey . strip ( ) . lower ( ) . replace ( '-' , '_' ) value = value . strip ( ) if lastkey in result : try : result [ lastkey ] . append ( value ) except AttributeError : result [ lastkey ] = [ result [ lastkey ] , value ] else : result [ lastkey ] = value for multikey in PKG_INFO_MULTIKEYS : if not isinstance ( result . get ( multikey , [ ] ) , list ) : result [ multikey ] = [ result [ multikey ] ] return result
6404	def get_feature ( vector , feature ) : if feature not in _FEATURE_MASK : raise AttributeError ( "feature must be one of: '" + "', '" . join ( ( 'consonantal' , 'sonorant' , 'syllabic' , 'labial' , 'round' , 'coronal' , 'anterior' , 'distributed' , 'dorsal' , 'high' , 'low' , 'back' , 'tense' , 'pharyngeal' , 'ATR' , 'voice' , 'spread_glottis' , 'constricted_glottis' , 'continuant' , 'strident' , 'lateral' , 'delayed_release' , 'nasal' , ) ) + "'" ) mask = _FEATURE_MASK [ feature ] pos_mask = mask >> 1 retvec = [ ] for char in vector : if char < 0 : retvec . append ( float ( 'NaN' ) ) else : masked = char & mask if masked == 0 : retvec . append ( 0 ) elif masked == mask : retvec . append ( 2 ) elif masked & pos_mask : retvec . append ( 1 ) else : retvec . append ( - 1 ) return retvec
13892	def _AssertIsLocal ( path ) : from six . moves . urllib . parse import urlparse if not _UrlIsLocal ( urlparse ( path ) ) : from . _exceptions import NotImplementedForRemotePathError raise NotImplementedForRemotePathError
4960	def get_earliest_start_date_from_program ( program ) : start_dates = [ ] for course in program . get ( 'courses' , [ ] ) : for run in course . get ( 'course_runs' , [ ] ) : if run . get ( 'start' ) : start_dates . append ( parse_lms_api_datetime ( run [ 'start' ] ) ) if not start_dates : return None return min ( start_dates )
9872	def start_response ( self , status , response_headers , exc_info = None ) : if exc_info : try : if self . headers_sent : raise finally : exc_info = None elif self . header_set : raise AssertionError ( "Headers already set!" ) if PY3K and not isinstance ( status , str ) : self . status = str ( status , 'ISO-8859-1' ) else : self . status = status try : self . header_set = Headers ( response_headers ) except UnicodeDecodeError : self . error = ( '500 Internal Server Error' , 'HTTP Headers should be bytes' ) self . err_log . error ( 'Received HTTP Headers from client that contain' ' invalid characters for Latin-1 encoding.' ) return self . write_warning
3225	def iter_project ( projects , key_file = None ) : def decorator ( func ) : @ wraps ( func ) def decorated_function ( * args , ** kwargs ) : item_list = [ ] exception_map = { } for project in projects : if isinstance ( project , string_types ) : kwargs [ 'project' ] = project if key_file : kwargs [ 'key_file' ] = key_file elif isinstance ( project , dict ) : kwargs [ 'project' ] = project [ 'project' ] kwargs [ 'key_file' ] = project [ 'key_file' ] itm , exc = func ( * args , ** kwargs ) item_list . extend ( itm ) exception_map . update ( exc ) return ( item_list , exception_map ) return decorated_function return decorator
1932	def get_description ( self , name : str ) -> str : if name not in self . _vars : raise ConfigError ( f"{self.name}.{name} not defined." ) return self . _vars [ name ] . description
5775	def _advapi32_sign ( private_key , data , hash_algorithm , rsa_pss_padding = False ) : algo = private_key . algorithm if algo == 'rsa' and hash_algorithm == 'raw' : padded_data = add_pkcs1v15_signature_padding ( private_key . byte_size , data ) return raw_rsa_private_crypt ( private_key , padded_data ) if algo == 'rsa' and rsa_pss_padding : hash_length = { 'sha1' : 20 , 'sha224' : 28 , 'sha256' : 32 , 'sha384' : 48 , 'sha512' : 64 } . get ( hash_algorithm , 0 ) padded_data = add_pss_padding ( hash_algorithm , hash_length , private_key . bit_size , data ) return raw_rsa_private_crypt ( private_key , padded_data ) if private_key . algorithm == 'dsa' and hash_algorithm == 'md5' : raise ValueError ( pretty_message ( ) ) hash_handle = None try : alg_id = { 'md5' : Advapi32Const . CALG_MD5 , 'sha1' : Advapi32Const . CALG_SHA1 , 'sha256' : Advapi32Const . CALG_SHA_256 , 'sha384' : Advapi32Const . CALG_SHA_384 , 'sha512' : Advapi32Const . CALG_SHA_512 , } [ hash_algorithm ] hash_handle_pointer = new ( advapi32 , 'HCRYPTHASH *' ) res = advapi32 . CryptCreateHash ( private_key . context_handle , alg_id , null ( ) , 0 , hash_handle_pointer ) handle_error ( res ) hash_handle = unwrap ( hash_handle_pointer ) res = advapi32 . CryptHashData ( hash_handle , data , len ( data ) , 0 ) handle_error ( res ) out_len = new ( advapi32 , 'DWORD *' ) res = advapi32 . CryptSignHashW ( hash_handle , Advapi32Const . AT_SIGNATURE , null ( ) , 0 , null ( ) , out_len ) handle_error ( res ) buffer_length = deref ( out_len ) buffer_ = buffer_from_bytes ( buffer_length ) res = advapi32 . CryptSignHashW ( hash_handle , Advapi32Const . AT_SIGNATURE , null ( ) , 0 , buffer_ , out_len ) handle_error ( res ) output = bytes_from_buffer ( buffer_ , deref ( out_len ) ) output = output [ : : - 1 ] if algo == 'dsa' : half_len = len ( output ) // 2 output = output [ half_len : ] + output [ : half_len ] output = algos . DSASignature . from_p1363 ( output ) . dump ( ) return output finally : if hash_handle : advapi32 . CryptDestroyHash ( hash_handle )
2563	def pull_tasks ( self , kill_event ) : logger . info ( "[TASK PULL THREAD] starting" ) poller = zmq . Poller ( ) poller . register ( self . task_incoming , zmq . POLLIN ) msg = self . create_reg_message ( ) logger . debug ( "Sending registration message: {}" . format ( msg ) ) self . task_incoming . send ( msg ) last_beat = time . time ( ) last_interchange_contact = time . time ( ) task_recv_counter = 0 poll_timer = 1 while not kill_event . is_set ( ) : time . sleep ( LOOP_SLOWDOWN ) ready_worker_count = self . ready_worker_queue . qsize ( ) pending_task_count = self . pending_task_queue . qsize ( ) logger . debug ( "[TASK_PULL_THREAD] ready workers:{}, pending tasks:{}" . format ( ready_worker_count , pending_task_count ) ) if time . time ( ) > last_beat + self . heartbeat_period : self . heartbeat ( ) last_beat = time . time ( ) if pending_task_count < self . max_queue_size and ready_worker_count > 0 : logger . debug ( "[TASK_PULL_THREAD] Requesting tasks: {}" . format ( ready_worker_count ) ) msg = ( ( ready_worker_count ) . to_bytes ( 4 , "little" ) ) self . task_incoming . send ( msg ) socks = dict ( poller . poll ( timeout = poll_timer ) ) if self . task_incoming in socks and socks [ self . task_incoming ] == zmq . POLLIN : _ , pkl_msg = self . task_incoming . recv_multipart ( ) tasks = pickle . loads ( pkl_msg ) last_interchange_contact = time . time ( ) if tasks == 'STOP' : logger . critical ( "[TASK_PULL_THREAD] Received stop request" ) kill_event . set ( ) break elif tasks == HEARTBEAT_CODE : logger . debug ( "Got heartbeat from interchange" ) else : poll_timer = 1 task_recv_counter += len ( tasks ) logger . debug ( "[TASK_PULL_THREAD] Got tasks: {} of {}" . format ( [ t [ 'task_id' ] for t in tasks ] , task_recv_counter ) ) for task in tasks : self . pending_task_queue . put ( task ) else : logger . debug ( "[TASK_PULL_THREAD] No incoming tasks" ) poll_timer = min ( self . heartbeat_period * 1000 , poll_timer * 2 ) if time . time ( ) > last_interchange_contact + self . heartbeat_threshold : logger . critical ( "[TASK_PULL_THREAD] Missing contact with interchange beyond heartbeat_threshold" ) kill_event . set ( ) logger . critical ( "[TASK_PULL_THREAD] Exiting" ) break
2272	def _win32_is_junction ( path ) : if not exists ( path ) : if os . path . isdir ( path ) : if not os . path . islink ( path ) : return True return False return jwfs . is_reparse_point ( path ) and not os . path . islink ( path )
6555	def copy ( self ) : return self . __class__ ( self . func , self . configurations , self . variables , self . vartype , name = self . name )
2746	def edit ( self ) : input_params = { "name" : self . name , "public_key" : self . public_key , } data = self . get_data ( "account/keys/%s" % self . id , type = PUT , params = input_params ) if data : self . id = data [ 'ssh_key' ] [ 'id' ]
13674	def add_directory ( self , * args , ** kwargs ) : exc = kwargs . get ( 'exclusions' , None ) for path in args : self . files . append ( DirectoryPath ( path , self , exclusions = exc ) )
7580	def get_evanno_table ( self , kvalues , max_var_multiple = 0 , quiet = False ) : if max_var_multiple : if max_var_multiple < 1 : raise ValueError ( 'max_variance_multiplier must be >1' ) table = _get_evanno_table ( self , kvalues , max_var_multiple , quiet ) return table
13666	def command_handle ( self ) : self . __results = self . execute ( self . args . command ) self . close ( ) self . logger . debug ( "results: {}" . format ( self . __results ) ) if not self . __results : self . unknown ( "{} return nothing." . format ( self . args . command ) ) if len ( self . __results ) != 1 : self . unknown ( "{} return more than one number." . format ( self . args . command ) ) self . __result = int ( self . __results [ 0 ] ) self . logger . debug ( "result: {}" . format ( self . __result ) ) if not isinstance ( self . __result , ( int , long ) ) : self . unknown ( "{} didn't return single number." . format ( self . args . command ) ) status = self . ok if self . __result > self . args . warning : status = self . warning if self . __result > self . args . critical : status = self . critical self . shortoutput = "{0} return {1}." . format ( self . args . command , self . __result ) [ self . longoutput . append ( line ) for line in self . __results if self . __results ] self . perfdata . append ( "{command}={result};{warn};{crit};0;" . format ( crit = self . args . critical , warn = self . args . warning , result = self . __result , command = self . args . command ) ) status ( self . output ( long_output_limit = None ) ) self . logger . debug ( "Return status and exit to Nagios." )
13130	def parse_single_computer ( entry ) : computer = Computer ( dns_hostname = get_field ( entry , 'dNSHostName' ) , description = get_field ( entry , 'description' ) , os = get_field ( entry , 'operatingSystem' ) , group_id = get_field ( entry , 'primaryGroupID' ) ) try : ip = str ( ipaddress . ip_address ( get_field ( entry , 'IPv4' ) ) ) except ValueError : ip = '' if ip : computer . ip = ip elif computer . dns_hostname : computer . ip = resolve_ip ( computer . dns_hostname ) return computer
4258	def url_from_path ( path ) : if os . sep != '/' : path = '/' . join ( path . split ( os . sep ) ) return quote ( path )
6483	def _process_pagination_values ( request ) : size = 20 page = 0 from_ = 0 if "page_size" in request . POST : size = int ( request . POST [ "page_size" ] ) max_page_size = getattr ( settings , "SEARCH_MAX_PAGE_SIZE" , 100 ) if not ( 0 < size <= max_page_size ) : raise ValueError ( _ ( 'Invalid page size of {page_size}' ) . format ( page_size = size ) ) if "page_index" in request . POST : page = int ( request . POST [ "page_index" ] ) from_ = page * size return size , from_ , page
10177	def run ( self , start_date = None , end_date = None , update_bookmark = True ) : if not Index ( self . event_index , using = self . client ) . exists ( ) : return lower_limit = start_date or self . get_bookmark ( ) if lower_limit is None : return upper_limit = min ( end_date or datetime . datetime . max , datetime . datetime . utcnow ( ) . replace ( microsecond = 0 ) , datetime . datetime . combine ( lower_limit + datetime . timedelta ( self . batch_size ) , datetime . datetime . min . time ( ) ) ) while upper_limit <= datetime . datetime . utcnow ( ) : self . indices = set ( ) self . new_bookmark = upper_limit . strftime ( self . doc_id_suffix ) bulk ( self . client , self . agg_iter ( lower_limit , upper_limit ) , stats_only = True , chunk_size = 50 ) current_search_client . indices . flush ( index = ',' . join ( self . indices ) , wait_if_ongoing = True ) if update_bookmark : self . set_bookmark ( ) self . indices = set ( ) lower_limit = lower_limit + datetime . timedelta ( self . batch_size ) upper_limit = min ( end_date or datetime . datetime . max , datetime . datetime . utcnow ( ) . replace ( microsecond = 0 ) , lower_limit + datetime . timedelta ( self . batch_size ) ) if lower_limit > upper_limit : break
11379	def extract_oembeds ( text , args = None ) : resource_type = width = height = None if args : dimensions = args . lower ( ) . split ( 'x' ) if len ( dimensions ) in ( 3 , 1 ) : resource_type = dimensions . pop ( ) if len ( dimensions ) == 2 : width , height = map ( lambda x : int ( x ) , dimensions ) client = OEmbedConsumer ( ) return client . extract ( text , width , height , resource_type )
9494	def compile ( code : list , consts : list , names : list , varnames : list , func_name : str = "<unknown, compiled>" , arg_count : int = 0 , kwarg_defaults : Tuple [ Any ] = ( ) , use_safety_wrapper : bool = True ) : varnames = tuple ( varnames ) consts = tuple ( consts ) names = tuple ( names ) code = util . flatten ( code ) if arg_count > len ( varnames ) : raise CompileError ( "arg_count > len(varnames)" ) if len ( kwarg_defaults ) > len ( varnames ) : raise CompileError ( "len(kwarg_defaults) > len(varnames)" ) bc = compile_bytecode ( code ) dis . dis ( bc ) if PY36 : pass else : if bc [ - 1 ] != tokens . RETURN_VALUE : raise CompileError ( "No default RETURN_VALUE. Add a `pyte.tokens.RETURN_VALUE` to the end of your " "bytecode if you don't need one." ) flags = 1 | 2 | 64 frame_data = inspect . stack ( ) [ 1 ] if sys . version_info [ 0 : 2 ] > ( 3 , 3 ) : stack_size = _simulate_stack ( dis . _get_instructions_bytes ( bc , constants = consts , names = names , varnames = varnames ) ) else : warnings . warn ( "Cannot check stack for safety." ) stack_size = 99 _optimize_warn_pass ( dis . _get_instructions_bytes ( bc , constants = consts , names = names , varnames = varnames ) ) obb = types . CodeType ( arg_count , 0 , len ( varnames ) , stack_size , flags , bc , consts , names , varnames , frame_data [ 1 ] , func_name , frame_data [ 2 ] , b'' , ( ) , ( ) ) f_globals = frame_data [ 0 ] . f_globals f = types . FunctionType ( obb , f_globals ) f . __name__ = func_name f . __defaults__ = kwarg_defaults if use_safety_wrapper : def __safety_wrapper ( * args , ** kwargs ) : try : return f ( * args , ** kwargs ) except SystemError as e : if 'opcode' not in ' ' . join ( e . args ) : raise msg = "Bytecode exception!" "\nFunction {} returned an invalid opcode." "\nFunction dissection:\n\n" . format ( f . __name__ ) file = io . StringIO ( ) with contextlib . redirect_stdout ( file ) : dis . dis ( f ) msg += file . getvalue ( ) raise SystemError ( msg ) from e returned_func = __safety_wrapper returned_func . wrapped = f else : returned_func = f return returned_func
13704	def iter_add_text ( self , lines , prepend = None , append = None ) : if ( prepend is None ) and ( append is None ) : yield from lines else : fmtpcs = [ '{prepend}' ] if prepend else [ ] fmtpcs . append ( '{line}' ) if append : fmtpcs . append ( '{append}' ) fmtstr = '' . join ( fmtpcs ) yield from ( fmtstr . format ( prepend = prepend , line = line , append = append ) for line in lines )
3870	async def get_events ( self , event_id = None , max_events = 50 ) : if event_id is None : conv_events = self . _events [ - 1 * max_events : ] else : conv_event = self . get_event ( event_id ) if self . _events [ 0 ] . id_ != event_id : conv_events = self . _events [ self . _events . index ( conv_event ) + 1 : ] else : logger . info ( 'Loading events for conversation {} before {}' . format ( self . id_ , conv_event . timestamp ) ) res = await self . _client . get_conversation ( hangouts_pb2 . GetConversationRequest ( request_header = self . _client . get_request_header ( ) , conversation_spec = hangouts_pb2 . ConversationSpec ( conversation_id = hangouts_pb2 . ConversationId ( id = self . id_ ) ) , include_event = True , max_events_per_conversation = max_events , event_continuation_token = self . _event_cont_token ) ) if res . conversation_state . HasField ( 'conversation' ) : self . update_conversation ( res . conversation_state . conversation ) self . _event_cont_token = ( res . conversation_state . event_continuation_token ) conv_events = [ self . _wrap_event ( event ) for event in res . conversation_state . event ] logger . info ( 'Loaded {} events for conversation {}' . format ( len ( conv_events ) , self . id_ ) ) for conv_event in reversed ( conv_events ) : if conv_event . id_ not in self . _events_dict : self . _events . insert ( 0 , conv_event ) self . _events_dict [ conv_event . id_ ] = conv_event else : logger . info ( 'Conversation %s ignoring duplicate event %s' , self . id_ , conv_event . id_ ) return conv_events
5923	def get_configuration ( filename = CONFIGNAME ) : global cfg , configuration cfg = GMXConfigParser ( filename = filename ) globals ( ) . update ( cfg . configuration ) configuration = cfg . configuration return cfg
5487	def send_payload ( self , params ) : data = json . dumps ( { 'jsonrpc' : self . version , 'method' : self . service_name , 'params' : params , 'id' : text_type ( uuid . uuid4 ( ) ) } ) data_binary = data . encode ( 'utf-8' ) url_request = Request ( self . service_url , data_binary , headers = self . headers ) return urlopen ( url_request ) . read ( )
1470	def getStmgrsRegSummary ( self , tmaster , callback = None ) : if not tmaster or not tmaster . host or not tmaster . stats_port : return reg_request = tmaster_pb2 . StmgrsRegistrationSummaryRequest ( ) request_str = reg_request . SerializeToString ( ) port = str ( tmaster . stats_port ) host = tmaster . host url = "http://{0}:{1}/stmgrsregistrationsummary" . format ( host , port ) request = tornado . httpclient . HTTPRequest ( url , body = request_str , method = 'POST' , request_timeout = 5 ) Log . debug ( 'Making HTTP call to fetch stmgrsregistrationsummary url: %s' , url ) try : client = tornado . httpclient . AsyncHTTPClient ( ) result = yield client . fetch ( request ) Log . debug ( "HTTP call complete." ) except tornado . httpclient . HTTPError as e : raise Exception ( str ( e ) ) responseCode = result . code if responseCode >= 400 : message = "Error in getting exceptions from Tmaster, code: " + responseCode Log . error ( message ) raise tornado . gen . Return ( { "message" : message } ) reg_response = tmaster_pb2 . StmgrsRegistrationSummaryResponse ( ) reg_response . ParseFromString ( result . body ) ret = { } for stmgr in reg_response . registered_stmgrs : ret [ stmgr ] = True for stmgr in reg_response . absent_stmgrs : ret [ stmgr ] = False raise tornado . gen . Return ( ret )
4080	def get_languages ( ) -> set : try : languages = cache [ 'languages' ] except KeyError : languages = LanguageTool . _get_languages ( ) cache [ 'languages' ] = languages return languages
2493	def create_annotation_node ( self , annotation ) : annotation_node = URIRef ( str ( annotation . spdx_id ) ) type_triple = ( annotation_node , RDF . type , self . spdx_namespace . Annotation ) self . graph . add ( type_triple ) annotator_node = Literal ( annotation . annotator . to_value ( ) ) self . graph . add ( ( annotation_node , self . spdx_namespace . annotator , annotator_node ) ) annotation_date_node = Literal ( annotation . annotation_date_iso_format ) annotation_triple = ( annotation_node , self . spdx_namespace . annotationDate , annotation_date_node ) self . graph . add ( annotation_triple ) if annotation . has_comment : comment_node = Literal ( annotation . comment ) comment_triple = ( annotation_node , RDFS . comment , comment_node ) self . graph . add ( comment_triple ) annotation_type_node = Literal ( annotation . annotation_type ) annotation_type_triple = ( annotation_node , self . spdx_namespace . annotationType , annotation_type_node ) self . graph . add ( annotation_type_triple ) return annotation_node
8968	def decryptMessage ( self , ciphertext , header , ad = None ) : if ad == None : ad = self . __ad plaintext = self . __decryptSavedMessage ( ciphertext , header , ad ) if plaintext : return plaintext if self . triggersStep ( header . dh_pub ) : self . __saveMessageKeys ( header . pn ) self . step ( header . dh_pub ) self . __saveMessageKeys ( header . n ) return self . __decrypt ( ciphertext , self . __skr . nextDecryptionKey ( ) , header , ad )
2244	def argflag ( key , argv = None ) : if argv is None : argv = sys . argv keys = [ key ] if isinstance ( key , six . string_types ) else key flag = any ( k in argv for k in keys ) return flag
8251	def swatch ( self , x , y , w = 35 , h = 35 , roundness = 0 ) : _ctx . fill ( self ) _ctx . rect ( x , y , w , h , roundness )
12540	def is_dicom_file ( filepath ) : if not os . path . exists ( filepath ) : raise IOError ( 'File {} not found.' . format ( filepath ) ) filename = os . path . basename ( filepath ) if filename == 'DICOMDIR' : return False try : _ = dicom . read_file ( filepath ) except Exception as exc : log . debug ( 'Checking if {0} was a DICOM, but returned ' 'False.' . format ( filepath ) ) return False return True
9232	def fetch_date_of_tag ( self , tag ) : if self . options . verbose > 1 : print ( "\tFetching date for tag {}" . format ( tag [ "name" ] ) ) gh = self . github user = self . options . user repo = self . options . project rc , data = gh . repos [ user ] [ repo ] . git . commits [ tag [ "commit" ] [ "sha" ] ] . get ( ) if rc == 200 : return data [ "committer" ] [ "date" ] self . raise_GitHubError ( rc , data , gh . getheaders ( ) )
11734	def get_resample_data ( self ) : if self . data is not None : if self . _pvpc_mean_daily is None : self . _pvpc_mean_daily = self . data [ 'data' ] . resample ( 'D' ) . mean ( ) if self . _pvpc_mean_monthly is None : self . _pvpc_mean_monthly = self . data [ 'data' ] . resample ( 'MS' ) . mean ( ) return self . _pvpc_mean_daily , self . _pvpc_mean_monthly
2152	def get ( self , pk = None , ** kwargs ) : self . _separate ( kwargs ) return super ( Resource , self ) . get ( pk = pk , ** kwargs )
8917	def _get_param ( self , param , allowed_values = None , optional = False ) : request_params = self . _request_params ( ) if param in request_params : value = request_params [ param ] . lower ( ) if allowed_values is not None : if value in allowed_values : self . params [ param ] = value else : raise OWSInvalidParameterValue ( "%s %s is not supported" % ( param , value ) , value = param ) elif optional : self . params [ param ] = None else : raise OWSMissingParameterValue ( 'Parameter "%s" is missing' % param , value = param ) return self . params [ param ]
6024	def convolve ( self , array ) : if self . shape [ 0 ] % 2 == 0 or self . shape [ 1 ] % 2 == 0 : raise exc . KernelException ( "PSF Kernel must be odd" ) return scipy . signal . convolve2d ( array , self , mode = 'same' )
11194	def freeze ( proto_dataset_uri ) : proto_dataset = dtoolcore . ProtoDataSet . from_uri ( uri = proto_dataset_uri , config_path = CONFIG_PATH ) num_items = len ( list ( proto_dataset . _identifiers ( ) ) ) max_files_limit = int ( dtoolcore . utils . get_config_value ( "DTOOL_MAX_FILES_LIMIT" , CONFIG_PATH , 10000 ) ) assert isinstance ( max_files_limit , int ) if num_items > max_files_limit : click . secho ( "Too many items ({} > {}) in proto dataset" . format ( num_items , max_files_limit ) , fg = "red" ) click . secho ( "1. Consider splitting the dataset into smaller datasets" ) click . secho ( "2. Consider packaging small files using tar" ) click . secho ( "3. Increase the limit using the DTOOL_MAX_FILES_LIMIT" ) click . secho ( " environment variable" ) sys . exit ( 2 ) handles = [ h for h in proto_dataset . _storage_broker . iter_item_handles ( ) ] for h in handles : if not valid_handle ( h ) : click . secho ( "Invalid item name: {}" . format ( h ) , fg = "red" ) click . secho ( "1. Consider renaming the item" ) click . secho ( "2. Consider removing the item" ) sys . exit ( 3 ) with click . progressbar ( length = len ( list ( proto_dataset . _identifiers ( ) ) ) , label = "Generating manifest" ) as progressbar : try : proto_dataset . freeze ( progressbar = progressbar ) except dtoolcore . storagebroker . DiskStorageBrokerValidationWarning as e : click . secho ( "" ) click . secho ( str ( e ) , fg = "red" , nl = False ) sys . exit ( 4 ) click . secho ( "Dataset frozen " , nl = False , fg = "green" ) click . secho ( proto_dataset_uri )
7248	def get_stdout ( self , workflow_id , task_id ) : url = '%(wf_url)s/%(wf_id)s/tasks/%(task_id)s/stdout' % { 'wf_url' : self . workflows_url , 'wf_id' : workflow_id , 'task_id' : task_id } r = self . gbdx_connection . get ( url ) r . raise_for_status ( ) return r . text
12349	def create ( self , name , region , size , image , ssh_keys = None , backups = None , ipv6 = None , private_networking = None , wait = True ) : if ssh_keys and not isinstance ( ssh_keys , ( list , tuple ) ) : raise TypeError ( "ssh_keys must be a list" ) resp = self . post ( name = name , region = region , size = size , image = image , ssh_keys = ssh_keys , private_networking = private_networking , backups = backups , ipv6 = ipv6 ) droplet = self . get ( resp [ self . singular ] [ 'id' ] ) if wait : droplet . wait ( ) droplet = self . get ( resp [ self . singular ] [ 'id' ] ) return droplet
3242	def boto3_cached_conn ( service , service_type = 'client' , future_expiration_minutes = 15 , account_number = None , assume_role = None , session_name = 'cloudaux' , region = 'us-east-1' , return_credentials = False , external_id = None , arn_partition = 'aws' ) : key = ( account_number , assume_role , session_name , external_id , region , service_type , service , arn_partition ) if key in CACHE : retval = _get_cached_creds ( key , service , service_type , region , future_expiration_minutes , return_credentials ) if retval : return retval role = None if assume_role : sts = boto3 . session . Session ( ) . client ( 'sts' ) if not all ( [ account_number , assume_role ] ) : raise ValueError ( "Account number and role to assume are both required" ) arn = 'arn:{partition}:iam::{0}:role/{1}' . format ( account_number , assume_role , partition = arn_partition ) assume_role_kwargs = { 'RoleArn' : arn , 'RoleSessionName' : session_name } if external_id : assume_role_kwargs [ 'ExternalId' ] = external_id role = sts . assume_role ( ** assume_role_kwargs ) if service_type == 'client' : conn = _client ( service , region , role ) elif service_type == 'resource' : conn = _resource ( service , region , role ) if role : CACHE [ key ] = role if return_credentials : return conn , role [ 'Credentials' ] return conn
12701	def get_subfields ( self , datafield , subfield , i1 = None , i2 = None , exception = False ) : if len ( datafield ) != 3 : raise ValueError ( "`datafield` parameter have to be exactly 3 chars long!" ) if len ( subfield ) != 1 : raise ValueError ( "Bad subfield specification - subfield have to be 1 char long!" ) if datafield not in self . datafields : if exception : raise KeyError ( datafield + " is not in datafields!" ) return [ ] output = [ ] for datafield in self . datafields [ datafield ] : if subfield not in datafield : continue for sfield in datafield [ subfield ] : if i1 and sfield . i1 != i1 : continue if i2 and sfield . i2 != i2 : continue output . append ( sfield ) if not output and exception : raise KeyError ( subfield + " couldn't be found in subfields!" ) return output
1815	def SETNLE ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , Operators . AND ( cpu . ZF == False , cpu . SF == cpu . OF ) , 1 , 0 ) )
11713	def instance ( self , counter = None ) : if not counter : history = self . history ( ) if not history : return history else : return Response . _from_json ( history [ 'pipelines' ] [ 0 ] ) return self . _get ( '/instance/{counter:d}' . format ( counter = counter ) )
3154	def get ( self , list_id , segment_id ) : return self . _mc_client . _get ( url = self . _build_path ( list_id , 'segments' , segment_id ) )
485	def _getCommonSteadyDBArgsDict ( ) : return dict ( creator = pymysql , host = Configuration . get ( 'nupic.cluster.database.host' ) , port = int ( Configuration . get ( 'nupic.cluster.database.port' ) ) , user = Configuration . get ( 'nupic.cluster.database.user' ) , passwd = Configuration . get ( 'nupic.cluster.database.passwd' ) , charset = 'utf8' , use_unicode = True , setsession = [ 'SET AUTOCOMMIT = 1' ] )
668	def sample ( self , rgen ) : x = rgen . poisson ( self . lambdaParameter ) return x , self . logDensity ( x )
2684	def cached_download ( url , name ) : clean_name = os . path . normpath ( name ) if clean_name != name : raise ValueError ( "{} is not normalized." . format ( name ) ) for dir_ in iter_data_dirs ( ) : path = os . path . join ( dir_ , name ) if os . path . exists ( path ) : return path dir_ = next ( iter_data_dirs ( True ) ) path = os . path . join ( dir_ , name ) log . info ( "Downloading {} to {}" . format ( url , path ) ) response = urlopen ( url ) if response . getcode ( ) != 200 : raise ValueError ( "HTTP {}" . format ( response . getcode ( ) ) ) dir_ = os . path . dirname ( path ) try : os . makedirs ( dir_ ) except OSError as e : if e . errno != errno . EEXIST : raise tmp_path = path + '.tmp' with open ( tmp_path , 'wb' ) as fh : while True : chunk = response . read ( 8196 ) if chunk : fh . write ( chunk ) else : break os . rename ( tmp_path , path ) return path
11869	def color_from_hls ( hue , light , sat ) : if light > 0.95 : return 256 elif light < 0.05 : return - 1 else : hue = ( - hue + 1 + 2.0 / 3.0 ) % 1 return int ( floor ( hue * 256 ) )
13413	def addMenu ( self ) : self . parent . multiLogLayout . addLayout ( self . logSelectLayout ) self . getPrograms ( logType , programName )
828	def getFieldDescription ( self , fieldName ) : description = self . getDescription ( ) + [ ( "end" , self . getWidth ( ) ) ] for i in xrange ( len ( description ) ) : ( name , offset ) = description [ i ] if ( name == fieldName ) : break if i >= len ( description ) - 1 : raise RuntimeError ( "Field name %s not found in this encoder" % fieldName ) return ( offset , description [ i + 1 ] [ 1 ] - offset )
5075	def is_course_run_enrollable ( course_run ) : now = datetime . datetime . now ( pytz . UTC ) end = parse_datetime_handle_invalid ( course_run . get ( 'end' ) ) enrollment_start = parse_datetime_handle_invalid ( course_run . get ( 'enrollment_start' ) ) enrollment_end = parse_datetime_handle_invalid ( course_run . get ( 'enrollment_end' ) ) return ( not end or end > now ) and ( not enrollment_start or enrollment_start < now ) and ( not enrollment_end or enrollment_end > now )
3731	def checkCAS ( CASRN ) : try : check = CASRN [ - 1 ] CASRN = CASRN [ : : - 1 ] [ 1 : ] productsum = 0 i = 1 for num in CASRN : if num == '-' : pass else : productsum += i * int ( num ) i += 1 return ( productsum % 10 == int ( check ) ) except : return False
3815	async def _on_receive_array ( self , array ) : if array [ 0 ] == 'noop' : pass else : wrapper = json . loads ( array [ 0 ] [ 'p' ] ) if '3' in wrapper : self . _client_id = wrapper [ '3' ] [ '2' ] logger . info ( 'Received new client_id: %r' , self . _client_id ) await self . _add_channel_services ( ) if '2' in wrapper : pblite_message = json . loads ( wrapper [ '2' ] [ '2' ] ) if pblite_message [ 0 ] == 'cbu' : batch_update = hangouts_pb2 . BatchUpdate ( ) pblite . decode ( batch_update , pblite_message , ignore_first_item = True ) for state_update in batch_update . state_update : logger . debug ( 'Received StateUpdate:\n%s' , state_update ) header = state_update . state_update_header self . _active_client_state = header . active_client_state await self . on_state_update . fire ( state_update ) else : logger . info ( 'Ignoring message: %r' , pblite_message [ 0 ] )
7384	def plot_nodes ( self , nodelist , theta , group ) : for i , node in enumerate ( nodelist ) : r = self . internal_radius + i * self . scale x , y = get_cartesian ( r , theta ) circle = plt . Circle ( xy = ( x , y ) , radius = self . dot_radius , color = self . node_colormap [ group ] , linewidth = 0 ) self . ax . add_patch ( circle )
2180	def fetch_access_token ( self , url , verifier = None , ** request_kwargs ) : if verifier : self . _client . client . verifier = verifier if not getattr ( self . _client . client , "verifier" , None ) : raise VerifierMissing ( "No client verifier has been set." ) token = self . _fetch_token ( url , ** request_kwargs ) log . debug ( "Resetting verifier attribute, should not be used anymore." ) self . _client . client . verifier = None return token
2264	def dict_union ( * args ) : if not args : return { } else : dictclass = OrderedDict if isinstance ( args [ 0 ] , OrderedDict ) else dict return dictclass ( it . chain . from_iterable ( d . items ( ) for d in args ) )
1521	def get_hostname ( ip_addr , cl_args ) : if is_self ( ip_addr ) : return get_self_hostname ( ) cmd = "hostname" ssh_cmd = ssh_remote_execute ( cmd , ip_addr , cl_args ) pid = subprocess . Popen ( ssh_cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) return_code = pid . wait ( ) output = pid . communicate ( ) if return_code != 0 : Log . error ( "Failed to get hostname for remote host %s with output:\n%s" % ( ip_addr , output ) ) sys . exit ( - 1 ) return output [ 0 ] . strip ( "\n" )
4655	def verify_authority ( self ) : try : if not self . blockchain . rpc . verify_authority ( self . json ( ) ) : raise InsufficientAuthorityError except Exception as e : raise e
7766	def _close_stream ( self ) : self . stream . close ( ) if self . stream . transport in self . _ml_handlers : self . _ml_handlers . remove ( self . stream . transport ) self . main_loop . remove_handler ( self . stream . transport ) self . stream = None self . uplink = None
6666	def update_merge ( d , u ) : import collections for k , v in u . items ( ) : if isinstance ( v , collections . Mapping ) : r = update_merge ( d . get ( k , dict ( ) ) , v ) d [ k ] = r else : d [ k ] = u [ k ] return d
12385	def parse ( text , encoding = 'utf8' ) : if isinstance ( text , six . binary_type ) : text = text . decode ( encoding ) return Query ( text , split_segments ( text ) )
809	def _storeSample ( self , inputVector , trueCatIndex , partition = 0 ) : if self . _samples is None : self . _samples = numpy . zeros ( ( 0 , len ( inputVector ) ) , dtype = RealNumpyDType ) assert self . _labels is None self . _labels = [ ] self . _samples = numpy . concatenate ( ( self . _samples , numpy . atleast_2d ( inputVector ) ) , axis = 0 ) self . _labels += [ trueCatIndex ] if self . _partitions is None : self . _partitions = [ ] if partition is None : partition = 0 self . _partitions += [ partition ]
1039	def column ( self ) : line , column = self . source_buffer . decompose_position ( self . begin_pos ) return column
11763	def utility ( self , state , player ) : "Return the value to player; 1 for win, -1 for loss, 0 otherwise." return if_ ( player == 'X' , state . utility , - state . utility )
10091	def setup ( app ) : if 'http' not in app . domains : httpdomain . setup ( app ) app . add_directive ( 'autopyramid' , RouteDirective )
12812	def rawDataReceived ( self , data ) : if self . _len_expected is not None : data , extra = data [ : self . _len_expected ] , data [ self . _len_expected : ] self . _len_expected -= len ( data ) else : extra = "" self . _buffer += data if self . _len_expected == 0 : data = self . _buffer . strip ( ) if data : lines = data . split ( "\r" ) for line in lines : try : message = self . factory . get_stream ( ) . get_connection ( ) . parse ( line ) if message : self . factory . get_stream ( ) . received ( [ message ] ) except ValueError : pass self . _buffer = "" self . _len_expected = None self . setLineMode ( extra )
10260	def remove_falsy_values ( counter : Mapping [ Any , int ] ) -> Mapping [ Any , int ] : return { label : count for label , count in counter . items ( ) if count }
10954	def model_to_data ( self , sigma = 0.0 ) : im = self . model . copy ( ) im += sigma * np . random . randn ( * im . shape ) self . set_image ( util . NullImage ( image = im ) )
156	def succ_key ( self , key , default = _sentinel ) : item = self . succ_item ( key , default ) return default if item is default else item [ 0 ]
7900	def process_configuration_success ( self , stanza ) : _unused = stanza self . configured = True self . handler . room_configured ( )
12625	def get_file_list ( file_dir , regex = '' ) : file_list = os . listdir ( file_dir ) file_list . sort ( ) if regex : file_list = search_list ( file_list , regex ) file_list = [ op . join ( file_dir , fname ) for fname in file_list ] return file_list
1777	def TEST ( cpu , src1 , src2 ) : temp = src1 . read ( ) & src2 . read ( ) cpu . SF = ( temp & ( 1 << ( src1 . size - 1 ) ) ) != 0 cpu . ZF = temp == 0 cpu . PF = cpu . _calculate_parity_flag ( temp ) cpu . CF = False cpu . OF = False
7656	def validate ( self , strict = True ) : valid = True try : jsonschema . validate ( self . __json__ , self . __schema__ ) except jsonschema . ValidationError as invalid : if strict : raise SchemaError ( str ( invalid ) ) else : warnings . warn ( str ( invalid ) ) valid = False return valid
10964	def trigger_update ( self , params , values ) : if self . _parent : self . _parent . trigger_update ( params , values ) else : self . update ( params , values )
4982	def get_available_course_modes ( self , request , course_run_id , enterprise_catalog ) : modes = EnrollmentApiClient ( ) . get_course_modes ( course_run_id ) if not modes : LOGGER . warning ( 'Unable to get course modes for course run id {course_run_id}.' . format ( course_run_id = course_run_id ) ) messages . add_generic_info_message_for_error ( request ) if enterprise_catalog : modes = [ mode for mode in modes if mode [ 'slug' ] in enterprise_catalog . enabled_course_modes ] modes . sort ( key = lambda course_mode : enterprise_catalog . enabled_course_modes . index ( course_mode [ 'slug' ] ) ) if not modes : LOGGER . info ( 'No matching course modes found for course run {course_run_id} in ' 'EnterpriseCustomerCatalog [{enterprise_catalog_uuid}]' . format ( course_run_id = course_run_id , enterprise_catalog_uuid = enterprise_catalog , ) ) messages . add_generic_info_message_for_error ( request ) return modes
12055	def ftp_login ( folder = None ) : pwDir = os . path . realpath ( __file__ ) for i in range ( 3 ) : pwDir = os . path . dirname ( pwDir ) pwFile = os . path . join ( pwDir , "passwd.txt" ) print ( " -- looking for login information in:\n [%s]" % pwFile ) try : with open ( pwFile ) as f : lines = f . readlines ( ) username = lines [ 0 ] . strip ( ) password = lines [ 1 ] . strip ( ) print ( " -- found a valid username/password" ) except : print ( " -- password lookup FAILED." ) username = TK_askPassword ( "FTP LOGIN" , "enter FTP username" ) password = TK_askPassword ( "FTP LOGIN" , "enter password for %s" % username ) if not username or not password : print ( " !! failed getting login info. aborting FTP effort." ) return print ( " username:" , username ) print ( " password:" , "*" * ( len ( password ) ) ) print ( " -- logging in to FTP ..." ) try : ftp = ftplib . FTP ( "swharden.com" ) ftp . login ( username , password ) if folder : ftp . cwd ( folder ) return ftp except : print ( " !! login failure !!" ) return False
6058	def bin_up_array_2d_using_mean ( array_2d , bin_up_factor ) : padded_array_2d = pad_2d_array_for_binning_up_with_bin_up_factor ( array_2d = array_2d , bin_up_factor = bin_up_factor ) binned_array_2d = np . zeros ( shape = ( padded_array_2d . shape [ 0 ] // bin_up_factor , padded_array_2d . shape [ 1 ] // bin_up_factor ) ) for y in range ( binned_array_2d . shape [ 0 ] ) : for x in range ( binned_array_2d . shape [ 1 ] ) : value = 0.0 for y1 in range ( bin_up_factor ) : for x1 in range ( bin_up_factor ) : padded_y = y * bin_up_factor + y1 padded_x = x * bin_up_factor + x1 value += padded_array_2d [ padded_y , padded_x ] binned_array_2d [ y , x ] = value / ( bin_up_factor ** 2.0 ) return binned_array_2d
7627	def add_namespace ( filename ) : with open ( filename , mode = 'r' ) as fileobj : __NAMESPACE__ . update ( json . load ( fileobj ) )
1662	def FilesBelongToSameModule ( filename_cc , filename_h ) : fileinfo_cc = FileInfo ( filename_cc ) if not fileinfo_cc . Extension ( ) . lstrip ( '.' ) in GetNonHeaderExtensions ( ) : return ( False , '' ) fileinfo_h = FileInfo ( filename_h ) if not fileinfo_h . Extension ( ) . lstrip ( '.' ) in GetHeaderExtensions ( ) : return ( False , '' ) filename_cc = filename_cc [ : - ( len ( fileinfo_cc . Extension ( ) ) ) ] matched_test_suffix = Search ( _TEST_FILE_SUFFIX , fileinfo_cc . BaseName ( ) ) if matched_test_suffix : filename_cc = filename_cc [ : - len ( matched_test_suffix . group ( 1 ) ) ] filename_cc = filename_cc . replace ( '/public/' , '/' ) filename_cc = filename_cc . replace ( '/internal/' , '/' ) filename_h = filename_h [ : - ( len ( fileinfo_h . Extension ( ) ) ) ] if filename_h . endswith ( '-inl' ) : filename_h = filename_h [ : - len ( '-inl' ) ] filename_h = filename_h . replace ( '/public/' , '/' ) filename_h = filename_h . replace ( '/internal/' , '/' ) files_belong_to_same_module = filename_cc . endswith ( filename_h ) common_path = '' if files_belong_to_same_module : common_path = filename_cc [ : - len ( filename_h ) ] return files_belong_to_same_module , common_path
2381	def parse_and_process_args ( self , args ) : parser = argparse . ArgumentParser ( prog = "python -m rflint" , description = "A style checker for robot framework plain text files." , formatter_class = argparse . RawDescriptionHelpFormatter , epilog = ( "You can use 'all' in place of RULENAME to refer to all rules. \n" "\n" "For example: '--ignore all --warn DuplicateTestNames' will ignore all\n" "rules except DuplicateTestNames.\n" "\n" "FORMAT is a string that performs a substitution on the following \n" "patterns: {severity}, {linenumber}, {char}, {message}, and {rulename}.\n" "\n" "For example: --format 'line: {linenumber}: message: {message}'. \n" "\n" "ARGUMENTFILE is a filename with contents that match the format of \n" "standard robot framework argument files\n" "\n" "If you give a directory as an argument, all files in the directory\n" "with the suffix .txt, .robot or .tsv will be processed. With the \n" "--recursive option, subfolders within the directory will also be\n" "processed." ) ) parser . add_argument ( "--error" , "-e" , metavar = "RULENAME" , action = SetErrorAction , help = "Assign a severity of ERROR to the given RULENAME" ) parser . add_argument ( "--ignore" , "-i" , metavar = "RULENAME" , action = SetIgnoreAction , help = "Ignore the given RULENAME" ) parser . add_argument ( "--warning" , "-w" , metavar = "RULENAME" , action = SetWarningAction , help = "Assign a severity of WARNING for the given RULENAME" ) parser . add_argument ( "--list" , "-l" , action = "store_true" , help = "show a list of known rules and exit" ) parser . add_argument ( "--describe" , "-d" , action = "store_true" , help = "describe the given rules" ) parser . add_argument ( "--no-filenames" , action = "store_false" , dest = "print_filenames" , default = True , help = "suppress the printing of filenames" ) parser . add_argument ( "--format" , "-f" , help = "Define the output format" , default = '{severity}: {linenumber}, {char}: {message} ({rulename})' ) parser . add_argument ( "--version" , action = "store_true" , default = False , help = "Display version number and exit" ) parser . add_argument ( "--verbose" , "-v" , action = "store_true" , default = False , help = "Give verbose output" ) parser . add_argument ( "--configure" , "-c" , action = ConfigureAction , help = "Configure a rule" ) parser . add_argument ( "--recursive" , "-r" , action = "store_true" , default = False , help = "Recursively scan subfolders in a directory" ) parser . add_argument ( "--rulefile" , "-R" , action = RulefileAction , help = "import additional rules from the given RULEFILE" ) parser . add_argument ( "--argumentfile" , "-A" , action = ArgfileLoader , help = "read arguments from the given file" ) parser . add_argument ( 'args' , metavar = "file" , nargs = argparse . REMAINDER ) ns = argparse . Namespace ( ) setattr ( ns , "app" , self ) args = parser . parse_args ( args , ns ) Rule . output_format = args . format return args
1566	def invoke_hook_spout_fail ( self , message_id , fail_latency_ns ) : if len ( self . task_hooks ) > 0 : spout_fail_info = SpoutFailInfo ( message_id = message_id , spout_task_id = self . get_task_id ( ) , fail_latency_ms = fail_latency_ns * system_constants . NS_TO_MS ) for task_hook in self . task_hooks : task_hook . spout_fail ( spout_fail_info )
4904	def populate_data_sharing_consent ( apps , schema_editor ) : DataSharingConsent = apps . get_model ( 'consent' , 'DataSharingConsent' ) EnterpriseCourseEnrollment = apps . get_model ( 'enterprise' , 'EnterpriseCourseEnrollment' ) User = apps . get_model ( 'auth' , 'User' ) for enrollment in EnterpriseCourseEnrollment . objects . all ( ) : user = User . objects . get ( pk = enrollment . enterprise_customer_user . user_id ) data_sharing_consent , __ = DataSharingConsent . objects . get_or_create ( username = user . username , enterprise_customer = enrollment . enterprise_customer_user . enterprise_customer , course_id = enrollment . course_id , ) if enrollment . consent_granted is not None : data_sharing_consent . granted = enrollment . consent_granted else : consent_state = enrollment . enterprise_customer_user . data_sharing_consent . first ( ) if consent_state is not None : data_sharing_consent . granted = consent_state . state in [ 'enabled' , 'external' ] else : data_sharing_consent . granted = False data_sharing_consent . save ( )
4435	async def get_tracks ( self , query ) : log . debug ( 'Requesting tracks for query {}' . format ( query ) ) async with self . http . get ( self . rest_uri + quote ( query ) , headers = { 'Authorization' : self . password } ) as res : return await res . json ( content_type = None )
12191	async def _get_socket_url ( self ) : data = await self . api . execute_method ( self . RTM_START_ENDPOINT , simple_latest = True , no_unreads = True , ) return data [ 'url' ]
4295	def less_than_version ( value ) : items = list ( map ( int , str ( value ) . split ( '.' ) ) ) if len ( items ) == 1 : items . append ( 0 ) items [ 1 ] += 1 if value == '1.11' : return '2.0' else : return '.' . join ( map ( str , items ) )
6327	def get_count ( self , ngram , corpus = None ) : r if not corpus : corpus = self . ngcorpus if not ngram : return corpus [ None ] if isinstance ( ngram , ( text_type , str ) ) : ngram = text_type ( ngram ) . split ( ) if ngram [ 0 ] in corpus : return self . get_count ( ngram [ 1 : ] , corpus [ ngram [ 0 ] ] ) return 0
4076	def cfg_to_args ( config ) : kwargs = { } opts_to_args = { 'metadata' : [ ( 'name' , 'name' ) , ( 'author' , 'author' ) , ( 'author-email' , 'author_email' ) , ( 'maintainer' , 'maintainer' ) , ( 'maintainer-email' , 'maintainer_email' ) , ( 'home-page' , 'url' ) , ( 'summary' , 'description' ) , ( 'description' , 'long_description' ) , ( 'download-url' , 'download_url' ) , ( 'classifier' , 'classifiers' ) , ( 'platform' , 'platforms' ) , ( 'license' , 'license' ) , ( 'keywords' , 'keywords' ) , ] , 'files' : [ ( 'packages_root' , 'package_dir' ) , ( 'packages' , 'packages' ) , ( 'modules' , 'py_modules' ) , ( 'scripts' , 'scripts' ) , ( 'package_data' , 'package_data' ) , ( 'data_files' , 'data_files' ) , ] , } opts_to_args [ 'metadata' ] . append ( ( 'requires-dist' , 'install_requires' ) ) if IS_PY2K and not which ( '3to2' ) : kwargs [ 'setup_requires' ] = [ '3to2' ] kwargs [ 'zip_safe' ] = False for section in opts_to_args : for option , argname in opts_to_args [ section ] : value = get_cfg_value ( config , section , option ) if value : kwargs [ argname ] = value if 'long_description' not in kwargs : kwargs [ 'long_description' ] = read_description_file ( config ) if 'package_dir' in kwargs : kwargs [ 'package_dir' ] = { '' : kwargs [ 'package_dir' ] } if 'keywords' in kwargs : kwargs [ 'keywords' ] = split_elements ( kwargs [ 'keywords' ] ) if 'package_data' in kwargs : kwargs [ 'package_data' ] = get_package_data ( kwargs [ 'package_data' ] ) if 'data_files' in kwargs : kwargs [ 'data_files' ] = get_data_files ( kwargs [ 'data_files' ] ) kwargs [ 'version' ] = get_version ( ) if not IS_PY2K : kwargs [ 'test_suite' ] = 'test' return kwargs
7859	def make_result_response ( self ) : if self . stanza_type not in ( "set" , "get" ) : raise ValueError ( "Results may only be generated for" " 'set' or 'get' iq" ) stanza = Iq ( stanza_type = "result" , from_jid = self . to_jid , to_jid = self . from_jid , stanza_id = self . stanza_id ) return stanza
2863	def ping ( self ) : self . _idle ( ) self . _transaction_start ( ) self . _i2c_start ( ) self . _i2c_write_bytes ( [ self . _address_byte ( False ) ] ) self . _i2c_stop ( ) response = self . _transaction_end ( ) if len ( response ) != 1 : raise RuntimeError ( 'Expected 1 response byte but received {0} byte(s).' . format ( len ( response ) ) ) return ( ( response [ 0 ] & 0x01 ) == 0x00 )
5853	def get_pif ( self , dataset_id , uid , dataset_version = None ) : failure_message = "An error occurred retrieving PIF {}" . format ( uid ) if dataset_version == None : response = self . _get ( routes . pif_dataset_uid ( dataset_id , uid ) , failure_message = failure_message ) else : response = self . _get ( routes . pif_dataset_version_uid ( dataset_id , uid , dataset_version ) , failure_message = failure_message ) return pif . loads ( response . content . decode ( "utf-8" ) )
5166	def __intermediate_dns_servers ( self , uci , address ) : if 'dns' in uci : return uci [ 'dns' ] if address [ 'proto' ] in [ 'dhcp' , 'dhcpv6' , 'none' ] : return None dns = self . netjson . get ( 'dns_servers' , None ) if dns : return ' ' . join ( dns )
1466	def setDefault ( self , constant , start , end ) : starttime = start / 60 * 60 if starttime < start : starttime += 60 endtime = end / 60 * 60 while starttime <= endtime : if starttime not in self . timeline or self . timeline [ starttime ] == 0 : self . timeline [ starttime ] = constant starttime += 60
11920	def get_object ( self ) : dataframe = self . filter_dataframe ( self . get_dataframe ( ) ) assert self . lookup_url_kwarg in self . kwargs , ( 'Expected view %s to be called with a URL keyword argument ' 'named "%s". Fix your URL conf, or set the `.lookup_field` ' 'attribute on the view correctly.' % ( self . __class__ . __name__ , self . lookup_url_kwarg ) ) try : obj = self . index_row ( dataframe ) except ( IndexError , KeyError , ValueError ) : raise Http404 self . check_object_permissions ( self . request , obj ) return obj
823	def addInstance ( self , groundTruth , prediction , record = None , result = None ) : self . value = self . avg ( prediction )
2793	def create ( self ) : params = { "name" : self . name , "type" : self . type , "dns_names" : self . dns_names , "private_key" : self . private_key , "leaf_certificate" : self . leaf_certificate , "certificate_chain" : self . certificate_chain } data = self . get_data ( "certificates/" , type = POST , params = params ) if data : self . id = data [ 'certificate' ] [ 'id' ] self . not_after = data [ 'certificate' ] [ 'not_after' ] self . sha1_fingerprint = data [ 'certificate' ] [ 'sha1_fingerprint' ] self . created_at = data [ 'certificate' ] [ 'created_at' ] self . type = data [ 'certificate' ] [ 'type' ] self . dns_names = data [ 'certificate' ] [ 'dns_names' ] self . state = data [ 'certificate' ] [ 'state' ] return self
13146	def remove_direct_link_triples ( train , valid , test ) : pairs = set ( ) merged = valid + test for t in merged : pairs . add ( ( t . head , t . tail ) ) filtered = filterfalse ( lambda t : ( t . head , t . tail ) in pairs or ( t . tail , t . head ) in pairs , train ) return list ( filtered )
8272	def _load ( self , top = 5 , blue = "blue" , archive = None , member = None ) : if archive is None : path = os . path . join ( self . cache , self . name + ".xml" ) xml = open ( path ) . read ( ) else : assert member is not None xml = archive . read ( member ) dom = parseString ( xml ) . documentElement attr = lambda e , a : e . attributes [ a ] . value for e in dom . getElementsByTagName ( "color" ) [ : top ] : w = float ( attr ( e , "weight" ) ) try : rgb = e . getElementsByTagName ( "rgb" ) [ 0 ] clr = color ( float ( attr ( rgb , "r" ) ) , float ( attr ( rgb , "g" ) ) , float ( attr ( rgb , "b" ) ) , float ( attr ( rgb , "a" ) ) , mode = "rgb" ) try : clr . name = attr ( e , "name" ) if clr . name == "blue" : clr = color ( blue ) except : pass except : name = attr ( e , "name" ) if name == "blue" : name = blue clr = color ( name ) for s in e . getElementsByTagName ( "shade" ) : self . ranges . append ( ( clr , shade ( attr ( s , "name" ) ) , w * float ( attr ( s , "weight" ) ) ) )
3252	def get_stores ( self , names = None , workspaces = None ) : if isinstance ( workspaces , Workspace ) : workspaces = [ workspaces ] elif isinstance ( workspaces , list ) and [ w for w in workspaces if isinstance ( w , Workspace ) ] : pass else : workspaces = self . get_workspaces ( names = workspaces ) stores = [ ] for ws in workspaces : ds_list = self . get_xml ( ws . datastore_url ) cs_list = self . get_xml ( ws . coveragestore_url ) wms_list = self . get_xml ( ws . wmsstore_url ) stores . extend ( [ datastore_from_index ( self , ws , n ) for n in ds_list . findall ( "dataStore" ) ] ) stores . extend ( [ coveragestore_from_index ( self , ws , n ) for n in cs_list . findall ( "coverageStore" ) ] ) stores . extend ( [ wmsstore_from_index ( self , ws , n ) for n in wms_list . findall ( "wmsStore" ) ] ) if names is None : names = [ ] elif isinstance ( names , basestring ) : names = [ s . strip ( ) for s in names . split ( ',' ) if s . strip ( ) ] if stores and names : return ( [ store for store in stores if store . name in names ] ) return stores
10308	def barh ( d , plt , title = None ) : labels = sorted ( d , key = d . get ) index = range ( len ( labels ) ) plt . yticks ( index , labels ) plt . barh ( index , [ d [ v ] for v in labels ] ) if title is not None : plt . title ( title )
8263	def swatch ( self , x , y , w = 35 , h = 35 , padding = 0 , roundness = 0 ) : for clr in self : clr . swatch ( x , y , w , h , roundness ) y += h + padding
203	def to_heatmaps ( self , only_nonempty = False , not_none_if_no_nonempty = False ) : from imgaug . augmentables . heatmaps import HeatmapsOnImage if not only_nonempty : return HeatmapsOnImage . from_0to1 ( self . arr , self . shape , min_value = 0.0 , max_value = 1.0 ) else : nonempty_mask = np . sum ( self . arr , axis = ( 0 , 1 ) ) > 0 + 1e-4 if np . sum ( nonempty_mask ) == 0 : if not_none_if_no_nonempty : nonempty_mask [ 0 ] = True else : return None , [ ] class_indices = np . arange ( self . arr . shape [ 2 ] ) [ nonempty_mask ] channels = self . arr [ ... , class_indices ] return HeatmapsOnImage ( channels , self . shape , min_value = 0.0 , max_value = 1.0 ) , class_indices
12842	def receive_id_from_server ( self ) : for message in self . pipe . receive ( ) : if isinstance ( message , IdFactory ) : self . actor_id_factory = message return True return False
10764	def _random_token ( self , bits = 128 ) : alphabet = string . ascii_letters + string . digits + '-_' num_letters = int ( math . ceil ( bits / 6.0 ) ) return '' . join ( random . choice ( alphabet ) for i in range ( num_letters ) )
11591	def _rc_sunion ( self , src , * args ) : args = list_or_args ( src , args ) src_set = self . smembers ( args . pop ( 0 ) ) if src_set is not set ( [ ] ) : for key in args : src_set . update ( self . smembers ( key ) ) return src_set
6159	def IIR_sos_header ( fname_out , SOS_mat ) : Ns , Mcol = SOS_mat . shape f = open ( fname_out , 'wt' ) f . write ( '//define a IIR SOS CMSIS-DSP coefficient array\n\n' ) f . write ( '#include <stdint.h>\n\n' ) f . write ( '#ifndef STAGES\n' ) f . write ( '#define STAGES %d\n' % Ns ) f . write ( '#endif\n' ) f . write ( '/*********************************************************/\n' ) f . write ( '/* IIR SOS Filter Coefficients */\n' ) f . write ( 'float32_t ba_coeff[%d] = { //b0,b1,b2,a1,a2,... by stage\n' % ( 5 * Ns ) ) for k in range ( Ns ) : if ( k < Ns - 1 ) : f . write ( ' %+-13e, %+-13e, %+-13e,\n' % ( SOS_mat [ k , 0 ] , SOS_mat [ k , 1 ] , SOS_mat [ k , 2 ] ) ) f . write ( ' %+-13e, %+-13e,\n' % ( - SOS_mat [ k , 4 ] , - SOS_mat [ k , 5 ] ) ) else : f . write ( ' %+-13e, %+-13e, %+-13e,\n' % ( SOS_mat [ k , 0 ] , SOS_mat [ k , 1 ] , SOS_mat [ k , 2 ] ) ) f . write ( ' %+-13e, %+-13e\n' % ( - SOS_mat [ k , 4 ] , - SOS_mat [ k , 5 ] ) ) f . write ( '};\n' ) f . write ( '/*********************************************************/\n' ) f . close ( )
13480	def _str_replacement ( self , target , replacement ) : self . data = self . data . replace ( target , replacement )
1568	def invoke_hook_bolt_ack ( self , heron_tuple , process_latency_ns ) : if len ( self . task_hooks ) > 0 : bolt_ack_info = BoltAckInfo ( heron_tuple = heron_tuple , acking_task_id = self . get_task_id ( ) , process_latency_ms = process_latency_ns * system_constants . NS_TO_MS ) for task_hook in self . task_hooks : task_hook . bolt_ack ( bolt_ack_info )
9189	def get_api_keys ( request ) : with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : cursor . execute ( ) api_keys = [ x [ 0 ] for x in cursor . fetchall ( ) ] return api_keys
12728	def axes ( self , axes ) : self . lmotor . axes = [ axes [ 0 ] ] self . ode_obj . setAxis ( tuple ( axes [ 0 ] ) )
4291	def cleanup_directory ( config_data ) : if os . path . exists ( config_data . project_directory ) : choice = False if config_data . noinput is False and not config_data . verbose : choice = query_yes_no ( 'The installation failed.\n' 'Do you want to clean up by removing {0}?\n' '\tWarning: this will delete all files in:\n' '\t\t{0}\n' 'Do you want to cleanup?' . format ( os . path . abspath ( config_data . project_directory ) ) , 'no' ) else : sys . stdout . write ( 'The installation has failed.\n' ) if config_data . skip_project_dir_check is False and ( choice or ( config_data . noinput and config_data . delete_project_dir ) ) : sys . stdout . write ( 'Removing everything under {0}\n' . format ( os . path . abspath ( config_data . project_directory ) ) ) shutil . rmtree ( config_data . project_directory , True )
6939	def checkplot_infokey_worker ( task ) : cpf , keys = task cpd = _read_checkplot_picklefile ( cpf ) resultkeys = [ ] for k in keys : try : resultkeys . append ( _dict_get ( cpd , k ) ) except Exception as e : resultkeys . append ( np . nan ) return resultkeys
10981	def accept ( group_id ) : membership = Membership . query . get_or_404 ( ( current_user . get_id ( ) , group_id ) ) try : membership . accept ( ) except Exception as e : flash ( str ( e ) , 'error' ) return redirect ( url_for ( '.invitations' , group_id = membership . group . id ) ) flash ( _ ( 'You are now part of %(name)s group.' , user = membership . user . email , name = membership . group . name ) , 'success' ) return redirect ( url_for ( '.invitations' , group_id = membership . group . id ) )
5362	def validate_config ( self ) : for key , key_config in self . params_map . items ( ) : if key_config [ 'required' ] : if key not in self . config : raise ValueError ( "Invalid Configuration! Required parameter '%s' was not provided to Sultan." ) for key in self . config . keys ( ) : if key not in self . params_map : raise ValueError ( "Invalid Configuration! The parameter '%s' provided is not used by Sultan!" % key )
12596	def _openpyxl_read_xl ( xl_path : str ) : try : wb = load_workbook ( filename = xl_path , read_only = True ) except : raise else : return wb
2191	def expired ( self , cfgstr = None , product = None ) : products = self . _rectify_products ( product ) certificate = self . _get_certificate ( cfgstr = cfgstr ) if certificate is None : is_expired = True elif products is None : is_expired = False elif not all ( map ( os . path . exists , products ) ) : is_expired = True else : product_file_hash = self . _product_file_hash ( products ) certificate_hash = certificate . get ( 'product_file_hash' , None ) is_expired = product_file_hash != certificate_hash return is_expired
129	def find_closest_point_index ( self , x , y , return_distance = False ) : ia . do_assert ( len ( self . exterior ) > 0 ) distances = [ ] for x2 , y2 in self . exterior : d = ( x2 - x ) ** 2 + ( y2 - y ) ** 2 distances . append ( d ) distances = np . sqrt ( distances ) closest_idx = np . argmin ( distances ) if return_distance : return closest_idx , distances [ closest_idx ] return closest_idx
506	def getLabels ( self , start = None , end = None ) : if len ( self . _recordsCache ) == 0 : return { 'isProcessing' : False , 'recordLabels' : [ ] } try : start = int ( start ) except Exception : start = 0 try : end = int ( end ) except Exception : end = self . _recordsCache [ - 1 ] . ROWID if end <= start : raise HTMPredictionModelInvalidRangeError ( "Invalid supplied range for 'getLabels'." , debugInfo = { 'requestRange' : { 'startRecordID' : start , 'endRecordID' : end } , 'numRecordsStored' : len ( self . _recordsCache ) } ) results = { 'isProcessing' : False , 'recordLabels' : [ ] } ROWIDX = numpy . array ( self . _knnclassifier . getParameter ( 'categoryRecencyList' ) ) validIdx = numpy . where ( ( ROWIDX >= start ) & ( ROWIDX < end ) ) [ 0 ] . tolist ( ) categories = self . _knnclassifier . getCategoryList ( ) for idx in validIdx : row = dict ( ROWID = int ( ROWIDX [ idx ] ) , labels = self . _categoryToLabelList ( categories [ idx ] ) ) results [ 'recordLabels' ] . append ( row ) return results
12932	def nav_to_vcf_dir ( ftp , build ) : if build == 'b37' : ftp . cwd ( DIR_CLINVAR_VCF_B37 ) elif build == 'b38' : ftp . cwd ( DIR_CLINVAR_VCF_B38 ) else : raise IOError ( "Genome build not recognized." )
6238	def add_point_light ( self , position , radius ) : self . point_lights . append ( PointLight ( position , radius ) )
8285	def _get_length ( self , segmented = False , precision = 10 ) : if not segmented : return sum ( self . _segment_lengths ( n = precision ) , 0.0 ) else : return self . _segment_lengths ( relative = True , n = precision )
8639	def accept_project_bid ( session , bid_id ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } bid_data = { 'action' : 'accept' } endpoint = 'bids/{}' . format ( bid_id ) response = make_put_request ( session , endpoint , headers = headers , params_data = bid_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : json_data = response . json ( ) raise BidNotAcceptedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
1139	def wrap ( text , width = 70 , ** kwargs ) : w = TextWrapper ( width = width , ** kwargs ) return w . wrap ( text )
12446	def resource ( ** kwargs ) : def inner ( function ) : name = kwargs . pop ( 'name' , None ) if name is None : name = utils . dasherize ( function . __name__ ) methods = kwargs . pop ( 'methods' , None ) if isinstance ( methods , six . string_types ) : methods = methods , handler = ( function , methods ) if name not in _resources : _handlers [ name ] = [ ] from armet import resources kwargs [ 'name' ] = name class LightweightResource ( resources . Resource ) : Meta = type ( str ( 'Meta' ) , ( ) , kwargs ) def route ( self , request , response ) : for handler , methods in _handlers [ name ] : if methods is None or request . method in methods : return handler ( request , response ) resources . Resource . route ( self ) _resources [ name ] = LightweightResource _handlers [ name ] . append ( handler ) return _resources [ name ] return inner
1204	def from_spec ( spec , kwargs = None ) : layer = util . get_object ( obj = spec , predefined_objects = tensorforce . core . networks . layers , kwargs = kwargs ) assert isinstance ( layer , Layer ) return layer
2809	def convert_constant ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting constant ...' ) params_list = params [ 'value' ] . numpy ( ) def target_layer ( x , value = params_list ) : return tf . constant ( value . tolist ( ) , shape = value . shape ) lambda_layer = keras . layers . Lambda ( target_layer ) layers [ scope_name + '_np' ] = params_list layers [ scope_name ] = lambda_layer ( layers [ list ( layers . keys ( ) ) [ 0 ] ] )
11974	def convert_nm ( nm , notation = IP_DOT , inotation = IP_UNKNOWN , check = True ) : return _convert ( nm , notation , inotation , _check = check , _isnm = True )
11004	def psffunc ( self , x , y , z , ** kwargs ) : if self . polychromatic : func = psfcalc . calculate_polychrome_pinhole_psf else : func = psfcalc . calculate_pinhole_psf x0 , y0 = [ psfcalc . vec_to_halfvec ( v ) for v in [ x , y ] ] vls = psfcalc . wrap_and_calc_psf ( x0 , y0 , z , func , ** kwargs ) return vls / vls . sum ( )
2571	def send_message ( self ) : start = time . time ( ) message = None if not self . initialized : message = self . construct_start_message ( ) self . initialized = True else : message = self . construct_end_message ( ) self . send_UDP_message ( message ) end = time . time ( ) return end - start
6741	def get_os_version ( ) : import warnings warnings . filterwarnings ( "ignore" , category = DeprecationWarning ) common_os_version = get_rc ( 'common_os_version' ) if common_os_version : return common_os_version with settings ( warn_only = True ) : with hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) : ret = _run_or_local ( 'cat /etc/lsb-release' ) if ret . succeeded : return OS ( type = LINUX , distro = UBUNTU , release = re . findall ( r'DISTRIB_RELEASE=([0-9\.]+)' , ret ) [ 0 ] ) ret = _run_or_local ( 'cat /etc/debian_version' ) if ret . succeeded : return OS ( type = LINUX , distro = DEBIAN , release = re . findall ( r'([0-9\.]+)' , ret ) [ 0 ] ) ret = _run_or_local ( 'cat /etc/fedora-release' ) if ret . succeeded : return OS ( type = LINUX , distro = FEDORA , release = re . findall ( r'release ([0-9]+)' , ret ) [ 0 ] ) raise Exception ( 'Unable to determine OS version.' )
7581	def parse ( self , psearch , dsearch ) : stable = "" with open ( self . repfile ) as orep : dat = orep . readlines ( ) for line in dat : if "Estimated Ln Prob of Data" in line : self . est_lnlik = float ( line . split ( ) [ - 1 ] ) if "Mean value of ln likelihood" in line : self . mean_lnlik = float ( line . split ( ) [ - 1 ] ) if "Variance of ln likelihood" in line : self . var_lnlik = float ( line . split ( ) [ - 1 ] ) if "Mean value of alpha" in line : self . alpha = float ( line . split ( ) [ - 1 ] ) nonline = psearch . search ( line ) popline = dsearch . search ( line ) if nonline : abc = line . strip ( ) . split ( ) outstr = "{}{}{}" . format ( " " . join ( [ abc [ 0 ] , abc [ 0 ] , abc [ 2 ] , abc [ 0 ] . split ( '.' ) [ 0 ] ] ) , " : " , " " . join ( abc [ 4 : ] ) ) self . inds += 1 stable += outstr + "\n" elif popline : abc = line . strip ( ) . split ( ) prop = [ "0.000" ] * self . kpop pidx = int ( abc [ 3 ] ) - 1 prop [ pidx ] = "1.000" outstr = "{}{}{}" . format ( " " . join ( [ abc [ 0 ] , abc [ 0 ] , abc [ 2 ] , abc [ 0 ] . split ( '.' ) [ 0 ] ] ) , " : " , " " . join ( prop ) ) self . inds += 1 stable += outstr + "\n" stable += "\n" return stable
5205	def proc_elms ( ** kwargs ) -> list : return [ ( ELEM_KEYS . get ( k , k ) , ELEM_VALS . get ( ELEM_KEYS . get ( k , k ) , dict ( ) ) . get ( v , v ) ) for k , v in kwargs . items ( ) if ( k in list ( ELEM_KEYS . keys ( ) ) + list ( ELEM_KEYS . values ( ) ) ) and ( k not in PRSV_COLS ) ]
7005	def apply_rf_classifier ( classifier , varfeaturesdir , outpickle , maxobjects = None ) : if isinstance ( classifier , str ) and os . path . exists ( classifier ) : with open ( classifier , 'rb' ) as infd : clfdict = pickle . load ( infd ) elif isinstance ( classifier , dict ) : clfdict = classifier else : LOGERROR ( "can't figure out the input classifier arg" ) return None if 'feature_names' not in clfdict : LOGERROR ( "feature_names not present in classifier input, " "can't figure out which ones to extract from " "varfeature pickles in %s" % varfeaturesdir ) return None featurestouse = clfdict [ 'feature_names' ] pklglob = clfdict [ 'collect_kwargs' ] [ 'pklglob' ] magcol = clfdict [ 'magcol' ] featfile = os . path . join ( os . path . dirname ( outpickle ) , 'actual-collected-features.pkl' ) features = collect_nonperiodic_features ( varfeaturesdir , magcol , featfile , pklglob = pklglob , featurestouse = featurestouse , maxobjects = maxobjects ) bestclf = clfdict [ 'best_classifier' ] predicted_labels = bestclf . predict ( features [ 'features_array' ] ) predicted_label_probs = bestclf . predict_proba ( features [ 'features_array' ] ) outdict = { 'features' : features , 'featfile' : featfile , 'classifier' : clfdict , 'predicted_labels' : predicted_labels , 'predicted_label_probs' : predicted_label_probs , } with open ( outpickle , 'wb' ) as outfd : pickle . dump ( outdict , outfd , pickle . HIGHEST_PROTOCOL ) return outdict
8250	def blend ( self , clr , factor = 0.5 ) : r = self . r * ( 1 - factor ) + clr . r * factor g = self . g * ( 1 - factor ) + clr . g * factor b = self . b * ( 1 - factor ) + clr . b * factor a = self . a * ( 1 - factor ) + clr . a * factor return Color ( r , g , b , a , mode = "rgb" )
8038	def code_mapping ( level , msg , default = 99 ) : try : return code_mappings_by_level [ level ] [ msg ] except KeyError : pass if msg . count ( '"' ) == 2 and ' "' in msg and msg . endswith ( '".' ) : txt = msg [ : msg . index ( ' "' ) ] return code_mappings_by_level [ level ] . get ( txt , default ) return default
9186	def get_moderation ( request ) : with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : cursor . execute ( ) moderations = [ x [ 0 ] for x in cursor . fetchall ( ) ] return moderations
13192	def geojson_to_gml ( gj , set_srs = True ) : tag = G ( gj [ 'type' ] ) if set_srs : tag . set ( 'srsName' , 'urn:ogc:def:crs:EPSG::4326' ) if gj [ 'type' ] == 'Point' : tag . append ( G . pos ( _reverse_geojson_coords ( gj [ 'coordinates' ] ) ) ) elif gj [ 'type' ] == 'LineString' : tag . append ( G . posList ( ' ' . join ( _reverse_geojson_coords ( ll ) for ll in gj [ 'coordinates' ] ) ) ) elif gj [ 'type' ] == 'Polygon' : rings = [ G . LinearRing ( G . posList ( ' ' . join ( _reverse_geojson_coords ( ll ) for ll in ring ) ) ) for ring in gj [ 'coordinates' ] ] tag . append ( G . exterior ( rings . pop ( 0 ) ) ) for ring in rings : tag . append ( G . interior ( ring ) ) elif gj [ 'type' ] in ( 'MultiPoint' , 'MultiLineString' , 'MultiPolygon' ) : single_type = gj [ 'type' ] [ 5 : ] member_tag = single_type [ 0 ] . lower ( ) + single_type [ 1 : ] + 'Member' for coord in gj [ 'coordinates' ] : tag . append ( G ( member_tag , geojson_to_gml ( { 'type' : single_type , 'coordinates' : coord } , set_srs = False ) ) ) else : raise NotImplementedError return tag
7570	def fastq_touchup_for_vsearch_merge ( read , outfile , reverse = False ) : counts = 0 with open ( outfile , 'w' ) as out : if read . endswith ( ".gz" ) : fr1 = gzip . open ( read , 'rb' ) else : fr1 = open ( read , 'rb' ) quarts = itertools . izip ( * [ iter ( fr1 ) ] * 4 ) writing = [ ] while 1 : try : lines = quarts . next ( ) except StopIteration : break if reverse : seq = lines [ 1 ] . strip ( ) [ : : - 1 ] else : seq = lines [ 1 ] . strip ( ) writing . append ( "" . join ( [ lines [ 0 ] , seq + "\n" , lines [ 2 ] , "B" * len ( seq ) ] ) ) counts += 1 if not counts % 1000 : out . write ( "\n" . join ( writing ) + "\n" ) writing = [ ] if writing : out . write ( "\n" . join ( writing ) ) out . close ( ) fr1 . close ( )
3774	def select_valid_methods ( self , T ) : r if self . forced : considered_methods = list ( self . user_methods ) else : considered_methods = list ( self . all_methods ) if self . user_methods : [ considered_methods . remove ( i ) for i in self . user_methods ] preferences = sorted ( [ self . ranked_methods . index ( i ) for i in considered_methods ] ) sorted_methods = [ self . ranked_methods [ i ] for i in preferences ] if self . user_methods : [ sorted_methods . insert ( 0 , i ) for i in reversed ( self . user_methods ) ] sorted_valid_methods = [ ] for method in sorted_methods : if self . test_method_validity ( T , method ) : sorted_valid_methods . append ( method ) return sorted_valid_methods
9815	def stop ( ctx , commit , yes ) : user , project_name = get_project_or_local ( ctx . obj . get ( 'project' ) ) if not yes and not click . confirm ( "Are sure you want to stop notebook " "for project `{}/{}`" . format ( user , project_name ) ) : click . echo ( 'Existing without stopping notebook.' ) sys . exit ( 1 ) if commit is None : commit = True try : PolyaxonClient ( ) . project . stop_notebook ( user , project_name , commit ) Printer . print_success ( 'Notebook is being deleted' ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not stop notebook project `{}`.' . format ( project_name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 )
13285	def clone ( src , dst_path , skip_globals , skip_dimensions , skip_variables ) : if os . path . exists ( dst_path ) : os . unlink ( dst_path ) dst = netCDF4 . Dataset ( dst_path , 'w' ) for attname in src . ncattrs ( ) : if attname not in skip_globals : setattr ( dst , attname , getattr ( src , attname ) ) unlimdim = None unlimdimname = False for dimname , dim in src . dimensions . items ( ) : if dimname in skip_dimensions : continue if dim . isunlimited ( ) : unlimdim = dim unlimdimname = dimname dst . createDimension ( dimname , None ) else : dst . createDimension ( dimname , len ( dim ) ) for varname , ncvar in src . variables . items ( ) : if varname in skip_variables : continue hasunlimdim = False if unlimdimname and unlimdimname in ncvar . dimensions : hasunlimdim = True filler = None if hasattr ( ncvar , '_FillValue' ) : filler = ncvar . _FillValue if ncvar . chunking == "contiguous" : var = dst . createVariable ( varname , ncvar . dtype , ncvar . dimensions , fill_value = filler ) else : var = dst . createVariable ( varname , ncvar . dtype , ncvar . dimensions , fill_value = filler , chunksizes = ncvar . chunking ( ) ) for attname in ncvar . ncattrs ( ) : if attname == '_FillValue' : continue else : setattr ( var , attname , getattr ( ncvar , attname ) ) nchunk = 1000 if hasunlimdim : if nchunk : start = 0 stop = len ( unlimdim ) step = nchunk if step < 1 : step = 1 for n in range ( start , stop , step ) : nmax = n + nchunk if nmax > len ( unlimdim ) : nmax = len ( unlimdim ) idata = ncvar [ n : nmax ] var [ n : nmax ] = idata else : idata = ncvar [ : ] var [ 0 : len ( unlimdim ) ] = idata else : idata = ncvar [ : ] var [ : ] = idata dst . sync ( ) src . close ( ) dst . close ( )
12037	def matrixValues ( matrix , key ) : assert key in matrix . dtype . names col = matrix . dtype . names . index ( key ) values = np . empty ( len ( matrix ) ) * np . nan for i in range ( len ( matrix ) ) : values [ i ] = matrix [ i ] [ col ] return values
9605	def fluent ( func ) : @ wraps ( func ) def fluent_interface ( instance , * args , ** kwargs ) : ret = func ( instance , * args , ** kwargs ) if ret is not None : return ret return instance return fluent_interface
242	def create_perf_attrib_tear_sheet ( returns , positions , factor_returns , factor_loadings , transactions = None , pos_in_dollars = True , return_fig = False , factor_partitions = FACTOR_PARTITIONS ) : portfolio_exposures , perf_attrib_data = perf_attrib . perf_attrib ( returns , positions , factor_returns , factor_loadings , transactions , pos_in_dollars = pos_in_dollars ) display ( Markdown ( "## Performance Relative to Common Risk Factors" ) ) perf_attrib . show_perf_attrib_stats ( returns , positions , factor_returns , factor_loadings , transactions , pos_in_dollars ) vertical_sections = 1 + 2 * max ( len ( factor_partitions ) , 1 ) current_section = 0 fig = plt . figure ( figsize = [ 14 , vertical_sections * 6 ] ) gs = gridspec . GridSpec ( vertical_sections , 1 , wspace = 0.5 , hspace = 0.5 ) perf_attrib . plot_returns ( perf_attrib_data , ax = plt . subplot ( gs [ current_section ] ) ) current_section += 1 if factor_partitions is not None : for factor_type , partitions in factor_partitions . iteritems ( ) : columns_to_select = perf_attrib_data . columns . intersection ( partitions ) perf_attrib . plot_factor_contribution_to_perf ( perf_attrib_data [ columns_to_select ] , ax = plt . subplot ( gs [ current_section ] ) , title = ( 'Cumulative common {} returns attribution' ) . format ( factor_type ) ) current_section += 1 for factor_type , partitions in factor_partitions . iteritems ( ) : perf_attrib . plot_risk_exposures ( portfolio_exposures [ portfolio_exposures . columns . intersection ( partitions ) ] , ax = plt . subplot ( gs [ current_section ] ) , title = 'Daily {} factor exposures' . format ( factor_type ) ) current_section += 1 else : perf_attrib . plot_factor_contribution_to_perf ( perf_attrib_data , ax = plt . subplot ( gs [ current_section ] ) ) current_section += 1 perf_attrib . plot_risk_exposures ( portfolio_exposures , ax = plt . subplot ( gs [ current_section ] ) ) gs . tight_layout ( fig ) if return_fig : return fig
7175	def main ( src , pyi_dir , target_dir , incremental , quiet , replace_any , hg , traceback ) : Config . incremental = incremental Config . replace_any = replace_any returncode = 0 for src_entry in src : for file , error , exc_type , tb in retype_path ( Path ( src_entry ) , pyi_dir = Path ( pyi_dir ) , targets = Path ( target_dir ) , src_explicitly_given = True , quiet = quiet , hg = hg , ) : print ( f'error: {file}: {error}' , file = sys . stderr ) if traceback : print ( 'Traceback (most recent call last):' , file = sys . stderr ) for line in tb : print ( line , file = sys . stderr , end = '' ) print ( f'{exc_type.__name__}: {error}' , file = sys . stderr ) returncode += 1 if not src and not quiet : print ( 'warning: no sources given' , file = sys . stderr ) sys . exit ( min ( returncode , 125 ) )
1298	def SetClipboardText ( text : str ) -> bool : if ctypes . windll . user32 . OpenClipboard ( 0 ) : ctypes . windll . user32 . EmptyClipboard ( ) textByteLen = ( len ( text ) + 1 ) * 2 hClipboardData = ctypes . windll . kernel32 . GlobalAlloc ( 0 , textByteLen ) hDestText = ctypes . windll . kernel32 . GlobalLock ( hClipboardData ) ctypes . cdll . msvcrt . wcsncpy ( ctypes . c_wchar_p ( hDestText ) , ctypes . c_wchar_p ( text ) , textByteLen // 2 ) ctypes . windll . kernel32 . GlobalUnlock ( hClipboardData ) ctypes . windll . user32 . SetClipboardData ( 13 , hClipboardData ) ctypes . windll . user32 . CloseClipboard ( ) return True return False
1556	def get_out_streamids ( self ) : if self . outputs is None : return set ( ) if not isinstance ( self . outputs , ( list , tuple ) ) : raise TypeError ( "Argument to outputs must be either list or tuple, given: %s" % str ( type ( self . outputs ) ) ) ret_lst = [ ] for output in self . outputs : if not isinstance ( output , ( str , Stream ) ) : raise TypeError ( "Outputs must be a list of strings or Streams, given: %s" % str ( output ) ) ret_lst . append ( Stream . DEFAULT_STREAM_ID if isinstance ( output , str ) else output . stream_id ) return set ( ret_lst )
3828	async def query_presence ( self , query_presence_request ) : response = hangouts_pb2 . QueryPresenceResponse ( ) await self . _pb_request ( 'presence/querypresence' , query_presence_request , response ) return response
4572	def hsv2rgb_raw ( hsv ) : HSV_SECTION_3 = 0x40 h , s , v = hsv invsat = 255 - s brightness_floor = ( v * invsat ) // 256 color_amplitude = v - brightness_floor section = h // HSV_SECTION_3 offset = h % HSV_SECTION_3 rampup = offset rampdown = ( HSV_SECTION_3 - 1 ) - offset rampup_amp_adj = ( rampup * color_amplitude ) // ( 256 // 4 ) rampdown_amp_adj = ( rampdown * color_amplitude ) // ( 256 // 4 ) rampup_adj_with_floor = rampup_amp_adj + brightness_floor rampdown_adj_with_floor = rampdown_amp_adj + brightness_floor r , g , b = ( 0 , 0 , 0 ) if section : if section == 1 : r = brightness_floor g = rampdown_adj_with_floor b = rampup_adj_with_floor else : r = rampup_adj_with_floor g = brightness_floor b = rampdown_adj_with_floor else : r = rampdown_adj_with_floor g = rampup_adj_with_floor b = brightness_floor return ( r , g , b )
11015	def publish ( context ) : header ( 'Recording changes...' ) run ( 'git add -A' ) header ( 'Displaying changes...' ) run ( 'git -c color.status=always status' ) if not click . confirm ( '\nContinue publishing' ) : run ( 'git reset HEAD --' ) abort ( context ) header ( 'Saving changes...' ) try : run ( 'git commit -m "{message}"' . format ( message = 'Publishing {}' . format ( choose_commit_emoji ( ) ) ) , capture = True ) except subprocess . CalledProcessError as e : if 'nothing to commit' not in e . stdout : raise else : click . echo ( 'Nothing to commit.' ) header ( 'Pushing to GitHub...' ) branch = get_branch ( ) run ( 'git push origin {branch}:{branch}' . format ( branch = branch ) ) pr_link = get_pr_link ( branch ) if pr_link : click . launch ( pr_link )
907	def replaceIterationCycle ( self , phaseSpecs ) : self . __phaseManager = _PhaseManager ( model = self . __model , phaseSpecs = phaseSpecs ) return
5545	def pyramid ( input_raster , output_dir , pyramid_type = None , output_format = None , resampling_method = None , scale_method = None , zoom = None , bounds = None , overwrite = False , debug = False ) : bounds = bounds if bounds else None options = dict ( pyramid_type = pyramid_type , scale_method = scale_method , output_format = output_format , resampling = resampling_method , zoom = zoom , bounds = bounds , overwrite = overwrite ) raster2pyramid ( input_raster , output_dir , options )
11586	def getnodefor ( self , name ) : "Return the node where the ``name`` would land to" node = self . _getnodenamefor ( name ) return { node : self . cluster [ 'nodes' ] [ node ] }
9996	def del_space ( self , name ) : if name not in self . spaces : raise ValueError ( "Space '%s' does not exist" % name ) if name in self . static_spaces : space = self . static_spaces [ name ] if space . is_derived : raise ValueError ( "%s has derived spaces" % repr ( space . interface ) ) else : self . static_spaces . del_item ( name ) self . model . spacegraph . remove_node ( space ) self . inherit ( ) self . model . spacegraph . update_subspaces ( self ) elif name in self . dynamic_spaces : self . dynamic_spaces . del_item ( name ) else : raise ValueError ( "Derived cells cannot be deleted" )
2333	def predict_dataset ( self , df ) : if len ( list ( df . columns ) ) == 2 : df . columns = [ "A" , "B" ] if self . model is None : raise AssertionError ( "Model has not been trained before predictions" ) df2 = DataFrame ( ) for idx , row in df . iterrows ( ) : df2 = df2 . append ( row , ignore_index = True ) df2 = df2 . append ( { 'A' : row [ "B" ] , 'B' : row [ "A" ] } , ignore_index = True ) return predict . predict ( deepcopy ( df2 ) , deepcopy ( self . model ) ) [ : : 2 ]
693	def loadExperiment ( path ) : if not os . path . isdir ( path ) : path = os . path . dirname ( path ) descriptionPyModule = loadExperimentDescriptionScriptFromDir ( path ) expIface = getExperimentDescriptionInterfaceFromModule ( descriptionPyModule ) return expIface . getModelDescription ( ) , expIface . getModelControl ( )
530	def getInputNames ( self ) : inputs = self . getSpec ( ) . inputs return [ inputs . getByIndex ( i ) [ 0 ] for i in xrange ( inputs . getCount ( ) ) ]
11139	def get_stats ( self ) : if self . __path is None : return 0 , 0 nfiles = 0 ndirs = 0 for fdict in self . get_repository_state ( ) : fdname = list ( fdict ) [ 0 ] if fdname == '' : continue if fdict [ fdname ] . get ( 'pyrepfileinfo' , False ) : nfiles += 1 elif fdict [ fdname ] . get ( 'pyrepdirinfo' , False ) : ndirs += 1 else : raise Exception ( 'Not sure what to do next. Please report issue' ) return ndirs , nfiles
5461	def get_job_and_task_param ( job_params , task_params , field ) : return job_params . get ( field , set ( ) ) | task_params . get ( field , set ( ) )
2788	def resize ( self , size_gigabytes , region ) : return self . get_data ( "volumes/%s/actions/" % self . id , type = POST , params = { "type" : "resize" , "size_gigabytes" : size_gigabytes , "region" : region } )
10149	def _ref ( self , resp , base_name = None ) : name = base_name or resp . get ( 'title' , '' ) or resp . get ( 'name' , '' ) pointer = self . json_pointer + name self . response_registry [ name ] = resp return { '$ref' : pointer }
6097	def luminosity_within_circle_in_units ( self , radius : dim . Length , unit_luminosity = 'eps' , kpc_per_arcsec = None , exposure_time = None ) : if not isinstance ( radius , dim . Length ) : radius = dim . Length ( value = radius , unit_length = 'arcsec' ) profile = self . new_profile_with_units_converted ( unit_length = radius . unit_length , unit_luminosity = unit_luminosity , kpc_per_arcsec = kpc_per_arcsec , exposure_time = exposure_time ) luminosity = quad ( profile . luminosity_integral , a = 0.0 , b = radius , args = ( 1.0 , ) ) [ 0 ] return dim . Luminosity ( luminosity , unit_luminosity )
13698	def wait ( self , timeout = None ) : if timeout is None : timeout = self . _timeout while self . _process . check_readable ( timeout ) : self . _flush ( )
6136	def add_model_string ( self , model_str , position = 1 , file_id = None ) : if file_id is None : file_id = self . make_unique_id ( 'inlined_input' ) ret_data = self . file_create ( File . from_string ( model_str , position , file_id ) ) return ret_data
771	def __constructMetricsModules ( self , metricSpecs ) : if not metricSpecs : return self . __metricSpecs = metricSpecs for spec in metricSpecs : if not InferenceElement . validate ( spec . inferenceElement ) : raise ValueError ( "Invalid inference element for metric spec: %r" % spec ) self . __metrics . append ( metrics . getModule ( spec ) ) self . __metricLabels . append ( spec . getLabel ( ) )
12535	def copy_files_to_other_folder ( self , output_folder , rename_files = True , mkdir = True , verbose = False ) : import shutil if not os . path . exists ( output_folder ) : os . mkdir ( output_folder ) if not rename_files : for dcmf in self . items : outf = os . path . join ( output_folder , os . path . basename ( dcmf ) ) if verbose : print ( '{} -> {}' . format ( dcmf , outf ) ) shutil . copyfile ( dcmf , outf ) else : n_pad = len ( self . items ) + 2 for idx , dcmf in enumerate ( self . items ) : outf = '{number:0{width}d}.dcm' . format ( width = n_pad , number = idx ) outf = os . path . join ( output_folder , outf ) if verbose : print ( '{} -> {}' . format ( dcmf , outf ) ) shutil . copyfile ( dcmf , outf )
7646	def note_hz_to_midi ( annotation ) : annotation . namespace = 'note_midi' data = annotation . pop_data ( ) for obs in data : annotation . append ( time = obs . time , duration = obs . duration , confidence = obs . confidence , value = 12 * ( np . log2 ( obs . value ) - np . log2 ( 440.0 ) ) + 69 ) return annotation
1392	def synch_topologies ( self ) : self . state_managers = statemanagerfactory . get_all_state_managers ( self . config . statemgr_config ) try : for state_manager in self . state_managers : state_manager . start ( ) except Exception as ex : Log . error ( "Found exception while initializing state managers: %s. Bailing out..." % ex ) traceback . print_exc ( ) sys . exit ( 1 ) def on_topologies_watch ( state_manager , topologies ) : Log . info ( "State watch triggered for topologies." ) Log . debug ( "Topologies: " + str ( topologies ) ) existingTopologies = self . getTopologiesForStateLocation ( state_manager . name ) existingTopNames = map ( lambda t : t . name , existingTopologies ) Log . debug ( "Existing topologies: " + str ( existingTopNames ) ) for name in existingTopNames : if name not in topologies : Log . info ( "Removing topology: %s in rootpath: %s" , name , state_manager . rootpath ) self . removeTopology ( name , state_manager . name ) for name in topologies : if name not in existingTopNames : self . addNewTopology ( state_manager , name ) for state_manager in self . state_managers : onTopologiesWatch = partial ( on_topologies_watch , state_manager ) state_manager . get_topologies ( onTopologiesWatch )
12965	def allOnlyFields ( self , fields , cascadeFetch = False ) : matchedKeys = self . getPrimaryKeys ( ) if matchedKeys : return self . getMultipleOnlyFields ( matchedKeys , fields , cascadeFetch = cascadeFetch ) return IRQueryableList ( [ ] , mdl = self . mdl )
7348	async def get_access_token ( consumer_key , consumer_secret , oauth_token , oauth_token_secret , oauth_verifier , ** kwargs ) : client = BasePeonyClient ( consumer_key = consumer_key , consumer_secret = consumer_secret , access_token = oauth_token , access_token_secret = oauth_token_secret , api_version = "" , suffix = "" ) response = await client . api . oauth . access_token . get ( _suffix = "" , oauth_verifier = oauth_verifier ) return parse_token ( response )
9528	def pbkdf2 ( password , salt , iterations , dklen = 0 , digest = None ) : if digest is None : digest = settings . CRYPTOGRAPHY_DIGEST if not dklen : dklen = digest . digest_size password = force_bytes ( password ) salt = force_bytes ( salt ) kdf = PBKDF2HMAC ( algorithm = digest , length = dklen , salt = salt , iterations = iterations , backend = settings . CRYPTOGRAPHY_BACKEND ) return kdf . derive ( password )
2461	def set_file_name ( self , doc , name ) : if self . has_package ( doc ) : doc . package . files . append ( file . File ( name ) ) self . reset_file_stat ( ) return True else : raise OrderError ( 'File::Name' )
3891	def html ( tag ) : return ( HTML_START . format ( tag = tag ) , HTML_END . format ( tag = tag ) )
6919	def _autocorr_func1 ( mags , lag , maglen , magmed , magstd ) : lagindex = nparange ( 1 , maglen - lag ) products = ( mags [ lagindex ] - magmed ) * ( mags [ lagindex + lag ] - magmed ) acorr = ( 1.0 / ( ( maglen - lag ) * magstd ) ) * npsum ( products ) return acorr
6943	def jhk_to_bmag ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , BJHK , BJH , BJK , BHK , BJ , BH , BK )
5101	def adjacency2graph ( adjacency , edge_type = None , adjust = 1 , ** kwargs ) : if isinstance ( adjacency , np . ndarray ) : adjacency = _matrix2dict ( adjacency ) elif isinstance ( adjacency , dict ) : adjacency = _dict2dict ( adjacency ) else : msg = ( "If the adjacency parameter is supplied it must be a " "dict, or a numpy.ndarray." ) raise TypeError ( msg ) if edge_type is None : edge_type = { } else : if isinstance ( edge_type , np . ndarray ) : edge_type = _matrix2dict ( edge_type , etype = True ) elif isinstance ( edge_type , dict ) : edge_type = _dict2dict ( edge_type ) for u , ty in edge_type . items ( ) : for v , et in ty . items ( ) : adjacency [ u ] [ v ] [ 'edge_type' ] = et g = nx . from_dict_of_dicts ( adjacency , create_using = nx . DiGraph ( ) ) adjacency = nx . to_dict_of_dicts ( g ) adjacency = _adjacency_adjust ( adjacency , adjust , True ) return nx . from_dict_of_dicts ( adjacency , create_using = nx . DiGraph ( ) )
6947	def jhk_to_sdssu ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , SDSSU_JHK , SDSSU_JH , SDSSU_JK , SDSSU_HK , SDSSU_J , SDSSU_H , SDSSU_K )
3612	def do_filter ( qs , keywords , exclude = False ) : and_q = Q ( ) for keyword , value in iteritems ( keywords ) : try : values = value . split ( "," ) if len ( values ) > 0 : or_q = Q ( ) for value in values : or_q |= Q ( ** { keyword : value } ) and_q &= or_q except AttributeError : and_q &= Q ( ** { keyword : value } ) if exclude : qs = qs . exclude ( and_q ) else : qs = qs . filter ( and_q ) return qs
108	def draw_grid ( images , rows = None , cols = None ) : nb_images = len ( images ) do_assert ( nb_images > 0 ) if is_np_array ( images ) : do_assert ( images . ndim == 4 ) else : do_assert ( is_iterable ( images ) and is_np_array ( images [ 0 ] ) and images [ 0 ] . ndim == 3 ) dts = [ image . dtype . name for image in images ] nb_dtypes = len ( set ( dts ) ) do_assert ( nb_dtypes == 1 , ( "All images provided to draw_grid() must have the same dtype, " + "found %d dtypes (%s)" ) % ( nb_dtypes , ", " . join ( dts ) ) ) cell_height = max ( [ image . shape [ 0 ] for image in images ] ) cell_width = max ( [ image . shape [ 1 ] for image in images ] ) channels = set ( [ image . shape [ 2 ] for image in images ] ) do_assert ( len ( channels ) == 1 , "All images are expected to have the same number of channels, " + "but got channel set %s with length %d instead." % ( str ( channels ) , len ( channels ) ) ) nb_channels = list ( channels ) [ 0 ] if rows is None and cols is None : rows = cols = int ( math . ceil ( math . sqrt ( nb_images ) ) ) elif rows is not None : cols = int ( math . ceil ( nb_images / rows ) ) elif cols is not None : rows = int ( math . ceil ( nb_images / cols ) ) do_assert ( rows * cols >= nb_images ) width = cell_width * cols height = cell_height * rows dt = images . dtype if is_np_array ( images ) else images [ 0 ] . dtype grid = np . zeros ( ( height , width , nb_channels ) , dtype = dt ) cell_idx = 0 for row_idx in sm . xrange ( rows ) : for col_idx in sm . xrange ( cols ) : if cell_idx < nb_images : image = images [ cell_idx ] cell_y1 = cell_height * row_idx cell_y2 = cell_y1 + image . shape [ 0 ] cell_x1 = cell_width * col_idx cell_x2 = cell_x1 + image . shape [ 1 ] grid [ cell_y1 : cell_y2 , cell_x1 : cell_x2 , : ] = image cell_idx += 1 return grid
5511	def register_memory ( ) : def get_mem ( proc ) : if os . name == 'posix' : mem = proc . memory_info_ex ( ) counter = mem . rss if 'shared' in mem . _fields : counter -= mem . shared return counter else : return proc . get_memory_info ( ) . rss if SERVER_PROC is not None : mem = get_mem ( SERVER_PROC ) for child in SERVER_PROC . children ( ) : mem += get_mem ( child ) server_memory . append ( bytes2human ( mem ) )
6458	def _m_degree ( self , term ) : mdeg = 0 last_was_vowel = False for letter in term : if letter in self . _vowels : last_was_vowel = True else : if last_was_vowel : mdeg += 1 last_was_vowel = False return mdeg
10737	def path_from_keywords ( keywords , into = 'path' ) : subdirs = [ ] def prepare_string ( s ) : s = str ( s ) s = re . sub ( '[][{},*"' + f"'{os.sep}]" , '_' , s ) if into == 'file' : s = s . replace ( '_' , ' ' ) if ' ' in s : s = s . title ( ) s = s . replace ( ' ' , '' ) return s if isinstance ( keywords , set ) : keywords_list = sorted ( keywords ) for property in keywords_list : subdirs . append ( prepare_string ( property ) ) else : keywords_list = sorted ( keywords . items ( ) ) for property , value in keywords_list : if Bool . valid ( value ) : subdirs . append ( ( '' if value else ( 'not_' if into == 'path' else 'not' ) ) + prepare_string ( property ) ) elif ( Float | Integer ) . valid ( value ) : subdirs . append ( '{}{}' . format ( prepare_string ( property ) , prepare_string ( value ) ) ) else : subdirs . append ( '{}{}{}' . format ( prepare_string ( property ) , '_' if into == 'path' else '' , prepare_string ( value ) ) ) if into == 'path' : out = os . path . join ( * subdirs ) else : out = '_' . join ( subdirs ) return out
10329	def get_path_effect ( graph , path , relationship_dict ) : causal_effect = [ ] for predecessor , successor in pairwise ( path ) : if pair_has_contradiction ( graph , predecessor , successor ) : return Effect . ambiguous edges = graph . get_edge_data ( predecessor , successor ) edge_key , edge_relation , _ = rank_edges ( edges ) relation = graph [ predecessor ] [ successor ] [ edge_key ] [ RELATION ] if relation not in relationship_dict or relationship_dict [ relation ] == 0 : return Effect . no_effect causal_effect . append ( relationship_dict [ relation ] ) final_effect = reduce ( lambda x , y : x * y , causal_effect ) return Effect . activation if final_effect == 1 else Effect . inhibition
3244	def get_security_group ( security_group , flags = FLAGS . ALL , ** kwargs ) : result = registry . build_out ( flags , start_with = security_group , pass_datastructure = True , ** kwargs ) result . pop ( 'security_group_rules' , [ ] ) return result
6756	def param_changed_to ( self , key , to_value , from_value = None ) : last_value = getattr ( self . last_manifest , key ) current_value = self . current_manifest . get ( key ) if from_value is not None : return last_value == from_value and current_value == to_value return last_value != to_value and current_value == to_value
13484	def ghpages ( ) : opts = options docroot = path ( opts . get ( 'docroot' , 'docs' ) ) if not docroot . exists ( ) : raise BuildFailure ( "Sphinx documentation root (%s) does not exist." % docroot ) builddir = docroot / opts . get ( "builddir" , ".build" ) builddir = builddir / 'html' if not builddir . exists ( ) : raise BuildFailure ( "Sphinx build directory (%s) does not exist." % builddir ) nojekyll = path ( builddir ) / '.nojekyll' nojekyll . touch ( ) sh ( 'ghp-import -p %s' % ( builddir ) )
13399	def resourcePath ( self , relative_path ) : from os import path import sys try : base_path = sys . _MEIPASS except Exception : base_path = path . dirname ( path . abspath ( __file__ ) ) return path . join ( base_path , relative_path )
6949	def jhk_to_sdssr ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , SDSSR_JHK , SDSSR_JH , SDSSR_JK , SDSSR_HK , SDSSR_J , SDSSR_H , SDSSR_K )
4756	def postprocess ( trun ) : plog = [ ] plog . append ( ( "trun" , process_trun ( trun ) ) ) for tsuite in trun [ "testsuites" ] : plog . append ( ( "tsuite" , process_tsuite ( tsuite ) ) ) for tcase in tsuite [ "testcases" ] : plog . append ( ( "tcase" , process_tcase ( tcase ) ) ) for task , success in plog : if not success : cij . err ( "rprtr::postprocess: FAILED for %r" % task ) return sum ( ( success for task , success in plog ) )
636	def numSegments ( self , cell = None ) : if cell is not None : return len ( self . _cells [ cell ] . _segments ) return self . _nextFlatIdx - len ( self . _freeFlatIdxs )
5701	def _feed_calendar_span ( gtfs , stats ) : n_feeds = _n_gtfs_sources ( gtfs ) [ 0 ] max_start = None min_end = None if n_feeds > 1 : for i in range ( n_feeds ) : feed_key = "feed_" + str ( i ) + "_" start_key = feed_key + "calendar_start" end_key = feed_key + "calendar_end" calendar_span = gtfs . conn . cursor ( ) . execute ( 'SELECT min(date), max(date) FROM trips, days ' 'WHERE trips.trip_I = days.trip_I AND trip_id LIKE ?;' , ( feed_key + '%' , ) ) . fetchone ( ) stats [ start_key ] = calendar_span [ 0 ] stats [ end_key ] = calendar_span [ 1 ] if calendar_span [ 0 ] is not None and calendar_span [ 1 ] is not None : if not max_start and not min_end : max_start = calendar_span [ 0 ] min_end = calendar_span [ 1 ] else : if gtfs . get_day_start_ut ( calendar_span [ 0 ] ) > gtfs . get_day_start_ut ( max_start ) : max_start = calendar_span [ 0 ] if gtfs . get_day_start_ut ( calendar_span [ 1 ] ) < gtfs . get_day_start_ut ( min_end ) : min_end = calendar_span [ 1 ] stats [ "latest_feed_start_date" ] = max_start stats [ "earliest_feed_end_date" ] = min_end else : stats [ "latest_feed_start_date" ] = stats [ "start_date" ] stats [ "earliest_feed_end_date" ] = stats [ "end_date" ] return stats
3216	def get_classic_link ( vpc , ** conn ) : result = { } try : cl_result = describe_vpc_classic_link ( VpcIds = [ vpc [ "id" ] ] , ** conn ) [ 0 ] result [ "Enabled" ] = cl_result [ "ClassicLinkEnabled" ] dns_result = describe_vpc_classic_link_dns_support ( VpcIds = [ vpc [ "id" ] ] , ** conn ) [ 0 ] result [ "DnsEnabled" ] = dns_result [ "ClassicLinkDnsSupported" ] except ClientError as e : if 'UnsupportedOperation' not in str ( e ) : raise e return result
13044	def main ( ) : config = Config ( ) pipes_dir = config . get ( 'pipes' , 'directory' ) pipes_config = config . get ( 'pipes' , 'config_file' ) pipes_config_path = os . path . join ( config . config_dir , pipes_config ) if not os . path . exists ( pipes_config_path ) : print_error ( "Please configure the named pipes first" ) return workers = create_pipe_workers ( pipes_config_path , pipes_dir ) if workers : for worker in workers : worker . start ( ) try : for worker in workers : worker . join ( ) except KeyboardInterrupt : print_notification ( "Shutting down" ) for worker in workers : worker . terminate ( ) worker . join ( )
6845	def check_ok ( self ) : import requests if not self . env . check_ok : return branch_name = self . _local ( 'git rev-parse --abbrev-ref HEAD' , capture = True ) . strip ( ) check_ok_paths = self . env . check_ok_paths or { } if branch_name in check_ok_paths : check = check_ok_paths [ branch_name ] if 'username' in check : auth = ( check [ 'username' ] , check [ 'password' ] ) else : auth = None ret = requests . get ( check [ 'url' ] , auth = auth ) passed = check [ 'text' ] in ret . content assert passed , 'Check failed: %s' % check [ 'url' ]
3386	def _random_point ( self ) : idx = np . random . randint ( self . n_warmup , size = min ( 2 , np . ceil ( np . sqrt ( self . n_warmup ) ) ) ) return self . warmup [ idx , : ] . mean ( axis = 0 )
7793	def set_fetcher ( self , fetcher_class ) : self . _lock . acquire ( ) try : self . _fetcher = fetcher_class finally : self . _lock . release ( )
2917	def _eval_args ( args , my_task ) : results = [ ] for arg in args : if isinstance ( arg , Attrib ) or isinstance ( arg , PathAttrib ) : results . append ( valueof ( my_task , arg ) ) else : results . append ( arg ) return results
2482	def parse ( self , data ) : try : return self . yacc . parse ( data , lexer = self . lex ) except : return None
7308	def set_list_field ( self , document , form_key , current_key , remaining_key , key_array_digit ) : document_field = document . _fields . get ( current_key ) list_value = translate_value ( document_field . field , self . form . cleaned_data [ form_key ] ) if list_value is None or ( not list_value and not bool ( list_value ) ) : return None current_list = getattr ( document , current_key , None ) if isinstance ( document_field . field , EmbeddedDocumentField ) : embedded_list_key = u"{0}_{1}" . format ( current_key , key_array_digit ) embedded_list_document = self . embedded_list_docs . get ( embedded_list_key , None ) if embedded_list_document is None : embedded_list_document = document_field . field . document_type_obj ( ) new_key , new_remaining_key_array = trim_field_key ( embedded_list_document , remaining_key ) self . process_document ( embedded_list_document , form_key , new_key ) list_value = embedded_list_document self . embedded_list_docs [ embedded_list_key ] = embedded_list_document if isinstance ( current_list , list ) : if embedded_list_document not in current_list : current_list . append ( embedded_list_document ) else : setattr ( document , current_key , [ embedded_list_document ] ) elif isinstance ( current_list , list ) : current_list . append ( list_value ) else : setattr ( document , current_key , [ list_value ] )
13880	def MoveFile ( source_filename , target_filename ) : _AssertIsLocal ( source_filename ) _AssertIsLocal ( target_filename ) import shutil shutil . move ( source_filename , target_filename )
12509	def get_img_info ( image ) : try : img = check_img ( image ) except Exception as exc : raise Exception ( 'Error reading file {0}.' . format ( repr_imgs ( image ) ) ) from exc else : return img . get_header ( ) , img . get_affine ( )
8723	def display ( content ) : if isinstance ( content , gp . GPServer ) : IPython . display . display ( GPAuthWidget ( content ) ) elif isinstance ( content , gp . GPTask ) : IPython . display . display ( GPTaskWidget ( content ) ) elif isinstance ( content , gp . GPJob ) : IPython . display . display ( GPJobWidget ( content ) ) else : IPython . display . display ( content )
3200	def create ( self , data ) : if 'recipients' not in data : raise KeyError ( 'The campaign must have recipients' ) if 'list_id' not in data [ 'recipients' ] : raise KeyError ( 'The campaign recipients must have a list_id' ) if 'settings' not in data : raise KeyError ( 'The campaign must have settings' ) if 'subject_line' not in data [ 'settings' ] : raise KeyError ( 'The campaign settings must have a subject_line' ) if 'from_name' not in data [ 'settings' ] : raise KeyError ( 'The campaign settings must have a from_name' ) if 'reply_to' not in data [ 'settings' ] : raise KeyError ( 'The campaign settings must have a reply_to' ) check_email ( data [ 'settings' ] [ 'reply_to' ] ) if 'type' not in data : raise KeyError ( 'The campaign must have a type' ) if not data [ 'type' ] in [ 'regular' , 'plaintext' , 'rss' , 'variate' , 'abspilt' ] : raise ValueError ( 'The campaign type must be one of "regular", "plaintext", "rss", or "variate"' ) if data [ 'type' ] == 'variate' : if 'variate_settings' not in data : raise KeyError ( 'The variate campaign must have variate_settings' ) if 'winner_criteria' not in data [ 'variate_settings' ] : raise KeyError ( 'The campaign variate_settings must have a winner_criteria' ) if data [ 'variate_settings' ] [ 'winner_criteria' ] not in [ 'opens' , 'clicks' , 'total_revenue' , 'manual' ] : raise ValueError ( 'The campaign variate_settings ' 'winner_criteria must be one of "opens", "clicks", "total_revenue", or "manual"' ) if data [ 'type' ] == 'rss' : if 'rss_opts' not in data : raise KeyError ( 'The rss campaign must have rss_opts' ) if 'feed_url' not in data [ 'rss_opts' ] : raise KeyError ( 'The campaign rss_opts must have a feed_url' ) if not data [ 'rss_opts' ] [ 'frequency' ] in [ 'daily' , 'weekly' , 'monthly' ] : raise ValueError ( 'The rss_opts frequency must be one of "daily", "weekly", or "monthly"' ) response = self . _mc_client . _post ( url = self . _build_path ( ) , data = data ) if response is not None : self . campaign_id = response [ 'id' ] else : self . campaign_id = None return response
11558	def get_stepper_version ( self , timeout = 20 ) : start_time = time . time ( ) while self . _command_handler . stepper_library_version <= 0 : if time . time ( ) - start_time > timeout : if self . verbose is True : print ( "Stepper Library Version Request timed-out. " "Did you send a stepper_request_library_version command?" ) return else : pass return self . _command_handler . stepper_library_version
5176	def fact ( self , name ) : facts = self . facts ( name = name ) return next ( fact for fact in facts )
7622	def tempo ( ref , est , ** kwargs ) : r ref = coerce_annotation ( ref , 'tempo' ) est = coerce_annotation ( est , 'tempo' ) ref_tempi = np . asarray ( [ o . value for o in ref ] ) ref_weight = ref . data [ 0 ] . confidence est_tempi = np . asarray ( [ o . value for o in est ] ) return mir_eval . tempo . evaluate ( ref_tempi , ref_weight , est_tempi , ** kwargs )
6153	def fir_remez_bsf ( f_pass1 , f_stop1 , f_stop2 , f_pass2 , d_pass , d_stop , fs = 1.0 , N_bump = 5 ) : n , ff , aa , wts = bandstop_order ( f_pass1 , f_stop1 , f_stop2 , f_pass2 , d_pass , d_stop , fsamp = fs ) if np . mod ( n , 2 ) != 0 : n += 1 N_taps = n N_taps += N_bump b = signal . remez ( N_taps , ff , aa [ 0 : : 2 ] , wts , Hz = 2 , maxiter = 25 , grid_density = 16 ) print ( 'N_bump must be odd to maintain odd filter length' ) print ( 'Remez filter taps = %d.' % N_taps ) return b
3374	def remove_cons_vars_from_problem ( model , what ) : context = get_context ( model ) model . solver . remove ( what ) if context : context ( partial ( model . solver . add , what ) )
10042	def admin_permission_factory ( ) : try : pkg_resources . get_distribution ( 'invenio-access' ) from invenio_access . permissions import DynamicPermission as Permission except pkg_resources . DistributionNotFound : from flask_principal import Permission return Permission ( action_admin_access )
10029	def add_arguments ( parser ) : parser . add_argument ( '-o' , '--old-environment' , help = 'Old environment name' , required = True ) parser . add_argument ( '-n' , '--new-environment' , help = 'New environment name' , required = True )
10257	def get_causal_sink_nodes ( graph : BELGraph , func ) -> Set [ BaseEntity ] : return { node for node in graph if node . function == func and is_causal_sink ( graph , node ) }
8883	def predict ( self , X ) : check_is_fitted ( self , [ 'inverse_influence_matrix' ] ) X = check_array ( X ) return self . __find_leverages ( X , self . inverse_influence_matrix ) <= self . threshold_value
7194	def histogram_stretch ( self , use_bands , ** kwargs ) : data = self . _read ( self [ use_bands , ... ] , ** kwargs ) data = np . rollaxis ( data . astype ( np . float32 ) , 0 , 3 ) return self . _histogram_stretch ( data , ** kwargs )
9304	def parse_date ( date_str ) : months = [ 'jan' , 'feb' , 'mar' , 'apr' , 'may' , 'jun' , 'jul' , 'aug' , 'sep' , 'oct' , 'nov' , 'dec' ] formats = { r'^(?:\w{3}, )?(\d{2}) (\w{3}) (\d{4})\D.*$' : lambda m : '{}-{:02d}-{}' . format ( m . group ( 3 ) , months . index ( m . group ( 2 ) . lower ( ) ) + 1 , m . group ( 1 ) ) , r'^\w+day, (\d{2})-(\w{3})-(\d{2})\D.*$' : lambda m : '{}{}-{:02d}-{}' . format ( str ( datetime . date . today ( ) . year ) [ : 2 ] , m . group ( 3 ) , months . index ( m . group ( 2 ) . lower ( ) ) + 1 , m . group ( 1 ) ) , r'^\w{3} (\w{3}) (\d{1,2}) \d{2}:\d{2}:\d{2} (\d{4})$' : lambda m : '{}-{:02d}-{:02d}' . format ( m . group ( 3 ) , months . index ( m . group ( 1 ) . lower ( ) ) + 1 , int ( m . group ( 2 ) ) ) , r'^(\d{4})(\d{2})(\d{2})T\d{6}Z$' : lambda m : '{}-{}-{}' . format ( * m . groups ( ) ) , r'^(\d{4}-\d{2}-\d{2})(?:[Tt].*)?$' : lambda m : m . group ( 1 ) , } out_date = None for regex , xform in formats . items ( ) : m = re . search ( regex , date_str ) if m : out_date = xform ( m ) break if out_date is None : raise DateFormatError else : return out_date
11478	def _streaming_file_md5 ( file_path ) : md5 = hashlib . md5 ( ) with open ( file_path , 'rb' ) as f : for chunk in iter ( lambda : f . read ( 128 * md5 . block_size ) , b'' ) : md5 . update ( chunk ) return md5 . hexdigest ( )
13240	def includes ( self , query_date , query_time = None ) : if self . start_date and query_date < self . start_date : return False if self . end_date and query_date > self . end_date : return False if query_date . weekday ( ) not in self . weekdays : return False if not query_time : return True if query_time >= self . period . start and query_time <= self . period . end : return True return False
9616	def is_displayed ( target ) : is_displayed = getattr ( target , 'is_displayed' , None ) if not is_displayed or not callable ( is_displayed ) : raise TypeError ( 'Target has no attribute \'is_displayed\' or not callable' ) if not is_displayed ( ) : raise WebDriverException ( 'element not visible' )
4946	def send_course_enrollment_statement ( lrs_configuration , course_enrollment ) : user_details = LearnerInfoSerializer ( course_enrollment . user ) course_details = CourseInfoSerializer ( course_enrollment . course ) statement = LearnerCourseEnrollmentStatement ( course_enrollment . user , course_enrollment . course , user_details . data , course_details . data , ) EnterpriseXAPIClient ( lrs_configuration ) . save_statement ( statement )
1338	def crossentropy ( label , logits ) : assert logits . ndim == 1 logits = logits - np . max ( logits ) e = np . exp ( logits ) s = np . sum ( e ) ce = np . log ( s ) - logits [ label ] return ce
110	def imshow ( image , backend = IMSHOW_BACKEND_DEFAULT ) : do_assert ( backend in [ "matplotlib" , "cv2" ] , "Expected backend 'matplotlib' or 'cv2', got %s." % ( backend , ) ) if backend == "cv2" : image_bgr = image if image . ndim == 3 and image . shape [ 2 ] in [ 3 , 4 ] : image_bgr = image [ ... , 0 : 3 ] [ ... , : : - 1 ] win_name = "imgaug-default-window" cv2 . namedWindow ( win_name , cv2 . WINDOW_NORMAL ) cv2 . imshow ( win_name , image_bgr ) cv2 . waitKey ( 0 ) cv2 . destroyWindow ( win_name ) else : import matplotlib . pyplot as plt dpi = 96 h , w = image . shape [ 0 ] / dpi , image . shape [ 1 ] / dpi w = max ( w , 6 ) fig , ax = plt . subplots ( figsize = ( w , h ) , dpi = dpi ) fig . canvas . set_window_title ( "imgaug.imshow(%s)" % ( image . shape , ) ) ax . imshow ( image , cmap = "gray" ) plt . show ( )
5240	def market_close ( self , session , mins ) -> Session : if session not in self . exch : return SessNA end_time = self . exch [ session ] [ - 1 ] return Session ( shift_time ( end_time , - int ( mins ) + 1 ) , end_time )
3569	def centralManager_didDiscoverPeripheral_advertisementData_RSSI_ ( self , manager , peripheral , data , rssi ) : logger . debug ( 'centralManager_didDiscoverPeripheral_advertisementData_RSSI called' ) device = device_list ( ) . get ( peripheral ) if device is None : device = device_list ( ) . add ( peripheral , CoreBluetoothDevice ( peripheral ) ) device . _update_advertised ( data )
9034	def instruction_in_grid ( self , instruction ) : row_position = self . _rows_in_grid [ instruction . row ] . xy x = instruction . index_of_first_consumed_mesh_in_row position = Point ( row_position . x + x , row_position . y ) return InstructionInGrid ( instruction , position )
8173	def _angle ( self ) : from math import atan , pi , degrees a = degrees ( atan ( self . vy / self . vx ) ) + 360 if self . vx < 0 : a += 180 return a
10795	def perfect_platonic_per_pixel ( N , R , scale = 11 , pos = None , zscale = 1.0 , returnpix = None ) : if scale % 2 != 1 : scale += 1 if pos is None : pos = np . array ( [ ( N - 1 ) / 2.0 ] * 3 ) s = 1.0 / scale f = zscale ** 2 i = pos . astype ( 'int' ) p = i + s * ( ( pos - i ) / s ) . astype ( 'int' ) pos = p + 1e-10 image = np . zeros ( ( N , ) * 3 ) x , y , z = np . meshgrid ( * ( xrange ( N ) , ) * 3 , indexing = 'ij' ) for x0 , y0 , z0 in zip ( x . flatten ( ) , y . flatten ( ) , z . flatten ( ) ) : ddd = np . sqrt ( f * ( x0 - pos [ 0 ] ) ** 2 + ( y0 - pos [ 1 ] ) ** 2 + ( z0 - pos [ 2 ] ) ** 2 ) if ddd > R + 4 : image [ x0 , y0 , z0 ] = 0.0 continue xp , yp , zp = np . meshgrid ( * ( np . linspace ( i - 0.5 + s / 2 , i + 0.5 - s / 2 , scale , endpoint = True ) for i in ( x0 , y0 , z0 ) ) , indexing = 'ij' ) ddd = np . sqrt ( f * ( xp - pos [ 0 ] ) ** 2 + ( yp - pos [ 1 ] ) ** 2 + ( zp - pos [ 2 ] ) ** 2 ) if returnpix is not None and returnpix == [ x0 , y0 , z0 ] : outpix = 1.0 * ( ddd < R ) vol = ( 1.0 * ( ddd < R ) + 0.0 * ( ddd == R ) ) . sum ( ) image [ x0 , y0 , z0 ] = vol / float ( scale ** 3 ) if returnpix : return image , pos , outpix return image , pos
13790	def MessageSetItemDecoder ( extensions_by_number ) : type_id_tag_bytes = encoder . TagBytes ( 2 , wire_format . WIRETYPE_VARINT ) message_tag_bytes = encoder . TagBytes ( 3 , wire_format . WIRETYPE_LENGTH_DELIMITED ) item_end_tag_bytes = encoder . TagBytes ( 1 , wire_format . WIRETYPE_END_GROUP ) local_ReadTag = ReadTag local_DecodeVarint = _DecodeVarint local_SkipField = SkipField def DecodeItem ( buffer , pos , end , message , field_dict ) : message_set_item_start = pos type_id = - 1 message_start = - 1 message_end = - 1 while 1 : ( tag_bytes , pos ) = local_ReadTag ( buffer , pos ) if tag_bytes == type_id_tag_bytes : ( type_id , pos ) = local_DecodeVarint ( buffer , pos ) elif tag_bytes == message_tag_bytes : ( size , message_start ) = local_DecodeVarint ( buffer , pos ) pos = message_end = message_start + size elif tag_bytes == item_end_tag_bytes : break else : pos = SkipField ( buffer , pos , end , tag_bytes ) if pos == - 1 : raise _DecodeError ( 'Missing group end tag.' ) if pos > end : raise _DecodeError ( 'Truncated message.' ) if type_id == - 1 : raise _DecodeError ( 'MessageSet item missing type_id.' ) if message_start == - 1 : raise _DecodeError ( 'MessageSet item missing message.' ) extension = extensions_by_number . get ( type_id ) if extension is not None : value = field_dict . get ( extension ) if value is None : value = field_dict . setdefault ( extension , extension . message_type . _concrete_class ( ) ) if value . _InternalParse ( buffer , message_start , message_end ) != message_end : raise _DecodeError ( 'Unexpected end-group tag.' ) else : if not message . _unknown_fields : message . _unknown_fields = [ ] message . _unknown_fields . append ( ( MESSAGE_SET_ITEM_TAG , buffer [ message_set_item_start : pos ] ) ) return pos return DecodeItem
5515	def setlocale ( name ) : with LOCALE_LOCK : old_locale = locale . setlocale ( locale . LC_ALL ) try : yield locale . setlocale ( locale . LC_ALL , name ) finally : locale . setlocale ( locale . LC_ALL , old_locale )
2666	def write ( self , buf , ** kwargs ) : self . i2c . writeto ( self . device_address , buf , ** kwargs ) if self . _debug : print ( "i2c_device.write:" , [ hex ( i ) for i in buf ] )
6471	def update ( self , points , values = None ) : self . values = values or [ None ] * len ( points ) if np is None : if self . option . function : warnings . warn ( 'numpy not available, function ignored' ) self . points = points self . minimum = min ( self . points ) self . maximum = max ( self . points ) self . current = self . points [ - 1 ] else : self . points = self . apply_function ( points ) self . minimum = np . min ( self . points ) self . maximum = np . max ( self . points ) self . current = self . points [ - 1 ] if self . maximum == self . minimum : self . extents = 1 else : self . extents = ( self . maximum - self . minimum ) self . extents = ( self . maximum - self . minimum )
1717	def replacement_template ( rep , source , span , npar ) : n = 0 res = '' while n < len ( rep ) - 1 : char = rep [ n ] if char == '$' : if rep [ n + 1 ] == '$' : res += '$' n += 2 continue elif rep [ n + 1 ] == '`' : res += source [ : span [ 0 ] ] n += 2 continue elif rep [ n + 1 ] == '\'' : res += source [ span [ 1 ] : ] n += 2 continue elif rep [ n + 1 ] in DIGS : dig = rep [ n + 1 ] if n + 2 < len ( rep ) and rep [ n + 2 ] in DIGS : dig += rep [ n + 2 ] num = int ( dig ) if not num or num > len ( npar ) : res += '$' + dig else : res += npar [ num - 1 ] if npar [ num - 1 ] else '' n += 1 + len ( dig ) continue res += char n += 1 if n < len ( rep ) : res += rep [ - 1 ] return res
2465	def set_file_chksum ( self , doc , chksum ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_chksum_set : self . file_chksum_set = True self . file ( doc ) . chk_sum = checksum_from_sha1 ( chksum ) return True else : raise CardinalityError ( 'File::CheckSum' ) else : raise OrderError ( 'File::CheckSum' )
7881	def _split_qname ( self , name , is_element ) : if name . startswith ( u"{" ) : namespace , name = name [ 1 : ] . split ( u"}" , 1 ) if namespace in STANZA_NAMESPACES : namespace = self . stanza_namespace elif is_element : raise ValueError ( u"Element with no namespace: {0!r}" . format ( name ) ) else : namespace = None return namespace , name
5301	def parse_colors ( path ) : if path . endswith ( ".txt" ) : return parse_rgb_txt_file ( path ) elif path . endswith ( ".json" ) : return parse_json_color_file ( path ) raise TypeError ( "colorful only supports .txt and .json files for colors" )
10267	def collapse_nodes_with_same_names ( graph : BELGraph ) -> None : survivor_mapping = defaultdict ( set ) victims = set ( ) it = tqdm ( itt . combinations ( graph , r = 2 ) , total = graph . number_of_nodes ( ) * ( graph . number_of_nodes ( ) - 1 ) / 2 ) for a , b in it : if b in victims : continue a_name , b_name = a . get ( NAME ) , b . get ( NAME ) if not a_name or not b_name or a_name . lower ( ) != b_name . lower ( ) : continue if a . keys ( ) != b . keys ( ) : continue for k in set ( a . keys ( ) ) - { NAME , NAMESPACE } : if a [ k ] != b [ k ] : continue survivor_mapping [ a ] . add ( b ) victims . add ( b ) collapse_nodes ( graph , survivor_mapping )
6235	def get_time ( self ) -> float : if self . paused : return self . pause_time return mixer . music . get_pos ( ) / 1000.0
2190	def _product_file_hash ( self , product = None ) : if self . hasher is None : return None else : products = self . _rectify_products ( product ) product_file_hash = [ util_hash . hash_file ( p , hasher = self . hasher , base = 'hex' ) for p in products ] return product_file_hash
13767	def add_bundle ( self , * args ) : for bundle in args : if not self . multitype and self . has_bundles ( ) : first_bundle = self . get_first_bundle ( ) if first_bundle . get_type ( ) != bundle . get_type ( ) : raise Exception ( 'Different bundle types for one Asset: %s[%s -> %s]' 'check types or set multitype parameter to True' % ( self . name , first_bundle . get_type ( ) , bundle . get_type ( ) ) ) self . bundles . append ( bundle ) return self
11779	def SyntheticRestaurant ( n = 20 ) : "Generate a DataSet with n examples." def gen ( ) : example = map ( random . choice , restaurant . values ) example [ restaurant . target ] = Fig [ 18 , 2 ] ( example ) return example return RestaurantDataSet ( [ gen ( ) for i in range ( n ) ] )
1957	def _init_arm_kernel_helpers ( self ) : page_data = bytearray ( b'\xf1\xde\xfd\xe7' * 1024 ) preamble = binascii . unhexlify ( 'ff0300ea' + '650400ea' + 'f0ff9fe5' + '430400ea' + '220400ea' + '810400ea' + '000400ea' + '870400ea' ) __kuser_cmpxchg64 = binascii . unhexlify ( '30002de9' + '08c09de5' + '30009ce8' + '010055e1' + '00005401' + '0100a013' + '0000a003' + '0c008c08' + '3000bde8' + '1eff2fe1' ) __kuser_dmb = binascii . unhexlify ( '5bf07ff5' + '1eff2fe1' ) __kuser_cmpxchg = binascii . unhexlify ( '003092e5' + '000053e1' + '0000a003' + '00108205' + '0100a013' + '1eff2fe1' ) self . _arm_tls_memory = self . current . memory . mmap ( None , 4 , 'rw ' ) __kuser_get_tls = binascii . unhexlify ( '04009FE5' + '010090e8' + '1eff2fe1' ) + struct . pack ( '<I' , self . _arm_tls_memory ) tls_area = b'\x00' * 12 version = struct . pack ( '<I' , 5 ) def update ( address , code ) : page_data [ address : address + len ( code ) ] = code update ( 0x000 , preamble ) update ( 0xf60 , __kuser_cmpxchg64 ) update ( 0xfa0 , __kuser_dmb ) update ( 0xfc0 , __kuser_cmpxchg ) update ( 0xfe0 , __kuser_get_tls ) update ( 0xff0 , tls_area ) update ( 0xffc , version ) self . current . memory . mmap ( 0xffff0000 , len ( page_data ) , 'r x' , page_data )
8101	def copy ( self , graph ) : g = styleguide ( graph ) g . order = self . order dict . __init__ ( g , [ ( k , v ) for k , v in self . iteritems ( ) ] ) return g
12612	def search_unique ( self , table_name , sample , unique_fields = None ) : return search_unique ( table = self . table ( table_name ) , sample = sample , unique_fields = unique_fields )
6185	def check_clean_status ( git_path = None ) : output = get_status ( git_path ) is_unmodified = ( len ( output . strip ( ) ) == 0 ) return is_unmodified
6111	def trace_to_next_plane ( self ) : return list ( map ( lambda positions , deflections : np . subtract ( positions , deflections ) , self . positions , self . deflections ) )
3409	def knock_out ( self ) : self . functional = False for reaction in self . reactions : if not reaction . functional : reaction . bounds = ( 0 , 0 )
3009	def has_credentials ( self ) : credentials = _credentials_from_request ( self . request ) return ( credentials and not credentials . invalid and credentials . has_scopes ( self . _get_scopes ( ) ) )
9993	def get_dynspace ( self , args , kwargs = None ) : node = get_node ( self , * convert_args ( args , kwargs ) ) key = node [ KEY ] if key in self . param_spaces : return self . param_spaces [ key ] else : last_self = self . system . self self . system . self = self try : space_args = self . eval_formula ( node ) finally : self . system . self = last_self if space_args is None : space_args = { "bases" : [ self ] } else : if "bases" in space_args : bases = get_impls ( space_args [ "bases" ] ) if isinstance ( bases , StaticSpaceImpl ) : space_args [ "bases" ] = [ bases ] elif bases is None : space_args [ "bases" ] = [ self ] else : space_args [ "bases" ] = bases else : space_args [ "bases" ] = [ self ] space_args [ "arguments" ] = node_get_args ( node ) space = self . _new_dynspace ( ** space_args ) self . param_spaces [ key ] = space space . inherit ( clear_value = False ) return space
10390	def workflow ( graph : BELGraph , node : BaseEntity , key : Optional [ str ] = None , tag : Optional [ str ] = None , default_score : Optional [ float ] = None , runs : Optional [ int ] = None , minimum_nodes : int = 1 , ) -> List [ 'Runner' ] : subgraph = generate_mechanism ( graph , node , key = key ) if subgraph . number_of_nodes ( ) <= minimum_nodes : return [ ] runners = multirun ( subgraph , node , key = key , tag = tag , default_score = default_score , runs = runs ) return list ( runners )
10558	def download ( self , songs , template = None ) : if not template : template = os . getcwd ( ) songnum = 0 total = len ( songs ) results = [ ] errors = { } pad = len ( str ( total ) ) for result in self . _download ( songs , template ) : song_id = songs [ songnum ] [ 'id' ] songnum += 1 downloaded , error = result if downloaded : logger . info ( "({num:>{pad}}/{total}) Successfully downloaded -- {file} ({song_id})" . format ( num = songnum , pad = pad , total = total , file = downloaded [ song_id ] , song_id = song_id ) ) results . append ( { 'result' : 'downloaded' , 'id' : song_id , 'filepath' : downloaded [ song_id ] } ) elif error : title = songs [ songnum ] . get ( 'title' , "<empty>" ) artist = songs [ songnum ] . get ( 'artist' , "<empty>" ) album = songs [ songnum ] . get ( 'album' , "<empty>" ) logger . info ( "({num:>{pad}}/{total}) Error on download -- {title} -- {artist} -- {album} ({song_id})" . format ( num = songnum , pad = pad , total = total , title = title , artist = artist , album = album , song_id = song_id ) ) results . append ( { 'result' : 'error' , 'id' : song_id , 'message' : error [ song_id ] } ) if errors : logger . info ( "\n\nThe following errors occurred:\n" ) for filepath , e in errors . items ( ) : logger . info ( "{file} | {error}" . format ( file = filepath , error = e ) ) logger . info ( "\nThese files may need to be synced again.\n" ) return results
6707	def run_as_root ( command , * args , ** kwargs ) : from burlap . common import run_or_dryrun , sudo_or_dryrun if env . user == 'root' : func = run_or_dryrun else : func = sudo_or_dryrun return func ( command , * args , ** kwargs )
12247	def create_bucket ( self , * args , ** kwargs ) : bucket = super ( S3Connection , self ) . create_bucket ( * args , ** kwargs ) if bucket : mimicdb . backend . sadd ( tpl . connection , bucket . name ) return bucket
6854	def getdevice_by_uuid ( uuid ) : with settings ( hide ( 'running' , 'warnings' , 'stdout' ) , warn_only = True ) : res = run_as_root ( 'blkid -U %s' % uuid ) if not res . succeeded : return None return res
1663	def UpdateIncludeState ( filename , include_dict , io = codecs ) : headerfile = None try : headerfile = io . open ( filename , 'r' , 'utf8' , 'replace' ) except IOError : return False linenum = 0 for line in headerfile : linenum += 1 clean_line = CleanseComments ( line ) match = _RE_PATTERN_INCLUDE . search ( clean_line ) if match : include = match . group ( 2 ) include_dict . setdefault ( include , linenum ) return True
7804	def verify_server ( self , server_name , srv_type = 'xmpp-client' ) : server_jid = JID ( server_name ) if "XmppAddr" not in self . alt_names and "DNS" not in self . alt_names and "SRV" not in self . alt_names : return self . verify_jid_against_common_name ( server_jid ) names = [ name for name in self . alt_names . get ( "DNS" , [ ] ) if not name . startswith ( u"*." ) ] names += self . alt_names . get ( "XmppAddr" , [ ] ) for name in names : logger . debug ( "checking {0!r} against {1!r}" . format ( server_jid , name ) ) try : jid = JID ( name ) except ValueError : logger . debug ( "Not a valid JID: {0!r}" . format ( name ) ) continue if jid == server_jid : logger . debug ( "Match!" ) return True if srv_type and self . verify_jid_against_srv_name ( server_jid , srv_type ) : return True wildcards = [ name [ 2 : ] for name in self . alt_names . get ( "DNS" , [ ] ) if name . startswith ( "*." ) ] if not wildcards or not "." in server_jid . domain : return False logger . debug ( "checking {0!r} against wildcard domains: {1!r}" . format ( server_jid , wildcards ) ) server_domain = JID ( domain = server_jid . domain . split ( "." , 1 ) [ 1 ] ) for domain in wildcards : logger . debug ( "checking {0!r} against {1!r}" . format ( server_domain , domain ) ) try : jid = JID ( domain ) except ValueError : logger . debug ( "Not a valid JID: {0!r}" . format ( name ) ) continue if jid == server_domain : logger . debug ( "Match!" ) return True return False
10948	def reset ( self , ** kwargs ) : self . aug_state . reset ( ) super ( LMAugmentedState , self ) . reset ( ** kwargs )
522	def _updateBoostFactorsGlobal ( self ) : if ( self . _localAreaDensity > 0 ) : targetDensity = self . _localAreaDensity else : inhibitionArea = ( ( 2 * self . _inhibitionRadius + 1 ) ** self . _columnDimensions . size ) inhibitionArea = min ( self . _numColumns , inhibitionArea ) targetDensity = float ( self . _numActiveColumnsPerInhArea ) / inhibitionArea targetDensity = min ( targetDensity , 0.5 ) self . _boostFactors = numpy . exp ( ( targetDensity - self . _activeDutyCycles ) * self . _boostStrength )
13791	def get_app_name ( ) : fn = getattr ( sys . modules [ '__main__' ] , '__file__' , None ) if fn is None : return '__main__' return os . path . splitext ( os . path . basename ( fn ) ) [ 0 ]
2527	def get_annotation_comment ( self , r_term ) : comment_list = list ( self . graph . triples ( ( r_term , RDFS . comment , None ) ) ) if len ( comment_list ) > 1 : self . error = True msg = 'Annotation can have at most one comment.' self . logger . log ( msg ) return else : return six . text_type ( comment_list [ 0 ] [ 2 ] )
6369	def recall ( self ) : r if self . _tp + self . _fn == 0 : return float ( 'NaN' ) return self . _tp / ( self . _tp + self . _fn )
2660	def _start_local_queue_process ( self ) : comm_q = Queue ( maxsize = 10 ) self . queue_proc = Process ( target = interchange . starter , args = ( comm_q , ) , kwargs = { "client_ports" : ( self . outgoing_q . port , self . incoming_q . port , self . command_client . port ) , "worker_ports" : self . worker_ports , "worker_port_range" : self . worker_port_range , "logdir" : "{}/{}" . format ( self . run_dir , self . label ) , "suppress_failure" : self . suppress_failure , "heartbeat_threshold" : self . heartbeat_threshold , "poll_period" : self . poll_period , "logging_level" : logging . DEBUG if self . worker_debug else logging . INFO } , ) self . queue_proc . start ( ) try : ( worker_task_port , worker_result_port ) = comm_q . get ( block = True , timeout = 120 ) except queue . Empty : logger . error ( "Interchange has not completed initialization in 120s. Aborting" ) raise Exception ( "Interchange failed to start" ) self . worker_task_url = "tcp://{}:{}" . format ( self . address , worker_task_port ) self . worker_result_url = "tcp://{}:{}" . format ( self . address , worker_result_port )
9705	def run ( self ) : while self . isRunning . is_set ( ) : try : try : self . monitorTUN ( ) except timeout_decorator . TimeoutError as error : pass self . checkSerial ( ) except KeyboardInterrupt : break
12976	def compat_convertHashedIndexes ( self , objs , conn = None ) : if conn is None : conn = self . _get_connection ( ) fields = [ ] for indexedField in self . indexedFields : origField = self . fields [ indexedField ] if 'hashIndex' not in origField . __class__ . __new__ . __code__ . co_varnames : continue if indexedField . hashIndex is True : hashingField = origField regField = origField . copy ( ) regField . hashIndex = False else : regField = origField hashingField = origField . copy ( ) hashingField . hashIndex = True fields . append ( ( origField , regField , hashingField ) ) objDicts = [ obj . asDict ( True , forStorage = True ) for obj in objs ] for objDict in objDicts : pipeline = conn . pipeline ( ) pk = objDict [ '_id' ] for origField , regField , hashingField in fields : val = objDict [ indexedField ] self . _rem_id_from_index ( regField , pk , val , pipeline ) self . _rem_id_from_index ( hashingField , pk , val , pipeline ) self . _add_id_to_index ( origField , pk , val , pipeline ) pipeline . execute ( )
4374	def get_messages_payload ( self , socket , timeout = None ) : try : msgs = socket . get_multiple_client_msgs ( timeout = timeout ) data = self . encode_payload ( msgs ) except Empty : data = "" return data
9510	def subseq ( self , start , end ) : return Fasta ( self . id , self . seq [ start : end ] )
627	def _hashCoordinate ( coordinate ) : coordinateStr = "," . join ( str ( v ) for v in coordinate ) hash = int ( int ( hashlib . md5 ( coordinateStr ) . hexdigest ( ) , 16 ) % ( 2 ** 64 ) ) return hash
11268	def join ( prev , sep , * args , ** kw ) : yield sep . join ( prev , * args , ** kw )
8399	def transform ( x ) : try : x = date2num ( x ) except AttributeError : x = [ pd . Timestamp ( item ) for item in x ] x = date2num ( x ) return x
4802	def is_child_of ( self , parent ) : self . is_file ( ) if not isinstance ( parent , str_types ) : raise TypeError ( 'given parent directory arg must be a path' ) val_abspath = os . path . abspath ( self . val ) parent_abspath = os . path . abspath ( parent ) if not val_abspath . startswith ( parent_abspath ) : self . _err ( 'Expected file <%s> to be a child of <%s>, but was not.' % ( val_abspath , parent_abspath ) ) return self
12609	def _query_data ( data , field_names = None , operators = '__eq__' ) : if field_names is None : field_names = list ( data . keys ( ) ) if isinstance ( field_names , str ) : field_names = [ field_names ] sample = OrderedDict ( [ ( fn , data [ fn ] ) for fn in field_names ] ) return _query_sample ( sample , operators = operators )
2078	def associate_notification_template ( self , job_template , notification_template , status ) : return self . _assoc ( 'notification_templates_%s' % status , job_template , notification_template )
6498	def remove ( self , doc_type , doc_ids , ** kwargs ) : try : actions = [ ] for doc_id in doc_ids : log . debug ( "Removing document of type %s and index %s" , doc_type , doc_id ) action = { '_op_type' : 'delete' , "_index" : self . index_name , "_type" : doc_type , "_id" : doc_id } actions . append ( action ) bulk ( self . _es , actions , ** kwargs ) except BulkIndexError as ex : valid_errors = [ error for error in ex . errors if error [ 'delete' ] [ 'status' ] != 404 ] if valid_errors : log . exception ( "An error occurred while removing documents from the index." ) raise
469	def create_vocab ( sentences , word_counts_output_file , min_word_count = 1 ) : tl . logging . info ( "Creating vocabulary." ) counter = Counter ( ) for c in sentences : counter . update ( c ) tl . logging . info ( " Total words: %d" % len ( counter ) ) word_counts = [ x for x in counter . items ( ) if x [ 1 ] >= min_word_count ] word_counts . sort ( key = lambda x : x [ 1 ] , reverse = True ) word_counts = [ ( "<PAD>" , 0 ) ] + word_counts tl . logging . info ( " Words in vocabulary: %d" % len ( word_counts ) ) with tf . gfile . FastGFile ( word_counts_output_file , "w" ) as f : f . write ( "\n" . join ( [ "%s %d" % ( w , c ) for w , c in word_counts ] ) ) tl . logging . info ( " Wrote vocabulary file: %s" % word_counts_output_file ) reverse_vocab = [ x [ 0 ] for x in word_counts ] unk_id = len ( reverse_vocab ) vocab_dict = dict ( [ ( x , y ) for ( y , x ) in enumerate ( reverse_vocab ) ] ) vocab = SimpleVocabulary ( vocab_dict , unk_id ) return vocab
515	def _avgConnectedSpanForColumn1D ( self , columnIndex ) : assert ( self . _inputDimensions . size == 1 ) connected = self . _connectedSynapses [ columnIndex ] . nonzero ( ) [ 0 ] if connected . size == 0 : return 0 else : return max ( connected ) - min ( connected ) + 1
12344	def well_images ( self , well_row , well_column ) : return list ( i for i in self . images if attribute ( i , 'u' ) == well_column and attribute ( i , 'v' ) == well_row )
3249	def get_base ( managed_policy , ** conn ) : managed_policy [ '_version' ] = 1 arn = _get_name_from_structure ( managed_policy , 'Arn' ) policy = get_policy ( arn , ** conn ) document = get_managed_policy_document ( arn , policy_metadata = policy , ** conn ) managed_policy . update ( policy [ 'Policy' ] ) managed_policy [ 'Document' ] = document managed_policy [ 'CreateDate' ] = get_iso_string ( managed_policy [ 'CreateDate' ] ) managed_policy [ 'UpdateDate' ] = get_iso_string ( managed_policy [ 'UpdateDate' ] ) return managed_policy
3690	def solve_T ( self , P , V , quick = True ) : r a , b = self . a , self . b if quick : x1 = - 1.j * 1.7320508075688772 + 1. x2 = V - b x3 = x2 / R x4 = V + b x5 = ( 1.7320508075688772 * ( x2 * x2 * ( - 4. * P * P * P * x3 + 27. * a * a / ( V * V * x4 * x4 ) ) / ( R * R ) ) ** 0.5 - 9. * a * x3 / ( V * x4 ) + 0j ) ** ( 1. / 3. ) return ( 3.3019272488946263 * ( 11.537996562459266 * P * x3 / ( x1 * x5 ) + 1.2599210498948732 * x1 * x5 ) ** 2 / 144.0 ) . real else : return ( ( - ( - 1 / 2 + sqrt ( 3 ) * 1j / 2 ) * ( sqrt ( 729 * ( - V * a + a * b ) ** 2 / ( R * V ** 2 + R * V * b ) ** 2 + 108 * ( - P * V + P * b ) ** 3 / R ** 3 ) / 2 + 27 * ( - V * a + a * b ) / ( 2 * ( R * V ** 2 + R * V * b ) ) + 0j ) ** ( 1 / 3 ) / 3 + ( - P * V + P * b ) / ( R * ( - 1 / 2 + sqrt ( 3 ) * 1j / 2 ) * ( sqrt ( 729 * ( - V * a + a * b ) ** 2 / ( R * V ** 2 + R * V * b ) ** 2 + 108 * ( - P * V + P * b ) ** 3 / R ** 3 ) / 2 + 27 * ( - V * a + a * b ) / ( 2 * ( R * V ** 2 + R * V * b ) ) + 0j ) ** ( 1 / 3 ) ) ) ** 2 ) . real
10701	def set_state ( _id , body ) : url = DEVICE_URL % _id if "mode" in body : url = MODES_URL % _id arequest = requests . put ( url , headers = HEADERS , data = json . dumps ( body ) ) status_code = str ( arequest . status_code ) if status_code != '202' : _LOGGER . error ( "State not accepted. " + status_code ) return False
8962	def freeze ( ctx , local = False ) : cmd = 'pip --disable-pip-version-check freeze{}' . format ( ' --local' if local else '' ) frozen = ctx . run ( cmd , hide = 'out' ) . stdout . replace ( '\x1b' , '#' ) with io . open ( 'frozen-requirements.txt' , 'w' , encoding = 'ascii' ) as out : out . write ( "# Requirements frozen by 'pip freeze' on {}\n" . format ( isodate ( ) ) ) out . write ( frozen ) notify . info ( "Frozen {} requirements." . format ( len ( frozen . splitlines ( ) ) , ) )
9068	def _lml_arbitrary_scale ( self ) : s = self . scale D = self . _D n = len ( self . _y ) lml = - self . _df * log2pi - n * log ( s ) lml -= sum ( npsum ( log ( d ) ) for d in D ) d = ( mTQ - yTQ for ( mTQ , yTQ ) in zip ( self . _mTQ , self . _yTQ ) ) lml -= sum ( ( i / j ) @ i for ( i , j ) in zip ( d , D ) ) / s return lml / 2
10441	def getmemorystat ( self , process_name ) : _stat_inst = ProcessStats ( process_name ) _stat_list = [ ] for p in _stat_inst . get_cpu_memory_stat ( ) : try : _stat_list . append ( round ( p . get_memory_percent ( ) , 2 ) ) except psutil . AccessDenied : pass return _stat_list
10419	def group_dict_set ( iterator : Iterable [ Tuple [ A , B ] ] ) -> Mapping [ A , Set [ B ] ] : d = defaultdict ( set ) for key , value in iterator : d [ key ] . add ( value ) return dict ( d )
12001	def _unsign_data ( self , data , options ) : if options [ 'signature_algorithm_id' ] not in self . signature_algorithms : raise Exception ( 'Unknown signature algorithm id: %d' % options [ 'signature_algorithm_id' ] ) signature_algorithm = self . signature_algorithms [ options [ 'signature_algorithm_id' ] ] algorithm = self . _get_algorithm_info ( signature_algorithm ) key_salt = '' if algorithm [ 'salt_size' ] : key_salt = data [ - algorithm [ 'salt_size' ] : ] data = data [ : - algorithm [ 'salt_size' ] ] key = self . _generate_key ( options [ 'signature_passphrase_id' ] , self . signature_passphrases , key_salt , algorithm ) data = self . _decode ( data , algorithm , key ) return data
7319	def _create_boundary ( message ) : if not message . is_multipart ( ) or message . get_boundary ( ) is not None : return message from future . backports . email . generator import Generator boundary = Generator . _make_boundary ( message . policy . linesep ) message . set_param ( 'boundary' , boundary ) return message
3338	def is_child_uri ( parentUri , childUri ) : return ( parentUri and childUri and childUri . rstrip ( "/" ) . startswith ( parentUri . rstrip ( "/" ) + "/" ) )
457	def _add_notice_to_docstring ( doc , no_doc_str , notice ) : if not doc : lines = [ no_doc_str ] else : lines = _normalize_docstring ( doc ) . splitlines ( ) notice = [ '' ] + notice if len ( lines ) > 1 : if lines [ 1 ] . strip ( ) : notice . append ( '' ) lines [ 1 : 1 ] = notice else : lines += notice return '\n' . join ( lines )
4814	def _word_ngrams ( self , tokens ) : if self . stop_words is not None : tokens = [ w for w in tokens if w not in self . stop_words ] min_n , max_n = self . ngram_range if max_n != 1 : original_tokens = tokens if min_n == 1 : tokens = list ( original_tokens ) min_n += 1 else : tokens = [ ] n_original_tokens = len ( original_tokens ) tokens_append = tokens . append space_join = " " . join for n in range ( min_n , min ( max_n + 1 , n_original_tokens + 1 ) ) : for i in range ( n_original_tokens - n + 1 ) : tokens_append ( space_join ( original_tokens [ i : i + n ] ) ) return tokens
7811	def _decode_subject ( self , subject ) : self . common_names = [ ] subject_name = [ ] for rdnss in subject : for rdns in rdnss : rdnss_list = [ ] for nameval in rdns : val_type = nameval . getComponentByName ( 'type' ) value = nameval . getComponentByName ( 'value' ) if val_type not in DN_OIDS : logger . debug ( "OID {0} not supported" . format ( val_type ) ) continue val_type = DN_OIDS [ val_type ] value = der_decoder . decode ( value , asn1Spec = DirectoryString ( ) ) [ 0 ] value = value . getComponent ( ) try : value = _decode_asn1_string ( value ) except UnicodeError : logger . debug ( "Cannot decode value: {0!r}" . format ( value ) ) continue if val_type == u"commonName" : self . common_names . append ( value ) rdnss_list . append ( ( val_type , value ) ) subject_name . append ( tuple ( rdnss_list ) ) self . subject_name = tuple ( subject_name )
6783	def lock ( self ) : self . init ( ) r = self . local_renderer if self . file_exists ( r . env . lockfile_path ) : raise exceptions . AbortDeployment ( 'Lock file %s exists. Perhaps another deployment is currently underway?' % r . env . lockfile_path ) else : self . vprint ( 'Locking %s.' % r . env . lockfile_path ) r . env . hostname = socket . gethostname ( ) r . run_or_local ( 'echo "{hostname}" > {lockfile_path}' )
3857	def is_quiet ( self ) : level = self . _conversation . self_conversation_state . notification_level return level == hangouts_pb2 . NOTIFICATION_LEVEL_QUIET
5280	def sklearn2pmml ( pipeline , pmml , user_classpath = [ ] , with_repr = False , debug = False , java_encoding = "UTF-8" ) : if debug : java_version = _java_version ( java_encoding ) if java_version is None : java_version = ( "java" , "N/A" ) print ( "python: {0}" . format ( platform . python_version ( ) ) ) print ( "sklearn: {0}" . format ( sklearn . __version__ ) ) print ( "sklearn.externals.joblib: {0}" . format ( joblib . __version__ ) ) print ( "pandas: {0}" . format ( pandas . __version__ ) ) print ( "sklearn_pandas: {0}" . format ( sklearn_pandas . __version__ ) ) print ( "sklearn2pmml: {0}" . format ( __version__ ) ) print ( "{0}: {1}" . format ( java_version [ 0 ] , java_version [ 1 ] ) ) if not isinstance ( pipeline , PMMLPipeline ) : raise TypeError ( "The pipeline object is not an instance of " + PMMLPipeline . __name__ + ". Use the 'sklearn2pmml.make_pmml_pipeline(obj)' utility function to translate a regular Scikit-Learn estimator or pipeline to a PMML pipeline" ) estimator = pipeline . _final_estimator cmd = [ "java" , "-cp" , os . pathsep . join ( _classpath ( user_classpath ) ) , "org.jpmml.sklearn.Main" ] dumps = [ ] try : if with_repr : pipeline . repr_ = repr ( pipeline ) if hasattr ( estimator , "download_mojo" ) : estimator_mojo = estimator . download_mojo ( ) dumps . append ( estimator_mojo ) estimator . _mojo_path = estimator_mojo pipeline_pkl = _dump ( pipeline , "pipeline" ) cmd . extend ( [ "--pkl-pipeline-input" , pipeline_pkl ] ) dumps . append ( pipeline_pkl ) cmd . extend ( [ "--pmml-output" , pmml ] ) if debug : print ( "Executing command:\n{0}" . format ( " " . join ( cmd ) ) ) try : process = Popen ( cmd , stdout = PIPE , stderr = PIPE , bufsize = 1 ) except OSError : raise RuntimeError ( "Java is not installed, or the Java executable is not on system path" ) output , error = process . communicate ( ) retcode = process . poll ( ) if debug or retcode : if ( len ( output ) > 0 ) : print ( "Standard output:\n{0}" . format ( _decode ( output , java_encoding ) ) ) else : print ( "Standard output is empty" ) if ( len ( error ) > 0 ) : print ( "Standard error:\n{0}" . format ( _decode ( error , java_encoding ) ) ) else : print ( "Standard error is empty" ) if retcode : raise RuntimeError ( "The JPMML-SkLearn conversion application has failed. The Java executable should have printed more information about the failure into its standard output and/or standard error streams" ) finally : if debug : print ( "Preserved joblib dump file(s): {0}" . format ( " " . join ( dumps ) ) ) else : for dump in dumps : os . remove ( dump )
1289	def baseline_optimizer_arguments ( self , states , internals , reward ) : arguments = dict ( time = self . global_timestep , variables = self . baseline . get_variables ( ) , arguments = dict ( states = states , internals = internals , reward = reward , update = tf . constant ( value = True ) , ) , fn_reference = self . baseline . reference , fn_loss = self . fn_baseline_loss , ) if self . global_model is not None : arguments [ 'global_variables' ] = self . global_model . baseline . get_variables ( ) return arguments
10827	def create ( cls , group , user , state = MembershipState . ACTIVE ) : with db . session . begin_nested ( ) : membership = cls ( user_id = user . get_id ( ) , id_group = group . id , state = state , ) db . session . add ( membership ) return membership
8778	def _check_collisions ( self , new_range , existing_ranges ) : def _contains ( num , r1 ) : return ( num >= r1 [ 0 ] and num <= r1 [ 1 ] ) def _is_overlap ( r1 , r2 ) : return ( _contains ( r1 [ 0 ] , r2 ) or _contains ( r1 [ 1 ] , r2 ) or _contains ( r2 [ 0 ] , r1 ) or _contains ( r2 [ 1 ] , r1 ) ) for existing_range in existing_ranges : if _is_overlap ( new_range , existing_range ) : return True return False
11527	def solr_advanced_search ( self , query , token = None , limit = 20 ) : parameters = dict ( ) parameters [ 'query' ] = query parameters [ 'limit' ] = limit if token : parameters [ 'token' ] = token response = self . request ( 'midas.solr.search.advanced' , parameters ) return response
7339	def log_error ( msg = None , exc_info = None , logger = None , ** kwargs ) : if logger is None : logger = _logger if not exc_info : exc_info = sys . exc_info ( ) if msg is None : msg = "" exc_class , exc_msg , _ = exc_info if all ( info is not None for info in exc_info ) : logger . error ( msg , exc_info = exc_info )
10480	def _performAction ( self , action ) : try : _a11y . AXUIElement . _performAction ( self , 'AX%s' % action ) except _a11y . ErrorUnsupported as e : sierra_ver = '10.12' if mac_ver ( ) [ 0 ] < sierra_ver : raise e else : pass
11971	def _detect ( ip , _isnm ) : ip = str ( ip ) if len ( ip ) > 1 : if ip [ 0 : 2 ] == '0x' : if _CHECK_FUNCT [ IP_HEX ] [ _isnm ] ( ip ) : return IP_HEX elif ip [ 0 ] == '0' : if _CHECK_FUNCT [ IP_OCT ] [ _isnm ] ( ip ) : return IP_OCT if _CHECK_FUNCT [ IP_DOT ] [ _isnm ] ( ip ) : return IP_DOT elif _isnm and _CHECK_FUNCT [ NM_BITS ] [ _isnm ] ( ip ) : return NM_BITS elif _CHECK_FUNCT [ IP_DEC ] [ _isnm ] ( ip ) : return IP_DEC elif _isnm and _CHECK_FUNCT [ NM_WILDCARD ] [ _isnm ] ( ip ) : return NM_WILDCARD elif _CHECK_FUNCT [ IP_BIN ] [ _isnm ] ( ip ) : return IP_BIN return IP_UNKNOWN
4410	async def disconnect ( self ) : if not self . is_connected : return await self . stop ( ) ws = self . _lavalink . bot . _connection . _get_websocket ( int ( self . guild_id ) ) await ws . voice_state ( self . guild_id , None )
12910	def intersection ( self , other , recursive = True ) : if not isinstance ( other , composite ) : raise AssertionError ( 'Cannot intersect composite and {} types' . format ( type ( other ) ) ) if self . meta_type != other . meta_type : return composite ( { } ) if self . meta_type == 'list' : keep = [ ] for item in self . _list : if item in other . _list : if recursive and isinstance ( item , composite ) : keep . extend ( item . intersection ( other . index ( item ) , recursive = True ) ) else : keep . append ( item ) return composite ( keep ) elif self . meta_type == 'dict' : keep = { } for key in self . _dict : item = self . _dict [ key ] if key in other . _dict : if recursive and isinstance ( item , composite ) and isinstance ( other . get ( key ) , composite ) : keep [ key ] = item . intersection ( other . get ( key ) , recursive = True ) elif item == other [ key ] : keep [ key ] = item return composite ( keep ) return
9117	def reset_jails ( confirm = True , keep_cleanser_master = True ) : if value_asbool ( confirm ) and not yesno ( ) : exit ( "Glad I asked..." ) reset_cleansers ( confirm = False ) jails = [ 'appserver' , 'webserver' , 'worker' ] if not value_asbool ( keep_cleanser_master ) : jails . append ( 'cleanser' ) with fab . warn_only ( ) : for jail in jails : fab . run ( 'ezjail-admin delete -fw {jail}' . format ( jail = jail ) ) fab . run ( 'rm /usr/jails/cleanser/usr/home/cleanser/.ssh/authorized_keys' )
8960	def clean ( _dummy_ctx , docs = False , backups = False , bytecode = False , dist = False , all = False , venv = False , tox = False , extra = '' ) : cfg = config . load ( ) notify . banner ( "Cleaning up project files" ) venv_dirs = [ 'bin' , 'include' , 'lib' , 'share' , 'local' , '.venv' ] patterns = [ 'build/' , 'pip-selfcheck.json' ] excludes = [ '.git/' , '.hg/' , '.svn/' , 'debian/*/' ] if docs or all : patterns . extend ( [ 'docs/_build/' , 'doc/_build/' ] ) if dist or all : patterns . append ( 'dist/' ) if backups or all : patterns . extend ( [ '**/*~' ] ) if bytecode or all : patterns . extend ( [ '**/*.py[co]' , '**/__pycache__/' , '*.egg-info/' , cfg . srcjoin ( '*.egg-info/' ) [ len ( cfg . project_root ) + 1 : ] , ] ) if venv : patterns . extend ( [ i + '/' for i in venv_dirs ] ) if tox : patterns . append ( '.tox/' ) else : excludes . append ( '.tox/' ) if extra : patterns . extend ( shlex . split ( extra ) ) patterns = [ antglob . includes ( i ) for i in patterns ] + [ antglob . excludes ( i ) for i in excludes ] if not venv : patterns . extend ( [ antglob . excludes ( i + '/' ) for i in venv_dirs ] ) fileset = antglob . FileSet ( cfg . project_root , patterns ) for name in fileset : notify . info ( 'rm {0}' . format ( name ) ) if name . endswith ( '/' ) : shutil . rmtree ( os . path . join ( cfg . project_root , name ) ) else : os . unlink ( os . path . join ( cfg . project_root , name ) )
1782	def AAS ( cpu ) : if ( cpu . AL & 0x0F > 9 ) or cpu . AF == 1 : cpu . AX = cpu . AX - 6 cpu . AH = cpu . AH - 1 cpu . AF = True cpu . CF = True else : cpu . AF = False cpu . CF = False cpu . AL = cpu . AL & 0x0f
3538	def yandex_metrica ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return YandexMetricaNode ( )
5649	def write_gtfs ( gtfs , output ) : output = os . path . abspath ( output ) uuid_str = "tmp_" + str ( uuid . uuid1 ( ) ) if output [ - 4 : ] == '.zip' : zip = True out_basepath = os . path . dirname ( os . path . abspath ( output ) ) if not os . path . exists ( out_basepath ) : raise IOError ( out_basepath + " does not exist, cannot write gtfs as a zip" ) tmp_dir = os . path . join ( out_basepath , str ( uuid_str ) ) else : zip = False out_basepath = output tmp_dir = os . path . join ( out_basepath + "_" + str ( uuid_str ) ) os . makedirs ( tmp_dir , exist_ok = True ) gtfs_table_to_writer = { "agency" : _write_gtfs_agencies , "calendar" : _write_gtfs_calendar , "calendar_dates" : _write_gtfs_calendar_dates , "feed_info" : _write_gtfs_feed_info , "routes" : _write_gtfs_routes , "shapes" : _write_gtfs_shapes , "stops" : _write_gtfs_stops , "stop_times" : _write_gtfs_stop_times , "transfers" : _write_gtfs_transfers , "trips" : _write_gtfs_trips , } for table , writer in gtfs_table_to_writer . items ( ) : fname_to_write = os . path . join ( tmp_dir , table + '.txt' ) print ( fname_to_write ) writer ( gtfs , open ( os . path . join ( tmp_dir , table + '.txt' ) , 'w' ) ) if zip : shutil . make_archive ( output [ : - 4 ] , 'zip' , tmp_dir ) shutil . rmtree ( tmp_dir ) else : print ( "moving " + str ( tmp_dir ) + " to " + out_basepath ) os . rename ( tmp_dir , out_basepath )
2147	def _configuration ( self , kwargs , config_item ) : if 'notification_configuration' not in config_item : if 'notification_type' not in kwargs : return nc = kwargs [ 'notification_configuration' ] = { } for field in Resource . configuration [ kwargs [ 'notification_type' ] ] : if field not in config_item : raise exc . TowerCLIError ( 'Required config field %s not' ' provided.' % field ) else : nc [ field ] = config_item [ field ] else : kwargs [ 'notification_configuration' ] = config_item [ 'notification_configuration' ]
2172	def new_state ( self ) : try : self . _state = self . state ( ) log . debug ( "Generated new state %s." , self . _state ) except TypeError : self . _state = self . state log . debug ( "Re-using previously supplied state %s." , self . _state ) return self . _state
7628	def namespace ( ns_key ) : if ns_key not in __NAMESPACE__ : raise NamespaceError ( 'Unknown namespace: {:s}' . format ( ns_key ) ) sch = copy . deepcopy ( JAMS_SCHEMA [ 'definitions' ] [ 'SparseObservation' ] ) for key in [ 'value' , 'confidence' ] : try : sch [ 'properties' ] [ key ] = __NAMESPACE__ [ ns_key ] [ key ] except KeyError : pass return sch
8467	def path ( self , value ) : if not value . endswith ( '/' ) : self . _path = '{v}/' . format ( v = value ) else : self . _path = value
13646	def hump_to_underscore ( name ) : new_name = '' pos = 0 for c in name : if pos == 0 : new_name = c . lower ( ) elif 65 <= ord ( c ) <= 90 : new_name += '_' + c . lower ( ) pass else : new_name += c pos += 1 pass return new_name
1585	def send_buffered_messages ( self ) : while not self . out_stream . is_empty ( ) and self . _stmgr_client . is_registered : tuple_set = self . out_stream . poll ( ) if isinstance ( tuple_set , tuple_pb2 . HeronTupleSet ) : tuple_set . src_task_id = self . my_pplan_helper . my_task_id self . gateway_metrics . update_sent_packet ( tuple_set . ByteSize ( ) ) self . _stmgr_client . send_message ( tuple_set )
3561	def advertised ( self ) : uuids = [ ] try : uuids = self . _props . Get ( _INTERFACE , 'UUIDs' ) except dbus . exceptions . DBusException as ex : if ex . get_dbus_name ( ) != 'org.freedesktop.DBus.Error.InvalidArgs' : raise ex return [ uuid . UUID ( str ( x ) ) for x in uuids ]
12828	def log_error ( self , text : str ) -> None : if self . log_errors : with self . _log_fp . open ( 'a+' ) as log_file : log_file . write ( f'{text}\n' )
8205	def flush ( self , frame ) : self . sink . render ( self . size_or_default ( ) , frame , self . _drawqueue ) self . reset_drawqueue ( )
8963	def isodate ( datestamp = None , microseconds = False ) : datestamp = datestamp or datetime . datetime . now ( ) if not microseconds : usecs = datetime . timedelta ( microseconds = datestamp . microsecond ) datestamp = datestamp - usecs return datestamp . isoformat ( b' ' if PY2 else u' ' )
10606	def prepare_to_run ( self ) : self . clock . reset ( ) for e in self . entities : e . prepare_to_run ( self . clock , self . period_count )
1610	def make_root_tuple_info ( stream_id , tuple_id ) : key = random . getrandbits ( TupleHelper . MAX_SFIXED64_RAND_BITS ) return RootTupleInfo ( stream_id = stream_id , tuple_id = tuple_id , insertion_time = time . time ( ) , key = key )
2287	def parallel_graph_evaluation ( data , adj_matrix , nb_runs = 16 , nb_jobs = None , ** kwargs ) : nb_jobs = SETTINGS . get_default ( nb_jobs = nb_jobs ) if nb_runs == 1 : return graph_evaluation ( data , adj_matrix , ** kwargs ) else : output = Parallel ( n_jobs = nb_jobs ) ( delayed ( graph_evaluation ) ( data , adj_matrix , idx = run , gpu_id = run % SETTINGS . GPU , ** kwargs ) for run in range ( nb_runs ) ) return np . mean ( output )
2165	def format_commands ( self , ctx , formatter ) : self . format_command_subsection ( ctx , formatter , self . list_misc_commands ( ) , 'Commands' ) self . format_command_subsection ( ctx , formatter , self . list_resource_commands ( ) , 'Resources' )
87	def is_integer_array ( val ) : return is_np_array ( val ) and issubclass ( val . dtype . type , np . integer )
637	def read ( cls , proto ) : protoCells = proto . cells connections = cls ( len ( protoCells ) ) for cellIdx , protoCell in enumerate ( protoCells ) : protoCell = protoCells [ cellIdx ] protoSegments = protoCell . segments connections . _cells [ cellIdx ] = CellData ( ) segments = connections . _cells [ cellIdx ] . _segments for segmentIdx , protoSegment in enumerate ( protoSegments ) : segment = Segment ( cellIdx , connections . _nextFlatIdx , connections . _nextSegmentOrdinal ) segments . append ( segment ) connections . _segmentForFlatIdx . append ( segment ) connections . _nextFlatIdx += 1 connections . _nextSegmentOrdinal += 1 synapses = segment . _synapses protoSynapses = protoSegment . synapses for synapseIdx , protoSynapse in enumerate ( protoSynapses ) : presynapticCell = protoSynapse . presynapticCell synapse = Synapse ( segment , presynapticCell , protoSynapse . permanence , ordinal = connections . _nextSynapseOrdinal ) connections . _nextSynapseOrdinal += 1 synapses . add ( synapse ) connections . _synapsesForPresynapticCell [ presynapticCell ] . add ( synapse ) connections . _numSynapses += 1 return connections
9814	def start ( ctx , file , u ) : specification = None job_config = None if file : specification = check_polyaxonfile ( file , log = False ) . specification if u : ctx . invoke ( upload , sync = False ) if specification : check_polyaxonfile_kind ( specification = specification , kind = specification . _NOTEBOOK ) job_config = specification . parsed_data user , project_name = get_project_or_local ( ctx . obj . get ( 'project' ) ) try : response = PolyaxonClient ( ) . project . start_notebook ( user , project_name , job_config ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not start notebook project `{}`.' . format ( project_name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) if response . status_code == 200 : Printer . print_header ( "A notebook for this project is already running on:" ) click . echo ( get_notebook_url ( user , project_name ) ) sys . exit ( 0 ) if response . status_code != 201 : Printer . print_error ( 'Something went wrong, Notebook was not created.' ) sys . exit ( 1 ) Printer . print_success ( 'Notebook is being deployed for project `{}`' . format ( project_name ) ) clint . textui . puts ( "It may take some time before you can access the notebook.\n" ) clint . textui . puts ( "Your notebook will be available on:\n" ) with clint . textui . indent ( 4 ) : clint . textui . puts ( get_notebook_url ( user , project_name ) )
7905	def forget ( self , rs ) : try : del self . rooms [ rs . room_jid . bare ( ) . as_unicode ( ) ] except KeyError : pass
11622	def _unrecognised ( chr ) : if options [ 'handleUnrecognised' ] == UNRECOGNISED_ECHO : return chr elif options [ 'handleUnrecognised' ] == UNRECOGNISED_SUBSTITUTE : return options [ 'substituteChar' ] else : raise ( KeyError , chr )
11950	def _set_global_verbosity_level ( is_verbose_output = False ) : global verbose_output verbose_output = is_verbose_output if verbose_output : jocker_lgr . setLevel ( logging . DEBUG ) else : jocker_lgr . setLevel ( logging . INFO )
6549	def fill_field ( self , ypos , xpos , tosend , length ) : if length < len ( tosend ) : raise FieldTruncateError ( 'length limit %d, but got "%s"' % ( length , tosend ) ) if xpos is not None and ypos is not None : self . move_to ( ypos , xpos ) self . delete_field ( ) self . send_string ( tosend )
4551	def draw_triangle ( setter , x0 , y0 , x1 , y1 , x2 , y2 , color = None , aa = False ) : draw_line ( setter , x0 , y0 , x1 , y1 , color , aa ) draw_line ( setter , x1 , y1 , x2 , y2 , color , aa ) draw_line ( setter , x2 , y2 , x0 , y0 , color , aa )
12083	def filesByExtension ( fnames ) : byExt = { "abf" : [ ] , "jpg" : [ ] , "tif" : [ ] } for fname in fnames : ext = os . path . splitext ( fname ) [ 1 ] . replace ( "." , '' ) . lower ( ) if not ext in byExt . keys ( ) : byExt [ ext ] = [ ] byExt [ ext ] = byExt [ ext ] + [ fname ] return byExt
7058	def s3_put_file ( local_file , bucket , client = None , raiseonfail = False ) : if not client : client = boto3 . client ( 's3' ) try : client . upload_file ( local_file , bucket , os . path . basename ( local_file ) ) return 's3://%s/%s' % ( bucket , os . path . basename ( local_file ) ) except Exception as e : LOGEXCEPTION ( 'could not upload %s to bucket: %s' % ( local_file , bucket ) ) if raiseonfail : raise return None
3533	def gosquared ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return GoSquaredNode ( )
2494	def package_verif_node ( self , package ) : verif_node = BNode ( ) type_triple = ( verif_node , RDF . type , self . spdx_namespace . PackageVerificationCode ) self . graph . add ( type_triple ) value_triple = ( verif_node , self . spdx_namespace . packageVerificationCodeValue , Literal ( package . verif_code ) ) self . graph . add ( value_triple ) excl_file_nodes = map ( lambda excl : Literal ( excl ) , package . verif_exc_files ) excl_predicate = self . spdx_namespace . packageVerificationCodeExcludedFile excl_file_triples = [ ( verif_node , excl_predicate , xcl_file ) for xcl_file in excl_file_nodes ] for trp in excl_file_triples : self . graph . add ( trp ) return verif_node
2538	def set_pkg_verif_code ( self , doc , code ) : self . assert_package_exists ( ) if not self . package_verif_set : self . package_verif_set = True doc . package . verif_code = code else : raise CardinalityError ( 'Package::VerificationCode' )
5600	def for_web ( self , data ) : rgba = self . _prepare_array_for_png ( data ) data = ma . masked_where ( rgba == self . nodata , rgba ) return memory_file ( data , self . profile ( ) ) , 'image/png'
9446	def transfer_call ( self , call_params ) : path = '/' + self . api_version + '/TransferCall/' method = 'POST' return self . request ( path , method , call_params )
12188	def message_is_to_me ( self , data ) : return ( data . get ( 'type' ) == 'message' and data . get ( 'text' , '' ) . startswith ( self . address_as ) )
3743	def ViswanathNatarajan2 ( T , A , B ) : mu = exp ( A + B / T ) mu = mu / 1000. mu = mu * 10 return mu
2297	def fit ( self , x , y ) : train = np . vstack ( ( np . array ( [ self . featurize_row ( row . iloc [ 0 ] , row . iloc [ 1 ] ) for idx , row in x . iterrows ( ) ] ) , np . array ( [ self . featurize_row ( row . iloc [ 1 ] , row . iloc [ 0 ] ) for idx , row in x . iterrows ( ) ] ) ) ) labels = np . vstack ( ( y , - y ) ) . ravel ( ) verbose = 1 if self . verbose else 0 self . clf = CLF ( verbose = verbose , min_samples_leaf = self . L , n_estimators = self . E , max_depth = self . max_depth , n_jobs = self . n_jobs ) . fit ( train , labels )
2012	def instruction ( self ) : try : _decoding_cache = getattr ( self , '_decoding_cache' ) except Exception : _decoding_cache = self . _decoding_cache = { } pc = self . pc if isinstance ( pc , Constant ) : pc = pc . value if pc in _decoding_cache : return _decoding_cache [ pc ] def getcode ( ) : bytecode = self . bytecode for pc_i in range ( pc , len ( bytecode ) ) : yield simplify ( bytecode [ pc_i ] ) . value while True : yield 0 instruction = EVMAsm . disassemble_one ( getcode ( ) , pc = pc , fork = DEFAULT_FORK ) _decoding_cache [ pc ] = instruction return instruction
6109	def xticks ( self ) : return np . linspace ( np . amin ( self . grid_stack . regular [ : , 1 ] ) , np . amax ( self . grid_stack . regular [ : , 1 ] ) , 4 )
2996	def marketNewsDF ( count = 10 , token = '' , version = '' ) : df = pd . DataFrame ( marketNews ( count , token , version ) ) _toDatetime ( df ) _reindex ( df , 'datetime' ) return df
5952	def start_logging ( logfile = "gromacs.log" ) : from . import log log . create ( "gromacs" , logfile = logfile ) logging . getLogger ( "gromacs" ) . info ( "GromacsWrapper %s STARTED logging to %r" , __version__ , logfile )
958	def aggregationToMonthsSeconds ( interval ) : seconds = interval . get ( 'microseconds' , 0 ) * 0.000001 seconds += interval . get ( 'milliseconds' , 0 ) * 0.001 seconds += interval . get ( 'seconds' , 0 ) seconds += interval . get ( 'minutes' , 0 ) * 60 seconds += interval . get ( 'hours' , 0 ) * 60 * 60 seconds += interval . get ( 'days' , 0 ) * 24 * 60 * 60 seconds += interval . get ( 'weeks' , 0 ) * 7 * 24 * 60 * 60 months = interval . get ( 'months' , 0 ) months += 12 * interval . get ( 'years' , 0 ) return { 'months' : months , 'seconds' : seconds }
12198	def to_cldf ( self , dest , mdname = 'cldf-metadata.json' ) : dest = Path ( dest ) if not dest . exists ( ) : dest . mkdir ( ) data = self . read ( ) if data [ self . source_table_name ] : sources = Sources ( ) for src in data [ self . source_table_name ] : sources . add ( Source ( src [ 'genre' ] , src [ 'id' ] , ** { k : v for k , v in src . items ( ) if k not in [ 'id' , 'genre' ] } ) ) sources . write ( dest / self . dataset . properties . get ( 'dc:source' , 'sources.bib' ) ) for table_type , items in data . items ( ) : try : table = self . dataset [ table_type ] table . common_props [ 'dc:extent' ] = table . write ( [ self . retranslate ( table , item ) for item in items ] , base = dest ) except KeyError : assert table_type == self . source_table_name , table_type return self . dataset . write_metadata ( dest / mdname )
7440	def get_params ( self , param = "" ) : fullcurdir = os . path . realpath ( os . path . curdir ) if not param : for index , ( key , value ) in enumerate ( self . paramsdict . items ( ) ) : if isinstance ( value , str ) : value = value . replace ( fullcurdir + "/" , "./" ) sys . stdout . write ( "{}{:<4}{:<28}{:<45}\n" . format ( self . _spacer , index , key , value ) ) else : try : if int ( param ) : return self . paramsdict . values ( ) [ int ( param ) ] except ( ValueError , TypeError , NameError , IndexError ) : try : return self . paramsdict [ param ] except KeyError : return 'key not recognized'
11016	def deploy ( context ) : config = context . obj header ( 'Generating HTML...' ) pelican ( config , '--verbose' , production = True ) header ( 'Removing unnecessary output...' ) unnecessary_paths = [ 'author' , 'category' , 'tag' , 'feeds' , 'tags.html' , 'authors.html' , 'categories.html' , 'archives.html' , ] for path in unnecessary_paths : remove_path ( os . path . join ( config [ 'OUTPUT_DIR' ] , path ) ) if os . environ . get ( 'TRAVIS' ) : header ( 'Setting up Git...' ) run ( 'git config user.name ' + run ( 'git show --format="%cN" -s' , capture = True ) ) run ( 'git config user.email ' + run ( 'git show --format="%cE" -s' , capture = True ) ) github_token = os . environ . get ( 'GITHUB_TOKEN' ) repo_slug = os . environ . get ( 'TRAVIS_REPO_SLUG' ) origin = 'https://{}@github.com/{}.git' . format ( github_token , repo_slug ) run ( 'git remote set-url origin ' + origin ) header ( 'Rewriting gh-pages branch...' ) run ( 'ghp-import -m "{message}" {dir}' . format ( message = 'Deploying {}' . format ( choose_commit_emoji ( ) ) , dir = config [ 'OUTPUT_DIR' ] , ) ) header ( 'Pushing to GitHub...' ) run ( 'git push origin gh-pages:gh-pages --force' )
5120	def next_event_description ( self ) : if self . _fancy_heap . size == 0 : event_type = 'Nothing' edge_index = None else : s = [ q . _key ( ) for q in self . edge2queue ] s . sort ( ) e = s [ 0 ] [ 1 ] q = self . edge2queue [ e ] event_type = 'Arrival' if q . next_event_description ( ) == 1 else 'Departure' edge_index = q . edge [ 2 ] return event_type , edge_index
12944	def save ( self , cascadeSave = True ) : saver = IndexedRedisSave ( self . __class__ ) return saver . save ( self , cascadeSave = cascadeSave )
11043	def listen ( self , reactor , endpoint_description ) : endpoint = serverFromString ( reactor , endpoint_description ) return endpoint . listen ( Site ( self . app . resource ( ) ) )
9533	def unsign ( self , signed_value , ttl = None ) : h_size , d_size = struct . calcsize ( '>cQ' ) , self . digest . digest_size fmt = '>cQ%ds%ds' % ( len ( signed_value ) - h_size - d_size , d_size ) try : version , timestamp , value , sig = struct . unpack ( fmt , signed_value ) except struct . error : raise BadSignature ( 'Signature is not valid' ) if version != self . version : raise BadSignature ( 'Signature version not supported' ) if ttl is not None : if isinstance ( ttl , datetime . timedelta ) : ttl = ttl . total_seconds ( ) age = abs ( time . time ( ) - timestamp ) if age > ttl + _MAX_CLOCK_SKEW : raise SignatureExpired ( 'Signature age %s > %s seconds' % ( age , ttl ) ) try : self . signature ( signed_value [ : - d_size ] ) . verify ( sig ) except InvalidSignature : raise BadSignature ( 'Signature "%s" does not match' % binascii . b2a_base64 ( sig ) ) return value
8881	def fit ( self , X , y = None ) : X = check_array ( X ) self . inverse_influence_matrix = self . __make_inverse_matrix ( X ) if self . threshold == 'auto' : self . threshold_value = 3 * ( 1 + X . shape [ 1 ] ) / X . shape [ 0 ] elif self . threshold == 'cv' : if y is None : raise ValueError ( "Y must be specified to find the optimal threshold." ) y = check_array ( y , accept_sparse = 'csc' , ensure_2d = False , dtype = None ) self . threshold_value = 0 score = 0 Y_pred , Y_true , AD = [ ] , [ ] , [ ] cv = KFold ( n_splits = 5 , random_state = 1 , shuffle = True ) for train_index , test_index in cv . split ( X ) : x_train = safe_indexing ( X , train_index ) x_test = safe_indexing ( X , test_index ) y_train = safe_indexing ( y , train_index ) y_test = safe_indexing ( y , test_index ) if self . reg_model is None : reg_model = RandomForestRegressor ( n_estimators = 500 , random_state = 1 ) . fit ( x_train , y_train ) else : reg_model = clone ( self . reg_model ) . fit ( x_train , y_train ) Y_pred . append ( reg_model . predict ( x_test ) ) Y_true . append ( y_test ) ad_model = self . __make_inverse_matrix ( x_train ) AD . append ( self . __find_leverages ( x_test , ad_model ) ) AD_ = unique ( hstack ( AD ) ) for z in AD_ : AD_new = hstack ( AD ) <= z if self . score == 'ba_ad' : val = balanced_accuracy_score_with_ad ( Y_true = hstack ( Y_true ) , Y_pred = hstack ( Y_pred ) , AD = AD_new ) elif self . score == 'rmse_ad' : val = rmse_score_with_ad ( Y_true = hstack ( Y_true ) , Y_pred = hstack ( Y_pred ) , AD = AD_new ) if val >= score : score = val self . threshold_value = z else : self . threshold_value = self . threshold return self
3705	def Townsend_Hales ( T , Tc , Vc , omega ) : r Tr = T / Tc return Vc / ( 1 + 0.85 * ( 1 - Tr ) + ( 1.692 + 0.986 * omega ) * ( 1 - Tr ) ** ( 1 / 3. ) )
6349	def _expand_alternates ( self , phonetic ) : alt_start = phonetic . find ( '(' ) if alt_start == - 1 : return self . _normalize_lang_attrs ( phonetic , False ) prefix = phonetic [ : alt_start ] alt_start += 1 alt_end = phonetic . find ( ')' , alt_start ) alt_string = phonetic [ alt_start : alt_end ] alt_end += 1 suffix = phonetic [ alt_end : ] alt_array = alt_string . split ( '|' ) result = '' for i in range ( len ( alt_array ) ) : alt = alt_array [ i ] alternate = self . _expand_alternates ( prefix + alt + suffix ) if alternate != '' and alternate != '[0]' : if result != '' : result += '|' result += alternate return result
9701	def send ( self , msg ) : slipDriver = sliplib . Driver ( ) slipData = slipDriver . send ( msg ) res = self . _serialPort . write ( slipData ) return res
7893	def leave ( self ) : if self . joined : p = MucPresence ( to_jid = self . room_jid , stanza_type = "unavailable" ) self . manager . stream . send ( p )
10895	def set_filter ( self , slices , values ) : self . filters = [ [ sl , values [ sl ] ] for sl in slices ]
8737	def get ( self , query , responseformat = "geojson" , verbosity = "body" , build = True ) : if build : full_query = self . _construct_ql_query ( query , responseformat = responseformat , verbosity = verbosity ) else : full_query = query if self . debug : logging . getLogger ( ) . info ( query ) r = self . _get_from_overpass ( full_query ) content_type = r . headers . get ( "content-type" ) if self . debug : print ( content_type ) if content_type == "text/csv" : result = [ ] reader = csv . reader ( StringIO ( r . text ) , delimiter = "\t" ) for row in reader : result . append ( row ) return result elif content_type in ( "text/xml" , "application/xml" , "application/osm3s+xml" ) : return r . text elif content_type == "application/json" : response = json . loads ( r . text ) if not build : return response if "elements" not in response : raise UnknownOverpassError ( "Received an invalid answer from Overpass." ) overpass_remark = response . get ( "remark" , None ) if overpass_remark and overpass_remark . startswith ( "runtime error" ) : raise ServerRuntimeError ( overpass_remark ) if responseformat is not "geojson" : return response return self . _as_geojson ( response [ "elements" ] )
4210	def csvd ( A ) : U , S , V = numpy . linalg . svd ( A ) return U , S , V
511	def _updateMinDutyCyclesGlobal ( self ) : self . _minOverlapDutyCycles . fill ( self . _minPctOverlapDutyCycles * self . _overlapDutyCycles . max ( ) )
476	def data_to_token_ids ( data_path , target_path , vocabulary_path , tokenizer = None , normalize_digits = True , UNK_ID = 3 , _DIGIT_RE = re . compile ( br"\d" ) ) : if not gfile . Exists ( target_path ) : tl . logging . info ( "Tokenizing data in %s" % data_path ) vocab , _ = initialize_vocabulary ( vocabulary_path ) with gfile . GFile ( data_path , mode = "rb" ) as data_file : with gfile . GFile ( target_path , mode = "w" ) as tokens_file : counter = 0 for line in data_file : counter += 1 if counter % 100000 == 0 : tl . logging . info ( " tokenizing line %d" % counter ) token_ids = sentence_to_token_ids ( line , vocab , tokenizer , normalize_digits , UNK_ID = UNK_ID , _DIGIT_RE = _DIGIT_RE ) tokens_file . write ( " " . join ( [ str ( tok ) for tok in token_ids ] ) + "\n" ) else : tl . logging . info ( "Target path %s exists" % target_path )
7773	def _quote ( data ) : data = data . replace ( b'\\' , b'\\\\' ) data = data . replace ( b'"' , b'\\"' ) return data
7056	def ec2_ssh ( ip_address , keypem_file , username = 'ec2-user' , raiseonfail = False ) : c = paramiko . client . SSHClient ( ) c . load_system_host_keys ( ) c . set_missing_host_key_policy ( paramiko . client . AutoAddPolicy ) privatekey = paramiko . RSAKey . from_private_key_file ( keypem_file ) try : c . connect ( ip_address , pkey = privatekey , username = 'ec2-user' ) return c except Exception as e : LOGEXCEPTION ( 'could not connect to EC2 instance at %s ' 'using keyfile: %s and user: %s' % ( ip_address , keypem_file , username ) ) if raiseonfail : raise return None
3832	async def send_chat_message ( self , send_chat_message_request ) : response = hangouts_pb2 . SendChatMessageResponse ( ) await self . _pb_request ( 'conversations/sendchatmessage' , send_chat_message_request , response ) return response
8807	def _make_job_dict ( job ) : body = { "id" : job . get ( 'id' ) , "action" : job . get ( 'action' ) , "completed" : job . get ( 'completed' ) , "tenant_id" : job . get ( 'tenant_id' ) , "created_at" : job . get ( 'created_at' ) , "transaction_id" : job . get ( 'transaction_id' ) , "parent_id" : job . get ( 'parent_id' , None ) } if not body [ 'transaction_id' ] : body [ 'transaction_id' ] = job . get ( 'id' ) completed = 0 for sub in job . subtransactions : if sub . get ( 'completed' ) : completed += 1 pct = 100 if job . get ( 'completed' ) else 0 if len ( job . subtransactions ) > 0 : pct = float ( completed ) / len ( job . subtransactions ) * 100.0 body [ 'transaction_percent' ] = int ( pct ) body [ 'completed_subtransactions' ] = completed body [ 'subtransactions' ] = len ( job . subtransactions ) return body
11715	def console_output ( self , instance = None ) : if instance is None : instance = self . instance ( ) for stage in instance [ 'stages' ] : for job in stage [ 'jobs' ] : if job [ 'result' ] not in self . final_results : continue artifact = self . artifact ( instance [ 'counter' ] , stage [ 'name' ] , job [ 'name' ] , stage [ 'counter' ] ) output = artifact . get ( 'cruise-output/console.log' ) yield ( { 'pipeline' : self . name , 'pipeline_counter' : instance [ 'counter' ] , 'stage' : stage [ 'name' ] , 'stage_counter' : stage [ 'counter' ] , 'job' : job [ 'name' ] , 'job_result' : job [ 'result' ] , } , output . body )
12251	def get_all_keys ( self , * args , ** kwargs ) : if kwargs . pop ( 'force' , None ) : headers = kwargs . get ( 'headers' , args [ 0 ] if len ( args ) else None ) or dict ( ) headers [ 'force' ] = True kwargs [ 'headers' ] = headers return super ( Bucket , self ) . get_all_keys ( * args , ** kwargs )
13032	def serve_forever ( self , poll_interval = 0.5 ) : logger . info ( 'Starting server on {}:{}...' . format ( self . server_name , self . server_port ) ) while True : try : self . poll_once ( poll_interval ) except ( KeyboardInterrupt , SystemExit ) : break self . handle_close ( ) logger . info ( 'Server stopped.' )
13771	def collect_links ( self , env = None ) : for asset in self . assets . values ( ) : if asset . has_bundles ( ) : asset . collect_files ( ) if env is None : env = self . config . env if env == static_bundle . ENV_PRODUCTION : self . _minify ( emulate = True ) self . _add_url_prefix ( )
4057	def _csljson_processor ( self , retrieved ) : items = [ ] json_kwargs = { } if self . preserve_json_order : json_kwargs [ "object_pairs_hook" ] = OrderedDict for csl in retrieved . entries : items . append ( json . loads ( csl [ "content" ] [ 0 ] [ "value" ] , ** json_kwargs ) ) self . url_params = None return items
13857	def sendmsg ( self , message , recipient_mobiles = [ ] , url = 'http://services.ambientmobile.co.za/sms' , concatenate_message = True , message_id = str ( time ( ) ) . replace ( "." , "" ) , reply_path = None , allow_duplicates = True , allow_invalid_numbers = True , ) : if not recipient_mobiles or not ( isinstance ( recipient_mobiles , list ) or isinstance ( recipient_mobiles , tuple ) ) : raise AmbientSMSError ( "Missing recipients" ) if not message or not len ( message ) : raise AmbientSMSError ( "Missing message" ) postXMLList = [ ] postXMLList . append ( "<api-key>%s</api-key>" % self . api_key ) postXMLList . append ( "<password>%s</password>" % self . password ) postXMLList . append ( "<recipients>%s</recipients>" % "" . join ( [ "<mobile>%s</mobile>" % m for m in recipient_mobiles ] ) ) postXMLList . append ( "<msg>%s</msg>" % message ) postXMLList . append ( "<concat>%s</concat>" % ( 1 if concatenate_message else 0 ) ) postXMLList . append ( "<message_id>%s</message_id>" % message_id ) postXMLList . append ( "<allow_duplicates>%s</allow_duplicates>" % ( 1 if allow_duplicates else 0 ) ) postXMLList . append ( "<allow_invalid_numbers>%s</allow_invalid_numbers>" % ( 1 if allow_invalid_numbers else 0 ) ) if reply_path : postXMLList . append ( "<reply_path>%s</reply_path>" % reply_path ) postXML = '<sms>%s</sms>' % "" . join ( postXMLList ) result = self . curl ( url , postXML ) status = result . get ( "status" , None ) if status and int ( status ) in [ 0 , 1 , 2 ] : return result else : raise AmbientSMSError ( int ( status ) )
7437	def stats ( self ) : nameordered = self . samples . keys ( ) nameordered . sort ( ) pd . options . display . max_rows = len ( self . samples ) statdat = pd . DataFrame ( [ self . samples [ i ] . stats for i in nameordered ] , index = nameordered ) . dropna ( axis = 1 , how = 'all' ) for column in statdat : if column not in [ "hetero_est" , "error_est" ] : statdat [ column ] = np . nan_to_num ( statdat [ column ] ) . astype ( int ) return statdat
2497	def handle_package_has_file_helper ( self , pkg_file ) : nodes = list ( self . graph . triples ( ( None , self . spdx_namespace . fileName , Literal ( pkg_file . name ) ) ) ) if len ( nodes ) == 1 : return nodes [ 0 ] [ 0 ] else : raise InvalidDocumentError ( 'handle_package_has_file_helper could not' + ' find file node for file: {0}' . format ( pkg_file . name ) )
4434	async def update_state ( self , data ) : guild_id = int ( data [ 'guildId' ] ) if guild_id in self . players : player = self . players . get ( guild_id ) player . position = data [ 'state' ] . get ( 'position' , 0 ) player . position_timestamp = data [ 'state' ] [ 'time' ]
1947	def sync_unicorn_to_manticore ( self ) : self . write_backs_disabled = True for reg in self . registers : val = self . _emu . reg_read ( self . _to_unicorn_id ( reg ) ) self . _cpu . write_register ( reg , val ) if len ( self . _mem_delta ) > 0 : logger . debug ( f"Syncing {len(self._mem_delta)} writes back into Manticore" ) for location in self . _mem_delta : value , size = self . _mem_delta [ location ] self . _cpu . write_int ( location , value , size * 8 ) self . write_backs_disabled = False self . _mem_delta = { }
10861	def param_particle_pos ( self , ind ) : ind = self . _vps ( listify ( ind ) ) return [ self . _i2p ( i , j ) for i in ind for j in [ 'z' , 'y' , 'x' ] ]
13401	def removeLogbook ( self , menu = None ) : if self . logMenuCount > 1 and menu is not None : menu . removeMenu ( ) self . logMenus . remove ( menu ) self . logMenuCount -= 1
2189	def _rectify_products ( self , product = None ) : products = self . product if product is None else product if products is None : return None if not isinstance ( products , ( list , tuple ) ) : products = [ products ] return products
13585	def add_link ( cls , attr , title = '' , display = '' ) : global klass_count klass_count += 1 fn_name = 'dyn_fn_%d' % klass_count cls . list_display . append ( fn_name ) if not title : title = attr . capitalize ( ) _display = display def _link ( self , obj ) : field_obj = admin_obj_attr ( obj , attr ) if not field_obj : return '' text = _obj_display ( field_obj , _display ) return admin_obj_link ( field_obj , text ) _link . short_description = title _link . allow_tags = True _link . admin_order_field = attr setattr ( cls , fn_name , _link )
6041	def unmasked_sparse_to_sparse ( self ) : return mapping_util . unmasked_sparse_to_sparse_from_mask_and_pixel_centres ( mask = self . regular_grid . mask , unmasked_sparse_grid_pixel_centres = self . unmasked_sparse_grid_pixel_centres , total_sparse_pixels = self . total_sparse_pixels ) . astype ( 'int' )
12218	def _bind_args ( sig , param_matchers , args , kwargs ) : bound = sig . bind ( * args , ** kwargs ) if not all ( param_matcher ( bound . arguments [ param_name ] ) for param_name , param_matcher in param_matchers ) : raise TypeError return bound
13834	def PrintMessage ( self , message ) : fields = message . ListFields ( ) if self . use_index_order : fields . sort ( key = lambda x : x [ 0 ] . index ) for field , value in fields : if _IsMapEntry ( field ) : for key in sorted ( value ) : entry_submsg = field . message_type . _concrete_class ( key = key , value = value [ key ] ) self . PrintField ( field , entry_submsg ) elif field . label == descriptor . FieldDescriptor . LABEL_REPEATED : for element in value : self . PrintField ( field , element ) else : self . PrintField ( field , value )
245	def get_low_liquidity_transactions ( transactions , market_data , last_n_days = None ) : txn_daily_w_bar = daily_txns_with_bar_data ( transactions , market_data ) txn_daily_w_bar . index . name = 'date' txn_daily_w_bar = txn_daily_w_bar . reset_index ( ) if last_n_days is not None : md = txn_daily_w_bar . date . max ( ) - pd . Timedelta ( days = last_n_days ) txn_daily_w_bar = txn_daily_w_bar [ txn_daily_w_bar . date > md ] bar_consumption = txn_daily_w_bar . assign ( max_pct_bar_consumed = ( txn_daily_w_bar . amount / txn_daily_w_bar . volume ) * 100 ) . sort_values ( 'max_pct_bar_consumed' , ascending = False ) max_bar_consumption = bar_consumption . groupby ( 'symbol' ) . first ( ) return max_bar_consumption [ [ 'date' , 'max_pct_bar_consumed' ] ]
10527	def cast_to_list ( position ) : @ wrapt . decorator def wrapper ( function , instance , args , kwargs ) : if not isinstance ( args [ position ] , list ) : args = list ( args ) args [ position ] = [ args [ position ] ] args = tuple ( args ) return function ( * args , ** kwargs ) return wrapper
2995	def _getJsonIEXCloud ( url , token = '' , version = 'beta' ) : url = _URL_PREFIX2 . format ( version = version ) + url resp = requests . get ( urlparse ( url ) . geturl ( ) , proxies = _PYEX_PROXIES , params = { 'token' : token } ) if resp . status_code == 200 : return resp . json ( ) raise PyEXception ( 'Response %d - ' % resp . status_code , resp . text )
1512	def distribute_package ( roles , cl_args ) : Log . info ( "Distributing heron package to nodes (this might take a while)..." ) masters = roles [ Role . MASTERS ] slaves = roles [ Role . SLAVES ] tar_file = tempfile . NamedTemporaryFile ( suffix = ".tmp" ) . name Log . debug ( "TAR file %s to %s" % ( cl_args [ "heron_dir" ] , tar_file ) ) make_tarfile ( tar_file , cl_args [ "heron_dir" ] ) dist_nodes = masters . union ( slaves ) scp_package ( tar_file , dist_nodes , cl_args )
1636	def CheckForFunctionLengths ( filename , clean_lines , linenum , function_state , error ) : lines = clean_lines . lines line = lines [ linenum ] joined_line = '' starting_func = False regexp = r'(\w(\w|::|\*|\&|\s)*)\(' match_result = Match ( regexp , line ) if match_result : function_name = match_result . group ( 1 ) . split ( ) [ - 1 ] if function_name == 'TEST' or function_name == 'TEST_F' or ( not Match ( r'[A-Z_]+$' , function_name ) ) : starting_func = True if starting_func : body_found = False for start_linenum in range ( linenum , clean_lines . NumLines ( ) ) : start_line = lines [ start_linenum ] joined_line += ' ' + start_line . lstrip ( ) if Search ( r'(;|})' , start_line ) : body_found = True break elif Search ( r'{' , start_line ) : body_found = True function = Search ( r'((\w|:)*)\(' , line ) . group ( 1 ) if Match ( r'TEST' , function ) : parameter_regexp = Search ( r'(\(.*\))' , joined_line ) if parameter_regexp : function += parameter_regexp . group ( 1 ) else : function += '()' function_state . Begin ( function ) break if not body_found : error ( filename , linenum , 'readability/fn_size' , 5 , 'Lint failed to find start of function body.' ) elif Match ( r'^\}\s*$' , line ) : function_state . Check ( error , filename , linenum ) function_state . End ( ) elif not Match ( r'^\s*$' , line ) : function_state . Count ( )
11825	def random_boggle ( n = 4 ) : cubes = [ cubes16 [ i % 16 ] for i in range ( n * n ) ] random . shuffle ( cubes ) return map ( random . choice , cubes )
9845	def ndmeshgrid ( * arrs ) : arrs = tuple ( arrs ) lens = list ( map ( len , arrs ) ) dim = len ( arrs ) sz = 1 for s in lens : sz *= s ans = [ ] for i , arr in enumerate ( arrs ) : slc = [ 1 ] * dim slc [ i ] = lens [ i ] arr2 = numpy . asanyarray ( arr ) . reshape ( slc ) for j , sz in enumerate ( lens ) : if j != i : arr2 = arr2 . repeat ( sz , axis = j ) ans . append ( arr2 ) return tuple ( ans )
635	def computeActivity ( self , activePresynapticCells , connectedPermanence ) : numActiveConnectedSynapsesForSegment = [ 0 ] * self . _nextFlatIdx numActivePotentialSynapsesForSegment = [ 0 ] * self . _nextFlatIdx threshold = connectedPermanence - EPSILON for cell in activePresynapticCells : for synapse in self . _synapsesForPresynapticCell [ cell ] : flatIdx = synapse . segment . flatIdx numActivePotentialSynapsesForSegment [ flatIdx ] += 1 if synapse . permanence > threshold : numActiveConnectedSynapsesForSegment [ flatIdx ] += 1 return ( numActiveConnectedSynapsesForSegment , numActivePotentialSynapsesForSegment )
7015	def concatenate_textlcs_for_objectid ( lcbasedir , objectid , aperture = 'TF1' , postfix = '.gz' , sortby = 'rjd' , normalize = True , recursive = True ) : LOGINFO ( 'looking for light curves for %s, aperture %s in directory: %s' % ( objectid , aperture , lcbasedir ) ) if recursive is False : matching = glob . glob ( os . path . join ( lcbasedir , '*%s*%s*%s' % ( objectid , aperture , postfix ) ) ) else : if sys . version_info [ : 2 ] > ( 3 , 4 ) : matching = glob . glob ( os . path . join ( lcbasedir , '**' , '*%s*%s*%s' % ( objectid , aperture , postfix ) ) , recursive = True ) LOGINFO ( 'found %s files: %s' % ( len ( matching ) , repr ( matching ) ) ) else : walker = os . walk ( lcbasedir ) matching = [ ] for root , dirs , _files in walker : for sdir in dirs : searchpath = os . path . join ( root , sdir , '*%s*%s*%s' % ( objectid , aperture , postfix ) ) foundfiles = glob . glob ( searchpath ) if foundfiles : matching . extend ( foundfiles ) LOGINFO ( 'found %s in dir: %s' % ( repr ( foundfiles ) , os . path . join ( root , sdir ) ) ) if matching and len ( matching ) > 0 : clcdict = concatenate_textlcs ( matching , sortby = sortby , normalize = normalize ) return clcdict else : LOGERROR ( 'did not find any light curves for %s and aperture %s' % ( objectid , aperture ) ) return None
288	def plot_returns ( returns , live_start_date = None , ax = None ) : if ax is None : ax = plt . gca ( ) ax . set_label ( '' ) ax . set_ylabel ( 'Returns' ) if live_start_date is not None : live_start_date = ep . utils . get_utc_timestamp ( live_start_date ) is_returns = returns . loc [ returns . index < live_start_date ] oos_returns = returns . loc [ returns . index >= live_start_date ] is_returns . plot ( ax = ax , color = 'g' ) oos_returns . plot ( ax = ax , color = 'r' ) else : returns . plot ( ax = ax , color = 'g' ) return ax
12168	def _dispatch_function ( self , event , listener , * args , ** kwargs ) : try : return listener ( * args , ** kwargs ) except Exception as exc : if event == self . LISTENER_ERROR_EVENT : raise return self . emit ( self . LISTENER_ERROR_EVENT , event , listener , exc )
3263	def get_workspace ( self , name ) : workspaces = self . get_workspaces ( names = name ) return self . _return_first_item ( workspaces )
5214	def earning ( ticker , by = 'Geo' , typ = 'Revenue' , ccy = None , level = None , ** kwargs ) -> pd . DataFrame : ovrd = 'G' if by [ 0 ] . upper ( ) == 'G' else 'P' new_kw = dict ( raw = True , Product_Geo_Override = ovrd ) header = bds ( tickers = ticker , flds = 'PG_Bulk_Header' , ** new_kw , ** kwargs ) if ccy : kwargs [ 'Eqy_Fund_Crncy' ] = ccy if level : kwargs [ 'PG_Hierarchy_Level' ] = level data = bds ( tickers = ticker , flds = f'PG_{typ}' , ** new_kw , ** kwargs ) return assist . format_earning ( data = data , header = header )
5666	def _run ( self ) : if self . _has_run : raise RuntimeError ( "This spreader instance has already been run: " "create a new Spreader object for a new run." ) i = 1 while self . event_heap . size ( ) > 0 and len ( self . _uninfected_stops ) > 0 : event = self . event_heap . pop_next_event ( ) this_stop = self . _stop_I_to_spreading_stop [ event . from_stop_I ] if event . arr_time_ut > self . start_time_ut + self . max_duration_ut : break if this_stop . can_infect ( event ) : target_stop = self . _stop_I_to_spreading_stop [ event . to_stop_I ] already_visited = target_stop . has_been_visited ( ) target_stop . visit ( event ) if not already_visited : self . _uninfected_stops . remove ( event . to_stop_I ) print ( i , self . event_heap . size ( ) ) transfer_distances = self . gtfs . get_straight_line_transfer_distances ( event . to_stop_I ) self . event_heap . add_walk_events_to_heap ( transfer_distances , event , self . start_time_ut , self . walk_speed , self . _uninfected_stops , self . max_duration_ut ) i += 1 self . _has_run = True
5663	def get_trip_points ( cur , route_id , offset = 0 , tripid_glob = '' ) : extra_where = '' if tripid_glob : extra_where = "AND trip_id GLOB '%s'" % tripid_glob cur . execute ( 'SELECT seq, lat, lon ' 'FROM (select trip_I from route ' ' LEFT JOIN trips USING (route_I) ' ' WHERE route_id=? %s limit 1 offset ? ) ' 'JOIN stop_times USING (trip_I) ' 'LEFT JOIN stop USING (stop_id) ' 'ORDER BY seq' % extra_where , ( route_id , offset ) ) stop_points = [ dict ( seq = row [ 0 ] , lat = row [ 1 ] , lon = row [ 2 ] ) for row in cur ] return stop_points
412	def save_model ( self , network = None , model_name = 'model' , ** kwargs ) : kwargs . update ( { 'model_name' : model_name } ) self . _fill_project_info ( kwargs ) params = network . get_all_params ( ) s = time . time ( ) kwargs . update ( { 'architecture' : network . all_graphs , 'time' : datetime . utcnow ( ) } ) try : params_id = self . model_fs . put ( self . _serialization ( params ) ) kwargs . update ( { 'params_id' : params_id , 'time' : datetime . utcnow ( ) } ) self . db . Model . insert_one ( kwargs ) print ( "[Database] Save model: SUCCESS, took: {}s" . format ( round ( time . time ( ) - s , 2 ) ) ) return True except Exception as e : exc_type , exc_obj , exc_tb = sys . exc_info ( ) fname = os . path . split ( exc_tb . tb_frame . f_code . co_filename ) [ 1 ] logging . info ( "{} {} {} {} {}" . format ( exc_type , exc_obj , fname , exc_tb . tb_lineno , e ) ) print ( "[Database] Save model: FAIL" ) return False
12947	def saveToExternal ( self , redisCon ) : if type ( redisCon ) == dict : conn = redis . Redis ( ** redisCon ) elif hasattr ( conn , '__class__' ) and issubclass ( conn . __class__ , redis . Redis ) : conn = redisCon else : raise ValueError ( 'saveToExternal "redisCon" param must either be a dictionary of connection parameters, or redis.Redis, or extension thereof' ) saver = self . saver forceID = saver . _getNextID ( conn ) myCopy = self . copy ( False ) return saver . save ( myCopy , usePipeline = True , forceID = forceID , conn = conn )
8236	def left_complement ( clr ) : left = split_complementary ( clr ) [ 1 ] colors = complementary ( clr ) colors [ 3 ] . h = left . h colors [ 4 ] . h = left . h colors [ 5 ] . h = left . h colors = colorlist ( colors [ 0 ] , colors [ 2 ] , colors [ 1 ] , colors [ 3 ] , colors [ 4 ] , colors [ 5 ] ) return colors
9580	def read_struct_array ( fd , endian , header ) : field_name_length = read_elements ( fd , endian , [ 'miINT32' ] ) if field_name_length > 32 : raise ParseError ( 'Unexpected field name length: {}' . format ( field_name_length ) ) fields = read_elements ( fd , endian , [ 'miINT8' ] , is_name = True ) if isinstance ( fields , basestring ) : fields = [ fields ] empty = lambda : [ list ( ) for i in range ( header [ 'dims' ] [ 0 ] ) ] array = { } for row in range ( header [ 'dims' ] [ 0 ] ) : for col in range ( header [ 'dims' ] [ 1 ] ) : for field in fields : vheader , next_pos , fd_var = read_var_header ( fd , endian ) data = read_var_array ( fd_var , endian , vheader ) if field not in array : array [ field ] = empty ( ) array [ field ] [ row ] . append ( data ) fd . seek ( next_pos ) for field in fields : rows = array [ field ] for i in range ( header [ 'dims' ] [ 0 ] ) : rows [ i ] = squeeze ( rows [ i ] ) array [ field ] = squeeze ( array [ field ] ) return array
2612	def deserialize_object ( buffers , g = None ) : bufs = list ( buffers ) pobj = buffer_to_bytes_py2 ( bufs . pop ( 0 ) ) canned = pickle . loads ( pobj ) if istype ( canned , sequence_types ) and len ( canned ) < MAX_ITEMS : for c in canned : _restore_buffers ( c , bufs ) newobj = uncan_sequence ( canned , g ) elif istype ( canned , dict ) and len ( canned ) < MAX_ITEMS : newobj = { } for k in sorted ( canned ) : c = canned [ k ] _restore_buffers ( c , bufs ) newobj [ k ] = uncan ( c , g ) else : _restore_buffers ( canned , bufs ) newobj = uncan ( canned , g ) return newobj , bufs
12463	def print_error ( message , wrap = True ) : if wrap : message = 'ERROR: {0}. Exit...' . format ( message . rstrip ( '.' ) ) colorizer = ( _color_wrap ( colorama . Fore . RED ) if colorama else lambda message : message ) return print ( colorizer ( message ) , file = sys . stderr )
9611	def _request ( self , method , url , body ) : if method != 'POST' and method != 'PUT' : body = None s = Session ( ) LOGGER . debug ( 'Method: {0}, Url: {1}, Body: {2}.' . format ( method , url , body ) ) req = Request ( method , url , json = body ) prepped = s . prepare_request ( req ) res = s . send ( prepped , timeout = self . _timeout or None ) res . raise_for_status ( ) return res . json ( )
4529	def set_device_brightness ( self , val ) : self . _chipset_brightness = ( val >> 3 ) self . _brightness_list = [ 0xE0 + self . _chipset_brightness ] * self . numLEDs self . _packet [ self . _start_frame : self . _pixel_stop : 4 ] = ( self . _brightness_list )
12058	def TK_askPassword ( title = "input" , msg = "type here:" ) : root = tkinter . Tk ( ) root . withdraw ( ) root . attributes ( "-topmost" , True ) root . lift ( ) value = tkinter . simpledialog . askstring ( title , msg ) root . destroy ( ) return value
9811	def grant ( username ) : try : PolyaxonClient ( ) . user . grant_superuser ( username ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not grant superuser role to user `{}`.' . format ( username ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Superuser role was granted successfully to user `{}`." . format ( username ) )
11624	def from_devanagari ( self , data ) : from indic_transliteration import sanscript return sanscript . transliterate ( data = data , _from = sanscript . DEVANAGARI , _to = self . name )
6750	def register ( self ) : self . _set_defaults ( ) all_satchels [ self . name . upper ( ) ] = self manifest_recorder [ self . name ] = self . record_manifest if self . required_system_packages : required_system_packages [ self . name . upper ( ) ] = self . required_system_packages
2198	def platform_data_dir ( ) : if LINUX : dpath_ = os . environ . get ( 'XDG_DATA_HOME' , '~/.local/share' ) elif DARWIN : dpath_ = '~/Library/Application Support' elif WIN32 : dpath_ = os . environ . get ( 'APPDATA' , '~/AppData/Roaming' ) else : raise '~/AppData/Local' dpath = normpath ( expanduser ( dpath_ ) ) return dpath
549	def _initPeriodicActivities ( self ) : updateModelDBResults = PeriodicActivityRequest ( repeating = True , period = 100 , cb = self . _updateModelDBResults ) updateJobResults = PeriodicActivityRequest ( repeating = True , period = 100 , cb = self . __updateJobResultsPeriodic ) checkCancelation = PeriodicActivityRequest ( repeating = True , period = 50 , cb = self . __checkCancelation ) checkMaturity = PeriodicActivityRequest ( repeating = True , period = 10 , cb = self . __checkMaturity ) updateJobResultsFirst = PeriodicActivityRequest ( repeating = False , period = 2 , cb = self . __updateJobResultsPeriodic ) periodicActivities = [ updateModelDBResults , updateJobResultsFirst , updateJobResults , checkCancelation ] if self . _isMaturityEnabled : periodicActivities . append ( checkMaturity ) return PeriodicActivityMgr ( requestedActivities = periodicActivities )
5013	def _call_search_students_recursively ( self , sap_search_student_url , all_inactive_learners , page_size , start_at ) : search_student_paginated_url = '{sap_search_student_url}&{pagination_criterion}' . format ( sap_search_student_url = sap_search_student_url , pagination_criterion = '$count=true&$top={page_size}&$skip={start_at}' . format ( page_size = page_size , start_at = start_at , ) , ) try : response = self . session . get ( search_student_paginated_url ) sap_inactive_learners = response . json ( ) except ( ConnectionError , Timeout ) : LOGGER . warning ( 'Unable to fetch inactive learners from SAP searchStudent API with url ' '"{%s}".' , search_student_paginated_url , ) return None if 'error' in sap_inactive_learners : LOGGER . warning ( 'SAP searchStudent API for customer %s and base url %s returned response with ' 'error message "%s" and with error code "%s".' , self . enterprise_configuration . enterprise_customer . name , self . enterprise_configuration . sapsf_base_url , sap_inactive_learners [ 'error' ] . get ( 'message' ) , sap_inactive_learners [ 'error' ] . get ( 'code' ) , ) return None new_page_start_at = page_size + start_at all_inactive_learners += sap_inactive_learners [ 'value' ] if sap_inactive_learners [ '@odata.count' ] > new_page_start_at : return self . _call_search_students_recursively ( sap_search_student_url , all_inactive_learners , page_size = page_size , start_at = new_page_start_at , ) return all_inactive_learners
10412	def summarize_node_filter ( graph : BELGraph , node_filters : NodePredicates ) -> None : passed = count_passed_node_filter ( graph , node_filters ) print ( '{}/{} nodes passed' . format ( passed , graph . number_of_nodes ( ) ) )
5576	def load_output_writer ( output_params , readonly = False ) : if not isinstance ( output_params , dict ) : raise TypeError ( "output_params must be a dictionary" ) driver_name = output_params [ "format" ] for v in pkg_resources . iter_entry_points ( DRIVERS_ENTRY_POINT ) : _driver = v . load ( ) if all ( [ hasattr ( _driver , attr ) for attr in [ "OutputData" , "METADATA" ] ] ) and ( _driver . METADATA [ "driver_name" ] == driver_name ) : return _driver . OutputData ( output_params , readonly = readonly ) raise MapcheteDriverError ( "no loader for driver '%s' could be found." % driver_name )
6763	def dumpload ( self , site = None , role = None ) : r = self . database_renderer ( site = site , role = role ) r . run ( 'pg_dump -c --host={host_string} --username={db_user} ' '--blobs --format=c {db_name} -n public | ' 'pg_restore -U {db_postgresql_postgres_user} --create ' '--dbname={db_name}' )
4689	def init_aes ( shared_secret , nonce ) : " Shared Secret " ss = hashlib . sha512 ( unhexlify ( shared_secret ) ) . digest ( ) " Seed " seed = bytes ( str ( nonce ) , "ascii" ) + hexlify ( ss ) seed_digest = hexlify ( hashlib . sha512 ( seed ) . digest ( ) ) . decode ( "ascii" ) " AES " key = unhexlify ( seed_digest [ 0 : 64 ] ) iv = unhexlify ( seed_digest [ 64 : 96 ] ) return AES . new ( key , AES . MODE_CBC , iv )
638	def getString ( cls , prop ) : if cls . _properties is None : cls . _readStdConfigFiles ( ) envValue = os . environ . get ( "%s%s" % ( cls . envPropPrefix , prop . replace ( '.' , '_' ) ) , None ) if envValue is not None : return envValue return cls . _properties [ prop ]
4864	def get_groups ( self , obj ) : if obj . user : return [ group . name for group in obj . user . groups . filter ( name__in = ENTERPRISE_PERMISSION_GROUPS ) ] return [ ]
7412	def run_tree_inference ( self , nexus , idx ) : tmpdir = tempfile . tempdir tmpfile = os . path . join ( tempfile . NamedTemporaryFile ( delete = False , prefix = str ( idx ) , dir = tmpdir , ) ) tmpfile . write ( nexus ) tmpfile . flush ( ) rax = raxml ( name = str ( idx ) , data = tmpfile . name , workdir = tmpdir , N = 1 , T = 2 ) rax . run ( force = True , block = True , quiet = True ) tmpfile . close ( ) order = get_order ( toytree . tree ( rax . trees . bestTree ) ) return "" . join ( order )
8905	def create_access_token ( self , valid_in_hours = 1 , data = None ) : data = data or { } token = AccessToken ( token = self . generate ( ) , expires_at = expires_at ( hours = valid_in_hours ) , data = data ) return token
4925	def get_missing_params_message ( self , parameter_state ) : params = ', ' . join ( name for name , present in parameter_state if not present ) return self . MISSING_REQUIRED_PARAMS_MSG . format ( params )
10018	def application_exists ( self ) : response = self . ebs . describe_applications ( application_names = [ self . app_name ] ) return len ( response [ 'DescribeApplicationsResponse' ] [ 'DescribeApplicationsResult' ] [ 'Applications' ] ) > 0
8720	def operation_upload ( uploader , sources , verify , do_compile , do_file , do_restart ) : sources , destinations = destination_from_source ( sources ) if len ( destinations ) == len ( sources ) : if uploader . prepare ( ) : for filename , dst in zip ( sources , destinations ) : if do_compile : uploader . file_remove ( os . path . splitext ( dst ) [ 0 ] + '.lc' ) uploader . write_file ( filename , dst , verify ) if do_compile and dst != 'init.lua' : uploader . file_compile ( dst ) uploader . file_remove ( dst ) if do_file : uploader . file_do ( os . path . splitext ( dst ) [ 0 ] + '.lc' ) elif do_file : uploader . file_do ( dst ) else : raise Exception ( 'Error preparing nodemcu for reception' ) else : raise Exception ( 'You must specify a destination filename for each file you want to upload.' ) if do_restart : uploader . node_restart ( ) log . info ( 'All done!' )
1541	def build_and_submit ( self ) : class_dict = self . _construct_topo_class_dict ( ) topo_cls = TopologyType ( self . topology_name , ( Topology , ) , class_dict ) topo_cls . write ( )
7184	def copy_arguments_to_annotations ( args , type_comment , * , is_method = False ) : if isinstance ( type_comment , ast3 . Ellipsis ) : return expected = len ( args . args ) if args . vararg : expected += 1 expected += len ( args . kwonlyargs ) if args . kwarg : expected += 1 actual = len ( type_comment ) if isinstance ( type_comment , list ) else 1 if expected != actual : if is_method and expected - actual == 1 : pass else : raise ValueError ( f"number of arguments in type comment doesn't match; " + f"expected {expected}, found {actual}" ) if isinstance ( type_comment , list ) : next_value = type_comment . pop else : _tc = type_comment def next_value ( index : int = 0 ) -> ast3 . expr : return _tc for arg in args . args [ expected - actual : ] : ensure_no_annotation ( arg . annotation ) arg . annotation = next_value ( 0 ) if args . vararg : ensure_no_annotation ( args . vararg . annotation ) args . vararg . annotation = next_value ( 0 ) for arg in args . kwonlyargs : ensure_no_annotation ( arg . annotation ) arg . annotation = next_value ( 0 ) if args . kwarg : ensure_no_annotation ( args . kwarg . annotation ) args . kwarg . annotation = next_value ( 0 )
5329	def get_raw ( config , backend_section , arthur ) : if arthur : task = TaskRawDataArthurCollection ( config , backend_section = backend_section ) else : task = TaskRawDataCollection ( config , backend_section = backend_section ) TaskProjects ( config ) . execute ( ) try : task . execute ( ) logging . info ( "Loading raw data finished!" ) except Exception as e : logging . error ( str ( e ) ) sys . exit ( - 1 )
7436	def _tuplecheck ( newvalue , dtype = str ) : if isinstance ( newvalue , list ) : newvalue = tuple ( newvalue ) if isinstance ( newvalue , str ) : newvalue = newvalue . rstrip ( ")" ) . strip ( "(" ) try : newvalue = tuple ( [ dtype ( i . strip ( ) ) for i in newvalue . split ( "," ) ] ) except TypeError : newvalue = tuple ( dtype ( newvalue ) ) except ValueError : LOGGER . info ( "Assembly.tuplecheck() failed to cast to {} - {}" . format ( dtype , newvalue ) ) raise except Exception as inst : LOGGER . info ( inst ) raise SystemExit ( "\nError: Param`{}` is not formatted correctly.\n({})\n" . format ( newvalue , inst ) ) return newvalue
9410	def _create_struct ( data , session ) : out = Struct ( ) for name in data . dtype . names : item = data [ name ] if isinstance ( item , np . ndarray ) and item . dtype . kind == 'O' : item = item . squeeze ( ) . tolist ( ) out [ name ] = _extract ( item , session ) return out
8828	def sg_gather_associated_ports ( context , group ) : if not group : return None if not hasattr ( group , "ports" ) or len ( group . ports ) <= 0 : return [ ] return group . ports
3882	def upgrade_name ( self , user_ ) : if user_ . name_type > self . name_type : self . full_name = user_ . full_name self . first_name = user_ . first_name self . name_type = user_ . name_type logger . debug ( 'Added %s name to User "%s": %s' , self . name_type . name . lower ( ) , self . full_name , self )
2182	def rebuild_auth ( self , prepared_request , response ) : if "Authorization" in prepared_request . headers : prepared_request . headers . pop ( "Authorization" , True ) prepared_request . prepare_auth ( self . auth ) return
9726	async def send_xml ( self , xml ) : return await asyncio . wait_for ( self . _protocol . send_command ( xml , command_type = QRTPacketType . PacketXML ) , timeout = self . _timeout , )
6252	def create_normal_matrix ( self , modelview ) : normal_m = Matrix33 . from_matrix44 ( modelview ) normal_m = normal_m . inverse normal_m = normal_m . transpose ( ) return normal_m
3515	def chartbeat_bottom ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return ChartbeatBottomNode ( )
1986	def load_stream ( self , key , binary = False ) : value = self . load_value ( key , binary = binary ) yield io . BytesIO ( value ) if binary else io . StringIO ( value )
12603	def duplicated_rows ( df , col_name ) : _check_cols ( df , [ col_name ] ) dups = df [ pd . notnull ( df [ col_name ] ) & df . duplicated ( subset = [ col_name ] ) ] return dups
10587	def get_account_descendants ( self , account ) : result = [ ] for child in account . accounts : self . _get_account_and_descendants_ ( child , result ) return result
10396	def unscored_nodes_iter ( self ) -> BaseEntity : for node , data in self . graph . nodes ( data = True ) : if self . tag not in data : yield node
7230	def create ( self , vectors ) : if type ( vectors ) is dict : vectors = [ vectors ] for vector in vectors : if not 'properties' in list ( vector . keys ( ) ) : raise Exception ( 'Vector does not contain "properties" field.' ) if not 'item_type' in list ( vector [ 'properties' ] . keys ( ) ) : raise Exception ( 'Vector does not contain "item_type".' ) if not 'ingest_source' in list ( vector [ 'properties' ] . keys ( ) ) : raise Exception ( 'Vector does not contain "ingest_source".' ) r = self . gbdx_connection . post ( self . create_url , data = json . dumps ( vectors ) ) r . raise_for_status ( ) return r . json ( )
1948	def write_back_memory ( self , where , expr , size ) : if self . write_backs_disabled : return if type ( expr ) is bytes : self . _emu . mem_write ( where , expr ) else : if issymbolic ( expr ) : data = [ Operators . CHR ( Operators . EXTRACT ( expr , offset , 8 ) ) for offset in range ( 0 , size , 8 ) ] concrete_data = [ ] for c in data : if issymbolic ( c ) : c = chr ( solver . get_value ( self . _cpu . memory . constraints , c ) ) concrete_data . append ( c ) data = concrete_data else : data = [ Operators . CHR ( Operators . EXTRACT ( expr , offset , 8 ) ) for offset in range ( 0 , size , 8 ) ] logger . debug ( f"Writing back {hr_size(size // 8)} to {hex(where)}: {data}" ) self . _emu . mem_write ( where , b'' . join ( b . encode ( 'utf-8' ) if type ( b ) is str else b for b in data ) )
10458	def clearContents ( cls ) : log_msg = 'Request to clear contents of pasteboard: general' logging . debug ( log_msg ) pb = AppKit . NSPasteboard . generalPasteboard ( ) pb . clearContents ( ) return True
11938	def add_message_for ( users , level , message_text , extra_tags = '' , date = None , url = None , fail_silently = False ) : BackendClass = stored_messages_settings . STORAGE_BACKEND backend = BackendClass ( ) m = backend . create_message ( level , message_text , extra_tags , date , url ) backend . archive_store ( users , m ) backend . inbox_store ( users , m )
3470	def get_coefficient ( self , metabolite_id ) : if isinstance ( metabolite_id , Metabolite ) : return self . _metabolites [ metabolite_id ] _id_to_metabolites = { m . id : m for m in self . _metabolites } return self . _metabolites [ _id_to_metabolites [ metabolite_id ] ]
8884	def fit ( self , X , y = None ) : X = check_array ( X ) self . _x_min = X . min ( axis = 0 ) self . _x_max = X . max ( axis = 0 ) return self
7073	def magbin_varind_gridsearch_worker ( task ) : simbasedir , gridpoint , magbinmedian = task try : res = get_recovered_variables_for_magbin ( simbasedir , magbinmedian , stetson_stdev_min = gridpoint [ 0 ] , inveta_stdev_min = gridpoint [ 1 ] , iqr_stdev_min = gridpoint [ 2 ] , statsonly = True ) return res except Exception as e : LOGEXCEPTION ( 'failed to get info for %s' % gridpoint ) return None
5831	def update ( self , id , configuration , name , description ) : data = { "configuration" : configuration , "name" : name , "description" : description } failure_message = "Dataview creation failed" self . _patch_json ( 'v1/data_views/' + id , data , failure_message = failure_message )
10107	def get_context_data ( self , ** kwargs ) : context = super ( TabView , self ) . get_context_data ( ** kwargs ) context . update ( kwargs ) process_tabs_kwargs = { 'tabs' : self . get_group_tabs ( ) , 'current_tab' : self , 'group_current_tab' : self , } context [ 'tabs' ] = self . _process_tabs ( ** process_tabs_kwargs ) context [ 'current_tab_id' ] = self . tab_id if self . tab_parent is not None : if self . tab_parent not in self . _registry : msg = '%s has no attribute _is_tab' % self . tab_parent . __class__ . __name__ raise ImproperlyConfigured ( msg ) parent = self . tab_parent ( ) process_parents_kwargs = { 'tabs' : parent . get_group_tabs ( ) , 'current_tab' : self , 'group_current_tab' : parent , } context [ 'parent_tabs' ] = self . _process_tabs ( ** process_parents_kwargs ) context [ 'parent_tab_id' ] = parent . tab_id if self . tab_id in self . _children : process_children_kwargs = { 'tabs' : [ t ( ) for t in self . _children [ self . tab_id ] ] , 'current_tab' : self , 'group_current_tab' : None , } context [ 'child_tabs' ] = self . _process_tabs ( ** process_children_kwargs ) return context
5337	def create_dashboard ( self , panel_file , data_sources = None , strict = True ) : es_enrich = self . conf [ 'es_enrichment' ] [ 'url' ] kibana_url = self . conf [ 'panels' ] [ 'kibiter_url' ] mboxes_sources = set ( [ 'pipermail' , 'hyperkitty' , 'groupsio' , 'nntp' ] ) if data_sources and any ( x in data_sources for x in mboxes_sources ) : data_sources = list ( data_sources ) data_sources . append ( 'mbox' ) if data_sources and ( 'supybot' in data_sources ) : data_sources = list ( data_sources ) data_sources . append ( 'irc' ) if data_sources and 'google_hits' in data_sources : data_sources = list ( data_sources ) data_sources . append ( 'googlehits' ) if data_sources and 'stackexchange' in data_sources : data_sources = list ( data_sources ) data_sources . append ( 'stackoverflow' ) if data_sources and 'phabricator' in data_sources : data_sources = list ( data_sources ) data_sources . append ( 'maniphest' ) try : import_dashboard ( es_enrich , kibana_url , panel_file , data_sources = data_sources , strict = strict ) except ValueError : logger . error ( "%s does not include release field. Not loading the panel." , panel_file ) except RuntimeError : logger . error ( "Can not load the panel %s" , panel_file )
9840	def __array ( self ) : try : tok = self . __consume ( ) except DXParserNoTokens : return if tok . equals ( 'type' ) : tok = self . __consume ( ) if not tok . iscode ( 'STRING' ) : raise DXParseError ( 'array: type was "%s", not a string.' % tok . text ) self . currentobject [ 'type' ] = tok . value ( ) elif tok . equals ( 'rank' ) : tok = self . __consume ( ) try : self . currentobject [ 'rank' ] = tok . value ( 'INTEGER' ) except ValueError : raise DXParseError ( 'array: rank was "%s", not an integer.' % tok . text ) elif tok . equals ( 'items' ) : tok = self . __consume ( ) try : self . currentobject [ 'size' ] = tok . value ( 'INTEGER' ) except ValueError : raise DXParseError ( 'array: items was "%s", not an integer.' % tok . text ) elif tok . equals ( 'data' ) : tok = self . __consume ( ) if not tok . iscode ( 'STRING' ) : raise DXParseError ( 'array: data was "%s", not a string.' % tok . text ) if tok . text != 'follows' : raise NotImplementedError ( 'array: Only the "data follows header" format is supported.' ) if not self . currentobject [ 'size' ] : raise DXParseError ( "array: missing number of items" ) self . currentobject [ 'array' ] = [ ] while len ( self . currentobject [ 'array' ] ) < self . currentobject [ 'size' ] : self . currentobject [ 'array' ] . extend ( self . dxfile . readline ( ) . strip ( ) . split ( ) ) elif tok . equals ( 'attribute' ) : attribute = self . __consume ( ) . value ( ) if not self . __consume ( ) . equals ( 'string' ) : raise DXParseError ( 'array: "string" expected.' ) value = self . __consume ( ) . value ( ) else : raise DXParseError ( 'array: ' + str ( tok ) + ' not recognized.' )
7425	def bedtools_merge ( data , sample ) : LOGGER . info ( "Entering bedtools_merge: %s" , sample . name ) mappedreads = os . path . join ( data . dirs . refmapping , sample . name + "-mapped-sorted.bam" ) cmd1 = [ ipyrad . bins . bedtools , "bamtobed" , "-i" , mappedreads ] cmd2 = [ ipyrad . bins . bedtools , "merge" , "-i" , "-" ] if 'pair' in data . paramsdict [ "datatype" ] : check_insert_size ( data , sample ) cmd2 . insert ( 2 , str ( data . _hackersonly [ "max_inner_mate_distance" ] ) ) cmd2 . insert ( 2 , "-d" ) else : cmd2 . insert ( 2 , str ( - 1 * data . _hackersonly [ "min_SE_refmap_overlap" ] ) ) cmd2 . insert ( 2 , "-d" ) LOGGER . info ( "stdv: bedtools merge cmds: %s %s" , cmd1 , cmd2 ) proc1 = sps . Popen ( cmd1 , stderr = sps . STDOUT , stdout = sps . PIPE ) proc2 = sps . Popen ( cmd2 , stderr = sps . STDOUT , stdout = sps . PIPE , stdin = proc1 . stdout ) result = proc2 . communicate ( ) [ 0 ] proc1 . stdout . close ( ) if proc2 . returncode : raise IPyradWarningExit ( "error in %s: %s" , cmd2 , result ) if os . path . exists ( ipyrad . __debugflag__ ) : with open ( os . path . join ( data . dirs . refmapping , sample . name + ".bed" ) , 'w' ) as outfile : outfile . write ( result ) nregions = len ( result . strip ( ) . split ( "\n" ) ) LOGGER . info ( "bedtools_merge: Got # regions: %s" , nregions ) return result
2688	def update_extend ( dst , src ) : for k , v in src . items ( ) : existing = dst . setdefault ( k , [ ] ) for x in v : if x not in existing : existing . append ( x )
4811	def evaluate ( best_processed_path , model ) : x_test_char , x_test_type , y_test = prepare_feature ( best_processed_path , option = 'test' ) y_predict = model . predict ( [ x_test_char , x_test_type ] ) y_predict = ( y_predict . ravel ( ) > 0.5 ) . astype ( int ) f1score = f1_score ( y_test , y_predict ) precision = precision_score ( y_test , y_predict ) recall = recall_score ( y_test , y_predict ) return f1score , precision , recall
12990	def overview ( ) : range_search = RangeSearch ( ) ranges = range_search . get_ranges ( ) if ranges : formatted_ranges = [ ] tags_lookup = { } for r in ranges : formatted_ranges . append ( { 'mask' : r . range } ) tags_lookup [ r . range ] = r . tags search = Host . search ( ) search = search . filter ( 'term' , status = 'up' ) search . aggs . bucket ( 'hosts' , 'ip_range' , field = 'address' , ranges = formatted_ranges ) response = search . execute ( ) print_line ( "{0:<18} {1:<6} {2}" . format ( "Range" , "Count" , "Tags" ) ) print_line ( "-" * 60 ) for entry in response . aggregations . hosts . buckets : print_line ( "{0:<18} {1:<6} {2}" . format ( entry . key , entry . doc_count , tags_lookup [ entry . key ] ) ) else : print_error ( "No ranges defined." )
10366	def complex_has_member ( graph : BELGraph , complex_node : ComplexAbundance , member_node : BaseEntity ) -> bool : return any ( v == member_node for _ , v , data in graph . out_edges ( complex_node , data = True ) if data [ RELATION ] == HAS_COMPONENT )
1579	def create_packet ( reqid , message ) : assert message . IsInitialized ( ) packet = '' typename = message . DESCRIPTOR . full_name datasize = HeronProtocol . get_size_to_pack_string ( typename ) + REQID . REQID_SIZE + HeronProtocol . get_size_to_pack_message ( message ) packet += HeronProtocol . pack_int ( datasize ) packet += HeronProtocol . pack_int ( len ( typename ) ) packet += typename packet += reqid . pack ( ) packet += HeronProtocol . pack_int ( message . ByteSize ( ) ) packet += message . SerializeToString ( ) return OutgoingPacket ( packet )
12236	def objective ( param_scales = ( 1 , 1 ) , xstar = None , seed = None ) : ndim = len ( param_scales ) def decorator ( func ) : @ wraps ( func ) def wrapper ( theta ) : return func ( theta ) def param_init ( ) : np . random . seed ( seed ) return np . random . randn ( ndim , ) * np . array ( param_scales ) wrapper . ndim = ndim wrapper . param_init = param_init wrapper . xstar = xstar return wrapper return decorator
13799	def log ( self , string ) : self . wfile . write ( json . dumps ( { 'log' : string } ) + NEWLINE )
2617	def read_state_file ( self , state_file ) : try : fh = open ( state_file , 'r' ) state = json . load ( fh ) self . vpc_id = state [ 'vpcID' ] self . sg_id = state [ 'sgID' ] self . sn_ids = state [ 'snIDs' ] self . instances = state [ 'instances' ] except Exception as e : logger . debug ( "Caught exception while reading state file: {0}" . format ( e ) ) raise e logger . debug ( "Done reading state from the local state file." )
12143	async def _push ( self , * args , ** kwargs ) : self . _data . append ( ( args , kwargs ) ) if self . _future is not None : future , self . _future = self . _future , None future . set_result ( True )
7105	def train ( self ) : for i , model in enumerate ( self . models ) : N = [ int ( i * len ( self . y ) ) for i in self . lc_range ] for n in N : X = self . X [ : n ] y = self . y [ : n ] e = Experiment ( X , y , model . estimator , self . scores , self . validation_method ) e . log_folder = self . log_folder e . train ( )
441	def count_params ( self ) : n_params = 0 for _i , p in enumerate ( self . all_params ) : n = 1 for s in p . get_shape ( ) : try : s = int ( s ) except Exception : s = 1 if s : n = n * s n_params = n_params + n return n_params
7866	def set_item ( self , key , value , timeout = None , timeout_callback = None ) : with self . _lock : logger . debug ( "expdict.__setitem__({0!r}, {1!r}, {2!r}, {3!r})" . format ( key , value , timeout , timeout_callback ) ) if not timeout : timeout = self . _default_timeout self . _timeouts [ key ] = ( time . time ( ) + timeout , timeout_callback ) return dict . __setitem__ ( self , key , value )
5550	def get_zoom_levels ( process_zoom_levels = None , init_zoom_levels = None ) : process_zoom_levels = _validate_zooms ( process_zoom_levels ) if init_zoom_levels is None : return process_zoom_levels else : init_zoom_levels = _validate_zooms ( init_zoom_levels ) if not set ( init_zoom_levels ) . issubset ( set ( process_zoom_levels ) ) : raise MapcheteConfigError ( "init zooms must be a subset of process zoom" ) return init_zoom_levels
13352	def monitor ( self , sleep = 5 ) : manager = FileModificationObjectManager ( ) timestamps = { } filebodies = { } for file in self . f_repository : timestamps [ file ] = self . _get_mtime ( file ) filebodies [ file ] = open ( file ) . read ( ) while True : for file in self . f_repository : mtime = timestamps [ file ] fbody = filebodies [ file ] modified = self . _check_modify ( file , mtime , fbody ) if not modified : continue new_mtime = self . _get_mtime ( file ) new_fbody = open ( file ) . read ( ) obj = FileModificationObject ( file , ( mtime , new_mtime ) , ( fbody , new_fbody ) ) timestamps [ file ] = new_mtime filebodies [ file ] = new_fbody manager . add_object ( obj ) yield obj time . sleep ( sleep )
3329	def acquire_read ( self , timeout = None ) : if timeout is not None : endtime = time ( ) + timeout me = currentThread ( ) self . __condition . acquire ( ) try : if self . __writer is me : self . __writercount += 1 return while True : if self . __writer is None : if self . __upgradewritercount or self . __pendingwriters : if me in self . __readers : self . __readers [ me ] += 1 return else : self . __readers [ me ] = self . __readers . get ( me , 0 ) + 1 return if timeout is not None : remaining = endtime - time ( ) if remaining <= 0 : raise RuntimeError ( "Acquiring read lock timed out" ) self . __condition . wait ( remaining ) else : self . __condition . wait ( ) finally : self . __condition . release ( )
363	def natural_keys ( text ) : def atoi ( text ) : return int ( text ) if text . isdigit ( ) else text return [ atoi ( c ) for c in re . split ( '(\d+)' , text ) ]
7744	def _timeout_cb ( self , method ) : self . _anything_done = True logger . debug ( "_timeout_cb() called for: {0!r}" . format ( method ) ) result = method ( ) rec = method . _pyxmpp_recurring if rec : self . _prepare_pending ( ) return True if rec is None and result is not None : logger . debug ( " auto-recurring, restarting in {0} s" . format ( result ) ) tag = glib . timeout_add ( int ( result * 1000 ) , self . _timeout_cb , method ) self . _timer_sources [ method ] = tag else : self . _timer_sources . pop ( method , None ) self . _prepare_pending ( ) return False
12325	def untokenize ( tokens ) : text = '' previous_line = '' last_row = 0 last_column = - 1 last_non_whitespace_token_type = None for ( token_type , token_string , start , end , line ) in tokens : if TOKENIZE_HAS_ENCODING and token_type == tokenize . ENCODING : continue ( start_row , start_column ) = start ( end_row , end_column ) = end if ( last_non_whitespace_token_type != tokenize . COMMENT and start_row > last_row and previous_line . endswith ( ( '\\\n' , '\\\r\n' , '\\\r' ) ) ) : text += previous_line [ len ( previous_line . rstrip ( ' \t\n\r\\' ) ) : ] if start_row > last_row : last_column = 0 if start_column > last_column : text += line [ last_column : start_column ] text += token_string previous_line = line last_row = end_row last_column = end_column if token_type not in WHITESPACE_TOKENS : last_non_whitespace_token_type = token_type return text
12907	def assert_json_type ( value : JsonValue , expected_type : JsonCheckType ) -> None : def type_name ( t : Union [ JsonCheckType , Type [ None ] ] ) -> str : if t is None : return "None" if isinstance ( t , JList ) : return "list" return t . __name__ if expected_type is None : if value is None : return elif expected_type == float : if isinstance ( value , float ) or isinstance ( value , int ) : return elif expected_type in [ str , int , bool , list , dict ] : if isinstance ( value , expected_type ) : return elif isinstance ( expected_type , JList ) : if isinstance ( value , list ) : for v in value : assert_json_type ( v , expected_type . value_type ) return else : raise TypeError ( "unsupported type" ) raise TypeError ( "wrong JSON type {} != {}" . format ( type_name ( expected_type ) , type_name ( type ( value ) ) ) )
3805	def calculate_P ( self , T , P , method ) : r if method == ELI_HANLEY_DENSE : Vmg = self . Vmg ( T , P ) if hasattr ( self . Vmg , '__call__' ) else self . Vmg Cvgm = self . Cvgm ( T ) if hasattr ( self . Cvgm , '__call__' ) else self . Cvgm kg = eli_hanley_dense ( T , self . MW , self . Tc , self . Vc , self . Zc , self . omega , Cvgm , Vmg ) elif method == CHUNG_DENSE : Vmg = self . Vmg ( T , P ) if hasattr ( self . Vmg , '__call__' ) else self . Vmg Cvgm = self . Cvgm ( T ) if hasattr ( self . Cvgm , '__call__' ) else self . Cvgm mug = self . mug ( T , P ) if hasattr ( self . mug , '__call__' ) else self . mug kg = chung_dense ( T , self . MW , self . Tc , self . Vc , self . omega , Cvgm , Vmg , mug , self . dipole ) elif method == STIEL_THODOS_DENSE : kg = self . T_dependent_property ( T ) Vmg = self . Vmg ( T , P ) if hasattr ( self . Vmg , '__call__' ) else self . Vmg kg = stiel_thodos_dense ( T , self . MW , self . Tc , self . Pc , self . Vc , self . Zc , Vmg , kg ) elif method == COOLPROP : kg = PropsSI ( 'L' , 'T' , T , 'P' , P , self . CASRN ) elif method in self . tabular_data : kg = self . interpolate_P ( T , P , method ) return kg
1546	def get_component_metrics ( component , cluster , env , topology , role ) : all_queries = metric_queries ( ) try : result = get_topology_metrics ( cluster , env , topology , component , [ ] , all_queries , [ 0 , - 1 ] , role ) return result [ "metrics" ] except Exception : Log . debug ( traceback . format_exc ( ) ) raise
2197	def log_part ( self ) : self . cap_stdout . seek ( self . _pos ) text = self . cap_stdout . read ( ) self . _pos = self . cap_stdout . tell ( ) self . parts . append ( text ) self . text = text
1492	def register_timer_task_in_sec ( self , task , second ) : second_in_float = float ( second ) expiration = time . time ( ) + second_in_float heappush ( self . timer_tasks , ( expiration , task ) )
3885	def get_user ( self , user_id ) : try : return self . _user_dict [ user_id ] except KeyError : logger . warning ( 'UserList returning unknown User for UserID %s' , user_id ) return User ( user_id , None , None , None , [ ] , False )
13367	def register_proxy_type ( cls , real_type , proxy_type ) : if distob . engine is None : cls . _initial_proxy_types [ real_type ] = proxy_type elif isinstance ( distob . engine , ObjectHub ) : distob . engine . _runtime_reg_proxy_type ( real_type , proxy_type ) else : distob . engine . _singleeng_reg_proxy_type ( real_type , proxy_type ) pass
6030	def grid_interpolate ( func ) : @ wraps ( func ) def wrapper ( profile , grid , grid_radial_minimum = None , * args , ** kwargs ) : if hasattr ( grid , "interpolator" ) : interpolator = grid . interpolator if grid . interpolator is not None : values = func ( profile , interpolator . interp_grid , grid_radial_minimum , * args , ** kwargs ) if values . ndim == 1 : return interpolator . interpolated_values_from_values ( values = values ) elif values . ndim == 2 : y_values = interpolator . interpolated_values_from_values ( values = values [ : , 0 ] ) x_values = interpolator . interpolated_values_from_values ( values = values [ : , 1 ] ) return np . asarray ( [ y_values , x_values ] ) . T return func ( profile , grid , grid_radial_minimum , * args , ** kwargs ) return wrapper
12920	def reload ( self ) : if len ( self ) == 0 : return [ ] ret = [ ] for obj in self : res = None try : res = obj . reload ( ) except Exception as e : res = e ret . append ( res ) return ret
13040	def process ( self , nemo ) : self . __nemo__ = nemo for annotation in self . __annotations__ : annotation . target . expanded = frozenset ( self . __getinnerreffs__ ( objectId = annotation . target . objectId , subreference = annotation . target . subreference ) )
3345	def parse_if_header_dict ( environ ) : if "wsgidav.conditions.if" in environ : return if "HTTP_IF" not in environ : environ [ "wsgidav.conditions.if" ] = None environ [ "wsgidav.ifLockTokenList" ] = [ ] return iftext = environ [ "HTTP_IF" ] . strip ( ) if not iftext . startswith ( "<" ) : iftext = "<*>" + iftext ifDict = dict ( [ ] ) ifLockList = [ ] resource1 = "*" for ( tmpURLVar , URLVar , _tmpContentVar , contentVar ) in reIfSeparator . findall ( iftext ) : if tmpURLVar != "" : resource1 = URLVar else : listTagContents = [ ] testflag = True for listitem in reIfTagListContents . findall ( contentVar ) : if listitem . upper ( ) != "NOT" : if listitem . startswith ( "[" ) : listTagContents . append ( ( testflag , "entity" , listitem . strip ( '"[]' ) ) ) else : listTagContents . append ( ( testflag , "locktoken" , listitem . strip ( "<>" ) ) ) ifLockList . append ( listitem . strip ( "<>" ) ) testflag = listitem . upper ( ) != "NOT" if resource1 in ifDict : listTag = ifDict [ resource1 ] else : listTag = [ ] ifDict [ resource1 ] = listTag listTag . append ( listTagContents ) environ [ "wsgidav.conditions.if" ] = ifDict environ [ "wsgidav.ifLockTokenList" ] = ifLockList _logger . debug ( "parse_if_header_dict\n{}" . format ( pformat ( ifDict ) ) ) return
4905	def create_course_completion ( self , user_id , payload ) : return self . _post ( urljoin ( self . enterprise_configuration . degreed_base_url , self . global_degreed_config . completion_status_api_path ) , payload , self . COMPLETION_PROVIDER_SCOPE )
9058	def gradient ( self ) : r self . _update_approx ( ) g = self . _ep . lml_derivatives ( self . _X ) ed = exp ( - self . logitdelta ) es = exp ( self . logscale ) grad = dict ( ) grad [ "logitdelta" ] = g [ "delta" ] * ( ed / ( 1 + ed ) ) / ( 1 + ed ) grad [ "logscale" ] = g [ "scale" ] * es grad [ "beta" ] = g [ "mean" ] return grad
10734	def fork ( self , name ) : fork = deepcopy ( self ) self [ name ] = fork return fork
13744	def create_table ( self ) : table = self . conn . create_table ( name = self . get_table_name ( ) , schema = self . get_schema ( ) , read_units = self . get_read_units ( ) , write_units = self . get_write_units ( ) , ) if table . status != 'ACTIVE' : table . refresh ( wait_for_active = True , retry_seconds = 1 ) return table
251	def get_turnover ( positions , transactions , denominator = 'AGB' ) : txn_vol = get_txn_vol ( transactions ) traded_value = txn_vol . txn_volume if denominator == 'AGB' : AGB = positions . drop ( 'cash' , axis = 1 ) . abs ( ) . sum ( axis = 1 ) denom = AGB . rolling ( 2 ) . mean ( ) denom . iloc [ 0 ] = AGB . iloc [ 0 ] / 2 elif denominator == 'portfolio_value' : denom = positions . sum ( axis = 1 ) else : raise ValueError ( "Unexpected value for denominator '{}'. The " "denominator parameter must be either 'AGB'" " or 'portfolio_value'." . format ( denominator ) ) denom . index = denom . index . normalize ( ) turnover = traded_value . div ( denom , axis = 'index' ) turnover = turnover . fillna ( 0 ) return turnover
13571	def configure ( server = None , username = None , password = None , tid = None , auto = False ) : if not server and not username and not password and not tid : if Config . has ( ) : if not yn_prompt ( "Override old configuration" , False ) : return False reset_db ( ) if not server : while True : server = input ( "Server url [https://tmc.mooc.fi/mooc/]: " ) . strip ( ) if len ( server ) == 0 : server = "https://tmc.mooc.fi/mooc/" if not server . endswith ( '/' ) : server += '/' if not ( server . startswith ( "http://" ) or server . startswith ( "https://" ) ) : ret = custom_prompt ( "Server should start with http:// or https://\n" + "R: Retry, H: Assume http://, S: Assume https://" , [ "r" , "h" , "s" ] , "r" ) if ret == "r" : continue if "://" in server : server = server . split ( "://" ) [ 1 ] if ret == "h" : server = "http://" + server elif ret == "s" : server = "https://" + server break print ( "Using URL: '{0}'" . format ( server ) ) while True : if not username : username = input ( "Username: " ) if not password : password = getpass ( "Password: " ) token = b64encode ( bytes ( "{0}:{1}" . format ( username , password ) , encoding = 'utf-8' ) ) . decode ( "utf-8" ) try : api . configure ( url = server , token = token , test = True ) except APIError as e : print ( e ) if auto is False and yn_prompt ( "Retry authentication" ) : username = password = None continue return False break if tid : select ( course = True , tid = tid , auto = auto ) else : select ( course = True )
8506	def _default ( self ) : try : iter ( self . default ) except TypeError : return repr ( self . default ) for v in self . default : if isinstance ( v , Unparseable ) : default = self . _default_value_only ( ) if default : return default return ', ' . join ( str ( v ) for v in self . default )
3122	def _verify_signature ( message , signature , certs ) : for pem in certs : verifier = Verifier . from_string ( pem , is_x509_cert = True ) if verifier . verify ( message , signature ) : return raise AppIdentityError ( 'Invalid token signature' )
9794	def _ignore_path ( cls , path , ignore_list = None , white_list = None ) : ignore_list = ignore_list or [ ] white_list = white_list or [ ] return ( cls . _matches_patterns ( path , ignore_list ) and not cls . _matches_patterns ( path , white_list ) )
4879	def get_paginated_response ( data , request ) : url = urlparse ( request . build_absolute_uri ( ) ) . _replace ( query = None ) . geturl ( ) next_page = None previous_page = None if data [ 'next' ] : next_page = "{base_url}?{query_parameters}" . format ( base_url = url , query_parameters = urlparse ( data [ 'next' ] ) . query , ) next_page = next_page . rstrip ( '?' ) if data [ 'previous' ] : previous_page = "{base_url}?{query_parameters}" . format ( base_url = url , query_parameters = urlparse ( data [ 'previous' ] or "" ) . query , ) previous_page = previous_page . rstrip ( '?' ) return Response ( OrderedDict ( [ ( 'count' , data [ 'count' ] ) , ( 'next' , next_page ) , ( 'previous' , previous_page ) , ( 'results' , data [ 'results' ] ) ] ) )
10631	def clear ( self ) : self . _compound_mfrs = self . _compound_mfrs * 0.0 self . _P = 1.0 self . _T = 25.0 self . _H = 0.0
6191	def volume ( self ) : return ( self . x2 - self . x1 ) * ( self . y2 - self . y1 ) * ( self . z2 - self . z1 )
12282	def get_resource ( self , p ) : for r in self . package [ 'resources' ] : if r [ 'relativepath' ] == p : r [ 'localfullpath' ] = os . path . join ( self . rootdir , p ) return r raise Exception ( "Invalid path" )
480	def id_to_word ( self , word_id ) : if word_id >= len ( self . reverse_vocab ) : return self . reverse_vocab [ self . unk_id ] else : return self . reverse_vocab [ word_id ]
328	def extract_interesting_date_ranges ( returns ) : returns_dupe = returns . copy ( ) returns_dupe . index = returns_dupe . index . map ( pd . Timestamp ) ranges = OrderedDict ( ) for name , ( start , end ) in PERIODS . items ( ) : try : period = returns_dupe . loc [ start : end ] if len ( period ) == 0 : continue ranges [ name ] = period except BaseException : continue return ranges
5310	def translate_rgb_to_ansi_code ( red , green , blue , offset , colormode ) : if colormode == terminal . NO_COLORS : return '' , '' if colormode == terminal . ANSI_8_COLORS or colormode == terminal . ANSI_16_COLORS : color_code = ansi . rgb_to_ansi16 ( red , green , blue ) start_code = ansi . ANSI_ESCAPE_CODE . format ( code = color_code + offset - ansi . FOREGROUND_COLOR_OFFSET ) end_code = ansi . ANSI_ESCAPE_CODE . format ( code = offset + ansi . COLOR_CLOSE_OFFSET ) return start_code , end_code if colormode == terminal . ANSI_256_COLORS : color_code = ansi . rgb_to_ansi256 ( red , green , blue ) start_code = ansi . ANSI_ESCAPE_CODE . format ( code = '{base};5;{code}' . format ( base = 8 + offset , code = color_code ) ) end_code = ansi . ANSI_ESCAPE_CODE . format ( code = offset + ansi . COLOR_CLOSE_OFFSET ) return start_code , end_code if colormode == terminal . TRUE_COLORS : start_code = ansi . ANSI_ESCAPE_CODE . format ( code = '{base};2;{red};{green};{blue}' . format ( base = 8 + offset , red = red , green = green , blue = blue ) ) end_code = ansi . ANSI_ESCAPE_CODE . format ( code = offset + ansi . COLOR_CLOSE_OFFSET ) return start_code , end_code raise ColorfulError ( 'invalid color mode "{0}"' . format ( colormode ) )
10850	def generate_sphere ( radius ) : rint = np . ceil ( radius ) . astype ( 'int' ) t = np . arange ( - rint , rint + 1 , 1 ) x , y , z = np . meshgrid ( t , t , t , indexing = 'ij' ) r = np . sqrt ( x * x + y * y + z * z ) sphere = r < radius return sphere
9501	def _disassemble ( self , lineno_width = 3 , mark_as_current = False ) : fields = [ ] if lineno_width : if self . starts_line is not None : lineno_fmt = "%%%dd" % lineno_width fields . append ( lineno_fmt % self . starts_line ) else : fields . append ( ' ' * lineno_width ) if mark_as_current : fields . append ( ' ) else : fields . append ( ' ' ) if self . is_jump_target : fields . append ( '>>' ) else : fields . append ( ' ' ) fields . append ( repr ( self . offset ) . rjust ( 4 ) ) fields . append ( self . opname . ljust ( 20 ) ) if self . arg is not None : fields . append ( repr ( self . arg ) . rjust ( 5 ) ) if self . argrepr : fields . append ( '(' + self . argrepr + ')' ) return ' ' . join ( fields ) . rstrip ( )
5874	def is_valid_filename ( self , image_node ) : src = self . parser . getAttribute ( image_node , attr = 'src' ) if not src : return False if self . badimages_names_re . search ( src ) : return False return True
11205	def name_from_string ( self , tzname_str ) : if not tzname_str . startswith ( '@' ) : return tzname_str name_splt = tzname_str . split ( ',-' ) try : offset = int ( name_splt [ 1 ] ) except : raise ValueError ( "Malformed timezone string." ) return self . load_name ( offset )
341	def google2_log_prefix ( level , timestamp = None , file_and_line = None ) : global _level_names now = timestamp or _time . time ( ) now_tuple = _time . localtime ( now ) now_microsecond = int ( 1e6 * ( now % 1.0 ) ) ( filename , line ) = file_and_line or _GetFileAndLine ( ) basename = _os . path . basename ( filename ) severity = 'I' if level in _level_names : severity = _level_names [ level ] [ 0 ] s = '%c%02d%02d %02d: %02d: %02d.%06d %5d %s: %d] ' % ( severity , now_tuple [ 1 ] , now_tuple [ 2 ] , now_tuple [ 3 ] , now_tuple [ 4 ] , now_tuple [ 5 ] , now_microsecond , _get_thread_id ( ) , basename , line ) return s
4390	def adsSyncReadStateReqEx ( port , address ) : sync_read_state_request = _adsDLL . AdsSyncReadStateReqEx ams_address_pointer = ctypes . pointer ( address . amsAddrStruct ( ) ) ads_state = ctypes . c_int ( ) ads_state_pointer = ctypes . pointer ( ads_state ) device_state = ctypes . c_int ( ) device_state_pointer = ctypes . pointer ( device_state ) error_code = sync_read_state_request ( port , ams_address_pointer , ads_state_pointer , device_state_pointer ) if error_code : raise ADSError ( error_code ) return ( ads_state . value , device_state . value )
656	def averageOnTimePerTimestep ( vectors , numSamples = None ) : if vectors . ndim == 1 : vectors . shape = ( - 1 , 1 ) numTimeSteps = len ( vectors ) numElements = len ( vectors [ 0 ] ) if numSamples is not None : import pdb pdb . set_trace ( ) countOn = numpy . random . randint ( 0 , numElements , numSamples ) vectors = vectors [ : , countOn ] durations = numpy . zeros ( vectors . shape , dtype = 'int32' ) for col in xrange ( vectors . shape [ 1 ] ) : _fillInOnTimes ( vectors [ : , col ] , durations [ : , col ] ) sums = vectors . sum ( axis = 1 ) sums . clip ( min = 1 , max = numpy . inf , out = sums ) avgDurations = durations . sum ( axis = 1 , dtype = 'float64' ) / sums avgOnTime = avgDurations . sum ( ) / ( avgDurations > 0 ) . sum ( ) freqCounts = _accumulateFrequencyCounts ( avgDurations ) return ( avgOnTime , freqCounts )
12993	def level_chunker ( text , getreffs , level = 1 ) : references = getreffs ( level = level ) return [ ( ref . split ( ":" ) [ - 1 ] , ref . split ( ":" ) [ - 1 ] ) for ref in references ]
10590	def report ( self , format = ReportFormat . printout , output_path = None ) : rpt = GlsRpt ( self , output_path ) return rpt . render ( format )
1431	def getInstancePid ( topology_info , instance_id ) : try : http_client = tornado . httpclient . AsyncHTTPClient ( ) endpoint = utils . make_shell_endpoint ( topology_info , instance_id ) url = "%s/pid/%s" % ( endpoint , instance_id ) Log . debug ( "HTTP call for url: %s" , url ) response = yield http_client . fetch ( url ) raise tornado . gen . Return ( response . body ) except tornado . httpclient . HTTPError as e : raise Exception ( str ( e ) )
1494	def _trigger_timers ( self ) : current = time . time ( ) while len ( self . timer_tasks ) > 0 and ( self . timer_tasks [ 0 ] [ 0 ] - current <= 0 ) : task = heappop ( self . timer_tasks ) [ 1 ] task ( )
13189	async def _upload_to_mongodb ( collection , jsonld ) : document = { 'data' : jsonld } query = { 'data.reportNumber' : jsonld [ 'reportNumber' ] } await collection . update ( query , document , upsert = True , multi = False )
7997	def set_peer_authenticated ( self , peer , restart_stream = False ) : with self . lock : self . peer_authenticated = True self . peer = peer if restart_stream : self . _restart_stream ( ) self . event ( AuthenticatedEvent ( self . peer ) )
1673	def PrintCategories ( ) : sys . stderr . write ( '' . join ( ' %s\n' % cat for cat in _ERROR_CATEGORIES ) ) sys . exit ( 0 )
2108	def login ( username , password , scope , client_id , client_secret , verbose ) : if not supports_oauth ( ) : raise exc . TowerCLIError ( 'This version of Tower does not support OAuth2.0. Set credentials using tower-cli config.' ) req = collections . namedtuple ( 'req' , 'headers' ) ( { } ) if client_id and client_secret : HTTPBasicAuth ( client_id , client_secret ) ( req ) req . headers [ 'Content-Type' ] = 'application/x-www-form-urlencoded' r = client . post ( '/o/token/' , data = { "grant_type" : "password" , "username" : username , "password" : password , "scope" : scope } , headers = req . headers ) elif client_id : req . headers [ 'Content-Type' ] = 'application/x-www-form-urlencoded' r = client . post ( '/o/token/' , data = { "grant_type" : "password" , "username" : username , "password" : password , "client_id" : client_id , "scope" : scope } , headers = req . headers ) else : HTTPBasicAuth ( username , password ) ( req ) r = client . post ( '/users/{}/personal_tokens/' . format ( username ) , data = { "description" : "Tower CLI" , "application" : None , "scope" : scope } , headers = req . headers ) if r . ok : result = r . json ( ) result . pop ( 'summary_fields' , None ) result . pop ( 'related' , None ) if client_id : token = result . pop ( 'access_token' , None ) else : token = result . pop ( 'token' , None ) if settings . verbose : result [ 'token' ] = token secho ( json . dumps ( result , indent = 1 ) , fg = 'blue' , bold = True ) config . main ( [ 'oauth_token' , token , '--scope=user' ] )
4464	def load_jam_audio ( jam_in , audio_file , validate = True , strict = True , fmt = 'auto' , ** kwargs ) : if isinstance ( jam_in , jams . JAMS ) : jam = jam_in else : jam = jams . load ( jam_in , validate = validate , strict = strict , fmt = fmt ) y , sr = librosa . load ( audio_file , ** kwargs ) if jam . file_metadata . duration is None : jam . file_metadata . duration = librosa . get_duration ( y = y , sr = sr ) return jam_pack ( jam , _audio = dict ( y = y , sr = sr ) )
4264	def build ( source , destination , debug , verbose , force , config , theme , title , ncpu ) : level = ( ( debug and logging . DEBUG ) or ( verbose and logging . INFO ) or logging . WARNING ) init_logging ( __name__ , level = level ) logger = logging . getLogger ( __name__ ) if not os . path . isfile ( config ) : logger . error ( "Settings file not found: %s" , config ) sys . exit ( 1 ) start_time = time . time ( ) settings = read_settings ( config ) for key in ( 'source' , 'destination' , 'theme' ) : arg = locals ( ) [ key ] if arg is not None : settings [ key ] = os . path . abspath ( arg ) logger . info ( "%12s : %s" , key . capitalize ( ) , settings [ key ] ) if not settings [ 'source' ] or not os . path . isdir ( settings [ 'source' ] ) : logger . error ( "Input directory not found: %s" , settings [ 'source' ] ) sys . exit ( 1 ) relative_check = True try : relative_check = os . path . relpath ( settings [ 'destination' ] , settings [ 'source' ] ) . startswith ( '..' ) except ValueError : pass if not relative_check : logger . error ( "Output directory should be outside of the input " "directory." ) sys . exit ( 1 ) if title : settings [ 'title' ] = title locale . setlocale ( locale . LC_ALL , settings [ 'locale' ] ) init_plugins ( settings ) gal = Gallery ( settings , ncpu = ncpu ) gal . build ( force = force ) for src , dst in settings [ 'files_to_copy' ] : src = os . path . join ( settings [ 'source' ] , src ) dst = os . path . join ( settings [ 'destination' ] , dst ) logger . debug ( 'Copy %s to %s' , src , dst ) copy ( src , dst , symlink = settings [ 'orig_link' ] , rellink = settings [ 'rel_link' ] ) stats = gal . stats def format_stats ( _type ) : opt = [ "{} {}" . format ( stats [ _type + '_' + subtype ] , subtype ) for subtype in ( 'skipped' , 'failed' ) if stats [ _type + '_' + subtype ] > 0 ] opt = ' ({})' . format ( ', ' . join ( opt ) ) if opt else '' return '{} {}s{}' . format ( stats [ _type ] , _type , opt ) print ( 'Done.\nProcessed {} and {} in {:.2f} seconds.' . format ( format_stats ( 'image' ) , format_stats ( 'video' ) , time . time ( ) - start_time ) )
9251	def generate_log_for_all_tags ( self ) : if self . options . verbose : print ( "Generating log..." ) self . issues2 = copy . deepcopy ( self . issues ) log1 = "" if self . options . with_unreleased : log1 = self . generate_unreleased_section ( ) log = "" for index in range ( len ( self . filtered_tags ) - 1 ) : log += self . do_generate_log_for_all_tags_part1 ( log , index ) if self . options . tag_separator and log1 : log = log1 + self . options . tag_separator + log else : log = log1 + log if len ( self . filtered_tags ) != 0 : log += self . do_generate_log_for_all_tags_part2 ( log ) return log
3203	def delete ( self , store_id , cart_id , line_id ) : self . store_id = store_id self . cart_id = cart_id self . line_id = line_id return self . _mc_client . _delete ( url = self . _build_path ( store_id , 'carts' , cart_id , 'lines' , line_id ) )
5609	def tiles_to_affine_shape ( tiles ) : if not tiles : raise TypeError ( "no tiles provided" ) pixel_size = tiles [ 0 ] . pixel_x_size left , bottom , right , top = ( min ( [ t . left for t in tiles ] ) , min ( [ t . bottom for t in tiles ] ) , max ( [ t . right for t in tiles ] ) , max ( [ t . top for t in tiles ] ) , ) return ( Affine ( pixel_size , 0 , left , 0 , - pixel_size , top ) , Shape ( width = int ( round ( ( right - left ) / pixel_size , 0 ) ) , height = int ( round ( ( top - bottom ) / pixel_size , 0 ) ) , ) )
10682	def H_mag ( self , T ) : tau = T / self . Tc_mag if tau <= 1.0 : h = ( - self . _A_mag / tau + self . _B_mag * ( tau ** 3 / 2 + tau ** 9 / 15 + tau ** 15 / 40 ) ) / self . _D_mag else : h = - ( tau ** - 5 / 2 + tau ** - 15 / 21 + tau ** - 25 / 60 ) / self . _D_mag return R * T * math . log ( self . beta0_mag + 1 ) * h
8746	def get_floatingips_count ( context , filters = None ) : LOG . info ( 'get_floatingips_count for tenant %s filters %s' % ( context . tenant_id , filters ) ) if filters is None : filters = { } filters [ '_deallocated' ] = False filters [ 'address_type' ] = ip_types . FLOATING count = db_api . ip_address_count_all ( context , filters ) LOG . info ( 'Found %s floating ips for tenant %s' % ( count , context . tenant_id ) ) return count
8865	def complete ( code , line , column , path , encoding , prefix ) : ret_val = [ ] try : script = jedi . Script ( code , line + 1 , column , path , encoding ) completions = script . completions ( ) print ( 'completions: %r' % completions ) except jedi . NotFoundError : completions = [ ] for completion in completions : ret_val . append ( { 'name' : completion . name , 'icon' : icon_from_typename ( completion . name , completion . type ) , 'tooltip' : completion . description } ) return ret_val
4671	def unlock ( self , pwd ) : if self . store . is_encrypted ( ) : return self . store . unlock ( pwd )
8678	def put ( self , name , value = None , modify = False , metadata = None , description = '' , encrypt = True , lock = False , key_type = 'secret' , add = False ) : def assert_key_is_unlocked ( existing_key ) : if existing_key and existing_key . get ( 'lock' ) : raise GhostError ( 'Key `{0}` is locked and therefore cannot be modified. ' 'Unlock the key and try again' . format ( name ) ) def assert_value_provided_for_new_key ( value , existing_key ) : if not value and not existing_key . get ( 'value' ) : raise GhostError ( 'You must provide a value for new keys' ) self . _assert_valid_stash ( ) self . _validate_key_schema ( value , key_type ) if value and encrypt and not isinstance ( value , dict ) : raise GhostError ( 'Value must be of type dict' ) key = self . _handle_existing_key ( name , modify or add ) assert_key_is_unlocked ( key ) assert_value_provided_for_new_key ( value , key ) new_key = dict ( name = name , lock = lock ) if value : if add : value = self . _update_existing_key ( key , value ) new_key [ 'value' ] = self . _encrypt ( value ) if encrypt else value else : new_key [ 'value' ] = key . get ( 'value' ) new_key [ 'description' ] = description or key . get ( 'description' ) new_key [ 'created_at' ] = key . get ( 'created_at' ) or _get_current_time ( ) new_key [ 'modified_at' ] = _get_current_time ( ) new_key [ 'metadata' ] = metadata or key . get ( 'metadata' ) new_key [ 'uid' ] = key . get ( 'uid' ) or str ( uuid . uuid4 ( ) ) new_key [ 'type' ] = key . get ( 'type' ) or key_type key_id = self . _storage . put ( new_key ) audit ( storage = self . _storage . db_path , action = 'MODIFY' if ( modify or add ) else 'PUT' , message = json . dumps ( dict ( key_name = new_key [ 'name' ] , value = 'HIDDEN' , description = new_key [ 'description' ] , uid = new_key [ 'uid' ] , metadata = json . dumps ( new_key [ 'metadata' ] ) , lock = new_key [ 'lock' ] , type = new_key [ 'type' ] ) ) ) return key_id
2117	def convert ( self , value , param , ctx ) : choice = super ( MappedChoice , self ) . convert ( value , param , ctx ) ix = self . choices . index ( choice ) return self . actual_choices [ ix ]
9591	def set_window_size ( self , width , height , window_handle = 'current' ) : self . _execute ( Command . SET_WINDOW_SIZE , { 'width' : int ( width ) , 'height' : int ( height ) , 'window_handle' : window_handle } )
280	def plot_annual_returns ( returns , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) x_axis_formatter = FuncFormatter ( utils . percentage ) ax . xaxis . set_major_formatter ( FuncFormatter ( x_axis_formatter ) ) ax . tick_params ( axis = 'x' , which = 'major' ) ann_ret_df = pd . DataFrame ( ep . aggregate_returns ( returns , 'yearly' ) ) ax . axvline ( 100 * ann_ret_df . values . mean ( ) , color = 'steelblue' , linestyle = '--' , lw = 4 , alpha = 0.7 ) ( 100 * ann_ret_df . sort_index ( ascending = False ) ) . plot ( ax = ax , kind = 'barh' , alpha = 0.70 , ** kwargs ) ax . axvline ( 0.0 , color = 'black' , linestyle = '-' , lw = 3 ) ax . set_ylabel ( 'Year' ) ax . set_xlabel ( 'Returns' ) ax . set_title ( "Annual returns" ) ax . legend ( [ 'Mean' ] , frameon = True , framealpha = 0.5 ) return ax
7387	def get_idx ( self , node ) : group = self . find_node_group_membership ( node ) return self . nodes [ group ] . index ( node )
2880	def get_event_definition ( self ) : messageEventDefinition = first ( self . xpath ( './/bpmn:messageEventDefinition' ) ) if messageEventDefinition is not None : return self . get_message_event_definition ( messageEventDefinition ) timerEventDefinition = first ( self . xpath ( './/bpmn:timerEventDefinition' ) ) if timerEventDefinition is not None : return self . get_timer_event_definition ( timerEventDefinition ) raise NotImplementedError ( 'Unsupported Intermediate Catch Event: %r' , ET . tostring ( self . node ) )
178	def subdivide ( self , points_per_edge ) : if len ( self . coords ) <= 1 or points_per_edge < 1 : return self . deepcopy ( ) coords = interpolate_points ( self . coords , nb_steps = points_per_edge , closed = False ) return self . deepcopy ( coords = coords )
3498	def find_carbon_sources ( model ) : try : model . slim_optimize ( error_value = None ) except OptimizationError : return [ ] reactions = model . reactions . get_by_any ( list ( model . medium ) ) reactions_fluxes = [ ( rxn , total_components_flux ( rxn . flux , reaction_elements ( rxn ) , consumption = True ) ) for rxn in reactions ] return [ rxn for rxn , c_flux in reactions_fluxes if c_flux > 0 ]
3032	def credentials_from_code ( client_id , client_secret , scope , code , redirect_uri = 'postmessage' , http = None , user_agent = None , token_uri = oauth2client . GOOGLE_TOKEN_URI , auth_uri = oauth2client . GOOGLE_AUTH_URI , revoke_uri = oauth2client . GOOGLE_REVOKE_URI , device_uri = oauth2client . GOOGLE_DEVICE_URI , token_info_uri = oauth2client . GOOGLE_TOKEN_INFO_URI , pkce = False , code_verifier = None ) : flow = OAuth2WebServerFlow ( client_id , client_secret , scope , redirect_uri = redirect_uri , user_agent = user_agent , auth_uri = auth_uri , token_uri = token_uri , revoke_uri = revoke_uri , device_uri = device_uri , token_info_uri = token_info_uri , pkce = pkce , code_verifier = code_verifier ) credentials = flow . step2_exchange ( code , http = http ) return credentials
5894	def render ( self , name , value , attrs = { } ) : if value is None : value = '' final_attrs = self . build_attrs ( attrs , name = name ) quill_app = apps . get_app_config ( 'quill' ) quill_config = getattr ( quill_app , self . config ) return mark_safe ( render_to_string ( quill_config [ 'template' ] , { 'final_attrs' : flatatt ( final_attrs ) , 'value' : value , 'id' : final_attrs [ 'id' ] , 'config' : self . config , } ) )
5324	def read_file_from_uri ( self , uri ) : logger . debug ( "Reading %s" % ( uri ) ) self . __check_looks_like_uri ( uri ) try : req = urllib . request . Request ( uri ) req . add_header ( 'Authorization' , 'token %s' % self . token ) r = urllib . request . urlopen ( req ) except urllib . error . HTTPError as err : if err . code == 404 : raise GithubFileNotFound ( 'File %s is not available. Check the URL to ensure it really exists' % uri ) else : raise return r . read ( ) . decode ( "utf-8" )
3115	def _get_flow_for_token ( csrf_token , request ) : flow_pickle = request . session . get ( _FLOW_KEY . format ( csrf_token ) , None ) return None if flow_pickle is None else jsonpickle . decode ( flow_pickle )
3328	def get_lock ( self , token , key = None ) : assert key in ( None , "type" , "scope" , "depth" , "owner" , "root" , "timeout" , "principal" , "token" , ) lock = self . storage . get ( token ) if key is None or lock is None : return lock return lock [ key ]
12324	def save ( self ) : if self . code : raise HolviError ( "Orders cannot be updated" ) send_json = self . to_holvi_dict ( ) send_json . update ( { 'pool' : self . api . connection . pool } ) url = six . u ( self . api . base_url + "order/" ) stat = self . api . connection . make_post ( url , send_json ) code = stat [ "details_uri" ] . split ( "/" ) [ - 2 ] return ( stat [ "checkout_uri" ] , self . api . get_order ( code ) )
7498	def shuffle_cols ( seqarr , newarr , cols ) : for idx in xrange ( cols . shape [ 0 ] ) : newarr [ : , idx ] = seqarr [ : , cols [ idx ] ] return newarr
1226	def tf_discounted_cumulative_reward ( self , terminal , reward , discount = None , final_reward = 0.0 , horizon = 0 ) : if discount is None : discount = self . discount def cumulate ( cumulative , reward_terminal_horizon_subtract ) : rew , is_terminal , is_over_horizon , sub = reward_terminal_horizon_subtract return tf . where ( condition = is_terminal , x = rew , y = tf . where ( condition = is_over_horizon , x = ( rew + cumulative * discount - sub ) , y = ( rew + cumulative * discount ) ) ) def len_ ( cumulative , term ) : return tf . where ( condition = term , x = tf . ones ( shape = ( ) , dtype = tf . int32 ) , y = cumulative + 1 ) reward = tf . reverse ( tensor = reward , axis = ( 0 , ) ) terminal = tf . reverse ( tensor = terminal , axis = ( 0 , ) ) lengths = tf . scan ( fn = len_ , elems = terminal , initializer = 0 ) off_horizon = tf . greater ( lengths , tf . fill ( dims = tf . shape ( lengths ) , value = horizon ) ) if horizon > 0 : horizon_subtractions = tf . map_fn ( lambda x : ( discount ** horizon ) * x , reward , dtype = tf . float32 ) horizon_subtractions = tf . concat ( [ np . zeros ( shape = ( horizon , ) ) , horizon_subtractions ] , axis = 0 ) horizon_subtractions = tf . slice ( horizon_subtractions , begin = ( 0 , ) , size = tf . shape ( reward ) ) else : horizon_subtractions = tf . zeros ( shape = tf . shape ( reward ) ) reward = tf . scan ( fn = cumulate , elems = ( reward , terminal , off_horizon , horizon_subtractions ) , initializer = final_reward if horizon != 1 else 0.0 ) return tf . reverse ( tensor = reward , axis = ( 0 , ) )
6049	def set_defaults ( key ) : def decorator ( func ) : @ functools . wraps ( func ) def wrapper ( phase , new_value ) : new_value = new_value or [ ] for item in new_value : galaxy = new_value [ item ] if isinstance ( item , str ) else item galaxy . redshift = galaxy . redshift or conf . instance . general . get ( "redshift" , key , float ) return func ( phase , new_value ) return wrapper return decorator
6913	def generate_sinusoidal_lightcurve ( times , mags = None , errs = None , paramdists = { 'period' : sps . uniform ( loc = 0.04 , scale = 500.0 ) , 'fourierorder' : [ 2 , 10 ] , 'amplitude' : sps . uniform ( loc = 0.1 , scale = 0.9 ) , 'phioffset' : 0.0 , } , magsarefluxes = False ) : if mags is None : mags = np . full_like ( times , 0.0 ) if errs is None : errs = np . full_like ( times , 0.0 ) epoch = npr . random ( ) * ( times . max ( ) - times . min ( ) ) + times . min ( ) period = paramdists [ 'period' ] . rvs ( size = 1 ) fourierorder = npr . randint ( paramdists [ 'fourierorder' ] [ 0 ] , high = paramdists [ 'fourierorder' ] [ 1 ] ) amplitude = paramdists [ 'amplitude' ] . rvs ( size = 1 ) if magsarefluxes and amplitude < 0.0 : amplitude = - amplitude elif not magsarefluxes and amplitude > 0.0 : amplitude = - amplitude ampcomps = [ abs ( amplitude / 2.0 ) / float ( x ) for x in range ( 1 , fourierorder + 1 ) ] phacomps = [ paramdists [ 'phioffset' ] * float ( x ) for x in range ( 1 , fourierorder + 1 ) ] modelmags , phase , ptimes , pmags , perrs = sinusoidal . sine_series_sum ( [ period , epoch , ampcomps , phacomps ] , times , mags , errs ) timeind = np . argsort ( ptimes ) mtimes = ptimes [ timeind ] mmags = modelmags [ timeind ] merrs = perrs [ timeind ] mphase = phase [ timeind ] modeldict = { 'vartype' : 'sinusoidal' , 'params' : { x : y for x , y in zip ( [ 'period' , 'epoch' , 'amplitude' , 'fourierorder' , 'fourieramps' , 'fourierphases' ] , [ period , epoch , amplitude , fourierorder , ampcomps , phacomps ] ) } , 'times' : mtimes , 'mags' : mmags , 'errs' : merrs , 'phase' : mphase , 'varperiod' : period , 'varamplitude' : amplitude } return modeldict
13106	def add_tag ( self , tag ) : self . tags = list ( set ( self . tags or [ ] ) | set ( [ tag ] ) )
1276	def tf_initialize ( self , x_init , base_value , target_value , estimated_improvement ) : self . base_value = base_value if estimated_improvement is None : estimated_improvement = tf . abs ( x = base_value ) first_step = super ( LineSearch , self ) . tf_initialize ( x_init ) improvement = tf . divide ( x = ( target_value - self . base_value ) , y = tf . maximum ( x = estimated_improvement , y = util . epsilon ) ) last_improvement = improvement - 1.0 if self . mode == 'linear' : deltas = [ - t * self . parameter for t in x_init ] self . estimated_incr = - estimated_improvement * self . parameter elif self . mode == 'exponential' : deltas = [ - t * self . parameter for t in x_init ] return first_step + ( deltas , improvement , last_improvement , estimated_improvement )
9449	def schedule_hangup ( self , call_params ) : path = '/' + self . api_version + '/ScheduleHangup/' method = 'POST' return self . request ( path , method , call_params )
9642	def set_trace ( context ) : try : import ipdb as pdb except ImportError : import pdb print ( "For best results, pip install ipdb." ) print ( "Variables that are available in the current context:" ) render = lambda s : template . Template ( s ) . render ( context ) availables = get_variables ( context ) pprint ( availables ) print ( 'Type `availables` to show this list.' ) print ( 'Type <variable_name> to access one.' ) print ( 'Use render("template string") to test template rendering' ) for var in availables : locals ( ) [ var ] = context [ var ] pdb . set_trace ( ) return ''
2803	def convert_slice ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting slice ...' ) if len ( params [ 'axes' ] ) > 1 : raise AssertionError ( 'Cannot convert slice by multiple dimensions' ) if params [ 'axes' ] [ 0 ] not in [ 0 , 1 , 2 , 3 ] : raise AssertionError ( 'Slice by dimension more than 3 or less than 0 is not supported' ) def target_layer ( x , axis = int ( params [ 'axes' ] [ 0 ] ) , start = int ( params [ 'starts' ] [ 0 ] ) , end = int ( params [ 'ends' ] [ 0 ] ) ) : if axis == 0 : return x [ start : end ] elif axis == 1 : return x [ : , start : end ] elif axis == 2 : return x [ : , : , start : end ] elif axis == 3 : return x [ : , : , : , start : end ] lambda_layer = keras . layers . Lambda ( target_layer ) layers [ scope_name ] = lambda_layer ( layers [ inputs [ 0 ] ] )
6150	def fir_remez_lpf ( f_pass , f_stop , d_pass , d_stop , fs = 1.0 , N_bump = 5 ) : n , ff , aa , wts = lowpass_order ( f_pass , f_stop , d_pass , d_stop , fsamp = fs ) N_taps = n N_taps += N_bump b = signal . remez ( N_taps , ff , aa [ 0 : : 2 ] , wts , Hz = 2 ) print ( 'Remez filter taps = %d.' % N_taps ) return b
1686	def Split ( self ) : googlename = self . RepositoryName ( ) project , rest = os . path . split ( googlename ) return ( project , ) + os . path . splitext ( rest )
3503	def add_loopless ( model , zero_cutoff = None ) : zero_cutoff = normalize_cutoff ( model , zero_cutoff ) internal = [ i for i , r in enumerate ( model . reactions ) if not r . boundary ] s_int = create_stoichiometric_matrix ( model ) [ : , numpy . array ( internal ) ] n_int = nullspace ( s_int ) . T max_bound = max ( max ( abs ( b ) for b in r . bounds ) for r in model . reactions ) prob = model . problem to_add = [ ] for i in internal : rxn = model . reactions [ i ] indicator = prob . Variable ( "indicator_" + rxn . id , type = "binary" ) on_off_constraint = prob . Constraint ( rxn . flux_expression - max_bound * indicator , lb = - max_bound , ub = 0 , name = "on_off_" + rxn . id ) delta_g = prob . Variable ( "delta_g_" + rxn . id ) delta_g_range = prob . Constraint ( delta_g + ( max_bound + 1 ) * indicator , lb = 1 , ub = max_bound , name = "delta_g_range_" + rxn . id ) to_add . extend ( [ indicator , on_off_constraint , delta_g , delta_g_range ] ) model . add_cons_vars ( to_add ) for i , row in enumerate ( n_int ) : name = "nullspace_constraint_" + str ( i ) nullspace_constraint = prob . Constraint ( Zero , lb = 0 , ub = 0 , name = name ) model . add_cons_vars ( [ nullspace_constraint ] ) coefs = { model . variables [ "delta_g_" + model . reactions [ ridx ] . id ] : row [ i ] for i , ridx in enumerate ( internal ) if abs ( row [ i ] ) > zero_cutoff } model . constraints [ name ] . set_linear_coefficients ( coefs )
2478	def reset ( self ) : self . reset_creation_info ( ) self . reset_document ( ) self . reset_package ( ) self . reset_file_stat ( ) self . reset_reviews ( ) self . reset_annotations ( ) self . reset_extr_lics ( )
5083	def get_learner_data_records ( self , enterprise_enrollment , completed_date = None , grade = None , is_passing = False ) : completed_timestamp = None course_completed = False if completed_date is not None : completed_timestamp = parse_datetime_to_epoch_millis ( completed_date ) course_completed = is_passing sapsf_user_id = enterprise_enrollment . enterprise_customer_user . get_remote_id ( ) if sapsf_user_id is not None : SapSuccessFactorsLearnerDataTransmissionAudit = apps . get_model ( 'sap_success_factors' , 'SapSuccessFactorsLearnerDataTransmissionAudit' ) return [ SapSuccessFactorsLearnerDataTransmissionAudit ( enterprise_course_enrollment_id = enterprise_enrollment . id , sapsf_user_id = sapsf_user_id , course_id = parse_course_key ( enterprise_enrollment . course_id ) , course_completed = course_completed , completed_timestamp = completed_timestamp , grade = grade , ) , SapSuccessFactorsLearnerDataTransmissionAudit ( enterprise_course_enrollment_id = enterprise_enrollment . id , sapsf_user_id = sapsf_user_id , course_id = enterprise_enrollment . course_id , course_completed = course_completed , completed_timestamp = completed_timestamp , grade = grade , ) , ] else : LOGGER . debug ( 'No learner data was sent for user [%s] because an SAP SuccessFactors user ID could not be found.' , enterprise_enrollment . enterprise_customer_user . username )
4176	def window_gaussian ( N , alpha = 2.5 ) : r t = linspace ( - ( N - 1 ) / 2. , ( N - 1 ) / 2. , N ) w = exp ( - 0.5 * ( alpha * t / ( N / 2. ) ) ** 2. ) return w
5824	def add_descriptor ( self , descriptor , role = 'ignore' , group_by_key = False ) : descriptor . validate ( ) if descriptor . key in self . configuration [ "roles" ] : raise ValueError ( "Cannot add a descriptor with the same name twice" ) self . configuration [ 'descriptors' ] . append ( descriptor . as_dict ( ) ) self . configuration [ "roles" ] [ descriptor . key ] = role if group_by_key : self . configuration [ "group_by" ] . append ( descriptor . key )
10955	def get_update_io_tiles ( self , params , values ) : otile = self . get_update_tile ( params , values ) if otile is None : return [ None ] * 3 ptile = self . get_padding_size ( otile ) or util . Tile ( 0 , dim = otile . dim ) otile = util . Tile . intersection ( otile , self . oshape ) if ( otile . shape <= 0 ) . any ( ) : raise UpdateError ( "update triggered invalid tile size" ) if ( ptile . shape < 0 ) . any ( ) or ( ptile . shape > self . oshape . shape ) . any ( ) : raise UpdateError ( "update triggered invalid padding tile size" ) outer = otile . pad ( ( ptile . shape + 1 ) // 2 ) inner , outer = outer . reflect_overhang ( self . oshape ) iotile = inner . translate ( - outer . l ) outer = util . Tile . intersection ( outer , self . oshape ) inner = util . Tile . intersection ( inner , self . oshape ) return outer , inner , iotile
7673	def trim ( self , start_time , end_time , strict = False ) : if self . file_metadata . duration is None : raise JamsError ( 'Duration must be set (jam.file_metadata.duration) before ' 'trimming can be performed.' ) if not ( 0 <= start_time <= end_time <= float ( self . file_metadata . duration ) ) : raise ParameterError ( 'start_time and end_time must be within the original file ' 'duration ({:f}) and end_time cannot be smaller than ' 'start_time.' . format ( float ( self . file_metadata . duration ) ) ) jam_trimmed = JAMS ( annotations = None , file_metadata = self . file_metadata , sandbox = self . sandbox ) jam_trimmed . annotations = self . annotations . trim ( start_time , end_time , strict = strict ) if 'trim' not in jam_trimmed . sandbox . keys ( ) : jam_trimmed . sandbox . update ( trim = [ { 'start_time' : start_time , 'end_time' : end_time } ] ) else : jam_trimmed . sandbox . trim . append ( { 'start_time' : start_time , 'end_time' : end_time } ) return jam_trimmed
11420	def record_xml_output ( rec , tags = None , order_fn = None ) : if tags is None : tags = [ ] if isinstance ( tags , str ) : tags = [ tags ] if tags and '001' not in tags : tags . append ( '001' ) marcxml = [ '<record>' ] fields = [ ] if rec is not None : for tag in rec : if not tags or tag in tags : for field in rec [ tag ] : fields . append ( ( tag , field ) ) if order_fn is None : record_order_fields ( fields ) else : record_order_fields ( fields , order_fn ) for field in fields : marcxml . append ( field_xml_output ( field [ 1 ] , field [ 0 ] ) ) marcxml . append ( '</record>' ) return '\n' . join ( marcxml )
6978	def read_kepler_pklc ( picklefile ) : if picklefile . endswith ( '.gz' ) : infd = gzip . open ( picklefile , 'rb' ) else : infd = open ( picklefile , 'rb' ) try : with infd : lcdict = pickle . load ( infd ) except UnicodeDecodeError : with open ( picklefile , 'rb' ) as infd : lcdict = pickle . load ( infd , encoding = 'latin1' ) LOGWARNING ( 'pickle %s was probably from Python 2 ' 'and failed to load without using "latin1" encoding. ' 'This is probably a numpy issue: ' 'http://stackoverflow.com/q/11305790' % picklefile ) return lcdict
12029	def setsweeps ( self ) : for sweep in range ( self . sweeps ) : self . setsweep ( sweep ) yield self . sweep
3923	def _set_title ( self ) : self . title = get_conv_name ( self . _conversation , show_unread = True , truncate = True ) self . _set_title_cb ( self , self . title )
6281	def keyboard_event ( self , key , action , modifier ) : if key == self . keys . ESCAPE : self . close ( ) return if key == self . keys . SPACE and action == self . keys . ACTION_PRESS : self . timer . toggle_pause ( ) if key == self . keys . D : if action == self . keys . ACTION_PRESS : self . sys_camera . move_right ( True ) elif action == self . keys . ACTION_RELEASE : self . sys_camera . move_right ( False ) elif key == self . keys . A : if action == self . keys . ACTION_PRESS : self . sys_camera . move_left ( True ) elif action == self . keys . ACTION_RELEASE : self . sys_camera . move_left ( False ) elif key == self . keys . W : if action == self . keys . ACTION_PRESS : self . sys_camera . move_forward ( True ) if action == self . keys . ACTION_RELEASE : self . sys_camera . move_forward ( False ) elif key == self . keys . S : if action == self . keys . ACTION_PRESS : self . sys_camera . move_backward ( True ) if action == self . keys . ACTION_RELEASE : self . sys_camera . move_backward ( False ) elif key == self . keys . Q : if action == self . keys . ACTION_PRESS : self . sys_camera . move_down ( True ) if action == self . keys . ACTION_RELEASE : self . sys_camera . move_down ( False ) elif key == self . keys . E : if action == self . keys . ACTION_PRESS : self . sys_camera . move_up ( True ) if action == self . keys . ACTION_RELEASE : self . sys_camera . move_up ( False ) if key == self . keys . X and action == self . keys . ACTION_PRESS : screenshot . create ( ) if key == self . keys . R and action == self . keys . ACTION_PRESS : project . instance . reload_programs ( ) if key == self . keys . RIGHT and action == self . keys . ACTION_PRESS : self . timer . set_time ( self . timer . get_time ( ) + 10.0 ) if key == self . keys . LEFT and action == self . keys . ACTION_PRESS : self . timer . set_time ( self . timer . get_time ( ) - 10.0 ) self . timeline . key_event ( key , action , modifier )
1510	def start_cluster ( cl_args ) : roles = read_and_parse_roles ( cl_args ) masters = roles [ Role . MASTERS ] slaves = roles [ Role . SLAVES ] zookeepers = roles [ Role . ZOOKEEPERS ] Log . info ( "Roles:" ) Log . info ( " - Master Servers: %s" % list ( masters ) ) Log . info ( " - Slave Servers: %s" % list ( slaves ) ) Log . info ( " - Zookeeper Servers: %s" % list ( zookeepers ) ) if not masters : Log . error ( "No master servers specified!" ) sys . exit ( - 1 ) if not slaves : Log . error ( "No slave servers specified!" ) sys . exit ( - 1 ) if not zookeepers : Log . error ( "No zookeeper servers specified!" ) sys . exit ( - 1 ) update_config_files ( cl_args ) dist_nodes = list ( masters . union ( slaves ) ) if not ( len ( dist_nodes ) == 1 and is_self ( dist_nodes [ 0 ] ) ) : distribute_package ( roles , cl_args ) start_master_nodes ( masters , cl_args ) start_slave_nodes ( slaves , cl_args ) start_api_server ( masters , cl_args ) start_heron_tools ( masters , cl_args ) Log . info ( "Heron standalone cluster complete!" )
2499	def create_doc ( self ) : doc_node = URIRef ( 'http://www.spdx.org/tools#SPDXRef-DOCUMENT' ) self . graph . add ( ( doc_node , RDF . type , self . spdx_namespace . SpdxDocument ) ) vers_literal = Literal ( str ( self . document . version ) ) self . graph . add ( ( doc_node , self . spdx_namespace . specVersion , vers_literal ) ) data_lics = URIRef ( self . document . data_license . url ) self . graph . add ( ( doc_node , self . spdx_namespace . dataLicense , data_lics ) ) doc_name = URIRef ( self . document . name ) self . graph . add ( ( doc_node , self . spdx_namespace . name , doc_name ) ) return doc_node
8717	def file_compile ( self , path ) : log . info ( 'Compile ' + path ) cmd = 'node.compile("%s")' % path res = self . __exchange ( cmd ) log . info ( res ) return res
4481	def storage ( self , provider = 'osfstorage' ) : stores = self . _json ( self . _get ( self . _storages_url ) , 200 ) stores = stores [ 'data' ] for store in stores : provides = self . _get_attribute ( store , 'attributes' , 'provider' ) if provides == provider : return Storage ( store , self . session ) raise RuntimeError ( "Project has no storage " "provider '{}'" . format ( provider ) )
11800	def infer_assignment ( self ) : "Return the partial assignment implied by the current inferences." self . support_pruning ( ) return dict ( ( v , self . curr_domains [ v ] [ 0 ] ) for v in self . vars if 1 == len ( self . curr_domains [ v ] ) )
2325	def predict_dataset ( self , x , ** kwargs ) : printout = kwargs . get ( "printout" , None ) pred = [ ] res = [ ] x . columns = [ "A" , "B" ] for idx , row in x . iterrows ( ) : a = scale ( row [ 'A' ] . reshape ( ( len ( row [ 'A' ] ) , 1 ) ) ) b = scale ( row [ 'B' ] . reshape ( ( len ( row [ 'B' ] ) , 1 ) ) ) pred . append ( self . predict_proba ( a , b , idx = idx ) ) if printout is not None : res . append ( [ row [ 'SampleID' ] , pred [ - 1 ] ] ) DataFrame ( res , columns = [ 'SampleID' , 'Predictions' ] ) . to_csv ( printout , index = False ) return pred
6180	def merge_DA_ph_times ( ph_times_d , ph_times_a ) : ph_times = np . hstack ( [ ph_times_d , ph_times_a ] ) a_em = np . hstack ( [ np . zeros ( ph_times_d . size , dtype = np . bool ) , np . ones ( ph_times_a . size , dtype = np . bool ) ] ) index_sort = ph_times . argsort ( ) return ph_times [ index_sort ] , a_em [ index_sort ]
11310	def get_object ( self , url , month_format = '%b' , day_format = '%d' ) : params = self . get_params ( url ) try : year = params [ self . _meta . year_part ] month = params [ self . _meta . month_part ] day = params [ self . _meta . day_part ] except KeyError : try : year , month , day = params [ '_0' ] , params [ '_1' ] , params [ '_2' ] except KeyError : raise OEmbedException ( 'Error extracting date from url parameters' ) try : tt = time . strptime ( '%s-%s-%s' % ( year , month , day ) , '%s-%s-%s' % ( '%Y' , month_format , day_format ) ) date = datetime . date ( * tt [ : 3 ] ) except ValueError : raise OEmbedException ( 'Error parsing date from: %s' % url ) if isinstance ( self . _meta . model . _meta . get_field ( self . _meta . date_field ) , DateTimeField ) : min_date = datetime . datetime . combine ( date , datetime . time . min ) max_date = datetime . datetime . combine ( date , datetime . time . max ) query = { '%s__range' % self . _meta . date_field : ( min_date , max_date ) } else : query = { self . _meta . date_field : date } for key , value in self . _meta . fields_to_match . iteritems ( ) : try : query [ value ] = params [ key ] except KeyError : raise OEmbedException ( '%s was not found in the urlpattern parameters. Valid names are: %s' % ( key , ', ' . join ( params . keys ( ) ) ) ) try : obj = self . get_queryset ( ) . get ( ** query ) except self . _meta . model . DoesNotExist : raise OEmbedException ( 'Requested object not found' ) return obj
3500	def assess_component ( model , reaction , side , flux_coefficient_cutoff = 0.001 , solver = None ) : reaction = model . reactions . get_by_any ( reaction ) [ 0 ] result_key = dict ( reactants = 'produced' , products = 'capacity' ) [ side ] get_components = attrgetter ( side ) with model as m : m . objective = reaction if _optimize_or_value ( m , solver = solver ) >= flux_coefficient_cutoff : return True simulation_results = { } demand_reactions = { } for component in get_components ( reaction ) : coeff = reaction . metabolites [ component ] demand = m . add_boundary ( component , type = 'demand' ) demand . metabolites [ component ] = coeff demand_reactions [ demand ] = ( component , coeff ) joint_demand = Reaction ( "joint_demand" ) for demand_reaction in demand_reactions : joint_demand += demand_reaction m . add_reactions ( [ joint_demand ] ) m . objective = joint_demand if _optimize_or_value ( m , solver = solver ) >= flux_coefficient_cutoff : return True for demand_reaction , ( component , coeff ) in iteritems ( demand_reactions ) : with m : m . objective = demand_reaction flux = _optimize_or_value ( m , solver = solver ) if flux_coefficient_cutoff > flux : simulation_results . update ( { component : { 'required' : flux_coefficient_cutoff / abs ( coeff ) , result_key : flux / abs ( coeff ) } } ) if len ( simulation_results ) == 0 : simulation_results = False return simulation_results
7552	def _getbins ( ) : if not _sys . maxsize > 2 ** 32 : _sys . exit ( "ipyrad requires 64bit architecture" ) _platform = _sys . platform if 'VIRTUAL_ENV' in _os . environ : ipyrad_path = _os . environ [ 'VIRTUAL_ENV' ] else : path = _os . path . abspath ( _os . path . dirname ( __file__ ) ) ipyrad_path = _os . path . dirname ( path ) ipyrad_path = _os . path . dirname ( path ) bin_path = _os . path . join ( ipyrad_path , "bin" ) if 'linux' in _platform : vsearch = _os . path . join ( _os . path . abspath ( bin_path ) , "vsearch-linux-x86_64" ) muscle = _os . path . join ( _os . path . abspath ( bin_path ) , "muscle-linux-x86_64" ) smalt = _os . path . join ( _os . path . abspath ( bin_path ) , "smalt-linux-x86_64" ) bwa = _os . path . join ( _os . path . abspath ( bin_path ) , "bwa-linux-x86_64" ) samtools = _os . path . join ( _os . path . abspath ( bin_path ) , "samtools-linux-x86_64" ) bedtools = _os . path . join ( _os . path . abspath ( bin_path ) , "bedtools-linux-x86_64" ) qmc = _os . path . join ( _os . path . abspath ( bin_path ) , "QMC-linux-x86_64" ) else : vsearch = _os . path . join ( _os . path . abspath ( bin_path ) , "vsearch-osx-x86_64" ) muscle = _os . path . join ( _os . path . abspath ( bin_path ) , "muscle-osx-x86_64" ) smalt = _os . path . join ( _os . path . abspath ( bin_path ) , "smalt-osx-x86_64" ) bwa = _os . path . join ( _os . path . abspath ( bin_path ) , "bwa-osx-x86_64" ) samtools = _os . path . join ( _os . path . abspath ( bin_path ) , "samtools-osx-x86_64" ) bedtools = _os . path . join ( _os . path . abspath ( bin_path ) , "bedtools-osx-x86_64" ) qmc = _os . path . join ( _os . path . abspath ( bin_path ) , "QMC-osx-x86_64" ) assert _cmd_exists ( muscle ) , "muscle not found here: " + muscle assert _cmd_exists ( vsearch ) , "vsearch not found here: " + vsearch assert _cmd_exists ( smalt ) , "smalt not found here: " + smalt assert _cmd_exists ( bwa ) , "bwa not found here: " + bwa assert _cmd_exists ( samtools ) , "samtools not found here: " + samtools assert _cmd_exists ( bedtools ) , "bedtools not found here: " + bedtools return vsearch , muscle , smalt , bwa , samtools , bedtools , qmc
10679	def S ( self , T ) : result = 0.0 if T < self . Tmax : lT = T else : lT = self . Tmax Tref = self . Tmin for c , e in zip ( self . _coefficients , self . _exponents ) : e_modified = e - 1.0 if e_modified == - 1.0 : result += c * math . log ( lT / Tref ) else : e_mod = e_modified + 1.0 result += c * ( lT ** e_mod - Tref ** e_mod ) / e_mod return result
3178	def get ( self , list_id , merge_id ) : self . list_id = list_id self . merge_id = merge_id return self . _mc_client . _get ( url = self . _build_path ( list_id , 'merge-fields' , merge_id ) )
13243	def weekdays ( self ) : if not self . root . xpath ( 'days' ) : return set ( range ( 7 ) ) return set ( int ( d ) - 1 for d in self . root . xpath ( 'days/day/text()' ) )
6045	def padded_blurred_image_2d_from_padded_image_1d_and_psf ( self , padded_image_1d , psf ) : padded_model_image_1d = self . convolve_array_1d_with_psf ( padded_array_1d = padded_image_1d , psf = psf ) return self . scaled_array_2d_from_array_1d ( array_1d = padded_model_image_1d )
9062	def fix ( self , param ) : if param == "delta" : super ( ) . _fix ( "logistic" ) else : self . _fix [ param ] = True
9432	def _c_func ( func , restype , argtypes , errcheck = None ) : func . restype = restype func . argtypes = argtypes if errcheck is not None : func . errcheck = errcheck return func
5079	def strip_html_tags ( text , allowed_tags = None ) : if text is None : return if allowed_tags is None : allowed_tags = ALLOWED_TAGS return bleach . clean ( text , tags = allowed_tags , attributes = [ 'id' , 'class' , 'style' , 'href' , 'title' ] , strip = True )
13882	def GetFileContents ( filename , binary = False , encoding = None , newline = None ) : source_file = OpenFile ( filename , binary = binary , encoding = encoding , newline = newline ) try : contents = source_file . read ( ) finally : source_file . close ( ) return contents
12543	def merge_images ( images , axis = 't' ) : if not images : return None axis_dim = { 'x' : 0 , 'y' : 1 , 'z' : 2 , 't' : 3 , } if axis not in axis_dim : raise ValueError ( 'Expected `axis` to be one of ({}), got {}.' . format ( set ( axis_dim . keys ( ) ) , axis ) ) img1 = images [ 0 ] for img in images : check_img_compatibility ( img1 , img ) image_data = [ ] for img in images : image_data . append ( check_img ( img ) . get_data ( ) ) work_axis = axis_dim [ axis ] ndim = image_data [ 0 ] . ndim if ndim - 1 < work_axis : image_data = [ np . expand_dims ( img , axis = work_axis ) for img in image_data ] return np . concatenate ( image_data , axis = work_axis )
8709	def write_lines ( self , data ) : lines = data . replace ( '\r' , '' ) . split ( '\n' ) for line in lines : self . __exchange ( line )
4634	def derive_private_key ( self , sequence ) : encoded = "%s %d" % ( str ( self ) , sequence ) a = bytes ( encoded , "ascii" ) s = hashlib . sha256 ( hashlib . sha512 ( a ) . digest ( ) ) . digest ( ) return PrivateKey ( hexlify ( s ) . decode ( "ascii" ) , prefix = self . pubkey . prefix )
3012	def locked_put ( self , credentials ) : filters = { self . key_name : self . key_value } query = self . session . query ( self . model_class ) . filter_by ( ** filters ) entity = query . first ( ) if not entity : entity = self . model_class ( ** filters ) setattr ( entity , self . property_name , credentials ) self . session . add ( entity )
10806	def validate ( cls , policy ) : return policy in [ cls . PUBLIC , cls . MEMBERS , cls . ADMINS ]
8335	def findPreviousSibling ( self , name = None , attrs = { } , text = None , ** kwargs ) : return self . _findOne ( self . findPreviousSiblings , name , attrs , text , ** kwargs )
5991	def weighted_regularization_matrix_from_pixel_neighbors ( regularization_weights , pixel_neighbors , pixel_neighbors_size ) : pixels = len ( regularization_weights ) regularization_matrix = np . zeros ( shape = ( pixels , pixels ) ) regularization_weight = regularization_weights ** 2.0 for i in range ( pixels ) : for j in range ( pixel_neighbors_size [ i ] ) : neighbor_index = pixel_neighbors [ i , j ] regularization_matrix [ i , i ] += regularization_weight [ neighbor_index ] regularization_matrix [ neighbor_index , neighbor_index ] += regularization_weight [ neighbor_index ] regularization_matrix [ i , neighbor_index ] -= regularization_weight [ neighbor_index ] regularization_matrix [ neighbor_index , i ] -= regularization_weight [ neighbor_index ] return regularization_matrix
1945	def _hook_write_mem ( self , uc , access , address , size , value , data ) : self . _mem_delta [ address ] = ( value , size ) return True
10500	def waitForCreation ( self , timeout = 10 , notification = 'AXCreated' ) : callback = AXCallbacks . returnElemCallback retelem = None args = ( retelem , ) return self . waitFor ( timeout , notification , callback = callback , args = args )
8330	def findAllNext ( self , name = None , attrs = { } , text = None , limit = None , ** kwargs ) : return self . _findAll ( name , attrs , text , limit , self . nextGenerator , ** kwargs )
6227	def _translate_string ( self , data , length ) : for index , char in enumerate ( data ) : if index == length : break yield self . _meta . characters - 1 - self . _ct [ char ]
966	def percentOverlap ( x1 , x2 , size ) : nonZeroX1 = np . count_nonzero ( x1 ) nonZeroX2 = np . count_nonzero ( x2 ) minX1X2 = min ( nonZeroX1 , nonZeroX2 ) percentOverlap = 0 if minX1X2 > 0 : percentOverlap = float ( np . dot ( x1 , x2 ) ) / float ( minX1X2 ) return percentOverlap
1037	def begin ( self ) : return Range ( self . source_buffer , self . begin_pos , self . begin_pos , expanded_from = self . expanded_from )
255	def apply_sector_mappings_to_round_trips ( round_trips , sector_mappings ) : sector_round_trips = round_trips . copy ( ) sector_round_trips . symbol = sector_round_trips . symbol . apply ( lambda x : sector_mappings . get ( x , 'No Sector Mapping' ) ) sector_round_trips = sector_round_trips . dropna ( axis = 0 ) return sector_round_trips
9289	def _send_login ( self ) : login_str = "user {0} pass {1} vers aprslib {3}{2}\r\n" login_str = login_str . format ( self . callsign , self . passwd , ( " filter " + self . filter ) if self . filter != "" else "" , __version__ ) self . logger . info ( "Sending login information" ) try : self . _sendall ( login_str ) self . sock . settimeout ( 5 ) test = self . sock . recv ( len ( login_str ) + 100 ) if is_py3 : test = test . decode ( 'latin-1' ) test = test . rstrip ( ) self . logger . debug ( "Server: %s" , test ) _ , _ , callsign , status , _ = test . split ( ' ' , 4 ) if callsign == "" : raise LoginError ( "Server responded with empty callsign???" ) if callsign != self . callsign : raise LoginError ( "Server: %s" % test ) if status != "verified," and self . passwd != "-1" : raise LoginError ( "Password is incorrect" ) if self . passwd == "-1" : self . logger . info ( "Login successful (receive only)" ) else : self . logger . info ( "Login successful" ) except LoginError as e : self . logger . error ( str ( e ) ) self . close ( ) raise except : self . close ( ) self . logger . error ( "Failed to login" ) raise LoginError ( "Failed to login" )
1271	def setup_components_and_tf_funcs ( self , custom_getter = None ) : self . network = Network . from_spec ( spec = self . network_spec , kwargs = dict ( summary_labels = self . summary_labels ) ) assert len ( self . internals_spec ) == 0 self . internals_spec = self . network . internals_spec ( ) for name in sorted ( self . internals_spec ) : internal = self . internals_spec [ name ] self . internals_input [ name ] = tf . placeholder ( dtype = util . tf_dtype ( internal [ 'type' ] ) , shape = ( None , ) + tuple ( internal [ 'shape' ] ) , name = ( 'internal-' + name ) ) if internal [ 'initialization' ] == 'zeros' : self . internals_init [ name ] = np . zeros ( shape = internal [ 'shape' ] ) else : raise TensorForceError ( "Invalid internal initialization value." ) custom_getter = super ( DistributionModel , self ) . setup_components_and_tf_funcs ( custom_getter ) self . distributions = self . create_distributions ( ) self . fn_kl_divergence = tf . make_template ( name_ = 'kl-divergence' , func_ = self . tf_kl_divergence , custom_getter_ = custom_getter ) return custom_getter
3510	def add_room ( model , solution = None , linear = False , delta = 0.03 , epsilon = 1E-03 ) : r if 'room_old_objective' in model . solver . variables : raise ValueError ( 'model is already adjusted for ROOM' ) if solution is None : solution = pfba ( model ) prob = model . problem variable = prob . Variable ( "room_old_objective" , ub = solution . objective_value ) constraint = prob . Constraint ( model . solver . objective . expression - variable , ub = 0.0 , lb = 0.0 , name = "room_old_objective_constraint" ) model . objective = prob . Objective ( Zero , direction = "min" , sloppy = True ) vars_and_cons = [ variable , constraint ] obj_vars = [ ] for rxn in model . reactions : flux = solution . fluxes [ rxn . id ] if linear : y = prob . Variable ( "y_" + rxn . id , lb = 0 , ub = 1 ) delta = epsilon = 0.0 else : y = prob . Variable ( "y_" + rxn . id , type = "binary" ) w_u = flux + ( delta * abs ( flux ) ) + epsilon upper_const = prob . Constraint ( rxn . flux_expression - y * ( rxn . upper_bound - w_u ) , ub = w_u , name = "room_constraint_upper_" + rxn . id ) w_l = flux - ( delta * abs ( flux ) ) - epsilon lower_const = prob . Constraint ( rxn . flux_expression - y * ( rxn . lower_bound - w_l ) , lb = w_l , name = "room_constraint_lower_" + rxn . id ) vars_and_cons . extend ( [ y , upper_const , lower_const ] ) obj_vars . append ( y ) model . add_cons_vars ( vars_and_cons ) model . objective . set_linear_coefficients ( { v : 1.0 for v in obj_vars } )
4844	def is_course_in_catalog ( self , catalog_id , course_id ) : try : course_run_id = str ( CourseKey . from_string ( course_id ) ) except InvalidKeyError : course_run_id = None endpoint = self . client . catalogs ( catalog_id ) . contains if course_run_id : resp = endpoint . get ( course_run_id = course_run_id ) else : resp = endpoint . get ( course_id = course_id ) return resp . get ( 'courses' , { } ) . get ( course_id , False )
720	def getOptimizationMetricInfo ( cls , searchJobParams ) : if searchJobParams [ "hsVersion" ] == "v2" : search = HypersearchV2 ( searchParams = searchJobParams ) else : raise RuntimeError ( "Unsupported hypersearch version \"%s\"" % ( searchJobParams [ "hsVersion" ] ) ) info = search . getOptimizationMetricInfo ( ) return info
5738	def enqueue ( self , f , * args , ** kwargs ) : task = Task ( uuid4 ( ) . hex , f , args , kwargs ) self . storage . put_task ( task ) return self . enqueue_task ( task )
1212	def WorkerAgentGenerator ( agent_class ) : if isinstance ( agent_class , str ) : agent_class = AgentsDictionary . get ( agent_class ) if not agent_class and agent_class . find ( '.' ) != - 1 : module_name , function_name = agent_class . rsplit ( '.' , 1 ) module = importlib . import_module ( module_name ) agent_class = getattr ( module , function_name ) class WorkerAgent ( agent_class ) : def __init__ ( self , model = None , ** kwargs ) : self . model = model if not issubclass ( agent_class , LearningAgent ) : kwargs . pop ( "network" ) super ( WorkerAgent , self ) . __init__ ( ** kwargs ) def initialize_model ( self ) : return self . model return WorkerAgent
2816	def convert_maxpool3 ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting pooling ...' ) if names == 'short' : tf_name = 'P' + random_string ( 7 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) if 'kernel_shape' in params : height , width , depth = params [ 'kernel_shape' ] else : height , width , depth = params [ 'kernel_size' ] if 'strides' in params : stride_height , stride_width , stride_depth = params [ 'strides' ] else : stride_height , stride_width , stride_depth = params [ 'stride' ] if 'pads' in params : padding_h , padding_w , padding_d , _ , _ = params [ 'pads' ] else : padding_h , padding_w , padding_d = params [ 'padding' ] input_name = inputs [ 0 ] if padding_h > 0 and padding_w > 0 and padding_d > 0 : padding_name = tf_name + '_pad' padding_layer = keras . layers . ZeroPadding3D ( padding = ( padding_h , padding_w , padding_d ) , name = padding_name ) layers [ padding_name ] = padding_layer ( layers [ inputs [ 0 ] ] ) input_name = padding_name pooling = keras . layers . MaxPooling3D ( pool_size = ( height , width , depth ) , strides = ( stride_height , stride_width , stride_depth ) , padding = 'valid' , name = tf_name ) layers [ scope_name ] = pooling ( layers [ input_name ] )
2085	def format_options ( self , ctx , formatter ) : field_opts = [ ] global_opts = [ ] local_opts = [ ] other_opts = [ ] for param in self . params : if param . name in SETTINGS_PARMS : opts = global_opts elif getattr ( param , 'help' , None ) and param . help . startswith ( '[FIELD]' ) : opts = field_opts param . help = param . help [ len ( '[FIELD]' ) : ] else : opts = local_opts rv = param . get_help_record ( ctx ) if rv is None : continue else : opts . append ( rv ) if self . add_help_option : help_options = self . get_help_option_names ( ctx ) if help_options : other_opts . append ( [ join_options ( help_options ) [ 0 ] , 'Show this message and exit.' ] ) if field_opts : with formatter . section ( 'Field Options' ) : formatter . write_dl ( field_opts ) if local_opts : with formatter . section ( 'Local Options' ) : formatter . write_dl ( local_opts ) if global_opts : with formatter . section ( 'Global Options' ) : formatter . write_dl ( global_opts ) if other_opts : with formatter . section ( 'Other Options' ) : formatter . write_dl ( other_opts )
411	def minibatches ( inputs = None , targets = None , batch_size = None , allow_dynamic_batch_size = False , shuffle = False ) : if len ( inputs ) != len ( targets ) : raise AssertionError ( "The length of inputs and targets should be equal" ) if shuffle : indices = np . arange ( len ( inputs ) ) np . random . shuffle ( indices ) for start_idx in range ( 0 , len ( inputs ) , batch_size ) : end_idx = start_idx + batch_size if end_idx > len ( inputs ) : if allow_dynamic_batch_size : end_idx = len ( inputs ) else : break if shuffle : excerpt = indices [ start_idx : end_idx ] else : excerpt = slice ( start_idx , end_idx ) if ( isinstance ( inputs , list ) or isinstance ( targets , list ) ) and ( shuffle == True ) : yield [ inputs [ i ] for i in excerpt ] , [ targets [ i ] for i in excerpt ] else : yield inputs [ excerpt ] , targets [ excerpt ]
677	def getDescription ( self ) : description = { 'name' : self . name , 'fields' : [ f . name for f in self . fields ] , 'numRecords by field' : [ f . numRecords for f in self . fields ] } return description
10651	def get_activity ( self , name ) : return [ a for a in self . activities if a . name == name ] [ 0 ]
4332	def noiseprof ( self , input_filepath , profile_path ) : if os . path . isdir ( profile_path ) : raise ValueError ( "profile_path {} is a directory." . format ( profile_path ) ) if os . path . dirname ( profile_path ) == '' and profile_path != '' : _abs_profile_path = os . path . join ( os . getcwd ( ) , profile_path ) else : _abs_profile_path = profile_path if not os . access ( os . path . dirname ( _abs_profile_path ) , os . W_OK ) : raise IOError ( "profile_path {} is not writeable." . format ( _abs_profile_path ) ) effect_args = [ 'noiseprof' , profile_path ] self . build ( input_filepath , None , extra_args = effect_args ) return None
4931	def transform_courserun_description ( self , content_metadata_item ) : description_with_locales = [ ] content_metadata_language_code = transform_language_code ( content_metadata_item . get ( 'content_language' , '' ) ) for locale in self . enterprise_configuration . get_locales ( default_locale = content_metadata_language_code ) : description_with_locales . append ( { 'locale' : locale , 'value' : ( content_metadata_item [ 'full_description' ] or content_metadata_item [ 'short_description' ] or content_metadata_item [ 'title' ] or '' ) } ) return description_with_locales
12574	def smooth_fwhm ( self , fwhm ) : if fwhm != self . _smooth_fwhm : self . _is_data_smooth = False self . _smooth_fwhm = fwhm
5442	def parse_file_provider ( uri ) : providers = { 'gs' : job_model . P_GCS , 'file' : job_model . P_LOCAL } provider_found = re . match ( r'^([A-Za-z][A-Za-z0-9+.-]{0,29})://' , uri ) if provider_found : prefix = provider_found . group ( 1 ) . lower ( ) else : prefix = 'file' if prefix in providers : return providers [ prefix ] else : raise ValueError ( 'File prefix not supported: %s://' % prefix )
9041	def as_instruction ( self , specification ) : instruction = self . _instruction_class ( specification ) type_ = instruction . type if type_ in self . _type_to_instruction : instruction . inherit_from ( self . _type_to_instruction [ type_ ] ) return instruction
8012	def check_paypal_api_key ( app_configs = None , ** kwargs ) : messages = [ ] mode = getattr ( djpaypal_settings , "PAYPAL_MODE" , None ) if mode not in VALID_MODES : msg = "Invalid PAYPAL_MODE specified: {}." . format ( repr ( mode ) ) hint = "PAYPAL_MODE must be one of {}" . format ( ", " . join ( repr ( k ) for k in VALID_MODES ) ) messages . append ( checks . Critical ( msg , hint = hint , id = "djpaypal.C001" ) ) for setting in "PAYPAL_CLIENT_ID" , "PAYPAL_CLIENT_SECRET" : if not getattr ( djpaypal_settings , setting , None ) : msg = "Invalid value specified for {}" . format ( setting ) hint = "Add PAYPAL_CLIENT_ID and PAYPAL_CLIENT_SECRET to your settings." messages . append ( checks . Critical ( msg , hint = hint , id = "djpaypal.C002" ) ) return messages
5031	def get ( self , request , template_id , view_type ) : template = get_object_or_404 ( EnrollmentNotificationEmailTemplate , pk = template_id ) if view_type not in self . view_type_contexts : return HttpResponse ( status = 404 ) base_context = self . view_type_contexts [ view_type ] . copy ( ) base_context . update ( { 'user_name' : self . get_user_name ( request ) } ) return HttpResponse ( template . render_html_template ( base_context ) , content_type = 'text/html' )
2953	def container_id ( self , name ) : container = self . _containers . get ( name , None ) if not container is None : return container . get ( 'id' , None ) return None
127	def area ( self ) : if len ( self . exterior ) < 3 : raise Exception ( "Cannot compute the polygon's area because it contains less than three points." ) poly = self . to_shapely_polygon ( ) return poly . area
5276	def _terminalSymbolsGenerator ( self ) : py2 = sys . version [ 0 ] < '3' UPPAs = list ( list ( range ( 0xE000 , 0xF8FF + 1 ) ) + list ( range ( 0xF0000 , 0xFFFFD + 1 ) ) + list ( range ( 0x100000 , 0x10FFFD + 1 ) ) ) for i in UPPAs : if py2 : yield ( unichr ( i ) ) else : yield ( chr ( i ) ) raise ValueError ( "To many input strings." )
1547	def configure ( level = logging . INFO , logfile = None ) : for handler in Log . handlers : if isinstance ( handler , logging . StreamHandler ) : Log . handlers . remove ( handler ) Log . setLevel ( level ) if logfile is not None : log_format = "[%(asctime)s] [%(levelname)s]: %(message)s" formatter = logging . Formatter ( fmt = log_format , datefmt = date_format ) file_handler = logging . FileHandler ( logfile ) file_handler . setFormatter ( formatter ) Log . addHandler ( file_handler ) else : log_format = "[%(asctime)s] %(log_color)s[%(levelname)s]%(reset)s: %(message)s" formatter = colorlog . ColoredFormatter ( fmt = log_format , datefmt = date_format ) stream_handler = logging . StreamHandler ( ) stream_handler . setFormatter ( formatter ) Log . addHandler ( stream_handler )
3908	def put ( self , coro ) : assert asyncio . iscoroutine ( coro ) self . _queue . put_nowait ( coro )
2359	def t_intnumber ( self , t ) : r'-?\d+' t . value = int ( t . value ) t . type = 'NUMBER' return t
5366	def replace_print ( fileobj = sys . stderr ) : printer = _Printer ( fileobj ) previous_stdout = sys . stdout sys . stdout = printer try : yield printer finally : sys . stdout = previous_stdout
7523	def reftrick ( iseq , consdict ) : altrefs = np . zeros ( ( iseq . shape [ 1 ] , 4 ) , dtype = np . uint8 ) altrefs [ : , 1 ] = 46 for col in xrange ( iseq . shape [ 1 ] ) : fcounts = np . zeros ( 111 , dtype = np . int64 ) counts = np . bincount ( iseq [ : , col ] ) fcounts [ : counts . shape [ 0 ] ] = counts fcounts [ 78 ] = 0 fcounts [ 45 ] = 0 for aidx in xrange ( consdict . shape [ 0 ] ) : nbases = fcounts [ consdict [ aidx , 0 ] ] for _ in xrange ( nbases ) : fcounts [ consdict [ aidx , 1 ] ] += 1 fcounts [ consdict [ aidx , 2 ] ] += 1 fcounts [ consdict [ aidx , 0 ] ] = 0 who = np . argmax ( fcounts ) altrefs [ col , 0 ] = who fcounts [ who ] = 0 who = np . argmax ( fcounts ) if who : altrefs [ col , 1 ] = who fcounts [ who ] = 0 who = np . argmax ( fcounts ) altrefs [ col , 2 ] = who fcounts [ who ] = 0 who = np . argmax ( fcounts ) altrefs [ col , 3 ] = who return altrefs
10718	def normalize_unitnumber ( unit_number ) : try : try : unit_number = int ( unit_number ) except ValueError : raise X10InvalidUnitNumber ( '%r not a valid unit number' % unit_number ) except TypeError : raise X10InvalidUnitNumber ( '%r not a valid unit number' % unit_number ) if not ( 1 <= unit_number <= 16 ) : raise X10InvalidUnitNumber ( '%r not a valid unit number' % unit_number ) return unit_number
11481	def _create_folder ( local_folder , parent_folder_id ) : new_folder = session . communicator . create_folder ( session . token , os . path . basename ( local_folder ) , parent_folder_id ) return new_folder [ 'folder_id' ]
5227	def to_str ( data : dict , fmt = '{key}={value}' , sep = ', ' , public_only = True ) -> str : if public_only : keys = list ( filter ( lambda vv : vv [ 0 ] != '_' , data . keys ( ) ) ) else : keys = list ( data . keys ( ) ) return '{' + sep . join ( [ to_str ( data = v , fmt = fmt , sep = sep ) if isinstance ( v , dict ) else fstr ( fmt = fmt , key = k , value = v ) for k , v in data . items ( ) if k in keys ] ) + '}'
3656	def add_cti_file ( self , file_path : str ) : if not os . path . exists ( file_path ) : self . _logger . warning ( 'Attempted to add {0} which does not exist.' . format ( file_path ) ) if file_path not in self . _cti_files : self . _cti_files . append ( file_path ) self . _logger . info ( 'Added {0} to the CTI file list.' . format ( file_path ) )
13217	def connection_dsn ( self , name = None ) : return ' ' . join ( "%s=%s" % ( param , value ) for param , value in self . _connect_options ( name ) )
7785	def error ( self , error_data ) : if not self . active : return if not self . _try_backup_item ( ) : self . _error_handler ( self . address , error_data ) self . cache . invalidate_object ( self . address ) self . _deactivate ( )
4871	def create ( self , validated_data ) : ret = [ ] for attrs in validated_data : if 'non_field_errors' not in attrs and not any ( isinstance ( attrs [ field ] , list ) for field in attrs ) : ret . append ( self . child . create ( attrs ) ) else : ret . append ( attrs ) return ret
1415	def create_pplan ( self , topologyName , pplan ) : if not pplan or not pplan . IsInitialized ( ) : raise_ ( StateException ( "Physical Plan protobuf not init properly" , StateException . EX_TYPE_PROTOBUF_ERROR ) , sys . exc_info ( ) [ 2 ] ) path = self . get_pplan_path ( topologyName ) LOG . info ( "Adding topology: {0} to path: {1}" . format ( topologyName , path ) ) pplanString = pplan . SerializeToString ( ) try : self . client . create ( path , value = pplanString , makepath = True ) return True except NoNodeError : raise_ ( StateException ( "NoNodeError while creating pplan" , StateException . EX_TYPE_NO_NODE_ERROR ) , sys . exc_info ( ) [ 2 ] ) except NodeExistsError : raise_ ( StateException ( "NodeExistsError while creating pplan" , StateException . EX_TYPE_NODE_EXISTS_ERROR ) , sys . exc_info ( ) [ 2 ] ) except ZookeeperError : raise_ ( StateException ( "Zookeeper while creating pplan" , StateException . EX_TYPE_ZOOKEEPER_ERROR ) , sys . exc_info ( ) [ 2 ] ) except Exception : raise
7461	def save_json ( data ) : datadict = OrderedDict ( [ ( "_version" , data . __dict__ [ "_version" ] ) , ( "_checkpoint" , data . __dict__ [ "_checkpoint" ] ) , ( "name" , data . __dict__ [ "name" ] ) , ( "dirs" , data . __dict__ [ "dirs" ] ) , ( "paramsdict" , data . __dict__ [ "paramsdict" ] ) , ( "samples" , data . __dict__ [ "samples" ] . keys ( ) ) , ( "populations" , data . __dict__ [ "populations" ] ) , ( "database" , data . __dict__ [ "database" ] ) , ( "clust_database" , data . __dict__ [ "clust_database" ] ) , ( "outfiles" , data . __dict__ [ "outfiles" ] ) , ( "barcodes" , data . __dict__ [ "barcodes" ] ) , ( "stats_files" , data . __dict__ [ "stats_files" ] ) , ( "_hackersonly" , data . __dict__ [ "_hackersonly" ] ) , ] ) sampledict = OrderedDict ( [ ] ) for key , sample in data . samples . iteritems ( ) : sampledict [ key ] = sample . _to_fulldict ( ) fulldumps = json . dumps ( { "assembly" : datadict , "samples" : sampledict } , cls = Encoder , sort_keys = False , indent = 4 , separators = ( "," , ":" ) , ) assemblypath = os . path . join ( data . dirs . project , data . name + ".json" ) if not os . path . exists ( data . dirs . project ) : os . mkdir ( data . dirs . project ) done = 0 while not done : try : with open ( assemblypath , 'w' ) as jout : jout . write ( fulldumps ) done = 1 except ( KeyboardInterrupt , SystemExit ) : print ( '.' ) continue
9630	def render_to_message ( self , extra_context = None , ** kwargs ) : if extra_context is None : extra_context = { } kwargs . setdefault ( 'headers' , { } ) . update ( self . headers ) context = self . get_context_data ( ** extra_context ) return self . message_class ( subject = self . render_subject ( context ) , body = self . render_body ( context ) , ** kwargs )
6104	def luminosities_of_galaxies_within_ellipses_in_units ( self , major_axis : dim . Length , unit_luminosity = 'eps' , exposure_time = None ) : return list ( map ( lambda galaxy : galaxy . luminosity_within_ellipse_in_units ( major_axis = major_axis , unit_luminosity = unit_luminosity , kpc_per_arcsec = self . kpc_per_arcsec , exposure_time = exposure_time ) , self . galaxies ) )
2599	def uncan ( obj , g = None ) : import_needed = False for cls , uncanner in iteritems ( uncan_map ) : if isinstance ( cls , string_types ) : import_needed = True break elif isinstance ( obj , cls ) : return uncanner ( obj , g ) if import_needed : _import_mapping ( uncan_map , _original_uncan_map ) return uncan ( obj , g ) return obj
264	def _stack_positions ( positions , pos_in_dollars = True ) : if pos_in_dollars : positions = get_percent_alloc ( positions ) positions = positions . drop ( 'cash' , axis = 'columns' ) positions = positions . stack ( ) positions . index = positions . index . set_names ( [ 'dt' , 'ticker' ] ) return positions
2023	def LT ( self , a , b ) : return Operators . ITEBV ( 256 , Operators . ULT ( a , b ) , 1 , 0 )
204	def from_heatmaps ( heatmaps , class_indices = None , nb_classes = None ) : if class_indices is None : return SegmentationMapOnImage ( heatmaps . arr_0to1 , shape = heatmaps . shape ) else : ia . do_assert ( nb_classes is not None ) ia . do_assert ( min ( class_indices ) >= 0 ) ia . do_assert ( max ( class_indices ) < nb_classes ) ia . do_assert ( len ( class_indices ) == heatmaps . arr_0to1 . shape [ 2 ] ) arr_0to1 = heatmaps . arr_0to1 arr_0to1_full = np . zeros ( ( arr_0to1 . shape [ 0 ] , arr_0to1 . shape [ 1 ] , nb_classes ) , dtype = np . float32 ) for heatmap_channel , mapped_channel in enumerate ( class_indices ) : arr_0to1_full [ : , : , mapped_channel ] = arr_0to1 [ : , : , heatmap_channel ] return SegmentationMapOnImage ( arr_0to1_full , shape = heatmaps . shape )
10486	def _generateFindR ( self , ** kwargs ) : for needle in self . _generateChildrenR ( ) : if needle . _match ( ** kwargs ) : yield needle
3053	def step1_get_device_and_user_codes ( self , http = None ) : if self . device_uri is None : raise ValueError ( 'The value of device_uri must not be None.' ) body = urllib . parse . urlencode ( { 'client_id' : self . client_id , 'scope' : self . scope , } ) headers = { 'content-type' : 'application/x-www-form-urlencoded' , } if self . user_agent is not None : headers [ 'user-agent' ] = self . user_agent if http is None : http = transport . get_http_object ( ) resp , content = transport . request ( http , self . device_uri , method = 'POST' , body = body , headers = headers ) content = _helpers . _from_bytes ( content ) if resp . status == http_client . OK : try : flow_info = json . loads ( content ) except ValueError as exc : raise OAuth2DeviceCodeError ( 'Could not parse server response as JSON: "{0}", ' 'error: "{1}"' . format ( content , exc ) ) return DeviceFlowInfo . FromResponse ( flow_info ) else : error_msg = 'Invalid response {0}.' . format ( resp . status ) try : error_dict = json . loads ( content ) if 'error' in error_dict : error_msg += ' Error: {0}' . format ( error_dict [ 'error' ] ) except ValueError : pass raise OAuth2DeviceCodeError ( error_msg )
7228	def paint ( self ) : snippet = { 'fill-extrusion-opacity' : VectorStyle . get_style_value ( self . opacity ) , 'fill-extrusion-color' : VectorStyle . get_style_value ( self . color ) , 'fill-extrusion-base' : VectorStyle . get_style_value ( self . base ) , 'fill-extrusion-height' : VectorStyle . get_style_value ( self . height ) } if self . translate : snippet [ 'fill-extrusion-translate' ] = self . translate return snippet
11646	def transform ( self , X ) : n = self . flip_ . shape [ 0 ] if X . ndim != 2 or X . shape [ 1 ] != n : msg = "X should have {} columns, the number of samples at fit time" raise TypeError ( msg . format ( self . flip_ . shape [ 0 ] ) ) return np . dot ( X , self . flip_ )
1197	def nested ( * managers ) : warn ( "With-statements now directly support multiple context managers" , DeprecationWarning , 3 ) exits = [ ] vars = [ ] exc = ( None , None , None ) try : for mgr in managers : exit = mgr . __exit__ enter = mgr . __enter__ vars . append ( enter ( ) ) exits . append ( exit ) yield vars except : exc = sys . exc_info ( ) finally : while exits : exit = exits . pop ( ) try : if exit ( * exc ) : exc = ( None , None , None ) except : exc = sys . exc_info ( ) if exc != ( None , None , None ) : raise exc [ 0 ] , exc [ 1 ] , exc [ 2 ]
4736	def info ( txt ) : print ( "%s# %s%s%s" % ( PR_EMPH_CC , get_time_stamp ( ) , txt , PR_NC ) ) sys . stdout . flush ( )
3340	def make_complete_url ( environ , localUri = None ) : url = environ [ "wsgi.url_scheme" ] + "://" if environ . get ( "HTTP_HOST" ) : url += environ [ "HTTP_HOST" ] else : url += environ [ "SERVER_NAME" ] if environ [ "wsgi.url_scheme" ] == "https" : if environ [ "SERVER_PORT" ] != "443" : url += ":" + environ [ "SERVER_PORT" ] else : if environ [ "SERVER_PORT" ] != "80" : url += ":" + environ [ "SERVER_PORT" ] url += compat . quote ( environ . get ( "SCRIPT_NAME" , "" ) ) if localUri is None : url += compat . quote ( environ . get ( "PATH_INFO" , "" ) ) if environ . get ( "QUERY_STRING" ) : url += "?" + environ [ "QUERY_STRING" ] else : url += localUri return url
5823	def to_dict ( self ) : return { "type" : self . type , "name" : self . name , "group_by_key" : self . group_by_key , "role" : self . role , "units" : self . units , "options" : self . build_options ( ) }
666	def sample ( self , rgen ) : rf = rgen . uniform ( 0 , self . sum ) index = bisect . bisect ( self . cdf , rf ) return self . keys [ index ] , numpy . log ( self . pmf [ index ] )
8613	def delete_volume ( self , datacenter_id , volume_id ) : response = self . _perform_request ( url = '/datacenters/%s/volumes/%s' % ( datacenter_id , volume_id ) , method = 'DELETE' ) return response
8964	def _get_registered_executable ( exe_name ) : registered = None if sys . platform . startswith ( 'win' ) : if os . path . splitext ( exe_name ) [ 1 ] . lower ( ) != '.exe' : exe_name += '.exe' import _winreg try : key = "SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\App Paths\\" + exe_name value = _winreg . QueryValue ( _winreg . HKEY_LOCAL_MACHINE , key ) registered = ( value , "from HKLM\\" + key ) except _winreg . error : pass if registered and not os . path . exists ( registered [ 0 ] ) : registered = None return registered
5632	def unindent ( lines ) : try : indent = min ( len ( line ) - len ( line . lstrip ( ) ) for line in lines if line ) except ValueError : return lines else : return [ line [ indent : ] for line in lines ]
9547	def add_record_predicate ( self , record_predicate , code = RECORD_PREDICATE_FALSE , message = MESSAGES [ RECORD_PREDICATE_FALSE ] , modulus = 1 ) : assert callable ( record_predicate ) , 'record predicate must be a callable function' t = record_predicate , code , message , modulus self . _record_predicates . append ( t )
2035	def SLOAD ( self , offset ) : storage_address = self . address self . _publish ( 'will_evm_read_storage' , storage_address , offset ) value = self . world . get_storage_data ( storage_address , offset ) self . _publish ( 'did_evm_read_storage' , storage_address , offset , value ) return value
11212	def _hash ( secret : bytes , data : bytes , alg : str ) -> bytes : algorithm = get_algorithm ( alg ) return hmac . new ( secret , msg = data , digestmod = algorithm ) . digest ( )
12179	def detect ( self ) : self . log . info ( "initializing AP detection on all sweeps..." ) t1 = cm . timeit ( ) for sweep in range ( self . abf . sweeps ) : self . detectSweep ( sweep ) self . log . info ( "AP analysis of %d sweeps found %d APs (completed in %s)" , self . abf . sweeps , len ( self . APs ) , cm . timeit ( t1 ) )
10986	def get_particles_featuring ( feature_rad , state_name = None , im_name = None , use_full_path = False , actual_rad = None , invert = True , featuring_params = { } , ** kwargs ) : state_name , im_name = _pick_state_im_name ( state_name , im_name , use_full_path = use_full_path ) s = states . load ( state_name ) if actual_rad is None : actual_rad = np . median ( s . obj_get_radii ( ) ) im = util . RawImage ( im_name , tile = s . image . tile ) pos = locate_spheres ( im , feature_rad , invert = invert , ** featuring_params ) _ = s . obj_remove_particle ( np . arange ( s . obj_get_radii ( ) . size ) ) s . obj_add_particle ( pos , np . ones ( pos . shape [ 0 ] ) * actual_rad ) s . set_image ( im ) _translate_particles ( s , invert = invert , ** kwargs ) return s
13577	def paste ( tid = None , review = False ) : submit ( pastebin = True , tid = tid , review = False )
1724	def execute ( self , js = None , use_compilation_plan = False ) : try : cache = self . __dict__ [ 'cache' ] except KeyError : cache = self . __dict__ [ 'cache' ] = { } hashkey = hashlib . md5 ( js . encode ( 'utf-8' ) ) . digest ( ) try : compiled = cache [ hashkey ] except KeyError : code = translate_js ( js , '' , use_compilation_plan = use_compilation_plan ) compiled = cache [ hashkey ] = compile ( code , '<EvalJS snippet>' , 'exec' ) exec ( compiled , self . _context )
1391	def convert_pb_kvs ( kvs , include_non_primitives = True ) : config = { } for kv in kvs : if kv . value : config [ kv . key ] = kv . value elif kv . serialized_value : if topology_pb2 . JAVA_SERIALIZED_VALUE == kv . type : jv = _convert_java_value ( kv , include_non_primitives = include_non_primitives ) if jv is not None : config [ kv . key ] = jv else : config [ kv . key ] = _raw_value ( kv ) return config
406	def retrieve_seq_length_op3 ( data , pad_val = 0 ) : data_shape_size = data . get_shape ( ) . ndims if data_shape_size == 3 : return tf . reduce_sum ( tf . cast ( tf . reduce_any ( tf . not_equal ( data , pad_val ) , axis = 2 ) , dtype = tf . int32 ) , 1 ) elif data_shape_size == 2 : return tf . reduce_sum ( tf . cast ( tf . not_equal ( data , pad_val ) , dtype = tf . int32 ) , 1 ) elif data_shape_size == 1 : raise ValueError ( "retrieve_seq_length_op3: data has wrong shape!" ) else : raise ValueError ( "retrieve_seq_length_op3: handling data_shape_size %s hasn't been implemented!" % ( data_shape_size ) )
12575	def apply_mask ( self , mask_img ) : self . set_mask ( mask_img ) return self . get_data ( masked = True , smoothed = True , safe_copy = True )
10283	def count_sources ( edge_iter : EdgeIterator ) -> Counter : return Counter ( u for u , _ , _ in edge_iter )
5613	def reproject_geometry ( geometry , src_crs = None , dst_crs = None , error_on_clip = False , validity_check = True , antimeridian_cutting = False ) : src_crs = _validated_crs ( src_crs ) dst_crs = _validated_crs ( dst_crs ) def _reproject_geom ( geometry , src_crs , dst_crs ) : if geometry . is_empty : return geometry else : out_geom = to_shape ( transform_geom ( src_crs . to_dict ( ) , dst_crs . to_dict ( ) , mapping ( geometry ) , antimeridian_cutting = antimeridian_cutting ) ) return _repair ( out_geom ) if validity_check else out_geom if src_crs == dst_crs or geometry . is_empty : return _repair ( geometry ) elif ( dst_crs . is_epsg_code and dst_crs . get ( "init" ) in CRS_BOUNDS and dst_crs . get ( "init" ) != "epsg:4326" ) : wgs84_crs = CRS ( ) . from_epsg ( 4326 ) crs_bbox = box ( * CRS_BOUNDS [ dst_crs . get ( "init" ) ] ) geometry_4326 = _reproject_geom ( geometry , src_crs , wgs84_crs ) if error_on_clip and not geometry_4326 . within ( crs_bbox ) : raise RuntimeError ( "geometry outside target CRS bounds" ) return _reproject_geom ( crs_bbox . intersection ( geometry_4326 ) , wgs84_crs , dst_crs ) else : return _reproject_geom ( geometry , src_crs , dst_crs )
4608	def nolist ( self , account ) : assert callable ( self . blockchain . account_whitelist ) return self . blockchain . account_whitelist ( account , lists = [ ] , account = self )
6095	def voronoi_regular_to_pix_from_grids_and_geometry ( regular_grid , regular_to_nearest_pix , pixel_centres , pixel_neighbors , pixel_neighbors_size ) : regular_to_pix = np . zeros ( ( regular_grid . shape [ 0 ] ) ) for regular_index in range ( regular_grid . shape [ 0 ] ) : nearest_pix_pixel_index = regular_to_nearest_pix [ regular_index ] while True : nearest_pix_pixel_center = pixel_centres [ nearest_pix_pixel_index ] sub_to_nearest_pix_distance = ( regular_grid [ regular_index , 0 ] - nearest_pix_pixel_center [ 0 ] ) ** 2 + ( regular_grid [ regular_index , 1 ] - nearest_pix_pixel_center [ 1 ] ) ** 2 closest_separation_from_pix_neighbor = 1.0e8 for neighbor_index in range ( pixel_neighbors_size [ nearest_pix_pixel_index ] ) : neighbor = pixel_neighbors [ nearest_pix_pixel_index , neighbor_index ] separation_from_neighbor = ( regular_grid [ regular_index , 0 ] - pixel_centres [ neighbor , 0 ] ) ** 2 + ( regular_grid [ regular_index , 1 ] - pixel_centres [ neighbor , 1 ] ) ** 2 if separation_from_neighbor < closest_separation_from_pix_neighbor : closest_separation_from_pix_neighbor = separation_from_neighbor closest_neighbor_index = neighbor_index neighboring_pix_pixel_index = pixel_neighbors [ nearest_pix_pixel_index , closest_neighbor_index ] sub_to_neighboring_pix_distance = closest_separation_from_pix_neighbor if sub_to_nearest_pix_distance <= sub_to_neighboring_pix_distance : regular_to_pix [ regular_index ] = nearest_pix_pixel_index break else : nearest_pix_pixel_index = neighboring_pix_pixel_index return regular_to_pix
4804	def when_called_with ( self , * some_args , ** some_kwargs ) : if not self . expected : raise TypeError ( 'expected exception not set, raises() must be called first' ) try : self . val ( * some_args , ** some_kwargs ) except BaseException as e : if issubclass ( type ( e ) , self . expected ) : return AssertionBuilder ( str ( e ) , self . description , self . kind ) else : self . _err ( 'Expected <%s> to raise <%s> when called with (%s), but raised <%s>.' % ( self . val . __name__ , self . expected . __name__ , self . _fmt_args_kwargs ( * some_args , ** some_kwargs ) , type ( e ) . __name__ ) ) self . _err ( 'Expected <%s> to raise <%s> when called with (%s).' % ( self . val . __name__ , self . expected . __name__ , self . _fmt_args_kwargs ( * some_args , ** some_kwargs ) ) )
12731	def create_body ( self , shape , name = None , ** kwargs ) : shape = shape . lower ( ) if name is None : for i in range ( 1 + len ( self . _bodies ) ) : name = '{}{}' . format ( shape , i ) if name not in self . _bodies : break self . _bodies [ name ] = Body . build ( shape , name , self , ** kwargs ) return self . _bodies [ name ]
12841	def _close ( self , conn ) : super ( PooledAIODatabase , self ) . _close ( conn ) for waiter in self . _waiters : if not waiter . done ( ) : logger . debug ( 'Release a waiter' ) waiter . set_result ( True ) break
6345	def stem ( self , word ) : terminate = False intact = True while not terminate : for n in range ( 6 , 0 , - 1 ) : if word [ - n : ] in self . _rule_table [ n ] : accept = False if len ( self . _rule_table [ n ] [ word [ - n : ] ] ) < 4 : for rule in self . _rule_table [ n ] [ word [ - n : ] ] : ( word , accept , intact , terminate , ) = self . _apply_rule ( word , rule , intact , terminate ) if accept : break else : rule = self . _rule_table [ n ] [ word [ - n : ] ] ( word , accept , intact , terminate ) = self . _apply_rule ( word , rule , intact , terminate ) if accept : break else : break return word
2649	def monitor ( pid , task_id , monitoring_hub_url , run_id , sleep_dur = 10 ) : import psutil radio = UDPRadio ( monitoring_hub_url , source_id = task_id ) simple = [ "cpu_num" , 'cpu_percent' , 'create_time' , 'cwd' , 'exe' , 'memory_percent' , 'nice' , 'name' , 'num_threads' , 'pid' , 'ppid' , 'status' , 'username' ] summable_values = [ 'cpu_percent' , 'memory_percent' , 'num_threads' ] pm = psutil . Process ( pid ) pm . cpu_percent ( ) first_msg = True while True : try : d = { "psutil_process_" + str ( k ) : v for k , v in pm . as_dict ( ) . items ( ) if k in simple } d [ "run_id" ] = run_id d [ "task_id" ] = task_id d [ 'resource_monitoring_interval' ] = sleep_dur d [ 'first_msg' ] = first_msg d [ 'timestamp' ] = datetime . datetime . now ( ) children = pm . children ( recursive = True ) d [ "psutil_cpu_count" ] = psutil . cpu_count ( ) d [ 'psutil_process_memory_virtual' ] = pm . memory_info ( ) . vms d [ 'psutil_process_memory_resident' ] = pm . memory_info ( ) . rss d [ 'psutil_process_time_user' ] = pm . cpu_times ( ) . user d [ 'psutil_process_time_system' ] = pm . cpu_times ( ) . system d [ 'psutil_process_children_count' ] = len ( children ) try : d [ 'psutil_process_disk_write' ] = pm . io_counters ( ) . write_bytes d [ 'psutil_process_disk_read' ] = pm . io_counters ( ) . read_bytes except psutil . _exceptions . AccessDenied : d [ 'psutil_process_disk_write' ] = 0 d [ 'psutil_process_disk_read' ] = 0 for child in children : for k , v in child . as_dict ( attrs = summable_values ) . items ( ) : d [ 'psutil_process_' + str ( k ) ] += v d [ 'psutil_process_time_user' ] += child . cpu_times ( ) . user d [ 'psutil_process_time_system' ] += child . cpu_times ( ) . system d [ 'psutil_process_memory_virtual' ] += child . memory_info ( ) . vms d [ 'psutil_process_memory_resident' ] += child . memory_info ( ) . rss try : d [ 'psutil_process_disk_write' ] += child . io_counters ( ) . write_bytes d [ 'psutil_process_disk_read' ] += child . io_counters ( ) . read_bytes except psutil . _exceptions . AccessDenied : d [ 'psutil_process_disk_write' ] += 0 d [ 'psutil_process_disk_read' ] += 0 finally : radio . send ( MessageType . TASK_INFO , task_id , d ) time . sleep ( sleep_dur ) first_msg = False
6702	def exists ( self , name ) : with self . settings ( hide ( 'running' , 'stdout' , 'warnings' ) , warn_only = True ) : return self . run ( 'getent group %(name)s' % locals ( ) ) . succeeded
478	def word_to_id ( self , word ) : if word in self . _vocab : return self . _vocab [ word ] else : return self . _unk_id
7474	def write_to_fullarr ( data , sample , sidx ) : LOGGER . info ( "writing fullarr %s %s" , sample . name , sidx ) with h5py . File ( data . clust_database , 'r+' ) as io5 : chunk = io5 [ "catgs" ] . attrs [ "chunksize" ] [ 0 ] catg = io5 [ "catgs" ] nall = io5 [ "nalleles" ] smpio = os . path . join ( data . dirs . across , sample . name + '.tmp.h5' ) with h5py . File ( smpio ) as indat : newcatg = indat [ "icatg" ] onall = indat [ "inall" ] for cidx in xrange ( 0 , catg . shape [ 0 ] , chunk ) : end = cidx + chunk catg [ cidx : end , sidx : sidx + 1 , : ] = np . expand_dims ( newcatg [ cidx : end , : ] , axis = 1 ) nall [ : , sidx : sidx + 1 ] = np . expand_dims ( onall , axis = 1 )
1430	def run ( command , parser , cl_args , unknown_args ) : Log . debug ( "Update Args: %s" , cl_args ) extra_lib_jars = jars . packing_jars ( ) action = "update topology%s" % ( ' in dry-run mode' if cl_args [ "dry_run" ] else '' ) dict_extra_args = { } try : dict_extra_args = build_extra_args_dict ( cl_args ) except Exception as err : return SimpleResult ( Status . InvocationError , err . message ) if cl_args [ 'deploy_mode' ] == config . SERVER_MODE : return cli_helper . run_server ( command , cl_args , action , dict_extra_args ) else : list_extra_args = convert_args_dict_to_list ( dict_extra_args ) return cli_helper . run_direct ( command , cl_args , action , list_extra_args , extra_lib_jars )
6628	def unpublish ( namespace , name , version , registry = None ) : registry = registry or Registry_Base_URL url = '%s/%s/%s/versions/%s' % ( registry , namespace , name , version ) headers = _headersForRegistry ( registry ) response = requests . delete ( url , headers = headers ) response . raise_for_status ( ) return None
5373	def file_exists ( file_path , credentials = None ) : if file_path . startswith ( 'gs://' ) : return _file_exists_in_gcs ( file_path , credentials ) else : return os . path . isfile ( file_path )
9223	def away_from_zero_round ( value , ndigits = 0 ) : if sys . version_info [ 0 ] >= 3 : p = 10 ** ndigits return float ( math . floor ( ( value * p ) + math . copysign ( 0.5 , value ) ) ) / p else : return round ( value , ndigits )
5284	def formset_valid ( self , formset ) : self . object_list = formset . save ( ) return super ( ModelFormSetMixin , self ) . formset_valid ( formset )
11022	def get_node ( self , string_key ) : pos = self . get_node_pos ( string_key ) if pos is None : return None return self . ring [ self . _sorted_keys [ pos ] ]
12525	def condor_call ( cmd , shell = True ) : log . info ( cmd ) ret = condor_submit ( cmd ) if ret != 0 : subprocess . call ( cmd , shell = shell )
3947	def decode ( message , pblite , ignore_first_item = False ) : if not isinstance ( pblite , list ) : logger . warning ( 'Ignoring invalid message: expected list, got %r' , type ( pblite ) ) return if ignore_first_item : pblite = pblite [ 1 : ] if pblite and isinstance ( pblite [ - 1 ] , dict ) : extra_fields = { int ( field_number ) : value for field_number , value in pblite [ - 1 ] . items ( ) } pblite = pblite [ : - 1 ] else : extra_fields = { } fields_values = itertools . chain ( enumerate ( pblite , start = 1 ) , extra_fields . items ( ) ) for field_number , value in fields_values : if value is None : continue try : field = message . DESCRIPTOR . fields_by_number [ field_number ] except KeyError : if value not in [ [ ] , '' , 0 ] : logger . debug ( 'Message %r contains unknown field %s with value ' '%r' , message . __class__ . __name__ , field_number , value ) continue if field . label == FieldDescriptor . LABEL_REPEATED : _decode_repeated_field ( message , field , value ) else : _decode_field ( message , field , value )
9100	def write_bel_namespace_mappings ( self , file : TextIO , ** kwargs ) -> None : json . dump ( self . _get_namespace_identifier_to_name ( ** kwargs ) , file , indent = 2 , sort_keys = True )
9155	def scaling ( self , x , y ) : self . drawer . append ( pgmagick . DrawableScaling ( float ( x ) , float ( y ) ) )
3346	def guess_mime_type ( url ) : ( mimetype , _mimeencoding ) = mimetypes . guess_type ( url ) if not mimetype : ext = os . path . splitext ( url ) [ 1 ] mimetype = _MIME_TYPES . get ( ext ) _logger . debug ( "mimetype({}): {}" . format ( url , mimetype ) ) if not mimetype : mimetype = "application/octet-stream" return mimetype
6639	def runScript ( self , scriptname , additional_environment = None ) : import subprocess import shlex command = self . getScript ( scriptname ) if command is None : logger . debug ( '%s has no script %s' , self , scriptname ) return 0 if not len ( command ) : logger . error ( "script %s of %s is empty" , scriptname , self . getName ( ) ) return 1 env = os . environ . copy ( ) if additional_environment is not None : env . update ( additional_environment ) errcode = 0 child = None try : logger . debug ( 'running script: %s' , command ) child = subprocess . Popen ( command , cwd = self . path , env = env ) child . wait ( ) if child . returncode : logger . error ( "script %s (from %s) exited with non-zero status %s" , scriptname , self . getName ( ) , child . returncode ) errcode = child . returncode child = None finally : if child is not None : tryTerminate ( child ) return errcode
2830	def convert_upsample ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting upsample...' ) if params [ 'mode' ] != 'nearest' : raise AssertionError ( 'Cannot convert non-nearest upsampling' ) if names == 'short' : tf_name = 'UPSL' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) if 'height_scale' in params : scale = ( params [ 'height_scale' ] , params [ 'width_scale' ] ) elif len ( inputs ) == 2 : scale = layers [ inputs [ - 1 ] + '_np' ] [ - 2 : ] upsampling = keras . layers . UpSampling2D ( size = scale , name = tf_name ) layers [ scope_name ] = upsampling ( layers [ inputs [ 0 ] ] )
5342	def __get_dash_menu ( self , kibiter_major ) : omenu = [ ] omenu . append ( self . menu_panels_common [ 'Overview' ] ) ds_menu = self . __get_menu_entries ( kibiter_major ) kafka_menu = None community_menu = None found_kafka = [ pos for pos , menu in enumerate ( ds_menu ) if menu [ 'name' ] == KAFKA_NAME ] if found_kafka : kafka_menu = ds_menu . pop ( found_kafka [ 0 ] ) found_community = [ pos for pos , menu in enumerate ( ds_menu ) if menu [ 'name' ] == COMMUNITY_NAME ] if found_community : community_menu = ds_menu . pop ( found_community [ 0 ] ) ds_menu . sort ( key = operator . itemgetter ( 'name' ) ) omenu += ds_menu if kafka_menu : omenu . append ( kafka_menu ) if community_menu : omenu . append ( community_menu ) omenu . append ( self . menu_panels_common [ 'Data Status' ] ) omenu . append ( self . menu_panels_common [ 'About' ] ) logger . debug ( "Menu for panels: %s" , json . dumps ( ds_menu , indent = 4 ) ) return omenu
4320	def set_globals ( self , dither = False , guard = False , multithread = False , replay_gain = False , verbosity = 2 ) : if not isinstance ( dither , bool ) : raise ValueError ( 'dither must be a boolean.' ) if not isinstance ( guard , bool ) : raise ValueError ( 'guard must be a boolean.' ) if not isinstance ( multithread , bool ) : raise ValueError ( 'multithread must be a boolean.' ) if not isinstance ( replay_gain , bool ) : raise ValueError ( 'replay_gain must be a boolean.' ) if verbosity not in VERBOSITY_VALS : raise ValueError ( 'Invalid value for VERBOSITY. Must be one {}' . format ( VERBOSITY_VALS ) ) global_args = [ ] if not dither : global_args . append ( '-D' ) if guard : global_args . append ( '-G' ) if multithread : global_args . append ( '--multi-threaded' ) if replay_gain : global_args . append ( '--replay-gain' ) global_args . append ( 'track' ) global_args . append ( '-V{}' . format ( verbosity ) ) self . globals = global_args return self
13089	def ensure_remote_branch_is_tracked ( branch ) : if branch == MASTER_BRANCH : return output = subprocess . check_output ( [ 'git' , 'branch' , '--list' ] ) for line in output . split ( '\n' ) : if line . strip ( ) == branch : break else : try : sys . stdout . write ( subprocess . check_output ( [ 'git' , 'checkout' , '--track' , 'origin/%s' % branch ] ) ) except subprocess . CalledProcessError : raise SystemExit ( 1 )
13123	def argparser ( self ) : core_parser = self . core_parser core_parser . add_argument ( '-r' , '--range' , type = str , help = "The range to search for use" ) return core_parser
9064	def fit ( self , verbose = True ) : if not self . _isfixed ( "logistic" ) : self . _maximize_scalar ( desc = "LMM" , rtol = 1e-6 , atol = 1e-6 , verbose = verbose ) if not self . _fix [ "beta" ] : self . _update_beta ( ) if not self . _fix [ "scale" ] : self . _update_scale ( )
749	def _removeUnlikelyPredictions ( cls , likelihoodsDict , minLikelihoodThreshold , maxPredictionsPerStep ) : maxVal = ( None , None ) for ( k , v ) in likelihoodsDict . items ( ) : if len ( likelihoodsDict ) <= 1 : break if maxVal [ 0 ] is None or v >= maxVal [ 1 ] : if maxVal [ 0 ] is not None and maxVal [ 1 ] < minLikelihoodThreshold : del likelihoodsDict [ maxVal [ 0 ] ] maxVal = ( k , v ) elif v < minLikelihoodThreshold : del likelihoodsDict [ k ] likelihoodsDict = dict ( sorted ( likelihoodsDict . iteritems ( ) , key = itemgetter ( 1 ) , reverse = True ) [ : maxPredictionsPerStep ] ) return likelihoodsDict
396	def choice_action_by_probs ( probs = ( 0.5 , 0.5 ) , action_list = None ) : if action_list is None : n_action = len ( probs ) action_list = np . arange ( n_action ) else : if len ( action_list ) != len ( probs ) : raise Exception ( "number of actions should equal to number of probabilities." ) return np . random . choice ( action_list , p = probs )
5251	def _init_services ( self ) : logger = _get_logger ( self . debug ) opened = self . _session . openService ( '//blp/refdata' ) ev = self . _session . nextEvent ( ) ev_name = _EVENT_DICT [ ev . eventType ( ) ] logger . info ( 'Event Type: {!r}' . format ( ev_name ) ) for msg in ev : logger . info ( 'Message Received:\n{}' . format ( msg ) ) if ev . eventType ( ) != blpapi . Event . SERVICE_STATUS : raise RuntimeError ( 'Expected a "SERVICE_STATUS" event but ' 'received a {!r}' . format ( ev_name ) ) if not opened : logger . warning ( 'Failed to open //blp/refdata' ) raise ConnectionError ( 'Could not open a //blp/refdata service' ) self . refDataService = self . _session . getService ( '//blp/refdata' ) opened = self . _session . openService ( '//blp/exrsvc' ) ev = self . _session . nextEvent ( ) ev_name = _EVENT_DICT [ ev . eventType ( ) ] logger . info ( 'Event Type: {!r}' . format ( ev_name ) ) for msg in ev : logger . info ( 'Message Received:\n{}' . format ( msg ) ) if ev . eventType ( ) != blpapi . Event . SERVICE_STATUS : raise RuntimeError ( 'Expected a "SERVICE_STATUS" event but ' 'received a {!r}' . format ( ev_name ) ) if not opened : logger . warning ( 'Failed to open //blp/exrsvc' ) raise ConnectionError ( 'Could not open a //blp/exrsvc service' ) self . exrService = self . _session . getService ( '//blp/exrsvc' ) return self
6725	def get_or_create ( name = None , group = None , config = None , extra = 0 , verbose = 0 , backend_opts = None ) : require ( 'vm_type' , 'vm_group' ) backend_opts = backend_opts or { } verbose = int ( verbose ) extra = int ( extra ) if config : config_fn = common . find_template ( config ) config = yaml . load ( open ( config_fn ) ) env . update ( config ) env . vm_type = ( env . vm_type or '' ) . lower ( ) assert env . vm_type , 'No VM type specified.' group = group or env . vm_group assert group , 'No VM group specified.' ret = exists ( name = name , group = group ) if not extra and ret : if verbose : print ( 'VM %s:%s exists.' % ( name , group ) ) return ret today = datetime . date . today ( ) release = int ( '%i%02i%02i' % ( today . year , today . month , today . day ) ) if not name : existing_instances = list_instances ( group = group , release = release , verbose = verbose ) name = env . vm_name_template . format ( index = len ( existing_instances ) + 1 ) if env . vm_type == EC2 : return get_or_create_ec2_instance ( name = name , group = group , release = release , verbose = verbose , backend_opts = backend_opts ) else : raise NotImplementedError
11812	def present ( self , results ) : "Present the results as a list." for ( score , d ) in results : doc = self . documents [ d ] print ( "%5.2f|%25s | %s" % ( 100 * score , doc . url , doc . title [ : 45 ] . expandtabs ( ) ) )
19	def sf01 ( arr ) : s = arr . shape return arr . swapaxes ( 0 , 1 ) . reshape ( s [ 0 ] * s [ 1 ] , * s [ 2 : ] )
11094	def n_dir ( self ) : self . assert_is_dir_and_exists ( ) n = 0 for _ in self . select_dir ( recursive = True ) : n += 1 return n
5094	def refresh_robots ( self ) : resp = requests . get ( urljoin ( self . ENDPOINT , 'dashboard' ) , headers = self . _headers ) resp . raise_for_status ( ) for robot in resp . json ( ) [ 'robots' ] : if robot [ 'mac_address' ] is None : continue try : self . _robots . add ( Robot ( name = robot [ 'name' ] , serial = robot [ 'serial' ] , secret = robot [ 'secret_key' ] , traits = robot [ 'traits' ] , endpoint = robot [ 'nucleo_url' ] ) ) except requests . exceptions . HTTPError : print ( "Your '{}' robot is offline." . format ( robot [ 'name' ] ) ) continue self . refresh_persistent_maps ( ) for robot in self . _robots : robot . has_persistent_maps = robot . serial in self . _persistent_maps
12032	def average ( self , t1 = 0 , t2 = None , setsweep = False ) : if setsweep : self . setsweep ( setsweep ) if t2 is None or t2 > self . sweepLength : t2 = self . sweepLength self . log . debug ( "resetting t2 to [%f]" , t2 ) t1 = max ( t1 , 0 ) if t1 > t2 : self . log . error ( "t1 cannot be larger than t2" ) return False I1 , I2 = int ( t1 * self . pointsPerSec ) , int ( t2 * self . pointsPerSec ) if I1 == I2 : return np . nan return np . average ( self . sweepY [ I1 : I2 ] )
13677	def prepare ( self ) : result_files = self . collect_files ( ) chain = self . prepare_handlers_chain if chain is None : chain = [ LessCompilerPrepareHandler ( ) ] for prepare_handler in chain : result_files = prepare_handler . prepare ( result_files , self ) return result_files
453	def get_variables_with_name ( name = None , train_only = True , verbose = False ) : if name is None : raise Exception ( "please input a name" ) logging . info ( " [*] geting variables with %s" % name ) if train_only : t_vars = tf . trainable_variables ( ) else : t_vars = tf . global_variables ( ) d_vars = [ var for var in t_vars if name in var . name ] if verbose : for idx , v in enumerate ( d_vars ) : logging . info ( " got {:3}: {:15} {}" . format ( idx , v . name , str ( v . get_shape ( ) ) ) ) return d_vars
1917	def fork ( self , state , expression , policy = 'ALL' , setstate = None ) : assert isinstance ( expression , Expression ) if setstate is None : setstate = lambda x , y : None solutions = state . concretize ( expression , policy ) if not solutions : raise ExecutorError ( "Forking on unfeasible constraint set" ) if len ( solutions ) == 1 : setstate ( state , solutions [ 0 ] ) return state logger . info ( "Forking. Policy: %s. Values: %s" , policy , ', ' . join ( f'0x{sol:x}' for sol in solutions ) ) self . _publish ( 'will_fork_state' , state , expression , solutions , policy ) children = [ ] for new_value in solutions : with state as new_state : new_state . constrain ( expression == new_value ) setstate ( new_state , new_value ) self . _publish ( 'did_fork_state' , new_state , expression , new_value , policy ) state_id = self . enqueue ( new_state ) children . append ( state_id ) logger . info ( "Forking current state into states %r" , children ) return None
9882	def alpha ( reliability_data = None , value_counts = None , value_domain = None , level_of_measurement = 'interval' , dtype = np . float64 ) : if ( reliability_data is None ) == ( value_counts is None ) : raise ValueError ( "Either reliability_data or value_counts must be provided, but not both." ) if value_counts is None : if type ( reliability_data ) is not np . ndarray : reliability_data = np . array ( reliability_data ) value_domain = value_domain or np . unique ( reliability_data [ ~ np . isnan ( reliability_data ) ] ) value_counts = _reliability_data_to_value_counts ( reliability_data , value_domain ) else : if value_domain : assert value_counts . shape [ 1 ] == len ( value_domain ) , "The value domain should be equal to the number of columns of value_counts." else : value_domain = tuple ( range ( value_counts . shape [ 1 ] ) ) distance_metric = _distance_metric ( level_of_measurement ) o = _coincidences ( value_counts , value_domain , dtype = dtype ) n_v = np . sum ( o , axis = 0 ) n = np . sum ( n_v ) e = _random_coincidences ( value_domain , n , n_v ) d = _distances ( value_domain , distance_metric , n_v ) return 1 - np . sum ( o * d ) / np . sum ( e * d )
10952	def build_funcs ( self ) : def m ( inds = None , slicer = None , flat = True ) : return sample ( self . model , inds = inds , slicer = slicer , flat = flat ) . copy ( ) def r ( inds = None , slicer = None , flat = True ) : return sample ( self . residuals , inds = inds , slicer = slicer , flat = flat ) . copy ( ) def l ( ) : return self . loglikelihood def r_e ( ** kwargs ) : return r ( ** kwargs ) , np . copy ( self . error ) def m_e ( ** kwargs ) : return m ( ** kwargs ) , np . copy ( self . error ) self . fisherinformation = partial ( self . _jtj , funct = m ) self . gradloglikelihood = partial ( self . _grad , funct = l ) self . hessloglikelihood = partial ( self . _hess , funct = l ) self . gradmodel = partial ( self . _grad , funct = m ) self . hessmodel = partial ( self . _hess , funct = m ) self . JTJ = partial ( self . _jtj , funct = r ) self . J = partial ( self . _grad , funct = r ) self . J_e = partial ( self . _grad , funct = r_e , nout = 2 ) self . gradmodel_e = partial ( self . _grad , funct = m_e , nout = 2 ) self . fisherinformation . __doc__ = _graddoc + _sampledoc self . gradloglikelihood . __doc__ = _graddoc self . hessloglikelihood . __doc__ = _graddoc self . gradmodel . __doc__ = _graddoc + _sampledoc self . hessmodel . __doc__ = _graddoc + _sampledoc self . JTJ . __doc__ = _graddoc + _sampledoc self . J . __doc__ = _graddoc + _sampledoc self . _dograddoc ( self . _grad_one_param ) self . _dograddoc ( self . _hess_two_param ) self . _dograddoc ( self . _grad ) self . _dograddoc ( self . _hess ) class _Statewrap ( object ) : def __init__ ( self , obj ) : self . obj = obj def __getitem__ ( self , d = None ) : if d is None : d = self . obj . params return util . delistify ( self . obj . get_values ( d ) , d ) self . state = _Statewrap ( self )
4133	def codestr2rst ( codestr , lang = 'python' ) : code_directive = "\n.. code-block:: {0}\n\n" . format ( lang ) indented_block = indent ( codestr , ' ' * 4 ) return code_directive + indented_block
5711	def get_descriptor_base_path ( descriptor ) : if isinstance ( descriptor , six . string_types ) : if os . path . exists ( descriptor ) : base_path = os . path . dirname ( os . path . abspath ( descriptor ) ) else : base_path = os . path . dirname ( descriptor ) else : base_path = '.' return base_path
863	def Enum ( * args , ** kwargs ) : def getLabel ( cls , val ) : return cls . __labels [ val ] def validate ( cls , val ) : return val in cls . __values def getValues ( cls ) : return list ( cls . __values ) def getLabels ( cls ) : return list ( cls . __labels . values ( ) ) def getValue ( cls , label ) : return cls . __labels [ label ] for arg in list ( args ) + kwargs . keys ( ) : if type ( arg ) is not str : raise TypeError ( "Enum arg {0} must be a string" . format ( arg ) ) if not __isidentifier ( arg ) : raise ValueError ( "Invalid enum value '{0}'. " "'{0}' is not a valid identifier" . format ( arg ) ) kwargs . update ( zip ( args , args ) ) newType = type ( "Enum" , ( object , ) , kwargs ) newType . __labels = dict ( ( v , k ) for k , v in kwargs . iteritems ( ) ) newType . __values = set ( newType . __labels . keys ( ) ) newType . getLabel = functools . partial ( getLabel , newType ) newType . validate = functools . partial ( validate , newType ) newType . getValues = functools . partial ( getValues , newType ) newType . getLabels = functools . partial ( getLabels , newType ) newType . getValue = functools . partial ( getValue , newType ) return newType
8184	def update ( self , iterations = 10 ) : self . alpha += 0.05 self . alpha = min ( self . alpha , 1.0 ) if self . layout . i == 0 : self . layout . prepare ( ) self . layout . i += 1 elif self . layout . i == 1 : self . layout . iterate ( ) elif self . layout . i < self . layout . n : n = min ( iterations , self . layout . i / 10 + 1 ) for i in range ( n ) : self . layout . iterate ( ) min_ , max = self . layout . bounds self . x = _ctx . WIDTH - max . x * self . d - min_ . x * self . d self . y = _ctx . HEIGHT - max . y * self . d - min_ . y * self . d self . x /= 2 self . y /= 2 return not self . layout . done
795	def getActiveJobCountForClientKey ( self , clientKey ) : with ConnectionFactory . get ( ) as conn : query = 'SELECT count(job_id) ' 'FROM %s ' 'WHERE client_key = %%s ' ' AND status != %%s' % self . jobsTableName conn . cursor . execute ( query , [ clientKey , self . STATUS_COMPLETED ] ) activeJobCount = conn . cursor . fetchone ( ) [ 0 ] return activeJobCount
13170	def path ( self , include_root = False ) : path = '%s[%d]' % ( self . tagname , self . index or 0 ) p = self . parent while p is not None : if p . parent or include_root : path = '%s[%d]/%s' % ( p . tagname , p . index or 0 , path ) p = p . parent return path
13538	def get_location ( self , location_id ) : url = "/2/locations/%s" % location_id return self . location_from_json ( self . _get_resource ( url ) [ "location" ] )
7365	def run_command ( args , ** kwargs ) : assert len ( args ) > 0 start_time = time . time ( ) process = AsyncProcess ( args , ** kwargs ) process . wait ( ) elapsed_time = time . time ( ) - start_time logger . info ( "%s took %0.4f seconds" , args [ 0 ] , elapsed_time )
8851	def open_file ( self , path , line = None ) : editor = None if path : interpreter , pyserver , args = self . _get_backend_parameters ( ) editor = self . tabWidget . open_document ( path , None , interpreter = interpreter , server_script = pyserver , args = args ) if editor : self . setup_editor ( editor ) self . recent_files_manager . open_file ( path ) self . menu_recents . update_actions ( ) if line is not None : TextHelper ( self . tabWidget . current_widget ( ) ) . goto_line ( line ) return editor
4102	def generate_gallery_rst ( app ) : try : plot_gallery = eval ( app . builder . config . plot_gallery ) except TypeError : plot_gallery = bool ( app . builder . config . plot_gallery ) gallery_conf . update ( app . config . sphinx_gallery_conf ) gallery_conf . update ( plot_gallery = plot_gallery ) gallery_conf . update ( abort_on_example_error = app . builder . config . abort_on_example_error ) app . config . sphinx_gallery_conf = gallery_conf app . config . html_static_path . append ( glr_path_static ( ) ) clean_gallery_out ( app . builder . outdir ) examples_dirs = gallery_conf [ 'examples_dirs' ] gallery_dirs = gallery_conf [ 'gallery_dirs' ] if not isinstance ( examples_dirs , list ) : examples_dirs = [ examples_dirs ] if not isinstance ( gallery_dirs , list ) : gallery_dirs = [ gallery_dirs ] mod_examples_dir = os . path . relpath ( gallery_conf [ 'mod_example_dir' ] , app . builder . srcdir ) seen_backrefs = set ( ) for examples_dir , gallery_dir in zip ( examples_dirs , gallery_dirs ) : examples_dir = os . path . relpath ( examples_dir , app . builder . srcdir ) gallery_dir = os . path . relpath ( gallery_dir , app . builder . srcdir ) for workdir in [ examples_dir , gallery_dir , mod_examples_dir ] : if not os . path . exists ( workdir ) : os . makedirs ( workdir ) fhindex = open ( os . path . join ( gallery_dir , 'index.rst' ) , 'w' ) fhindex . write ( generate_dir_rst ( examples_dir , gallery_dir , gallery_conf , seen_backrefs ) ) for directory in sorted ( os . listdir ( examples_dir ) ) : if os . path . isdir ( os . path . join ( examples_dir , directory ) ) : src_dir = os . path . join ( examples_dir , directory ) target_dir = os . path . join ( gallery_dir , directory ) fhindex . write ( generate_dir_rst ( src_dir , target_dir , gallery_conf , seen_backrefs ) ) fhindex . flush ( )
3700	def solubility_parameter ( T = 298.15 , Hvapm = None , Vml = None , CASRN = '' , AvailableMethods = False , Method = None ) : r def list_methods ( ) : methods = [ ] if T and Hvapm and Vml : methods . append ( DEFINITION ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == DEFINITION : if ( not Hvapm ) or ( not T ) or ( not Vml ) : delta = None else : if Hvapm < R * T or Vml < 0 : delta = None else : delta = ( ( Hvapm - R * T ) / Vml ) ** 0.5 elif Method == NONE : delta = None else : raise Exception ( 'Failure in in function' ) return delta
13712	def invalidate_cache ( self ) : if self . _use_cache : self . _cache_version += 1 self . _cache . increment ( 'cached_httpbl_{0}_version' . format ( self . _api_key ) )
5922	def create ( logger_name , logfile = 'gromacs.log' ) : logger = logging . getLogger ( logger_name ) logger . setLevel ( logging . DEBUG ) logfile = logging . FileHandler ( logfile ) logfile_formatter = logging . Formatter ( '%(asctime)s %(name)-12s %(levelname)-8s %(message)s' ) logfile . setFormatter ( logfile_formatter ) logger . addHandler ( logfile ) console = logging . StreamHandler ( ) console . setLevel ( logging . INFO ) formatter = logging . Formatter ( '%(name)-12s: %(levelname)-8s %(message)s' ) console . setFormatter ( formatter ) logger . addHandler ( console ) return logger
13090	def main ( branch ) : try : output = subprocess . check_output ( [ 'git' , 'rev-parse' ] ) . decode ( 'utf-8' ) sys . stdout . write ( output ) except subprocess . CalledProcessError : return ensure_remote_branch_is_tracked ( branch ) subprocess . check_call ( [ 'git' , 'checkout' , '--quiet' , branch ] ) subprocess . check_call ( [ 'git' , 'pull' , '--quiet' ] ) subprocess . check_call ( [ 'git' , 'checkout' , '--quiet' , '%s~0' % branch ] ) subprocess . check_call ( [ 'find' , '.' , '-name' , '"*.pyc"' , '-delete' ] ) print ( 'Your branch is up to date with branch \'origin/%s\'.' % branch )
3633	def cardInfo ( self , resource_id ) : base_id = baseId ( resource_id ) if base_id in self . players : return self . players [ base_id ] else : url = '{0}{1}.json' . format ( card_info_url , base_id ) return requests . get ( url , timeout = self . timeout ) . json ( )
10232	def list_abundance_cartesian_expansion ( graph : BELGraph ) -> None : for u , v , k , d in list ( graph . edges ( keys = True , data = True ) ) : if CITATION not in d : continue if isinstance ( u , ListAbundance ) and isinstance ( v , ListAbundance ) : for u_member , v_member in itt . product ( u . members , v . members ) : graph . add_qualified_edge ( u_member , v_member , relation = d [ RELATION ] , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) elif isinstance ( u , ListAbundance ) : for member in u . members : graph . add_qualified_edge ( member , v , relation = d [ RELATION ] , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) elif isinstance ( v , ListAbundance ) : for member in v . members : graph . add_qualified_edge ( u , member , relation = d [ RELATION ] , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) _remove_list_abundance_nodes ( graph )
4727	def s20_to_gen ( self , pugrp , punit , chunk , sectr ) : cmd = [ "nvm_addr s20_to_gen" , self . envs [ "DEV_PATH" ] , "%d %d %d %d" % ( pugrp , punit , chunk , sectr ) ] status , stdout , _ = cij . ssh . command ( cmd , shell = True ) if status : raise RuntimeError ( "cij.liblight.s20_to_gen: cmd fail" ) return int ( re . findall ( r"val: ([0-9a-fx]+)" , stdout ) [ 0 ] , 16 )
1869	def MOVZX ( cpu , op0 , op1 ) : op0 . write ( Operators . ZEXTEND ( op1 . read ( ) , op0 . size ) )
11963	def _dec_to_dot ( ip ) : first = int ( ( ip >> 24 ) & 255 ) second = int ( ( ip >> 16 ) & 255 ) third = int ( ( ip >> 8 ) & 255 ) fourth = int ( ip & 255 ) return '%d.%d.%d.%d' % ( first , second , third , fourth )
970	def _getEphemeralMembers ( self ) : e = BacktrackingTM . _getEphemeralMembers ( self ) if self . makeCells4Ephemeral : e . extend ( [ 'cells4' ] ) return e
3462	def double_reaction_deletion ( model , reaction_list1 = None , reaction_list2 = None , method = "fba" , solution = None , processes = None , ** kwargs ) : reaction_list1 , reaction_list2 = _element_lists ( model . reactions , reaction_list1 , reaction_list2 ) return _multi_deletion ( model , 'reaction' , element_lists = [ reaction_list1 , reaction_list2 ] , method = method , solution = solution , processes = processes , ** kwargs )
4309	def _validate_num_channels ( input_filepath_list , combine_type ) : channels = [ file_info . channels ( f ) for f in input_filepath_list ] if not core . all_equal ( channels ) : raise IOError ( "Input files do not have the same number of channels. The " "{} combine type requires that all files have the same " "number of channels" . format ( combine_type ) )
2541	def set_pkg_desc ( self , doc , text ) : self . assert_package_exists ( ) if not self . package_desc_set : self . package_desc_set = True doc . package . description = text else : raise CardinalityError ( 'Package::Description' )
9341	def MetaOrdered ( parallel , done , turnstile ) : class Ordered : def __init__ ( self , iterref ) : if parallel . master : done [ ... ] = 0 self . iterref = iterref parallel . barrier ( ) @ classmethod def abort ( self ) : turnstile . release ( ) def __enter__ ( self ) : while self . iterref != done : pass turnstile . acquire ( ) return self def __exit__ ( self , * args ) : done [ ... ] += 1 turnstile . release ( ) return Ordered
4520	def set ( self , ring , angle , color ) : pixel = self . angleToPixel ( angle , ring ) self . _set_base ( pixel , color )
352	def load_celebA_dataset ( path = 'data' ) : data_dir = 'celebA' filename , drive_id = "img_align_celeba.zip" , "0B7EVK8r0v71pZjFTYXZWM3FlRnM" save_path = os . path . join ( path , filename ) image_path = os . path . join ( path , data_dir ) if os . path . exists ( image_path ) : logging . info ( '[*] {} already exists' . format ( save_path ) ) else : exists_or_mkdir ( path ) download_file_from_google_drive ( drive_id , save_path ) zip_dir = '' with zipfile . ZipFile ( save_path ) as zf : zip_dir = zf . namelist ( ) [ 0 ] zf . extractall ( path ) os . remove ( save_path ) os . rename ( os . path . join ( path , zip_dir ) , image_path ) data_files = load_file_list ( path = image_path , regx = '\\.jpg' , printable = False ) for i , _v in enumerate ( data_files ) : data_files [ i ] = os . path . join ( image_path , data_files [ i ] ) return data_files
12395	def register ( self , method , args , kwargs ) : invoc = self . dump_invoc ( * args , ** kwargs ) self . registry . append ( ( invoc , method . __name__ ) )
7142	def balance ( self , unlocked = False ) : return self . _backend . balances ( account = self . index ) [ 1 if unlocked else 0 ]
4521	def get ( self , ring , angle ) : pixel = self . angleToPixel ( angle , ring ) return self . _get_base ( pixel )
10960	def scramble_positions ( p , delete_frac = 0.1 ) : probs = [ 1 - delete_frac , delete_frac ] m = np . random . choice ( [ True , False ] , p . shape [ 0 ] , p = probs ) jumble = np . random . randn ( m . sum ( ) , 3 ) return p [ m ] + jumble
5023	def get_integrated_channels ( self , options ) : channel_classes = self . get_channel_classes ( options . get ( 'channel' ) ) filter_kwargs = { 'active' : True , 'enterprise_customer__active' : True , } enterprise_customer = self . get_enterprise_customer ( options . get ( 'enterprise_customer' ) ) if enterprise_customer : filter_kwargs [ 'enterprise_customer' ] = enterprise_customer for channel_class in channel_classes : for integrated_channel in channel_class . objects . filter ( ** filter_kwargs ) : yield integrated_channel
5495	def make_aware ( dt ) : return dt if dt . tzinfo else dt . replace ( tzinfo = timezone . utc )
13546	def get_users ( self , params = { } ) : param_list = [ ( k , params [ k ] ) for k in sorted ( params ) ] url = "/2/users/?%s" % urlencode ( param_list ) data = self . _get_resource ( url ) users = [ ] for entry in data [ "users" ] : users . append ( self . user_from_json ( entry ) ) return users
1421	def load ( file_object ) : marshaller = JavaObjectUnmarshaller ( file_object ) marshaller . add_transformer ( DefaultObjectTransformer ( ) ) return marshaller . readObject ( )
12521	def to_file ( self , output_file , smooth_fwhm = 0 , outdtype = None ) : outmat , mask_indices , mask_shape = self . to_matrix ( smooth_fwhm , outdtype ) exporter = ExportData ( ) content = { 'data' : outmat , 'labels' : self . labels , 'mask_indices' : mask_indices , 'mask_shape' : mask_shape , } if self . others : content . update ( self . others ) log . debug ( 'Creating content in file {}.' . format ( output_file ) ) try : exporter . save_variables ( output_file , content ) except Exception as exc : raise Exception ( 'Error saving variables to file {}.' . format ( output_file ) ) from exc
11439	def _get_children_by_tag_name ( node , name ) : try : return [ child for child in node . childNodes if child . nodeName == name ] except TypeError : return [ ]
6789	def get_settings ( self , site = None , role = None ) : r = self . local_renderer _stdout = sys . stdout _stderr = sys . stderr if not self . verbose : sys . stdout = StringIO ( ) sys . stderr = StringIO ( ) try : sys . path . insert ( 0 , r . env . src_dir ) tmp_site = self . genv . SITE if site and site . endswith ( '_secure' ) : site = site [ : - 7 ] site = site or self . genv . SITE or self . genv . default_site self . set_site ( site ) tmp_role = self . genv . ROLE if role : self . set_role ( role ) try : if r . env . delete_module_with_prefixes : for name in sorted ( sys . modules ) : for prefix in r . env . delete_module_with_prefixes : if name . startswith ( prefix ) : if self . verbose : print ( 'Deleting module %s prior to re-import.' % name ) del sys . modules [ name ] break for name in list ( sys . modules ) : for s in r . env . delete_module_containing : if s in name : del sys . modules [ name ] break if r . env . settings_module in sys . modules : del sys . modules [ r . env . settings_module ] if 'django_settings_module' in r . genv : r . env . settings_module = r . genv . django_settings_module else : r . env . settings_module = r . env . settings_module or r . genv . dj_settings_module if self . verbose : print ( 'r.env.settings_module:' , r . env . settings_module , r . format ( r . env . settings_module ) ) module = import_module ( r . format ( r . env . settings_module ) ) if site : assert site == module . SITE , 'Unable to set SITE to "%s" Instead it is set to "%s".' % ( site , module . SITE ) import imp imp . reload ( module ) except ImportError as e : print ( 'Warning: Could not import settings for site "%s": %s' % ( site , e ) , file = _stdout ) traceback . print_exc ( file = _stdout ) return finally : if tmp_site : self . set_site ( tmp_site ) if tmp_role : self . set_role ( tmp_role ) finally : sys . stdout = _stdout sys . stderr = _stderr sys . path . remove ( r . env . src_dir ) return module
7658	def append_records ( self , records ) : for obs in records : if isinstance ( obs , Observation ) : self . append ( ** obs . _asdict ( ) ) else : self . append ( ** obs )
6087	def contribution_maps_1d_from_hyper_images_and_galaxies ( hyper_model_image_1d , hyper_galaxy_images_1d , hyper_galaxies , hyper_minimum_values ) : return list ( map ( lambda hyper_galaxy , hyper_galaxy_image_1d , hyper_minimum_value : hyper_galaxy . contributions_from_model_image_and_galaxy_image ( model_image = hyper_model_image_1d , galaxy_image = hyper_galaxy_image_1d , minimum_value = hyper_minimum_value ) , hyper_galaxies , hyper_galaxy_images_1d , hyper_minimum_values ) )
4122	def centerdc_2_twosided ( data ) : N = len ( data ) newpsd = np . concatenate ( ( data [ N // 2 : ] , ( cshift ( data [ 0 : N // 2 ] , - 1 ) ) ) ) return newpsd
12650	def is_fnmatch_regex ( string ) : is_regex = False regex_chars = [ '!' , '*' , '$' ] for c in regex_chars : if string . find ( c ) > - 1 : return True return is_regex
12514	def crop_img ( image , rtol = 1e-8 , copy = True ) : img = check_img ( image ) data = img . get_data ( ) infinity_norm = max ( - data . min ( ) , data . max ( ) ) passes_threshold = np . logical_or ( data < - rtol * infinity_norm , data > rtol * infinity_norm ) if data . ndim == 4 : passes_threshold = np . any ( passes_threshold , axis = - 1 ) coords = np . array ( np . where ( passes_threshold ) ) start = coords . min ( axis = 1 ) end = coords . max ( axis = 1 ) + 1 start = np . maximum ( start - 1 , 0 ) end = np . minimum ( end + 1 , data . shape [ : 3 ] ) slices = [ slice ( s , e ) for s , e in zip ( start , end ) ] return _crop_img_to ( img , slices , copy = copy )
13079	def render ( self , template , ** kwargs ) : kwargs [ "cache_key" ] = "%s" % kwargs [ "url" ] . values ( ) kwargs [ "lang" ] = self . get_locale ( ) kwargs [ "assets" ] = self . assets kwargs [ "main_collections" ] = self . main_collections ( kwargs [ "lang" ] ) kwargs [ "cache_active" ] = self . cache is not None kwargs [ "cache_time" ] = 0 kwargs [ "cache_key" ] , kwargs [ "cache_key_i18n" ] = self . make_cache_keys ( request . endpoint , kwargs [ "url" ] ) kwargs [ "template" ] = template for plugin in self . __plugins_render_views__ : kwargs . update ( plugin . render ( ** kwargs ) ) return render_template ( kwargs [ "template" ] , ** kwargs )
5008	def _create_session ( self ) : session = requests . Session ( ) session . timeout = self . SESSION_TIMEOUT oauth_access_token , expires_at = SAPSuccessFactorsAPIClient . get_oauth_access_token ( self . enterprise_configuration . sapsf_base_url , self . enterprise_configuration . key , self . enterprise_configuration . secret , self . enterprise_configuration . sapsf_company_id , self . enterprise_configuration . sapsf_user_id , self . enterprise_configuration . user_type ) session . headers [ 'Authorization' ] = 'Bearer {}' . format ( oauth_access_token ) session . headers [ 'content-type' ] = 'application/json' self . session = session self . expires_at = expires_at
2206	def compressuser ( path , home = '~' ) : path = normpath ( path ) userhome_dpath = userhome ( ) if path . startswith ( userhome_dpath ) : if len ( path ) == len ( userhome_dpath ) : path = home elif path [ len ( userhome_dpath ) ] == os . path . sep : path = home + path [ len ( userhome_dpath ) : ] return path
537	def run ( self ) : descriptionPyModule = helpers . loadExperimentDescriptionScriptFromDir ( self . _experimentDir ) expIface = helpers . getExperimentDescriptionInterfaceFromModule ( descriptionPyModule ) expIface . normalizeStreamSources ( ) modelDescription = expIface . getModelDescription ( ) self . _modelControl = expIface . getModelControl ( ) streamDef = self . _modelControl [ 'dataset' ] from nupic . data . stream_reader import StreamReader readTimeout = 0 self . _inputSource = StreamReader ( streamDef , isBlocking = False , maxTimeout = readTimeout ) fieldStats = self . _getFieldStats ( ) self . _model = ModelFactory . create ( modelDescription ) self . _model . setFieldStatistics ( fieldStats ) self . _model . enableLearning ( ) self . _model . enableInference ( self . _modelControl . get ( "inferenceArgs" , None ) ) self . __metricMgr = MetricsManager ( self . _modelControl . get ( 'metrics' , None ) , self . _model . getFieldInfo ( ) , self . _model . getInferenceType ( ) ) self . __loggedMetricPatterns = self . _modelControl . get ( "loggedMetrics" , [ ] ) self . _optimizedMetricLabel = self . __getOptimizedMetricLabel ( ) self . _reportMetricLabels = matchPatterns ( self . _reportKeyPatterns , self . _getMetricLabels ( ) ) self . _periodic = self . _initPeriodicActivities ( ) numIters = self . _modelControl . get ( 'iterationCount' , - 1 ) learningOffAt = None iterationCountInferOnly = self . _modelControl . get ( 'iterationCountInferOnly' , 0 ) if iterationCountInferOnly == - 1 : self . _model . disableLearning ( ) elif iterationCountInferOnly > 0 : assert numIters > iterationCountInferOnly , "when iterationCountInferOnly " "is specified, iterationCount must be greater than " "iterationCountInferOnly." learningOffAt = numIters - iterationCountInferOnly self . __runTaskMainLoop ( numIters , learningOffAt = learningOffAt ) self . _finalize ( ) return ( self . _cmpReason , None )
1443	def execute_tuple ( self , stream_id , source_component , latency_in_ns ) : self . update_count ( self . EXEC_COUNT , key = stream_id ) self . update_reduced_metric ( self . EXEC_LATENCY , latency_in_ns , stream_id ) self . update_count ( self . EXEC_TIME_NS , incr_by = latency_in_ns , key = stream_id ) global_stream_id = source_component + "/" + stream_id self . update_count ( self . EXEC_COUNT , key = global_stream_id ) self . update_reduced_metric ( self . EXEC_LATENCY , latency_in_ns , global_stream_id ) self . update_count ( self . EXEC_TIME_NS , incr_by = latency_in_ns , key = global_stream_id )
868	def resetCustomConfig ( cls ) : _getLogger ( ) . info ( "Resetting all custom configuration properties; " "caller=%r" , traceback . format_stack ( ) ) super ( Configuration , cls ) . clear ( ) _CustomConfigurationFileWrapper . clear ( persistent = True )
13201	def format_short_title ( self , format = 'html5' , deparagraph = True , mathjax = False , smart = True , extra_args = None ) : if self . short_title is None : return None output_text = convert_lsstdoc_tex ( self . short_title , 'html5' , deparagraph = deparagraph , mathjax = mathjax , smart = smart , extra_args = extra_args ) return output_text
7877	def _bind_success ( self , stanza ) : payload = stanza . get_payload ( ResourceBindingPayload ) jid = payload . jid if not jid : raise BadRequestProtocolError ( u"<jid/> element mising in" " the bind response" ) self . stream . me = jid self . stream . event ( AuthorizedEvent ( self . stream . me ) )
6096	def voronoi_sub_to_pix_from_grids_and_geometry ( sub_grid , regular_to_nearest_pix , sub_to_regular , pixel_centres , pixel_neighbors , pixel_neighbors_size ) : sub_to_pix = np . zeros ( ( sub_grid . shape [ 0 ] ) ) for sub_index in range ( sub_grid . shape [ 0 ] ) : nearest_pix_pixel_index = regular_to_nearest_pix [ sub_to_regular [ sub_index ] ] while True : nearest_pix_pixel_center = pixel_centres [ nearest_pix_pixel_index ] sub_to_nearest_pix_distance = ( sub_grid [ sub_index , 0 ] - nearest_pix_pixel_center [ 0 ] ) ** 2 + ( sub_grid [ sub_index , 1 ] - nearest_pix_pixel_center [ 1 ] ) ** 2 closest_separation_from_pix_to_neighbor = 1.0e8 for neighbor_index in range ( pixel_neighbors_size [ nearest_pix_pixel_index ] ) : neighbor = pixel_neighbors [ nearest_pix_pixel_index , neighbor_index ] separation_from_neighbor = ( sub_grid [ sub_index , 0 ] - pixel_centres [ neighbor , 0 ] ) ** 2 + ( sub_grid [ sub_index , 1 ] - pixel_centres [ neighbor , 1 ] ) ** 2 if separation_from_neighbor < closest_separation_from_pix_to_neighbor : closest_separation_from_pix_to_neighbor = separation_from_neighbor closest_neighbor_index = neighbor_index neighboring_pix_pixel_index = pixel_neighbors [ nearest_pix_pixel_index , closest_neighbor_index ] sub_to_neighboring_pix_distance = closest_separation_from_pix_to_neighbor if sub_to_nearest_pix_distance <= sub_to_neighboring_pix_distance : sub_to_pix [ sub_index ] = nearest_pix_pixel_index break else : nearest_pix_pixel_index = neighboring_pix_pixel_index return sub_to_pix
12654	def convert_dcm2nii ( input_dir , output_dir , filename ) : if not op . exists ( input_dir ) : raise IOError ( 'Expected an existing folder in {}.' . format ( input_dir ) ) if not op . exists ( output_dir ) : raise IOError ( 'Expected an existing output folder in {}.' . format ( output_dir ) ) tmpdir = tempfile . TemporaryDirectory ( prefix = 'dcm2nii_' ) arguments = '-o "{}" -i y' . format ( tmpdir . name ) try : call_out = call_dcm2nii ( input_dir , arguments ) except : raise else : log . info ( 'Converted "{}" to nifti.' . format ( input_dir ) ) filenames = glob ( op . join ( tmpdir . name , '*.nii*' ) ) cleaned_filenames = remove_dcm2nii_underprocessed ( filenames ) filepaths = [ ] for srcpath in cleaned_filenames : dstpath = op . join ( output_dir , filename ) realpath = copy_w_plus ( srcpath , dstpath ) filepaths . append ( realpath ) basename = op . basename ( remove_ext ( srcpath ) ) aux_files = set ( glob ( op . join ( tmpdir . name , '{}.*' . format ( basename ) ) ) ) - set ( glob ( op . join ( tmpdir . name , '{}.nii*' . format ( basename ) ) ) ) for aux_file in aux_files : aux_dstpath = copy_w_ext ( aux_file , output_dir , remove_ext ( op . basename ( realpath ) ) ) filepaths . append ( aux_dstpath ) return filepaths
5220	def save_intraday ( data : pd . DataFrame , ticker : str , dt , typ = 'TRADE' ) : cur_dt = pd . Timestamp ( dt ) . strftime ( '%Y-%m-%d' ) logger = logs . get_logger ( save_intraday , level = 'debug' ) info = f'{ticker} / {cur_dt} / {typ}' data_file = hist_file ( ticker = ticker , dt = dt , typ = typ ) if not data_file : return if data . empty : logger . warning ( f'data is empty for {info} ...' ) return exch = const . exch_info ( ticker = ticker ) if exch . empty : return end_time = pd . Timestamp ( const . market_timing ( ticker = ticker , dt = dt , timing = 'FINISHED' ) ) . tz_localize ( exch . tz ) now = pd . Timestamp ( 'now' , tz = exch . tz ) - pd . Timedelta ( '1H' ) if end_time > now : logger . debug ( f'skip saving cause market close ({end_time}) < now - 1H ({now}) ...' ) return logger . info ( f'saving data to {data_file} ...' ) files . create_folder ( data_file , is_file = True ) data . to_parquet ( data_file )
12265	def wrap ( f_df , xref , size = 1 ) : memoized_f_df = lrucache ( lambda x : f_df ( restruct ( x , xref ) ) , size ) objective = compose ( first , memoized_f_df ) gradient = compose ( destruct , second , memoized_f_df ) return objective , gradient
3872	def get_all ( self , include_archived = False ) : return [ conv for conv in self . _conv_dict . values ( ) if not conv . is_archived or include_archived ]
4487	def update ( self , fp ) : if 'b' not in fp . mode : raise ValueError ( "File has to be opened in binary mode." ) url = self . _upload_url if fp . peek ( 1 ) : response = self . _put ( url , data = fp ) else : response = self . _put ( url , data = b'' ) if response . status_code != 200 : msg = ( 'Could not update {} (status ' 'code: {}).' . format ( self . path , response . status_code ) ) raise RuntimeError ( msg )
401	def cross_entropy_seq_with_mask ( logits , target_seqs , input_mask , return_details = False , name = None ) : targets = tf . reshape ( target_seqs , [ - 1 ] ) weights = tf . to_float ( tf . reshape ( input_mask , [ - 1 ] ) ) losses = tf . nn . sparse_softmax_cross_entropy_with_logits ( logits = logits , labels = targets , name = name ) * weights loss = tf . divide ( tf . reduce_sum ( losses ) , tf . reduce_sum ( weights ) , name = "seq_loss_with_mask" ) if return_details : return loss , losses , weights , targets else : return loss
1367	def start_connect ( self ) : Log . debug ( "In start_connect() of %s" % self . _get_classname ( ) ) self . create_socket ( socket . AF_INET , socket . SOCK_STREAM ) self . _connecting = True self . connect ( self . endpoint )
9414	def _make_user_class ( session , name ) : attrs = session . eval ( 'fieldnames(%s);' % name , nout = 1 ) . ravel ( ) . tolist ( ) methods = session . eval ( 'methods(%s);' % name , nout = 1 ) . ravel ( ) . tolist ( ) ref = weakref . ref ( session ) doc = _DocDescriptor ( ref , name ) values = dict ( __doc__ = doc , _name = name , _ref = ref , _attrs = attrs , __module__ = 'oct2py.dynamic' ) for method in methods : doc = _MethodDocDescriptor ( ref , name , method ) cls_name = '%s_%s' % ( name , method ) method_values = dict ( __doc__ = doc ) method_cls = type ( str ( cls_name ) , ( OctaveUserClassMethod , ) , method_values ) values [ method ] = method_cls ( ref , method , name ) for attr in attrs : values [ attr ] = OctaveUserClassAttr ( ref , attr , attr ) return type ( str ( name ) , ( OctaveUserClass , ) , values )
1940	def get_func_signature ( self , hsh : bytes ) -> Optional [ str ] : if not isinstance ( hsh , ( bytes , bytearray ) ) : raise TypeError ( 'The selector argument must be a concrete byte array' ) return self . _function_signatures_by_selector . get ( hsh )
5132	def generate_transition_matrix ( g , seed = None ) : g = _test_graph ( g ) if isinstance ( seed , numbers . Integral ) : np . random . seed ( seed ) nV = g . number_of_nodes ( ) mat = np . zeros ( ( nV , nV ) ) for v in g . nodes ( ) : ind = [ e [ 1 ] for e in sorted ( g . out_edges ( v ) ) ] deg = len ( ind ) if deg == 1 : mat [ v , ind ] = 1 elif deg > 1 : probs = np . ceil ( np . random . rand ( deg ) * 100 ) / 100. if np . isclose ( np . sum ( probs ) , 0 ) : probs [ np . random . randint ( deg ) ] = 1 mat [ v , ind ] = probs / np . sum ( probs ) return mat
2074	def convert_input_vector ( y , index ) : if y is None : return None if isinstance ( y , pd . Series ) : return y elif isinstance ( y , np . ndarray ) : if len ( np . shape ( y ) ) == 1 : return pd . Series ( y , name = 'target' , index = index ) elif len ( np . shape ( y ) ) == 2 and np . shape ( y ) [ 0 ] == 1 : return pd . Series ( y [ 0 , : ] , name = 'target' , index = index ) elif len ( np . shape ( y ) ) == 2 and np . shape ( y ) [ 1 ] == 1 : return pd . Series ( y [ : , 0 ] , name = 'target' , index = index ) else : raise ValueError ( 'Unexpected input shape: %s' % ( str ( np . shape ( y ) ) ) ) elif np . isscalar ( y ) : return pd . Series ( [ y ] , name = 'target' , index = index ) elif isinstance ( y , list ) : if len ( y ) == 0 or ( len ( y ) > 0 and not isinstance ( y [ 0 ] , list ) ) : return pd . Series ( y , name = 'target' , index = index ) elif len ( y ) > 0 and isinstance ( y [ 0 ] , list ) and len ( y [ 0 ] ) == 1 : flatten = lambda y : [ item for sublist in y for item in sublist ] return pd . Series ( flatten ( y ) , name = 'target' , index = index ) elif len ( y ) == 1 and isinstance ( y [ 0 ] , list ) : return pd . Series ( y [ 0 ] , name = 'target' , index = index ) else : raise ValueError ( 'Unexpected input shape' ) elif isinstance ( y , pd . DataFrame ) : if len ( list ( y ) ) == 0 : return pd . Series ( y , name = 'target' ) if len ( list ( y ) ) == 1 : return y . iloc [ : , 0 ] else : raise ValueError ( 'Unexpected input shape: %s' % ( str ( y . shape ) ) ) else : return pd . Series ( y , name = 'target' , index = index )
7738	def check_unassigned ( self , data ) : for char in data : for lookup in self . unassigned : if lookup ( char ) : raise StringprepError ( "Unassigned character: {0!r}" . format ( char ) ) return data
5494	def expand_mentions ( text , embed_names = True ) : if embed_names : mention_format = "@<{name} {url}>" else : mention_format = "@<{url}>" def handle_mention ( match ) : source = get_source_by_name ( match . group ( 1 ) ) if source is None : return "@{0}" . format ( match . group ( 1 ) ) return mention_format . format ( name = source . nick , url = source . url ) return short_mention_re . sub ( handle_mention , text )
4101	def mdl_eigen ( s , N ) : r import numpy as np kmdl = [ ] n = len ( s ) for k in range ( 0 , n - 1 ) : ak = 1. / ( n - k ) * np . sum ( s [ k + 1 : ] ) gk = np . prod ( s [ k + 1 : ] ** ( 1. / ( n - k ) ) ) kmdl . append ( - ( n - k ) * N * np . log ( gk / ak ) + 0.5 * k * ( 2. * n - k ) * np . log ( N ) ) return kmdl
4602	def merge ( * projects ) : result = { } for project in projects : for name , section in ( project or { } ) . items ( ) : if name not in PROJECT_SECTIONS : raise ValueError ( UNKNOWN_SECTION_ERROR % name ) if section is None : result [ name ] = type ( result [ name ] ) ( ) continue if name in NOT_MERGEABLE + SPECIAL_CASE : result [ name ] = section continue if section and not isinstance ( section , ( dict , str ) ) : cname = section . __class__ . __name__ raise ValueError ( SECTION_ISNT_DICT_ERROR % ( name , cname ) ) if name == 'animation' : adesc = load . load_if_filename ( section ) if adesc : section = adesc . get ( 'animation' , { } ) section [ 'run' ] = adesc . get ( 'run' , { } ) result_section = result . setdefault ( name , { } ) section = construct . to_type ( section ) for k , v in section . items ( ) : if v is None : result_section . pop ( k , None ) else : result_section [ k ] = v return result
10507	def server_bind ( self , * args , ** kwargs ) : self . socket . setsockopt ( socket . SOL_SOCKET , socket . SO_REUSEADDR , 1 ) SimpleXMLRPCServer . server_bind ( self , * args , ** kwargs )
4789	def is_digit ( self ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a string' ) if len ( self . val ) == 0 : raise ValueError ( 'val is empty' ) if not self . val . isdigit ( ) : self . _err ( 'Expected <%s> to contain only digits, but did not.' % self . val ) return self
768	def getMetricDetails ( self , metricLabel ) : try : metricIndex = self . __metricLabels . index ( metricLabel ) except IndexError : return None return self . __metrics [ metricIndex ] . getMetric ( )
8087	def font ( self , fontpath = None , fontsize = None ) : if fontpath is not None : self . _canvas . fontfile = fontpath else : return self . _canvas . fontfile if fontsize is not None : self . _canvas . fontsize = fontsize
8454	def _cookiecutter_configs_have_changed ( template , old_version , new_version ) : temple . check . is_git_ssh_path ( template ) repo_path = temple . utils . get_repo_path ( template ) github_client = temple . utils . GithubClient ( ) api = '/repos/{}/contents/cookiecutter.json' . format ( repo_path ) old_config_resp = github_client . get ( api , params = { 'ref' : old_version } ) old_config_resp . raise_for_status ( ) new_config_resp = github_client . get ( api , params = { 'ref' : new_version } ) new_config_resp . raise_for_status ( ) return old_config_resp . json ( ) [ 'content' ] != new_config_resp . json ( ) [ 'content' ]
7750	def __try_handlers ( self , handler_list , stanza , stanza_type = None ) : if stanza_type is None : stanza_type = stanza . stanza_type payload = stanza . get_all_payload ( ) classes = [ p . __class__ for p in payload ] keys = [ ( p . __class__ , p . handler_key ) for p in payload ] for handler in handler_list : type_filter = handler . _pyxmpp_stanza_handled [ 1 ] class_filter = handler . _pyxmpp_payload_class_handled extra_filter = handler . _pyxmpp_payload_key if type_filter != stanza_type : continue if class_filter : if extra_filter is None and class_filter not in classes : continue if extra_filter and ( class_filter , extra_filter ) not in keys : continue response = handler ( stanza ) if self . _process_handler_result ( response ) : return True return False
13611	def add_arguments ( cls ) : return [ ( ( '--yes' , ) , dict ( action = 'store_true' , help = 'clean .git repo' ) ) , ( ( '--variable' , '-s' ) , dict ( nargs = '+' , help = 'set extra variable,format is name:value' ) ) , ( ( '--skip-builtin' , ) , dict ( action = 'store_true' , help = 'skip replace builtin variable' ) ) , ]
2219	def register ( self , type ) : def _decorator ( func ) : if isinstance ( type , tuple ) : for t in type : self . func_registry [ t ] = func else : self . func_registry [ type ] = func return func return _decorator
3448	def minimal_medium ( model , min_objective_value = 0.1 , exports = False , minimize_components = False , open_exchanges = False ) : exchange_rxns = find_boundary_types ( model , "exchange" ) if isinstance ( open_exchanges , bool ) : open_bound = 1000 else : open_bound = open_exchanges with model as mod : if open_exchanges : LOGGER . debug ( "Opening exchanges for %d imports." , len ( exchange_rxns ) ) for rxn in exchange_rxns : rxn . bounds = ( - open_bound , open_bound ) LOGGER . debug ( "Applying objective value constraints." ) obj_const = mod . problem . Constraint ( mod . objective . expression , lb = min_objective_value , name = "medium_obj_constraint" ) mod . add_cons_vars ( [ obj_const ] ) mod . solver . update ( ) mod . objective = Zero LOGGER . debug ( "Adding new media objective." ) tol = mod . solver . configuration . tolerances . feasibility if minimize_components : add_mip_obj ( mod ) if isinstance ( minimize_components , bool ) : minimize_components = 1 seen = set ( ) best = num_components = mod . slim_optimize ( ) if mod . solver . status != OPTIMAL : LOGGER . warning ( "Minimization of medium was infeasible." ) return None exclusion = mod . problem . Constraint ( Zero , ub = 0 ) mod . add_cons_vars ( [ exclusion ] ) mod . solver . update ( ) media = [ ] for i in range ( minimize_components ) : LOGGER . info ( "Finding alternative medium #%d." , ( i + 1 ) ) vars = [ mod . variables [ "ind_" + s ] for s in seen ] if len ( seen ) > 0 : exclusion . set_linear_coefficients ( dict . fromkeys ( vars , 1 ) ) exclusion . ub = best - 1 num_components = mod . slim_optimize ( ) if mod . solver . status != OPTIMAL or num_components > best : break medium = _as_medium ( exchange_rxns , tol , exports = exports ) media . append ( medium ) seen . update ( medium [ medium > 0 ] . index ) if len ( media ) > 1 : medium = pd . concat ( media , axis = 1 , sort = True ) . fillna ( 0.0 ) medium . sort_index ( axis = 1 , inplace = True ) else : medium = media [ 0 ] else : add_linear_obj ( mod ) mod . slim_optimize ( ) if mod . solver . status != OPTIMAL : LOGGER . warning ( "Minimization of medium was infeasible." ) return None medium = _as_medium ( exchange_rxns , tol , exports = exports ) return medium
573	def rApply ( d , f ) : remainingDicts = [ ( d , ( ) ) ] while len ( remainingDicts ) > 0 : current , prevKeys = remainingDicts . pop ( ) for k , v in current . iteritems ( ) : keys = prevKeys + ( k , ) if isinstance ( v , dict ) : remainingDicts . insert ( 0 , ( v , keys ) ) else : f ( v , keys )
10005	def clear_descendants ( self , source , clear_source = True ) : removed = self . cellgraph . clear_descendants ( source , clear_source ) for node in removed : del node [ OBJ ] . data [ node [ KEY ] ]
4353	def socketio_manage ( environ , namespaces , request = None , error_handler = None , json_loads = None , json_dumps = None ) : socket = environ [ 'socketio' ] socket . _set_environ ( environ ) socket . _set_namespaces ( namespaces ) if request : socket . _set_request ( request ) if error_handler : socket . _set_error_handler ( error_handler ) if json_loads : socket . _set_json_loads ( json_loads ) if json_dumps : socket . _set_json_dumps ( json_dumps ) receiver_loop = socket . _spawn_receiver_loop ( ) gevent . joinall ( [ receiver_loop ] ) return
546	def _writePrediction ( self , result ) : self . __predictionCache . append ( result ) if self . _isBestModel : self . __flushPredictionCache ( )
13394	def setting ( self , name_hyphen ) : if name_hyphen in self . _instance_settings : value = self . _instance_settings [ name_hyphen ] [ 1 ] else : msg = "No setting named '%s'" % name_hyphen raise UserFeedback ( msg ) if hasattr ( value , 'startswith' ) and value . startswith ( "$" ) : env_var = value . lstrip ( "$" ) if env_var in os . environ : return os . getenv ( env_var ) else : msg = "'%s' is not defined in your environment" % env_var raise UserFeedback ( msg ) elif hasattr ( value , 'startswith' ) and value . startswith ( "\$" ) : return value . replace ( "\$" , "$" ) else : return value
1333	def predictions_and_gradient ( self , image = None , label = None , strict = True , return_details = False ) : assert self . has_gradient ( ) if image is None : image = self . __original_image if label is None : label = self . __original_class in_bounds = self . in_bounds ( image ) assert not strict or in_bounds self . _total_prediction_calls += 1 self . _total_gradient_calls += 1 predictions , gradient = self . __model . predictions_and_gradient ( image , label ) is_adversarial , is_best , distance = self . __is_adversarial ( image , predictions , in_bounds ) assert predictions . ndim == 1 assert gradient . shape == image . shape if return_details : return predictions , gradient , is_adversarial , is_best , distance else : return predictions , gradient , is_adversarial
5995	def plot_mask ( mask , units , kpc_per_arcsec , pointsize , zoom_offset_pixels ) : if mask is not None : plt . gca ( ) edge_pixels = mask . masked_grid_index_to_pixel [ mask . edge_pixels ] + 0.5 if zoom_offset_pixels is not None : edge_pixels -= zoom_offset_pixels edge_arcsec = mask . grid_pixels_to_grid_arcsec ( grid_pixels = edge_pixels ) edge_units = convert_grid_units ( array = mask , grid_arcsec = edge_arcsec , units = units , kpc_per_arcsec = kpc_per_arcsec ) plt . scatter ( y = edge_units [ : , 0 ] , x = edge_units [ : , 1 ] , s = pointsize , c = 'k' )
133	def clip_out_of_image ( self , image ) : import shapely . geometry if self . is_out_of_image ( image , fully = True , partly = False ) : return [ ] h , w = image . shape [ 0 : 2 ] if ia . is_np_array ( image ) else image [ 0 : 2 ] poly_shapely = self . to_shapely_polygon ( ) poly_image = shapely . geometry . Polygon ( [ ( 0 , 0 ) , ( w , 0 ) , ( w , h ) , ( 0 , h ) ] ) multipoly_inter_shapely = poly_shapely . intersection ( poly_image ) if not isinstance ( multipoly_inter_shapely , shapely . geometry . MultiPolygon ) : ia . do_assert ( isinstance ( multipoly_inter_shapely , shapely . geometry . Polygon ) ) multipoly_inter_shapely = shapely . geometry . MultiPolygon ( [ multipoly_inter_shapely ] ) polygons = [ ] for poly_inter_shapely in multipoly_inter_shapely . geoms : polygons . append ( Polygon . from_shapely ( poly_inter_shapely , label = self . label ) ) polygons_reordered = [ ] for polygon in polygons : found = False for x , y in self . exterior : closest_idx , dist = polygon . find_closest_point_index ( x = x , y = y , return_distance = True ) if dist < 1e-6 : polygon_reordered = polygon . change_first_point_by_index ( closest_idx ) polygons_reordered . append ( polygon_reordered ) found = True break ia . do_assert ( found ) return polygons_reordered
13749	def many_to_one ( clsname , ** kw ) : @ declared_attr def m2o ( cls ) : cls . _references ( ( cls . __name__ , clsname ) ) return relationship ( clsname , ** kw ) return m2o
7923	def as_unicode ( self ) : result = self . domain if self . local : result = self . local + u'@' + result if self . resource : result = result + u'/' + self . resource if not JID . cache . has_key ( result ) : JID . cache [ result ] = self return result
12585	def spatialimg_to_hdfpath ( file_path , spatial_img , h5path = None , append = True ) : if h5path is None : h5path = '/img' mode = 'w' if os . path . exists ( file_path ) : if append : mode = 'a' with h5py . File ( file_path , mode ) as f : try : h5img = f . create_group ( h5path ) spatialimg_to_hdfgroup ( h5img , spatial_img ) except ValueError as ve : raise Exception ( 'Error creating group ' + h5path ) from ve
4204	def rlevinson ( a , efinal ) : a = numpy . array ( a ) realdata = numpy . isrealobj ( a ) assert a [ 0 ] == 1 , 'First coefficient of the prediction polynomial must be unity' p = len ( a ) if p < 2 : raise ValueError ( 'Polynomial should have at least two coefficients' ) if realdata == True : U = numpy . zeros ( ( p , p ) ) else : U = numpy . zeros ( ( p , p ) , dtype = complex ) U [ : , p - 1 ] = numpy . conj ( a [ - 1 : : - 1 ] ) p = p - 1 e = numpy . zeros ( p ) e [ - 1 ] = efinal for k in range ( p - 1 , 0 , - 1 ) : [ a , e [ k - 1 ] ] = levdown ( a , e [ k ] ) U [ : , k ] = numpy . concatenate ( ( numpy . conj ( a [ - 1 : : - 1 ] . transpose ( ) ) , [ 0 ] * ( p - k ) ) ) e0 = e [ 0 ] / ( 1. - abs ( a [ 1 ] ** 2 ) ) U [ 0 , 0 ] = 1 kr = numpy . conj ( U [ 0 , 1 : ] ) kr = kr . transpose ( ) R = numpy . zeros ( 1 , dtype = complex ) k = 1 R0 = e0 R [ 0 ] = - numpy . conj ( U [ 0 , 1 ] ) * R0 for k in range ( 1 , p ) : r = - sum ( numpy . conj ( U [ k - 1 : : - 1 , k ] ) * R [ - 1 : : - 1 ] ) - kr [ k ] * e [ k - 1 ] R = numpy . insert ( R , len ( R ) , r ) R = numpy . insert ( R , 0 , e0 ) return R , U , kr , e
5408	def _operation_status_message ( self ) : msg = None action = None if not google_v2_operations . is_done ( self . _op ) : last_event = google_v2_operations . get_last_event ( self . _op ) if last_event : msg = last_event [ 'description' ] action_id = last_event . get ( 'details' , { } ) . get ( 'actionId' ) if action_id : action = google_v2_operations . get_action_by_id ( self . _op , action_id ) else : msg = 'Pending' else : failed_events = google_v2_operations . get_failed_events ( self . _op ) if failed_events : failed_event = failed_events [ - 1 ] msg = failed_event . get ( 'details' , { } ) . get ( 'stderr' ) action_id = failed_event . get ( 'details' , { } ) . get ( 'actionId' ) if action_id : action = google_v2_operations . get_action_by_id ( self . _op , action_id ) if not msg : error = google_v2_operations . get_error ( self . _op ) if error : msg = error [ 'message' ] else : msg = 'Success' return msg , action
9059	def gradient ( self ) : L = self . L n = self . L . shape [ 0 ] grad = { "Lu" : zeros ( ( n , n , n * self . _L . shape [ 1 ] ) ) } for ii in range ( self . _L . shape [ 0 ] * self . _L . shape [ 1 ] ) : row = ii // self . _L . shape [ 1 ] col = ii % self . _L . shape [ 1 ] grad [ "Lu" ] [ row , : , ii ] = L [ : , col ] grad [ "Lu" ] [ : , row , ii ] += L [ : , col ] return grad
10710	def get_water_heaters ( self ) : water_heaters = [ ] for location in self . locations : _location_id = location . get ( "id" ) for device in location . get ( "equipment" ) : if device . get ( "type" ) == "Water Heater" : water_heater_modes = self . api_interface . get_modes ( device . get ( "id" ) ) water_heater_usage = self . api_interface . get_usage ( device . get ( "id" ) ) water_heater = self . api_interface . get_device ( device . get ( "id" ) ) vacations = self . api_interface . get_vacations ( ) device_vacations = [ ] for vacation in vacations : for equipment in vacation . get ( "participatingEquipment" ) : if equipment . get ( "id" ) == water_heater . get ( "id" ) : device_vacations . append ( EcoNetVacation ( vacation , self . api_interface ) ) water_heaters . append ( EcoNetWaterHeater ( water_heater , water_heater_modes , water_heater_usage , _location_id , device_vacations , self . api_interface ) ) return water_heaters
63	def is_partly_within_image ( self , image ) : shape = normalize_shape ( image ) height , width = shape [ 0 : 2 ] eps = np . finfo ( np . float32 ) . eps img_bb = BoundingBox ( x1 = 0 , x2 = width - eps , y1 = 0 , y2 = height - eps ) return self . intersection ( img_bb ) is not None
3368	def _valid_atoms ( model , expression ) : atoms = expression . atoms ( optlang . interface . Variable ) return all ( a . problem is model . solver for a in atoms )
4431	async def ensure_voice ( self , ctx ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . is_connected : if not ctx . author . voice or not ctx . author . voice . channel : await ctx . send ( 'You aren\'t connected to any voice channel.' ) raise commands . CommandInvokeError ( 'Author not connected to voice channel.' ) permissions = ctx . author . voice . channel . permissions_for ( ctx . me ) if not permissions . connect or not permissions . speak : await ctx . send ( 'Missing permissions `CONNECT` and/or `SPEAK`.' ) raise commands . CommandInvokeError ( 'Bot has no permissions CONNECT and/or SPEAK' ) player . store ( 'channel' , ctx . channel . id ) await player . connect ( ctx . author . voice . channel . id ) else : if player . connected_channel . id != ctx . author . voice . channel . id : return await ctx . send ( 'Join my voice channel!' )
13729	def balance_over_time ( address ) : forged_blocks = None txhistory = Address . transactions ( address ) delegates = Delegate . delegates ( ) for i in delegates : if address == i . address : forged_blocks = Delegate . blocks ( i . pubkey ) balance_over_time = [ ] balance = 0 block = 0 Balance = namedtuple ( 'balance' , 'timestamp amount' ) for tx in txhistory : if forged_blocks : while forged_blocks [ block ] . timestamp <= tx . timestamp : balance += ( forged_blocks [ block ] . reward + forged_blocks [ block ] . totalFee ) balance_over_time . append ( Balance ( timestamp = forged_blocks [ block ] . timestamp , amount = balance ) ) block += 1 if tx . senderId == address : balance -= ( tx . amount + tx . fee ) res = Balance ( timestamp = tx . timestamp , amount = balance ) balance_over_time . append ( res ) if tx . recipientId == address : balance += tx . amount res = Balance ( timestamp = tx . timestamp , amount = balance ) balance_over_time . append ( res ) if forged_blocks and block <= len ( forged_blocks ) - 1 : if forged_blocks [ block ] . timestamp > txhistory [ - 1 ] . timestamp : for i in forged_blocks [ block : ] : balance += ( i . reward + i . totalFee ) res = Balance ( timestamp = i . timestamp , amount = balance ) balance_over_time . append ( res ) return balance_over_time
11382	def do_url_scheme ( parser , token ) : args = token . split_contents ( ) if len ( args ) != 1 : raise template . TemplateSyntaxError ( '%s takes no parameters.' % args [ 0 ] ) return OEmbedURLSchemeNode ( )
7619	def chord ( ref , est , ** kwargs ) : r namespace = 'chord' ref = coerce_annotation ( ref , namespace ) est = coerce_annotation ( est , namespace ) ref_interval , ref_value = ref . to_interval_values ( ) est_interval , est_value = est . to_interval_values ( ) return mir_eval . chord . evaluate ( ref_interval , ref_value , est_interval , est_value , ** kwargs )
10880	def listify ( a ) : if a is None : return [ ] elif not isinstance ( a , ( tuple , list , np . ndarray ) ) : return [ a ] return list ( a )
7537	def branch_assembly ( args , parsedict ) : data = getassembly ( args , parsedict ) bargs = args . branch newname = bargs [ 0 ] if newname . endswith ( ".txt" ) : newname = newname [ : - 4 ] if len ( bargs ) > 1 : if any ( [ x . stats . state == 6 for x in data . samples . values ( ) ] ) : pass subsamples = bargs [ 1 : ] if bargs [ 1 ] == "-" : fails = [ i for i in subsamples [ 1 : ] if i not in data . samples . keys ( ) ] if any ( fails ) : raise IPyradWarningExit ( "\ \n Failed: unrecognized names requested, check spelling:\n {}" . format ( "\n " . join ( [ i for i in fails ] ) ) ) print ( " dropping {} samples" . format ( len ( subsamples ) - 1 ) ) subsamples = list ( set ( data . samples . keys ( ) ) - set ( subsamples ) ) if os . path . exists ( bargs [ 1 ] ) : new_data = data . branch ( newname , infile = bargs [ 1 ] ) else : new_data = data . branch ( newname , subsamples ) else : new_data = data . branch ( newname , None ) print ( " creating a new branch called '{}' with {} Samples" . format ( new_data . name , len ( new_data . samples ) ) ) print ( " writing new params file to {}" . format ( "params-" + new_data . name + ".txt\n" ) ) new_data . write_params ( "params-" + new_data . name + ".txt" , force = args . force )
2935	def parse_node ( self , node ) : if node . get ( 'id' ) in self . parsed_nodes : return self . parsed_nodes [ node . get ( 'id' ) ] ( node_parser , spec_class ) = self . parser . _get_parser_class ( node . tag ) if not node_parser or not spec_class : raise ValidationException ( "There is no support implemented for this task type." , node = node , filename = self . filename ) np = node_parser ( self , spec_class , node ) task_spec = np . parse_node ( ) return task_spec
3749	def calculate_P ( self , T , P , method ) : r if method == COOLPROP : mu = PropsSI ( 'V' , 'T' , T , 'P' , P , self . CASRN ) elif method in self . tabular_data : mu = self . interpolate_P ( T , P , method ) return mu
502	def _labelToCategoryNumber ( self , label ) : if label not in self . saved_categories : self . saved_categories . append ( label ) return pow ( 2 , self . saved_categories . index ( label ) )
5110	def simulate ( self , n = 1 , t = None , nA = None , nD = None ) : if t is None and nD is None and nA is None : for dummy in range ( n ) : self . next_event ( ) elif t is not None : then = self . _current_t + t while self . _current_t < then and self . _time < infty : self . next_event ( ) elif nD is not None : num_departures = self . num_departures + nD while self . num_departures < num_departures and self . _time < infty : self . next_event ( ) elif nA is not None : num_arrivals = self . _oArrivals + nA while self . _oArrivals < num_arrivals and self . _time < infty : self . next_event ( )
9517	def trim_Ns ( self ) : i = 0 while i < len ( self ) and self . seq [ i ] in 'nN' : i += 1 self . seq = self . seq [ i : ] self . qual = self . qual [ i : ] self . seq = self . seq . rstrip ( 'Nn' ) self . qual = self . qual [ : len ( self . seq ) ]
11174	def settingshelp ( self , width = 0 ) : out = [ ] out . append ( self . _wrap ( self . docs [ 'title' ] , width = width ) ) if self . docs [ 'description' ] : out . append ( self . _wrap ( self . docs [ 'description' ] , indent = 2 , width = width ) ) out . append ( '' ) out . append ( 'SETTINGS:' ) out . append ( self . strsettings ( indent = 2 , width = width ) ) out . append ( '' ) return '\n' . join ( out )
8487	def init ( self , hosts = None , cacert = None , client_cert = None , client_key = None ) : try : import etcd self . module = etcd except ImportError : pass if not self . module : return self . _parse_jetconfig ( ) hosts = env ( 'PYCONFIG_ETCD_HOSTS' , hosts ) protocol = env ( 'PYCONFIG_ETCD_PROTOCOL' , None ) cacert = env ( 'PYCONFIG_ETCD_CACERT' , cacert ) client_cert = env ( 'PYCONFIG_ETCD_CERT' , client_cert ) client_key = env ( 'PYCONFIG_ETCD_KEY' , client_key ) username = None password = None auth = env ( 'PYCONFIG_ETCD_AUTH' , None ) if auth : auth = auth . split ( ':' ) auth . append ( '' ) username = auth [ 0 ] password = auth [ 1 ] hosts = self . _parse_hosts ( hosts ) if hosts is None : return kw = { } kw [ 'allow_reconnect' ] = True if protocol : kw [ 'protocol' ] = protocol if username : kw [ 'username' ] = username if password : kw [ 'password' ] = password if cacert : kw [ 'ca_cert' ] = os . path . abspath ( cacert ) if client_cert and client_key : kw [ 'cert' ] = ( ( os . path . abspath ( client_cert ) , os . path . abspath ( client_key ) ) ) elif client_cert : kw [ 'cert' ] = os . path . abspath ( client_cert ) if cacert or client_cert or client_key : kw [ 'protocol' ] = 'https' self . client = self . module . Client ( hosts , ** kw )
394	def cross_entropy_reward_loss ( logits , actions , rewards , name = None ) : cross_entropy = tf . nn . sparse_softmax_cross_entropy_with_logits ( labels = actions , logits = logits , name = name ) return tf . reduce_sum ( tf . multiply ( cross_entropy , rewards ) )
11050	def _dispatch_event ( self ) : data = self . _prepare_data ( ) if data is not None : self . _handler ( self . _event , data ) self . _reset_event_data ( )
3074	def authorize_view ( self ) : args = request . args . to_dict ( ) args [ 'scopes' ] = request . args . getlist ( 'scopes' ) return_url = args . pop ( 'return_url' , None ) if return_url is None : return_url = request . referrer or '/' flow = self . _make_flow ( return_url = return_url , ** args ) auth_url = flow . step1_get_authorize_url ( ) return redirect ( auth_url )
2448	def set_pkg_file_name ( self , doc , name ) : self . assert_package_exists ( ) if not self . package_file_name_set : self . package_file_name_set = True doc . package . file_name = name return True else : raise CardinalityError ( 'Package::FileName' )
5316	def setup ( self , colormode = None , colorpalette = None , extend_colors = False ) : if colormode : self . colormode = colormode if colorpalette : if extend_colors : self . update_palette ( colorpalette ) else : self . colorpalette = colorpalette
8138	def contrast ( self , value = 1.0 ) : c = ImageEnhance . Contrast ( self . img ) self . img = c . enhance ( value )
10810	def get_by_name ( cls , name ) : try : return cls . query . filter_by ( name = name ) . one ( ) except NoResultFound : return None
4930	def transform_courserun_title ( self , content_metadata_item ) : title = content_metadata_item . get ( 'title' ) or '' course_run_start = content_metadata_item . get ( 'start' ) if course_run_start : if course_available_for_enrollment ( content_metadata_item ) : title += ' ({starts}: {:%B %Y})' . format ( parse_lms_api_datetime ( course_run_start ) , starts = _ ( 'Starts' ) ) else : title += ' ({:%B %Y} - {enrollment_closed})' . format ( parse_lms_api_datetime ( course_run_start ) , enrollment_closed = _ ( 'Enrollment Closed' ) ) title_with_locales = [ ] content_metadata_language_code = transform_language_code ( content_metadata_item . get ( 'content_language' , '' ) ) for locale in self . enterprise_configuration . get_locales ( default_locale = content_metadata_language_code ) : title_with_locales . append ( { 'locale' : locale , 'value' : title } ) return title_with_locales
11426	def record_strip_empty_volatile_subfields ( rec ) : for tag in rec . keys ( ) : for field in rec [ tag ] : field [ 0 ] [ : ] = [ subfield for subfield in field [ 0 ] if subfield [ 1 ] [ : 9 ] != "VOLATILE:" ]
5611	def memory_file ( data = None , profile = None ) : memfile = MemoryFile ( ) profile . update ( width = data . shape [ - 2 ] , height = data . shape [ - 1 ] ) with memfile . open ( ** profile ) as dataset : dataset . write ( data ) return memfile
10025	def get_versions ( self ) : response = self . ebs . describe_application_versions ( application_name = self . app_name ) return response [ 'DescribeApplicationVersionsResponse' ] [ 'DescribeApplicationVersionsResult' ] [ 'ApplicationVersions' ]
8806	def calc_periods ( hour = 0 , minute = 0 ) : period_end = datetime . datetime . utcnow ( ) . replace ( hour = hour , minute = minute , second = 0 , microsecond = 0 ) period_start = period_end - datetime . timedelta ( days = 1 ) period_end -= datetime . timedelta ( seconds = 1 ) return ( period_start , period_end )
4207	def arcovar ( x , order ) : r from spectrum import corrmtx import scipy . linalg X = corrmtx ( x , order , 'covariance' ) Xc = np . matrix ( X [ : , 1 : ] ) X1 = np . array ( X [ : , 0 ] ) a , _residues , _rank , _singular_values = scipy . linalg . lstsq ( - Xc , X1 ) Cz = np . dot ( X1 . conj ( ) . transpose ( ) , Xc ) e = np . dot ( X1 . conj ( ) . transpose ( ) , X1 ) + np . dot ( Cz , a ) assert e . imag < 1e-4 , 'wierd behaviour' e = float ( e . real ) return a , e
4412	def fetch ( self , key : object , default = None ) : return self . _user_data . get ( key , default )
9607	def vformat ( self , format_string , args , kwargs ) : self . _used_kwargs = { } self . _unused_kwargs = { } return super ( MemorizeFormatter , self ) . vformat ( format_string , args , kwargs )
7227	def paint ( self ) : snippet = { 'fill-opacity' : VectorStyle . get_style_value ( self . opacity ) , 'fill-color' : VectorStyle . get_style_value ( self . color ) , 'fill-outline-color' : VectorStyle . get_style_value ( self . outline_color ) } if self . translate : snippet [ 'fill-translate' ] = self . translate return snippet
10160	def ci ( ctx ) : opts = [ '' ] if os . environ . get ( 'TRAVIS' , '' ) . lower ( ) == 'true' : opts += [ 'test.pytest' ] else : opts += [ 'test.tox' ] ctx . run ( "invoke --echo --pty clean --all build --docs check --reports{}" . format ( ' ' . join ( opts ) ) )
13745	def get_table ( self ) : if hasattr ( self , '_table' ) : table = self . _table else : try : table = self . conn . get_table ( self . get_table_name ( ) ) except boto . exception . DynamoDBResponseError : if self . auto_create_table : table = self . create_table ( ) else : raise self . _table = table return table
9276	def apply_exclude_tags ( self , all_tags ) : filtered = copy . deepcopy ( all_tags ) for tag in all_tags : if tag [ "name" ] not in self . options . exclude_tags : self . warn_if_tag_not_found ( tag , "exclude-tags" ) else : filtered . remove ( tag ) return filtered
10292	def expand_internal ( universe : BELGraph , graph : BELGraph , edge_predicates : EdgePredicates = None ) -> None : edge_filter = and_edge_predicates ( edge_predicates ) for u , v in itt . product ( graph , repeat = 2 ) : if graph . has_edge ( u , v ) or not universe . has_edge ( u , v ) : continue rs = defaultdict ( list ) for key , data in universe [ u ] [ v ] . items ( ) : if not edge_filter ( universe , u , v , key ) : continue rs [ data [ RELATION ] ] . append ( ( key , data ) ) if 1 == len ( rs ) : relation = list ( rs ) [ 0 ] for key , data in rs [ relation ] : graph . add_edge ( u , v , key = key , ** data ) else : log . debug ( 'Multiple relationship types found between %s and %s' , u , v )
10278	def get_neurommsig_score ( graph : BELGraph , genes : List [ Gene ] , ora_weight : Optional [ float ] = None , hub_weight : Optional [ float ] = None , top_percent : Optional [ float ] = None , topology_weight : Optional [ float ] = None ) -> float : ora_weight = ora_weight or 1.0 hub_weight = hub_weight or 1.0 topology_weight = topology_weight or 1.0 total_weight = ora_weight + hub_weight + topology_weight genes = list ( genes ) ora_score = neurommsig_gene_ora ( graph , genes ) hub_score = neurommsig_hubs ( graph , genes , top_percent = top_percent ) topology_score = neurommsig_topology ( graph , genes ) weighted_sum = ( ora_weight * ora_score + hub_weight * hub_score + topology_weight * topology_score ) return weighted_sum / total_weight
6808	def configure_hdmi ( self ) : r = self . local_renderer r . enable_attr ( filename = '/boot/config.txt' , key = 'hdmi_force_hotplug' , value = 1 , use_sudo = True , ) r . enable_attr ( filename = '/boot/config.txt' , key = 'hdmi_drive' , value = 2 , use_sudo = True , )
9897	def boottime ( ) : global __boottime if __boottime is None : up = uptime ( ) if up is None : return None if __boottime is None : _boottime_linux ( ) if datetime is None : raise RuntimeError ( 'datetime module required.' ) return datetime . fromtimestamp ( __boottime or time . time ( ) - up )
2131	def get ( self , pk = None , ** kwargs ) : if kwargs . pop ( 'include_debug_header' , True ) : debug . log ( 'Getting the role record.' , header = 'details' ) data , self . endpoint = self . data_endpoint ( kwargs ) response = self . read ( pk = pk , fail_on_no_results = True , fail_on_multiple_results = True , ** data ) item_dict = response [ 'results' ] [ 0 ] self . configure_display ( item_dict ) return item_dict
7846	def add_item ( self , jid , node = None , name = None , action = None ) : return DiscoItem ( self , jid , node , name , action )
10239	def count_citations_by_annotation ( graph : BELGraph , annotation : str ) -> Mapping [ str , typing . Counter [ str ] ] : citations = defaultdict ( lambda : defaultdict ( set ) ) for u , v , data in graph . edges ( data = True ) : if not edge_has_annotation ( data , annotation ) or CITATION not in data : continue k = data [ ANNOTATIONS ] [ annotation ] citations [ k ] [ u , v ] . add ( ( data [ CITATION ] [ CITATION_TYPE ] , data [ CITATION ] [ CITATION_REFERENCE ] . strip ( ) ) ) return { k : Counter ( itt . chain . from_iterable ( v . values ( ) ) ) for k , v in citations . items ( ) }
13105	def cmpToDataStore_uri ( base , ds1 , ds2 ) : ret = difflib . get_close_matches ( base . uri , [ ds1 . uri , ds2 . uri ] , 1 , cutoff = 0.5 ) if len ( ret ) <= 0 : return 0 if ret [ 0 ] == ds1 . uri : return - 1 return 1
9220	def parse_args ( self , args , scope ) : arguments = list ( zip ( args , [ ' ' ] * len ( args ) ) ) if args and args [ 0 ] else None zl = itertools . zip_longest if sys . version_info [ 0 ] == 3 else itertools . izip_longest if self . args : parsed = [ v if hasattr ( v , 'parse' ) else v for v in copy . copy ( self . args ) ] args = args if isinstance ( args , list ) else [ args ] vars = [ self . _parse_arg ( var , arg , scope ) for arg , var in zl ( [ a for a in args ] , parsed ) ] for var in vars : if var : var . parse ( scope ) if not arguments : arguments = [ v . value for v in vars if v ] if not arguments : arguments = '' Variable ( [ '@arguments' , None , arguments ] ) . parse ( scope )
13132	def parse_user ( entry , domain_groups ) : result = { } distinguished_name = get_field ( entry , 'distinguishedName' ) result [ 'domain' ] = "." . join ( distinguished_name . split ( ',DC=' ) [ 1 : ] ) result [ 'name' ] = get_field ( entry , 'name' ) result [ 'username' ] = get_field ( entry , 'sAMAccountName' ) result [ 'description' ] = get_field ( entry , 'description' ) result [ 'sid' ] = get_field ( entry , 'objectSid' ) . split ( '-' ) [ - 1 ] primary_group = get_field ( entry , 'primaryGroupID' ) member_of = entry [ 'attributes' ] . get ( 'memberOf' , [ ] ) groups = [ ] for member in member_of : for e in member . split ( ',' ) : if e . startswith ( 'CN=' ) : groups . append ( e [ 3 : ] ) groups . append ( domain_groups . get ( primary_group , '' ) ) result [ 'groups' ] = groups flags = [ ] try : uac = int ( get_field ( entry , 'userAccountControl' ) ) for flag , value in uac_flags . items ( ) : if uac & value : flags . append ( flag ) except ValueError : pass result [ 'flags' ] = flags return result
5868	def _activate_organization_course_relationship ( relationship ) : relationship = internal . OrganizationCourse . objects . get ( id = relationship . id , active = False , organization__active = True ) _activate_record ( relationship )
9632	def render_subject ( self , context ) : rendered = self . subject_template . render ( unescape ( context ) ) return rendered . strip ( )
1962	def sys_rt_sigaction ( self , signum , act , oldact ) : return self . sys_sigaction ( signum , act , oldact )
2555	def add ( self , * args ) : for obj in args : if isinstance ( obj , numbers . Number ) : obj = str ( obj ) if isinstance ( obj , basestring ) : obj = escape ( obj ) self . children . append ( obj ) elif isinstance ( obj , dom_tag ) : ctx = dom_tag . _with_contexts [ _get_thread_context ( ) ] if ctx and ctx [ - 1 ] : ctx [ - 1 ] . used . add ( obj ) self . children . append ( obj ) obj . parent = self obj . setdocument ( self . document ) elif isinstance ( obj , dict ) : for attr , value in obj . items ( ) : self . set_attribute ( * dom_tag . clean_pair ( attr , value ) ) elif hasattr ( obj , '__iter__' ) : for subobj in obj : self . add ( subobj ) else : raise ValueError ( '%r not a tag or string.' % obj ) if len ( args ) == 1 : return args [ 0 ] return args
13115	def parse_ips ( ips , netmask , include_public ) : hs = HostSearch ( ) rs = RangeSearch ( ) ranges = [ ] ips = list ( set ( ips ) ) included_ips = [ ] print_success ( "Found {} ips" . format ( len ( ips ) ) ) for ip in ips : ip_address = ipaddress . ip_address ( ip ) if include_public or ip_address . is_private : if len ( ips ) < 15 : print_success ( "Found ip: {}" . format ( ip ) ) host = hs . id_to_object ( ip ) host . add_tag ( 'dns_discover' ) host . save ( ) r = str ( ipaddress . IPv4Network ( "{}/{}" . format ( ip , netmask ) , strict = False ) ) ranges . append ( r ) included_ips . append ( ip ) else : print_notification ( "Excluding ip {}" . format ( ip ) ) ranges = list ( set ( ranges ) ) print_success ( "Found {} ranges" . format ( len ( ranges ) ) ) for rng in ranges : if len ( ranges ) < 15 : print_success ( "Found range: {}" . format ( rng ) ) r = rs . id_to_object ( rng ) r . add_tag ( 'dns_discover' ) r . save ( ) stats = { } stats [ 'ips' ] = included_ips stats [ 'ranges' ] = ranges return stats
210	def pad_to_aspect_ratio ( self , aspect_ratio , mode = "constant" , cval = 0.0 , return_pad_amounts = False ) : arr_0to1_padded , pad_amounts = ia . pad_to_aspect_ratio ( self . arr_0to1 , aspect_ratio = aspect_ratio , mode = mode , cval = cval , return_pad_amounts = True ) heatmaps = HeatmapsOnImage . from_0to1 ( arr_0to1_padded , shape = self . shape , min_value = self . min_value , max_value = self . max_value ) if return_pad_amounts : return heatmaps , pad_amounts else : return heatmaps
497	def _constructClassificationRecord ( self , inputs ) : allSPColumns = inputs [ "spBottomUpOut" ] activeSPColumns = allSPColumns . nonzero ( ) [ 0 ] score = anomaly . computeRawAnomalyScore ( activeSPColumns , self . _prevPredictedColumns ) spSize = len ( allSPColumns ) allTPCells = inputs [ 'tpTopDownOut' ] tpSize = len ( inputs [ 'tpLrnActiveStateT' ] ) classificationVector = numpy . array ( [ ] ) if self . classificationVectorType == 1 : classificationVector = numpy . zeros ( tpSize ) activeCellMatrix = inputs [ "tpLrnActiveStateT" ] . reshape ( tpSize , 1 ) activeCellIdx = numpy . where ( activeCellMatrix > 0 ) [ 0 ] if activeCellIdx . shape [ 0 ] > 0 : classificationVector [ numpy . array ( activeCellIdx , dtype = numpy . uint16 ) ] = 1 elif self . classificationVectorType == 2 : classificationVector = numpy . zeros ( spSize + spSize ) if activeSPColumns . shape [ 0 ] > 0 : classificationVector [ activeSPColumns ] = 1.0 errorColumns = numpy . setdiff1d ( self . _prevPredictedColumns , activeSPColumns ) if errorColumns . shape [ 0 ] > 0 : errorColumnIndexes = ( numpy . array ( errorColumns , dtype = numpy . uint16 ) + spSize ) classificationVector [ errorColumnIndexes ] = 1.0 else : raise TypeError ( "Classification vector type must be either 'tpc' or" " 'sp_tpe', current value is %s" % ( self . classificationVectorType ) ) numPredictedCols = len ( self . _prevPredictedColumns ) predictedColumns = allTPCells . nonzero ( ) [ 0 ] self . _prevPredictedColumns = copy . deepcopy ( predictedColumns ) if self . _anomalyVectorLength is None : self . _anomalyVectorLength = len ( classificationVector ) result = _CLAClassificationRecord ( ROWID = self . _iteration , anomalyScore = score , anomalyVector = classificationVector . nonzero ( ) [ 0 ] . tolist ( ) , anomalyLabel = [ ] ) return result
3687	def a_alpha_and_derivatives ( self , T , full = True , quick = True ) : r if not full : return self . a * ( 1 + self . kappa * ( 1 - ( T / self . Tc ) ** 0.5 ) ) ** 2 else : if quick : Tc , kappa = self . Tc , self . kappa x0 = T ** 0.5 x1 = Tc ** - 0.5 x2 = kappa * ( x0 * x1 - 1. ) - 1. x3 = self . a * kappa a_alpha = self . a * x2 * x2 da_alpha_dT = x1 * x2 * x3 / x0 d2a_alpha_dT2 = x3 * ( - 0.5 * T ** - 1.5 * x1 * x2 + 0.5 / ( T * Tc ) * kappa ) else : a_alpha = self . a * ( 1 + self . kappa * ( 1 - ( T / self . Tc ) ** 0.5 ) ) ** 2 da_alpha_dT = - self . a * self . kappa * sqrt ( T / self . Tc ) * ( self . kappa * ( - sqrt ( T / self . Tc ) + 1. ) + 1. ) / T d2a_alpha_dT2 = self . a * self . kappa * ( self . kappa / self . Tc - sqrt ( T / self . Tc ) * ( self . kappa * ( sqrt ( T / self . Tc ) - 1. ) - 1. ) / T ) / ( 2. * T ) return a_alpha , da_alpha_dT , d2a_alpha_dT2
10214	def summarize_subgraph_node_overlap ( graph : BELGraph , node_predicates = None , annotation : str = 'Subgraph' ) : r1 = group_nodes_by_annotation_filtered ( graph , node_predicates = node_predicates , annotation = annotation ) return calculate_tanimoto_set_distances ( r1 )
12410	def all_package_versions ( package ) : info = PyPI . package_info ( package ) return info and sorted ( info [ 'releases' ] . keys ( ) , key = lambda x : x . split ( ) , reverse = True ) or [ ]
2614	def _write_submit_script ( self , template , script_filename , job_name , configs ) : try : submit_script = Template ( template ) . substitute ( jobname = job_name , ** configs ) with open ( script_filename , 'w' ) as f : f . write ( submit_script ) except KeyError as e : logger . error ( "Missing keys for submit script : %s" , e ) raise ( SchedulerMissingArgs ( e . args , self . sitename ) ) except IOError as e : logger . error ( "Failed writing to submit script: %s" , script_filename ) raise ( ScriptPathError ( script_filename , e ) ) except Exception as e : print ( "Template : " , template ) print ( "Args : " , job_name ) print ( "Kwargs : " , configs ) logger . error ( "Uncategorized error: %s" , e ) raise ( e ) return True
8948	def info ( msg ) : _flush ( ) sys . stdout . write ( msg + '\n' ) sys . stdout . flush ( )
6110	def unmasked_blurred_image_of_galaxies_from_psf ( self , padded_grid_stack , psf ) : return [ padded_grid_stack . unmasked_blurred_image_from_psf_and_unmasked_image ( psf , image ) if not galaxy . has_pixelization else None for galaxy , image in zip ( self . galaxies , self . image_plane_image_1d_of_galaxies ) ]
3673	def draw_2d ( self , width = 300 , height = 300 , Hs = False ) : r try : from rdkit . Chem import Draw from rdkit . Chem . Draw import IPythonConsole if Hs : mol = self . rdkitmol_Hs else : mol = self . rdkitmol return Draw . MolToImage ( mol , size = ( width , height ) ) except : return 'Rdkit is required for this feature.'
3048	def _implicit_credentials_from_files ( ) : credentials_filename = _get_environment_variable_file ( ) if not credentials_filename : credentials_filename = _get_well_known_file ( ) if os . path . isfile ( credentials_filename ) : extra_help = ( ' (produced automatically when running' ' "gcloud auth login" command)' ) else : credentials_filename = None else : extra_help = ( ' (pointed to by ' + GOOGLE_APPLICATION_CREDENTIALS + ' environment variable)' ) if not credentials_filename : return SETTINGS . env_name = DEFAULT_ENV_NAME try : return _get_application_default_credential_from_file ( credentials_filename ) except ( ApplicationDefaultCredentialsError , ValueError ) as error : _raise_exception_for_reading_json ( credentials_filename , extra_help , error )
2290	def create_graph_from_data ( self , data ) : warnings . warn ( "An exhaustive search of the causal structure of CGNN without" " skeleton is super-exponential in the number of variables." ) nb_vars = len ( list ( data . columns ) ) data = scale ( data . values ) . astype ( 'float32' ) candidates = [ np . reshape ( np . array ( i ) , ( nb_vars , nb_vars ) ) for i in itertools . product ( [ 0 , 1 ] , repeat = nb_vars * nb_vars ) if ( np . trace ( np . reshape ( np . array ( i ) , ( nb_vars , nb_vars ) ) ) == 0 and nx . is_directed_acyclic_graph ( nx . DiGraph ( np . reshape ( np . array ( i ) , ( nb_vars , nb_vars ) ) ) ) ) ] warnings . warn ( "A total of {} graphs will be evaluated." . format ( len ( candidates ) ) ) scores = [ parallel_graph_evaluation ( data , i , nh = self . nh , nb_runs = self . nb_runs , gpu = self . gpu , nb_jobs = self . nb_jobs , lr = self . lr , train_epochs = self . train_epochs , test_epochs = self . test_epochs , verbose = self . verbose ) for i in candidates ] final_candidate = candidates [ scores . index ( min ( scores ) ) ] output = np . zeros ( final_candidate . shape ) for ( i , j ) , x in np . ndenumerate ( final_candidate ) : if x > 0 : cand = final_candidate cand [ i , j ] = 0 output [ i , j ] = min ( scores ) - scores [ candidates . index ( cand ) ] return nx . DiGraph ( candidates [ output ] , { idx : i for idx , i in enumerate ( data . columns ) } )
6007	def load_image ( image_path , image_hdu , pixel_scale ) : return ScaledSquarePixelArray . from_fits_with_pixel_scale ( file_path = image_path , hdu = image_hdu , pixel_scale = pixel_scale )
2155	def set_or_reset_runtime_param ( self , key , value ) : if self . _runtime . has_option ( 'general' , key ) : self . _runtime = self . _new_parser ( ) if value is None : return settings . _runtime . set ( 'general' , key . replace ( 'tower_' , '' ) , six . text_type ( value ) )
6433	def eudex_hamming ( src , tar , weights = 'exponential' , max_length = 8 , normalized = False ) : return Eudex ( ) . dist_abs ( src , tar , weights , max_length , normalized )
2934	def merge_option_and_config_str ( cls , option_name , config , options ) : opt = getattr ( options , option_name , None ) if opt : config . set ( CONFIG_SECTION_NAME , option_name , opt ) elif config . has_option ( CONFIG_SECTION_NAME , option_name ) : setattr ( options , option_name , config . get ( CONFIG_SECTION_NAME , option_name ) )
1125	def Rule ( name , loc = None ) : @ llrule ( loc , lambda parser : getattr ( parser , name ) . expected ( parser ) ) def rule ( parser ) : return getattr ( parser , name ) ( ) return rule
5959	def ma ( self ) : a = self . array return numpy . ma . MaskedArray ( a , mask = numpy . logical_not ( numpy . isfinite ( a ) ) )
5294	def get_params_for_field ( self , field_name , sort_type = None ) : if not sort_type : if self . initial_sort == field_name : sort_type = 'desc' if self . initial_sort_type == 'asc' else 'asc' else : sort_type = 'asc' self . initial_params [ self . sort_param_name ] = self . sort_fields [ field_name ] self . initial_params [ self . sort_type_param_name ] = sort_type return '?%s' % self . initial_params . urlencode ( )
13528	def printoptions ( ) : x = json . dumps ( environment . options , indent = 4 , sort_keys = True , skipkeys = True , cls = MyEncoder ) print ( x )
3195	def delete ( self , list_id , subscriber_hash ) : subscriber_hash = check_subscriber_hash ( subscriber_hash ) self . list_id = list_id self . subscriber_hash = subscriber_hash return self . _mc_client . _delete ( url = self . _build_path ( list_id , 'members' , subscriber_hash ) )
4847	def _load_data ( self , resource , detail_resource = None , resource_id = None , querystring = None , traverse_pagination = False , default = DEFAULT_VALUE_SAFEGUARD , ) : default_val = default if default != self . DEFAULT_VALUE_SAFEGUARD else { } querystring = querystring if querystring else { } cache_key = utils . get_cache_key ( resource = resource , querystring = querystring , traverse_pagination = traverse_pagination , resource_id = resource_id ) response = cache . get ( cache_key ) if not response : endpoint = getattr ( self . client , resource ) ( resource_id ) endpoint = getattr ( endpoint , detail_resource ) if detail_resource else endpoint response = endpoint . get ( ** querystring ) if traverse_pagination : results = utils . traverse_pagination ( response , endpoint ) response = { 'count' : len ( results ) , 'next' : 'None' , 'previous' : 'None' , 'results' : results , } if response : cache . set ( cache_key , response , settings . ENTERPRISE_API_CACHE_TIMEOUT ) return response or default_val
2034	def MSTORE8 ( self , address , value ) : if istainted ( self . pc ) : for taint in get_taints ( self . pc ) : value = taint_with ( value , taint ) self . _allocate ( address , 1 ) self . _store ( address , Operators . EXTRACT ( value , 0 , 8 ) , 1 )
3739	def Hf_g ( CASRN , AvailableMethods = False , Method = None ) : r def list_methods ( ) : methods = [ ] if CASRN in ATcT_g . index : methods . append ( ATCT_G ) if CASRN in TRC_gas_data . index and not np . isnan ( TRC_gas_data . at [ CASRN , 'Hf' ] ) : methods . append ( TRC ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == ATCT_G : _Hfg = float ( ATcT_g . at [ CASRN , 'Hf_298K' ] ) elif Method == TRC : _Hfg = float ( TRC_gas_data . at [ CASRN , 'Hf' ] ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' ) return _Hfg
5111	def _get_queues ( g , queues , edge , edge_type ) : INT = numbers . Integral if isinstance ( queues , INT ) : queues = [ queues ] elif queues is None : if edge is not None : if isinstance ( edge , tuple ) : if isinstance ( edge [ 0 ] , INT ) and isinstance ( edge [ 1 ] , INT ) : queues = [ g . edge_index [ edge ] ] elif isinstance ( edge [ 0 ] , collections . Iterable ) : if np . array ( [ len ( e ) == 2 for e in edge ] ) . all ( ) : queues = [ g . edge_index [ e ] for e in edge ] else : queues = [ g . edge_index [ edge ] ] elif edge_type is not None : if isinstance ( edge_type , collections . Iterable ) : edge_type = set ( edge_type ) else : edge_type = set ( [ edge_type ] ) tmp = [ ] for e in g . edges ( ) : if g . ep ( e , 'edge_type' ) in edge_type : tmp . append ( g . edge_index [ e ] ) queues = np . array ( tmp , int ) if queues is None : queues = range ( g . number_of_edges ( ) ) return queues
9248	def generate_header ( self , newer_tag_name , newer_tag_link , newer_tag_time , older_tag_link , project_url ) : log = "" time_string = newer_tag_time . strftime ( self . options . date_format ) if self . options . release_url : release_url = self . options . release_url . format ( newer_tag_link ) else : release_url = u"{project_url}/tree/{newer_tag_link}" . format ( project_url = project_url , newer_tag_link = newer_tag_link ) if not self . options . unreleased_with_date and newer_tag_name == self . options . unreleased_label : log += u"## [{newer_tag_name}]({release_url})\n\n" . format ( newer_tag_name = newer_tag_name , release_url = release_url ) else : log += u"## [{newer_tag_name}]({release_url}) " u"({time_string})\n" . format ( newer_tag_name = newer_tag_name , release_url = release_url , time_string = time_string ) if self . options . compare_link and older_tag_link != REPO_CREATED_TAG_NAME : log += u"[Full Changelog]" log += u"({project_url}/compare/{older_tag_link}" . format ( project_url = project_url , older_tag_link = older_tag_link , ) log += u"...{newer_tag_link})\n\n" . format ( newer_tag_link = newer_tag_link ) return log
1561	def get_sources ( self , component_id ) : StreamId = namedtuple ( 'StreamId' , 'id, component_name' ) if component_id in self . inputs : ret = { } for istream in self . inputs . get ( component_id ) : key = StreamId ( id = istream . stream . id , component_name = istream . stream . component_name ) ret [ key ] = istream . gtype return ret else : return None
11539	def set_pin_type ( self , pin , ptype ) : if type ( pin ) is list : for p in pin : self . set_pin_type ( p , ptype ) return pin_id = self . _pin_mapping . get ( pin , None ) if type ( ptype ) is not ahio . PortType : raise KeyError ( 'ptype must be of type ahio.PortType' ) elif pin_id : self . _set_pin_type ( pin_id , ptype ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
4561	def simpixel ( new = 0 , autoraise = True ) : simpixel_driver . open_browser ( new = new , autoraise = autoraise )
4121	def twosided_2_centerdc ( data ) : N = len ( data ) newpsd = np . concatenate ( ( cshift ( data [ N // 2 : ] , 1 ) , data [ 0 : N // 2 ] ) ) newpsd [ 0 ] = data [ - 1 ] return newpsd
12792	def post ( self , url = None , post_data = { } , parse_data = False , key = None , parameters = None , listener = None ) : return self . _fetch ( "POST" , url , post_data = post_data , parse_data = parse_data , key = key , parameters = parameters , listener = listener , full_return = True )
11861	def consistent_with ( event , evidence ) : "Is event consistent with the given evidence?" return every ( lambda ( k , v ) : evidence . get ( k , v ) == v , event . items ( ) )
4134	def get_md5sum ( src_file ) : with open ( src_file , 'r' ) as src_data : src_content = src_data . read ( ) if sys . version_info [ 0 ] == 3 : src_content = src_content . encode ( 'utf-8' ) src_md5 = hashlib . md5 ( src_content ) . hexdigest ( ) return src_md5
2886	def is_connected ( self , callback ) : index = self . _weakly_connected_index ( callback ) if index is not None : return True if self . hard_subscribers is None : return False return callback in self . _hard_callbacks ( )
4146	def WelchPeriodogram ( data , NFFT = None , sampling = 1. , ** kargs ) : r from pylab import psd spectrum = Spectrum ( data , sampling = 1. ) P = psd ( data , NFFT , Fs = sampling , ** kargs ) spectrum . psd = P [ 0 ] return P , spectrum
6066	def einstein_radius_rescaled ( self ) : return ( ( 3 - self . slope ) / ( 1 + self . axis_ratio ) ) * self . einstein_radius ** ( self . slope - 1 )
2231	def lookup ( self , data ) : query_hash_type = data . __class__ key = ( query_hash_type . __module__ , query_hash_type . __name__ ) try : hash_type , hash_func = self . keyed_extensions [ key ] except KeyError : raise TypeError ( 'No registered hash func for hashable type=%r' % ( query_hash_type ) ) return hash_func
11185	def publish ( quiet , dataset_uri ) : access_uri = http_publish ( dataset_uri ) if not quiet : click . secho ( "Dataset accessible at " , nl = False , fg = "green" ) click . secho ( access_uri )
6163	def QPSK_rx ( fc , N_symb , Rs , EsN0 = 100 , fs = 125 , lfsr_len = 10 , phase = 0 , pulse = 'src' ) : Ns = int ( np . round ( fs / Rs ) ) print ( 'Ns = ' , Ns ) print ( 'Rs = ' , fs / float ( Ns ) ) print ( 'EsN0 = ' , EsN0 , 'dB' ) print ( 'phase = ' , phase , 'degrees' ) print ( 'pulse = ' , pulse ) x , b , data = QPSK_bb ( N_symb , Ns , lfsr_len , pulse ) x = cpx_AWGN ( x , EsN0 , Ns ) n = np . arange ( len ( x ) ) xc = x * np . exp ( 1j * 2 * np . pi * fc / float ( fs ) * n ) * np . exp ( 1j * phase ) return xc , b , data
7778	def __from_xml ( self , data ) : ns = get_node_ns ( data ) if ns and ns . getContent ( ) != VCARD_NS : raise ValueError ( "Not in the %r namespace" % ( VCARD_NS , ) ) if data . name != "vCard" : raise ValueError ( "Bad root element name: %r" % ( data . name , ) ) n = data . children dns = get_node_ns ( data ) while n : if n . type != 'element' : n = n . next continue ns = get_node_ns ( n ) if ( ns and dns and ns . getContent ( ) != dns . getContent ( ) ) : n = n . next continue if not self . components . has_key ( n . name ) : n = n . next continue cl , tp = self . components [ n . name ] if tp in ( "required" , "optional" ) : if self . content . has_key ( n . name ) : raise ValueError ( "Duplicate %s" % ( n . name , ) ) try : self . content [ n . name ] = cl ( n . name , n ) except Empty : pass elif tp == "multi" : if not self . content . has_key ( n . name ) : self . content [ n . name ] = [ ] try : self . content [ n . name ] . append ( cl ( n . name , n ) ) except Empty : pass n = n . next
5636	def mod2md ( module , title , title_api_section , toc = True , maxdepth = 0 ) : docstr = module . __doc__ text = doctrim ( docstr ) lines = text . split ( '\n' ) sections = find_sections ( lines ) if sections : level = min ( n for n , t in sections ) - 1 else : level = 1 api_md = [ ] api_sec = [ ] if title_api_section and module . __all__ : sections . append ( ( level + 1 , title_api_section ) ) for name in module . __all__ : api_sec . append ( ( level + 2 , "`" + name + "`" ) ) api_md += [ '' , '' ] entry = module . __dict__ [ name ] if entry . __doc__ : md , sec = doc2md ( entry . __doc__ , "`" + name + "`" , min_level = level + 2 , more_info = True , toc = False ) api_sec += sec api_md += md sections += api_sec head = next ( ( i for i , l in enumerate ( lines ) if is_heading ( l ) ) , 0 ) md = [ make_heading ( level , title ) , "" , ] + lines [ : head ] if toc : md += make_toc ( sections , maxdepth ) md += [ '' ] md += _doc2md ( lines [ head : ] ) md += [ '' , '' , make_heading ( level + 1 , title_api_section ) , ] if toc : md += [ '' ] md += make_toc ( api_sec , 1 ) md += api_md return "\n" . join ( md )
11308	def get_image ( self , obj ) : if self . _meta . image_field : return getattr ( obj , self . _meta . image_field )
618	def parseBool ( s ) : l = s . lower ( ) if l in ( "true" , "t" , "1" ) : return True if l in ( "false" , "f" , "0" ) : return False raise Exception ( "Unable to convert string '%s' to a boolean value" % s )
13424	def get_message ( self , message_id ) : url = "/2/messages/%s" % message_id return self . message_from_json ( self . _get_resource ( url ) [ "message" ] )
4620	def unlock ( self , password ) : self . password = password if self . config_key in self . config and self . config [ self . config_key ] : self . _decrypt_masterpassword ( ) else : self . _new_masterpassword ( password ) self . _save_encrypted_masterpassword ( )
11140	def reset ( self ) : self . __path = None self . __repo = { 'repository_unique_name' : str ( uuid . uuid1 ( ) ) , 'create_utctime' : time . time ( ) , 'last_update_utctime' : None , 'pyrep_version' : str ( __version__ ) , 'repository_information' : '' , 'walk_repo' : [ ] }
9771	def stop ( ctx , yes ) : user , project_name , _job = get_job_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'job' ) ) if not yes and not click . confirm ( "Are sure you want to stop " "job `{}`" . format ( _job ) ) : click . echo ( 'Existing without stopping job.' ) sys . exit ( 0 ) try : PolyaxonClient ( ) . job . stop ( user , project_name , _job ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not stop job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Job is being stopped." )
2441	def add_annotation_date ( self , doc , annotation_date ) : if len ( doc . annotations ) != 0 : if not self . annotation_date_set : self . annotation_date_set = True date = utils . datetime_from_iso_format ( annotation_date ) if date is not None : doc . annotations [ - 1 ] . annotation_date = date return True else : raise SPDXValueError ( 'Annotation::AnnotationDate' ) else : raise CardinalityError ( 'Annotation::AnnotationDate' ) else : raise OrderError ( 'Annotation::AnnotationDate' )
8811	def filter_factory ( global_conf , ** local_conf ) : conf = global_conf . copy ( ) conf . update ( local_conf ) def wrapper ( app ) : return ResponseAsyncIdAdder ( app , conf ) return wrapper
10874	def get_hsym_asym ( rho , z , get_hdet = False , include_K3_det = True , ** kwargs ) : K1 , Kprefactor = get_K ( rho , z , K = 1 , get_hdet = get_hdet , Kprefactor = None , return_Kprefactor = True , ** kwargs ) K2 = get_K ( rho , z , K = 2 , get_hdet = get_hdet , Kprefactor = Kprefactor , return_Kprefactor = False , ** kwargs ) if get_hdet and not include_K3_det : K3 = 0 * K1 else : K3 = get_K ( rho , z , K = 3 , get_hdet = get_hdet , Kprefactor = Kprefactor , return_Kprefactor = False , ** kwargs ) hsym = K1 * K1 . conj ( ) + K2 * K2 . conj ( ) + 0.5 * ( K3 * K3 . conj ( ) ) hasym = K1 * K2 . conj ( ) + K2 * K1 . conj ( ) + 0.5 * ( K3 * K3 . conj ( ) ) return hsym . real , hasym . real
3366	def _process_flux_dataframe ( flux_dataframe , fva , threshold , floatfmt ) : abs_flux = flux_dataframe [ 'flux' ] . abs ( ) flux_threshold = threshold * abs_flux . max ( ) if fva is None : flux_dataframe = flux_dataframe . loc [ abs_flux >= flux_threshold , : ] . copy ( ) else : flux_dataframe = flux_dataframe . loc [ ( abs_flux >= flux_threshold ) | ( flux_dataframe [ 'fmin' ] . abs ( ) >= flux_threshold ) | ( flux_dataframe [ 'fmax' ] . abs ( ) >= flux_threshold ) , : ] . copy ( ) if fva is None : flux_dataframe [ 'is_input' ] = ( flux_dataframe [ 'flux' ] >= 0 ) flux_dataframe [ 'flux' ] = flux_dataframe [ 'flux' ] . abs ( ) else : def get_direction ( flux , fmin , fmax ) : if flux < 0 : return - 1 elif flux > 0 : return 1 elif ( fmax > 0 ) & ( fmin <= 0 ) : return 1 elif ( fmax < 0 ) & ( fmin >= 0 ) : return - 1 elif ( ( fmax + fmin ) / 2 ) < 0 : return - 1 else : return 1 sign = flux_dataframe . apply ( lambda x : get_direction ( x . flux , x . fmin , x . fmax ) , 1 ) flux_dataframe [ 'is_input' ] = sign == 1 flux_dataframe . loc [ : , [ 'flux' , 'fmin' , 'fmax' ] ] = flux_dataframe . loc [ : , [ 'flux' , 'fmin' , 'fmax' ] ] . multiply ( sign , 0 ) . astype ( 'float' ) . round ( 6 ) flux_dataframe . loc [ : , [ 'flux' , 'fmin' , 'fmax' ] ] = flux_dataframe . loc [ : , [ 'flux' , 'fmin' , 'fmax' ] ] . applymap ( lambda x : x if abs ( x ) > 1E-6 else 0 ) if fva is not None : flux_dataframe [ 'fva_fmt' ] = flux_dataframe . apply ( lambda x : ( "[{0.fmin:" + floatfmt + "}, {0.fmax:" + floatfmt + "}]" ) . format ( x ) , 1 ) flux_dataframe = flux_dataframe . sort_values ( by = [ 'flux' , 'fmax' , 'fmin' , 'id' ] , ascending = [ False , False , False , True ] ) else : flux_dataframe = flux_dataframe . sort_values ( by = [ 'flux' , 'id' ] , ascending = [ False , True ] ) return flux_dataframe
9804	def activate ( username ) : try : PolyaxonClient ( ) . user . activate_user ( username ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not activate user `{}`.' . format ( username ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "User `{}` was activated successfully." . format ( username ) )
8131	def merge ( self , layers ) : layers . sort ( ) if layers [ 0 ] == 0 : del layers [ 0 ] self . flatten ( layers )
3302	def _get_checked_path ( path , config , must_exist = True , allow_none = True ) : if path in ( None , "" ) : if allow_none : return None raise ValueError ( "Invalid path {!r}" . format ( path ) ) config_file = config . get ( "_config_file" ) if config_file and not os . path . isabs ( path ) : path = os . path . normpath ( os . path . join ( os . path . dirname ( config_file ) , path ) ) else : path = os . path . abspath ( path ) if must_exist and not os . path . exists ( path ) : raise ValueError ( "Invalid path {!r}" . format ( path ) ) return path
13679	def register_json ( self , data ) : j = json . loads ( data ) self . last_data_timestamp = datetime . datetime . utcnow ( ) . replace ( microsecond = 0 ) . isoformat ( ) try : for v in j : self . data [ v [ self . id_key ] ] = { } self . data [ v [ self . id_key ] ] [ self . id_key ] = v [ self . id_key ] self . data [ v [ self . id_key ] ] [ self . value_key ] = v [ self . value_key ] if self . unit_key in v : self . data [ v [ self . id_key ] ] [ self . unit_key ] = v [ self . unit_key ] if self . threshold_key in v : self . data [ v [ self . id_key ] ] [ self . threshold_key ] = v [ self . threshold_key ] for k in self . other_keys : if k in v : self . data [ v [ self . id_key ] ] [ k ] = v [ k ] if self . sensor_time_key in v : self . data [ v [ self . sensor_time_key ] ] [ self . sensor_time_key ] = v [ self . sensor_time_key ] self . data [ v [ self . id_key ] ] [ self . time_key ] = self . last_data_timestamp except KeyError as e : print ( "The main key was not found on the serial input line: " + str ( e ) ) except ValueError as e : print ( "No valid JSON string received. Waiting for the next turn." ) print ( "The error was: " + str ( e ) )
13024	def get ( self , pk ) : if type ( pk ) == str : try : pk = int ( pk ) except ValueError : pass return self . select ( "SELECT {0} FROM " + self . table + " WHERE " + self . pk + " = {1};" , self . columns , pk )
13713	def run ( self ) : self . log . debug ( 'consumer is running...' ) self . running = True while self . running : self . upload ( ) self . log . debug ( 'consumer exited.' )
4891	def export ( self ) : enrollment_queryset = EnterpriseCourseEnrollment . objects . select_related ( 'enterprise_customer_user' ) . filter ( enterprise_customer_user__enterprise_customer = self . enterprise_customer , enterprise_customer_user__active = True , ) . order_by ( 'course_id' ) course_details = None for enterprise_enrollment in enrollment_queryset : course_id = enterprise_enrollment . course_id if course_details is None or course_details [ 'course_id' ] != course_id : if self . course_api is None : self . course_api = CourseApiClient ( ) course_details = self . course_api . get_course_details ( course_id ) if course_details is None : LOGGER . error ( "No course run details found for enrollment [%d]: [%s]" , enterprise_enrollment . pk , course_id ) continue consent = DataSharingConsent . objects . proxied_get ( username = enterprise_enrollment . enterprise_customer_user . username , course_id = enterprise_enrollment . course_id , enterprise_customer = enterprise_enrollment . enterprise_customer_user . enterprise_customer ) if not consent . granted or enterprise_enrollment . audit_reporting_disabled : continue if course_details . get ( 'pacing' ) == 'instructor' : completed_date , grade , is_passing = self . _collect_certificate_data ( enterprise_enrollment ) else : completed_date , grade , is_passing = self . _collect_grades_data ( enterprise_enrollment , course_details ) records = self . get_learner_data_records ( enterprise_enrollment = enterprise_enrollment , completed_date = completed_date , grade = grade , is_passing = is_passing , ) if records : for record in records : yield record
10493	def clickMouseButtonLeft ( self , coord , interval = None ) : modFlags = 0 self . _queueMouseButton ( coord , Quartz . kCGMouseButtonLeft , modFlags ) if interval : self . _postQueuedEvents ( interval = interval ) else : self . _postQueuedEvents ( )
1805	def SETA ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , Operators . OR ( cpu . CF , cpu . ZF ) == False , 1 , 0 ) )
2516	def p_file_notice ( self , f_term , predicate ) : try : for _ , _ , notice in self . graph . triples ( ( f_term , predicate , None ) ) : self . builder . set_file_notice ( self . doc , six . text_type ( notice ) ) except CardinalityError : self . more_than_one_error ( 'file notice' )
3675	def charge ( self ) : r try : if not self . rdkitmol : return charge_from_formula ( self . formula ) else : return Chem . GetFormalCharge ( self . rdkitmol ) except : return charge_from_formula ( self . formula )
2386	def create_model_path ( model_path ) : if not model_path . startswith ( "/" ) and not model_path . startswith ( "models/" ) : model_path = "/" + model_path if not model_path . startswith ( "models" ) : model_path = "models" + model_path if not model_path . endswith ( ".p" ) : model_path += ".p" return model_path
2209	def parse_requirements_alt ( fname = 'requirements.txt' ) : import requirements from os . path import dirname , join , exists require_fpath = join ( dirname ( __file__ ) , fname ) if exists ( require_fpath ) : with open ( require_fpath , 'r' ) as file : requires = list ( requirements . parse ( file ) ) packages = [ r . name for r in requires ] return packages return [ ]
12076	def frameAndSave ( abf , tag = "" , dataType = "plot" , saveAsFname = False , closeWhenDone = True ) : print ( "closeWhenDone" , closeWhenDone ) plt . tight_layout ( ) plt . subplots_adjust ( top = .93 , bottom = .07 ) plt . annotate ( tag , ( .01 , .99 ) , xycoords = 'figure fraction' , ha = 'left' , va = 'top' , family = 'monospace' , size = 10 , alpha = .5 ) msgBot = "%s [%s]" % ( abf . ID , abf . protocomment ) plt . annotate ( msgBot , ( .01 , .01 ) , xycoords = 'figure fraction' , ha = 'left' , va = 'bottom' , family = 'monospace' , size = 10 , alpha = .5 ) fname = tag . lower ( ) . replace ( " " , '_' ) + ".jpg" fname = dataType + "_" + fname plt . tight_layout ( ) if IMAGE_SAVE : abf . log . info ( "saving [%s]" , fname ) try : if saveAsFname : saveAs = os . path . abspath ( saveAsFname ) else : saveAs = os . path . abspath ( abf . outPre + fname ) if not os . path . exists ( abf . outFolder ) : os . mkdir ( abf . outFolder ) plt . savefig ( saveAs ) except Exception as E : abf . log . error ( "saving [%s] failed! 'pip install pillow'?" , fname ) print ( E ) if IMAGE_SHOW == True : if closeWhenDone == False : print ( "NOT SHOWING (because closeWhenDone==True and showing would mess things up)" ) else : abf . log . info ( "showing [%s]" , fname ) plt . show ( ) if closeWhenDone : print ( "closing figure" ) plt . close ( 'all' )
8537	def push ( self , ip_packet ) : data_len = len ( ip_packet . data . data ) seq_id = ip_packet . data . seq if data_len == 0 : self . _next_seq_id = seq_id return False if self . _next_seq_id != - 1 and seq_id != self . _next_seq_id : return False self . _next_seq_id = seq_id + data_len with self . _lock_packets : self . _length += len ( ip_packet . data . data ) self . _remaining += len ( ip_packet . data . data ) self . _packets . append ( ip_packet ) return True
7506	def _dump_qmc ( self ) : io5 = h5py . File ( self . database . output , 'r' ) self . files . qdump = os . path . join ( self . dirs , self . name + ".quartets.txt" ) LOGGER . info ( "qdump file %s" , self . files . qdump ) outfile = open ( self . files . qdump , 'w' ) for idx in xrange ( 0 , self . params . nquartets , self . _chunksize ) : masked_quartets = io5 [ "quartets" ] [ idx : idx + self . _chunksize , : ] quarts = [ list ( j ) for j in masked_quartets ] chunk = [ "{},{}|{},{}" . format ( * i ) for i in quarts ] outfile . write ( "\n" . join ( chunk ) + "\n" ) outfile . close ( ) io5 . close ( )
11658	def transform ( self , X ) : X = check_array ( X , copy = self . copy ) X *= self . scale_ X += self . min_ if self . truncate : np . maximum ( self . feature_range [ 0 ] , X , out = X ) np . minimum ( self . feature_range [ 1 ] , X , out = X ) return X
7843	def get_type ( self ) : item_type = self . xmlnode . prop ( "type" ) if not item_type : item_type = "?" return item_type . decode ( "utf-8" )
2523	def p_file_lic_conc ( self , f_term , predicate ) : try : for _ , _ , licenses in self . graph . triples ( ( f_term , predicate , None ) ) : if ( licenses , RDF . type , self . spdx_namespace [ 'ConjunctiveLicenseSet' ] ) in self . graph : lics = self . handle_conjunctive_list ( licenses ) self . builder . set_concluded_license ( self . doc , lics ) elif ( licenses , RDF . type , self . spdx_namespace [ 'DisjunctiveLicenseSet' ] ) in self . graph : lics = self . handle_disjunctive_list ( licenses ) self . builder . set_concluded_license ( self . doc , lics ) else : try : lics = self . handle_lics ( licenses ) self . builder . set_concluded_license ( self . doc , lics ) except SPDXValueError : self . value_error ( 'FILE_SINGLE_LICS' , licenses ) except CardinalityError : self . more_than_one_error ( 'file {0}' . format ( predicate ) )
558	def bestModelInSprint ( self , sprintIdx ) : swarms = self . getAllSwarms ( sprintIdx ) bestModelId = None bestErrScore = numpy . inf for swarmId in swarms : ( modelId , errScore ) = self . _hsObj . _resultsDB . bestModelIdAndErrScore ( swarmId ) if errScore < bestErrScore : bestModelId = modelId bestErrScore = errScore return ( bestModelId , bestErrScore )
13319	def deactivate ( ) : if 'CPENV_ACTIVE' not in os . environ or 'CPENV_CLEAN_ENV' not in os . environ : raise EnvironmentError ( 'Can not deactivate environment...' ) utils . restore_env_from_file ( os . environ [ 'CPENV_CLEAN_ENV' ] )
13166	def parse_query ( query ) : parts = query . split ( '/' ) norm = [ ] for p in parts : p = p . strip ( ) if p : norm . append ( p ) elif '' not in norm : norm . append ( '' ) return norm
11942	def stored_messages_archive ( context , num_elements = 10 ) : if "user" in context : user = context [ "user" ] if user . is_authenticated ( ) : qs = MessageArchive . objects . select_related ( "message" ) . filter ( user = user ) return { "messages" : qs [ : num_elements ] , "count" : qs . count ( ) , }
12501	def _smooth_data_array ( arr , affine , fwhm , copy = True ) : if arr . dtype . kind == 'i' : if arr . dtype == np . int64 : arr = arr . astype ( np . float64 ) else : arr = arr . astype ( np . float32 ) if copy : arr = arr . copy ( ) arr [ np . logical_not ( np . isfinite ( arr ) ) ] = 0 try : affine = affine [ : 3 , : 3 ] fwhm_sigma_ratio = np . sqrt ( 8 * np . log ( 2 ) ) vox_size = np . sqrt ( np . sum ( affine ** 2 , axis = 0 ) ) sigma = fwhm / ( fwhm_sigma_ratio * vox_size ) for n , s in enumerate ( sigma ) : ndimage . gaussian_filter1d ( arr , s , output = arr , axis = n ) except : raise ValueError ( 'Error smoothing the array.' ) else : return arr
11058	def start ( self ) : self . bot_start_time = datetime . now ( ) self . webserver = Webserver ( self . config [ 'webserver' ] [ 'host' ] , self . config [ 'webserver' ] [ 'port' ] ) self . plugins . load ( ) self . plugins . load_state ( ) self . _find_event_handlers ( ) self . sc = ThreadedSlackClient ( self . config [ 'slack_token' ] ) self . always_send_dm = [ '_unauthorized_' ] if 'always_send_dm' in self . config : self . always_send_dm . extend ( map ( lambda x : '!' + x , self . config [ 'always_send_dm' ] ) ) logging . getLogger ( 'Rocket.Errors.ThreadPool' ) . setLevel ( logging . INFO ) self . is_setup = True if self . test_mode : self . metrics [ 'startup_time' ] = ( datetime . now ( ) - self . bot_start_time ) . total_seconds ( ) * 1000.0
12665	def apply_mask ( image , mask_img ) : img = check_img ( image ) mask = check_img ( mask_img ) check_img_compatibility ( img , mask ) vol = img . get_data ( ) mask_data , _ = load_mask_data ( mask ) return vol [ mask_data ] , mask_data
9690	def start ( self ) : self . receiver = self . Receiver ( self . read , self . write , self . send_lock , self . senders , self . frames_received , callback = self . receive_callback , fcs_nack = self . fcs_nack , ) self . receiver . start ( )
6421	def encode ( self , word ) : word = unicode_normalize ( 'NFKD' , text_type ( word . upper ( ) ) ) word = word . translate ( { 198 : 'AE' , 338 : 'OE' } ) word = '' . join ( c for c in word if c in self . _uc_set ) for rule in self . _rule_order : regex , repl = self . _rule_table [ rule ] if isinstance ( regex , text_type ) : word = word . replace ( regex , repl ) else : word = regex . sub ( repl , word ) return word
6452	def dist_abs ( self , src , tar ) : if tar == src : return 0 elif not src : return len ( tar ) elif not tar : return len ( src ) src_bag = Counter ( src ) tar_bag = Counter ( tar ) return max ( sum ( ( src_bag - tar_bag ) . values ( ) ) , sum ( ( tar_bag - src_bag ) . values ( ) ) , )
10621	def get_element_mass_dictionary ( self ) : element_symbols = self . material . elements element_masses = self . get_element_masses ( ) return { s : m for s , m in zip ( element_symbols , element_masses ) }
7361	async def _connect ( self ) : logger . debug ( "connecting to the stream" ) await self . client . setup if self . session is None : self . session = self . client . _session kwargs = await self . client . headers . prepare_request ( ** self . kwargs ) request = self . client . error_handler ( self . session . request ) return await request ( timeout = 0 , ** kwargs )
8646	def create_milestone_request ( session , project_id , bid_id , description , amount ) : milestone_request_data = { 'project_id' : project_id , 'bid_id' : bid_id , 'description' : description , 'amount' : amount , } response = make_post_request ( session , 'milestone_requests' , json_data = milestone_request_data ) json_data = response . json ( ) if response . status_code == 200 : milestone_request_data = json_data [ 'result' ] return MilestoneRequest ( milestone_request_data ) else : raise MilestoneRequestNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
7756	def _set_response_handlers ( self , stanza , res_handler , err_handler , timeout_handler = None , timeout = None ) : self . fix_out_stanza ( stanza ) to_jid = stanza . to_jid if to_jid : to_jid = unicode ( to_jid ) if timeout_handler : def callback ( dummy1 , dummy2 ) : timeout_handler ( ) self . _iq_response_handlers . set_item ( ( stanza . stanza_id , to_jid ) , ( res_handler , err_handler ) , timeout , callback ) else : self . _iq_response_handlers . set_item ( ( stanza . stanza_id , to_jid ) , ( res_handler , err_handler ) , timeout )
13524	def safe_joinall ( greenlets , timeout = None , raise_error = False ) : greenlets = list ( greenlets ) try : gevent . joinall ( greenlets , timeout = timeout , raise_error = raise_error ) except gevent . GreenletExit : [ greenlet . kill ( ) for greenlet in greenlets if not greenlet . ready ( ) ] raise return greenlets
12161	def userFolder ( ) : path = os . path . expanduser ( "~" ) + "/.swhlab/" if not os . path . exists ( path ) : print ( "creating" , path ) os . mkdir ( path ) return os . path . abspath ( path )
5184	def edges ( self , ** kwargs ) : edges = self . _query ( 'edges' , ** kwargs ) for edge in edges : identifier_source = edge [ 'source_type' ] + '[' + edge [ 'source_title' ] + ']' identifier_target = edge [ 'target_type' ] + '[' + edge [ 'target_title' ] + ']' yield Edge ( source = self . resources [ identifier_source ] , target = self . resources [ identifier_target ] , relationship = edge [ 'relationship' ] , node = edge [ 'certname' ] )
6753	def local_renderer ( self ) : if not self . _local_renderer : r = self . create_local_renderer ( ) self . _local_renderer = r return self . _local_renderer
3985	def _dusty_hosts_config ( hosts_specs ) : rules = '' . join ( [ '{} {}\n' . format ( spec [ 'forwarded_ip' ] , spec [ 'host_address' ] ) for spec in hosts_specs ] ) return config_file . create_config_section ( rules )
6961	def _time_independent_equals ( a , b ) : if len ( a ) != len ( b ) : return False result = 0 if isinstance ( a [ 0 ] , int ) : for x , y in zip ( a , b ) : result |= x ^ y else : for x , y in zip ( a , b ) : result |= ord ( x ) ^ ord ( y ) return result == 0
12045	def originFormat ( thing ) : if type ( thing ) is list and type ( thing [ 0 ] ) is dict : return originFormat_listOfDicts ( thing ) if type ( thing ) is list and type ( thing [ 0 ] ) is list : return originFormat_listOfDicts ( dictFlat ( thing ) ) else : print ( " !! I don't know how to format this object!" ) print ( thing )
7902	def set_stream ( self , stream ) : self . jid = stream . me self . stream = stream for r in self . rooms . values ( ) : r . set_stream ( stream )
1309	def _CreateInput ( structure ) -> INPUT : if isinstance ( structure , MOUSEINPUT ) : return INPUT ( InputType . Mouse , _INPUTUnion ( mi = structure ) ) if isinstance ( structure , KEYBDINPUT ) : return INPUT ( InputType . Keyboard , _INPUTUnion ( ki = structure ) ) if isinstance ( structure , HARDWAREINPUT ) : return INPUT ( InputType . Hardware , _INPUTUnion ( hi = structure ) ) raise TypeError ( 'Cannot create INPUT structure!' )
2629	def scale_out ( self , blocks = 1 , block_size = 1 ) : self . config [ 'sites.jetstream.{0}' . format ( self . pool ) ] [ 'flavor' ] count = 0 if blocks == 1 : block_id = len ( self . blocks ) self . blocks [ block_id ] = [ ] for instance_id in range ( 0 , block_size ) : instances = self . server_manager . create ( 'parsl-{0}-{1}' . format ( block_id , instance_id ) , self . client . images . get ( '87e08a17-eae2-4ce4-9051-c561d9a54bde' ) , self . client . flavors . list ( ) [ 0 ] , min_count = 1 , max_count = 1 , userdata = setup_script . format ( engine_config = self . engine_config ) , key_name = 'TG-MCB090174-api-key' , security_groups = [ 'global-ssh' ] , nics = [ { "net-id" : '724a50cf-7f11-4b3b-a884-cd7e6850e39e' , "net-name" : 'PARSL-priv-net' , "v4-fixed-ip" : '' } ] ) self . blocks [ block_id ] . extend ( [ instances ] ) count += 1 return count
918	def warning ( self , msg , * args , ** kwargs ) : self . _baseLogger . warning ( self , self . getExtendedMsg ( msg ) , * args , ** kwargs )
895	def getPredictiveCells ( self ) : previousCell = None predictiveCells = [ ] for segment in self . activeSegments : if segment . cell != previousCell : predictiveCells . append ( segment . cell ) previousCell = segment . cell return predictiveCells
12904	def toIndex ( self , value ) : if self . _isIrNull ( value ) : ret = IR_NULL_STR else : ret = self . _toIndex ( value ) if self . isIndexHashed is False : return ret return md5 ( tobytes ( ret ) ) . hexdigest ( )
717	def __getHyperSearchJobIDFilePath ( cls , permWorkDir , outputLabel ) : basePath = permWorkDir filename = "%s_HyperSearchJobID.pkl" % ( outputLabel , ) filepath = os . path . join ( basePath , filename ) return filepath
12503	def _smooth_array ( arr , affine , fwhm = None , ensure_finite = True , copy = True , ** kwargs ) : if arr . dtype . kind == 'i' : if arr . dtype == np . int64 : arr = arr . astype ( np . float64 ) else : arr = arr . astype ( np . float32 ) if copy : arr = arr . copy ( ) if ensure_finite : arr [ np . logical_not ( np . isfinite ( arr ) ) ] = 0 if fwhm == 'fast' : arr = _fast_smooth_array ( arr ) elif fwhm is not None : affine = affine [ : 3 , : 3 ] fwhm_over_sigma_ratio = np . sqrt ( 8 * np . log ( 2 ) ) vox_size = np . sqrt ( np . sum ( affine ** 2 , axis = 0 ) ) sigma = fwhm / ( fwhm_over_sigma_ratio * vox_size ) for n , s in enumerate ( sigma ) : ndimage . gaussian_filter1d ( arr , s , output = arr , axis = n , ** kwargs ) return arr
715	def __saveHyperSearchJobID ( cls , permWorkDir , outputLabel , hyperSearchJob ) : jobID = hyperSearchJob . getJobID ( ) filePath = cls . __getHyperSearchJobIDFilePath ( permWorkDir = permWorkDir , outputLabel = outputLabel ) if os . path . exists ( filePath ) : _backupFile ( filePath ) d = dict ( hyperSearchJobID = jobID ) with open ( filePath , "wb" ) as jobIdPickleFile : pickle . dump ( d , jobIdPickleFile )
10436	def verifypartialtablecell ( self , window_name , object_name , row_index , column_index , row_text ) : try : value = getcellvalue ( window_name , object_name , row_index , column_index ) if re . searchmatch ( row_text , value ) : return 1 except LdtpServerException : pass return 0
9488	def generate_simple_call ( opcode : int , index : int ) : bs = b"" bs += opcode . to_bytes ( 1 , byteorder = "little" ) if isinstance ( index , int ) : if PY36 : bs += index . to_bytes ( 1 , byteorder = "little" ) else : bs += index . to_bytes ( 2 , byteorder = "little" ) else : bs += index return bs
7232	def get ( self , ID , index = 'vector-web-s' ) : url = self . get_url % index r = self . gbdx_connection . get ( url + ID ) r . raise_for_status ( ) return r . json ( )
5326	def __create_arthur_json ( self , repo , backend_args ) : backend_args = self . _compose_arthur_params ( self . backend_section , repo ) if self . backend_section == 'git' : backend_args [ 'gitpath' ] = os . path . join ( self . REPOSITORY_DIR , repo ) backend_args [ 'tag' ] = self . backend_tag ( repo ) ajson = { "tasks" : [ { } ] } ajson [ "tasks" ] [ 0 ] [ 'task_id' ] = self . backend_tag ( repo ) ajson [ "tasks" ] [ 0 ] [ 'backend' ] = self . backend_section . split ( ":" ) [ 0 ] ajson [ "tasks" ] [ 0 ] [ 'backend_args' ] = backend_args ajson [ "tasks" ] [ 0 ] [ 'category' ] = backend_args [ 'category' ] ajson [ "tasks" ] [ 0 ] [ 'archive' ] = { } ajson [ "tasks" ] [ 0 ] [ 'scheduler' ] = { "delay" : self . ARTHUR_TASK_DELAY } es_col_url = self . _get_collection_url ( ) es_index = self . conf [ self . backend_section ] [ 'raw_index' ] es = ElasticSearch ( es_col_url , es_index ) connector = get_connector_from_name ( self . backend_section ) klass = connector [ 0 ] signature = inspect . signature ( klass . fetch ) last_activity = None filter_ = { "name" : "tag" , "value" : backend_args [ 'tag' ] } if 'from_date' in signature . parameters : last_activity = es . get_last_item_field ( 'metadata__updated_on' , [ filter_ ] ) if last_activity : ajson [ "tasks" ] [ 0 ] [ 'backend_args' ] [ 'from_date' ] = last_activity . isoformat ( ) elif 'offset' in signature . parameters : last_activity = es . get_last_item_field ( 'offset' , [ filter_ ] ) if last_activity : ajson [ "tasks" ] [ 0 ] [ 'backend_args' ] [ 'offset' ] = last_activity if last_activity : logging . info ( "Getting raw item with arthur since %s" , last_activity ) return ( ajson )
6417	def stem ( self , word ) : word = normalize ( 'NFKD' , text_type ( word . lower ( ) ) ) word = '' . join ( c for c in word if c in { 'a' , 'b' , 'c' , 'd' , 'e' , 'f' , 'g' , 'h' , 'i' , 'j' , 'k' , 'l' , 'm' , 'n' , 'o' , 'p' , 'q' , 'r' , 's' , 't' , 'u' , 'v' , 'w' , 'x' , 'y' , 'z' , } ) word = word . replace ( 'j' , 'i' ) . replace ( 'v' , 'u' ) if word [ - 3 : ] == 'que' : if word [ : - 3 ] in self . _keep_que or word == 'que' : return { 'n' : word , 'v' : word } else : word = word [ : - 3 ] noun = word verb = word for endlen in range ( 4 , 0 , - 1 ) : if word [ - endlen : ] in self . _n_endings [ endlen ] : if len ( word ) - 2 >= endlen : noun = word [ : - endlen ] else : noun = word break for endlen in range ( 6 , 0 , - 1 ) : if word [ - endlen : ] in self . _v_endings_strip [ endlen ] : if len ( word ) - 2 >= endlen : verb = word [ : - endlen ] else : verb = word break if word [ - endlen : ] in self . _v_endings_alter [ endlen ] : if word [ - endlen : ] in { 'iuntur' , 'erunt' , 'untur' , 'iunt' , 'unt' , } : new_word = word [ : - endlen ] + 'i' addlen = 1 elif word [ - endlen : ] in { 'beris' , 'bor' , 'bo' } : new_word = word [ : - endlen ] + 'bi' addlen = 2 else : new_word = word [ : - endlen ] + 'eri' addlen = 3 if len ( new_word ) >= 2 + addlen : verb = new_word else : verb = word break return { 'n' : noun , 'v' : verb }
12193	def _respond ( self , channel , text ) : result = self . _format_message ( channel , text ) if result is not None : logger . info ( 'Sending message: %r' , truncate ( result , max_len = 50 ) , ) self . socket . send_str ( result )
6057	def resized_array_2d_from_array_2d_and_resized_shape ( array_2d , resized_shape , origin = ( - 1 , - 1 ) , pad_value = 0.0 ) : y_is_even = int ( array_2d . shape [ 0 ] ) % 2 == 0 x_is_even = int ( array_2d . shape [ 1 ] ) % 2 == 0 if origin is ( - 1 , - 1 ) : if y_is_even : y_centre = int ( array_2d . shape [ 0 ] / 2 ) elif not y_is_even : y_centre = int ( array_2d . shape [ 0 ] / 2 ) if x_is_even : x_centre = int ( array_2d . shape [ 1 ] / 2 ) elif not x_is_even : x_centre = int ( array_2d . shape [ 1 ] / 2 ) origin = ( y_centre , x_centre ) resized_array = np . zeros ( shape = resized_shape ) if y_is_even : y_min = origin [ 0 ] - int ( resized_shape [ 0 ] / 2 ) y_max = origin [ 0 ] + int ( ( resized_shape [ 0 ] / 2 ) ) + 1 elif not y_is_even : y_min = origin [ 0 ] - int ( resized_shape [ 0 ] / 2 ) y_max = origin [ 0 ] + int ( ( resized_shape [ 0 ] / 2 ) ) + 1 if x_is_even : x_min = origin [ 1 ] - int ( resized_shape [ 1 ] / 2 ) x_max = origin [ 1 ] + int ( ( resized_shape [ 1 ] / 2 ) ) + 1 elif not x_is_even : x_min = origin [ 1 ] - int ( resized_shape [ 1 ] / 2 ) x_max = origin [ 1 ] + int ( ( resized_shape [ 1 ] / 2 ) ) + 1 for y_resized , y in enumerate ( range ( y_min , y_max ) ) : for x_resized , x in enumerate ( range ( x_min , x_max ) ) : if y >= 0 and y < array_2d . shape [ 0 ] and x >= 0 and x < array_2d . shape [ 1 ] : if y_resized >= 0 and y_resized < resized_shape [ 0 ] and x_resized >= 0 and x_resized < resized_shape [ 1 ] : resized_array [ y_resized , x_resized ] = array_2d [ y , x ] else : if y_resized >= 0 and y_resized < resized_shape [ 0 ] and x_resized >= 0 and x_resized < resized_shape [ 1 ] : resized_array [ y_resized , x_resized ] = pad_value return resized_array
5948	def remove_legend ( ax = None ) : from pylab import gca , draw if ax is None : ax = gca ( ) ax . legend_ = None draw ( )
11064	def _ignore_event ( self , message ) : if hasattr ( message , 'subtype' ) and message . subtype in self . ignored_events : return True return False
2602	def engine_file ( self ) : return os . path . join ( self . ipython_dir , 'profile_{0}' . format ( self . profile ) , 'security/ipcontroller-engine.json' )
2457	def set_pkg_license_declared ( self , doc , lic ) : self . assert_package_exists ( ) if not self . package_license_declared_set : self . package_license_declared_set = True if validations . validate_lics_conc ( lic ) : doc . package . license_declared = lic return True else : raise SPDXValueError ( 'Package::LicenseDeclared' ) else : raise CardinalityError ( 'Package::LicenseDeclared' )
12544	def nifti_out ( f ) : @ wraps ( f ) def wrapped ( * args , ** kwargs ) : r = f ( * args , ** kwargs ) img = read_img ( args [ 0 ] ) return nib . Nifti1Image ( r , affine = img . get_affine ( ) , header = img . header ) return wrapped
10323	def spanning_2d_grid ( length ) : ret = nx . grid_2d_graph ( length + 2 , length ) for i in range ( length ) : ret . node [ ( 0 , i ) ] [ 'span' ] = 0 ret [ ( 0 , i ) ] [ ( 1 , i ) ] [ 'span' ] = 0 ret . node [ ( length + 1 , i ) ] [ 'span' ] = 1 ret [ ( length + 1 , i ) ] [ ( length , i ) ] [ 'span' ] = 1 return ret
9602	def wait_for_elements ( self , using , value , timeout = 10000 , interval = 1000 , asserter = is_displayed ) : if not callable ( asserter ) : raise TypeError ( 'Asserter must be callable.' ) @ retry ( retry_on_exception = lambda ex : isinstance ( ex , WebDriverException ) , stop_max_delay = timeout , wait_fixed = interval ) def _wait_for_elements ( ctx , using , value ) : els = ctx . elements ( using , value ) if not len ( els ) : raise WebDriverException ( 'no such element' ) else : el = els [ 0 ] asserter ( el ) return els return _wait_for_elements ( self , using , value )
3485	def _create_parameter ( model , pid , value , sbo = None , constant = True , units = None , flux_udef = None ) : parameter = model . createParameter ( ) parameter . setId ( pid ) parameter . setValue ( value ) parameter . setConstant ( constant ) if sbo : parameter . setSBOTerm ( sbo ) if units : parameter . setUnits ( flux_udef . getId ( ) )
5375	def simple_pattern_exists_in_gcs ( file_pattern , credentials = None ) : if '*' not in file_pattern : return _file_exists_in_gcs ( file_pattern , credentials ) if not file_pattern . startswith ( 'gs://' ) : raise ValueError ( 'file name must start with gs://' ) gcs_service = _get_storage_service ( credentials ) bucket_name , prefix = file_pattern [ len ( 'gs://' ) : ] . split ( '/' , 1 ) if '*' in bucket_name : raise ValueError ( 'Wildcards may not appear in the bucket name' ) assert '*' in prefix prefix_no_wildcard = prefix [ : prefix . index ( '*' ) ] request = gcs_service . objects ( ) . list ( bucket = bucket_name , prefix = prefix_no_wildcard ) response = request . execute ( ) if 'items' not in response : return False items_list = [ i [ 'name' ] for i in response [ 'items' ] ] return any ( fnmatch . fnmatch ( i , prefix ) for i in items_list )
8786	def diag_port ( self , context , port_id , ** kwargs ) : LOG . info ( "diag_port %s" % port_id ) try : port = self . _client . show_port ( port_id ) except Exception as e : msg = "failed fetching downstream port: %s" % ( str ( e ) ) LOG . exception ( msg ) raise IronicException ( msg = msg ) return { "downstream_port" : port }
9838	def __gridpositions ( self ) : try : tok = self . __consume ( ) except DXParserNoTokens : return if tok . equals ( 'counts' ) : shape = [ ] try : while True : self . __peek ( ) . value ( 'INTEGER' ) tok = self . __consume ( ) shape . append ( tok . value ( 'INTEGER' ) ) except ( DXParserNoTokens , ValueError ) : pass if len ( shape ) == 0 : raise DXParseError ( 'gridpositions: no shape parameters' ) self . currentobject [ 'shape' ] = shape elif tok . equals ( 'origin' ) : origin = [ ] try : while ( self . __peek ( ) . iscode ( 'INTEGER' ) or self . __peek ( ) . iscode ( 'REAL' ) ) : tok = self . __consume ( ) origin . append ( tok . value ( ) ) except DXParserNoTokens : pass if len ( origin ) == 0 : raise DXParseError ( 'gridpositions: no origin parameters' ) self . currentobject [ 'origin' ] = origin elif tok . equals ( 'delta' ) : d = [ ] try : while ( self . __peek ( ) . iscode ( 'INTEGER' ) or self . __peek ( ) . iscode ( 'REAL' ) ) : tok = self . __consume ( ) d . append ( tok . value ( ) ) except DXParserNoTokens : pass if len ( d ) == 0 : raise DXParseError ( 'gridpositions: missing delta parameters' ) try : self . currentobject [ 'delta' ] . append ( d ) except KeyError : self . currentobject [ 'delta' ] = [ d ] else : raise DXParseError ( 'gridpositions: ' + str ( tok ) + ' not recognized.' )
7527	def align_and_parse ( handle , max_internal_indels = 5 , is_gbs = False ) : try : with open ( handle , 'rb' ) as infile : clusts = infile . read ( ) . split ( "//\n//\n" ) clusts = [ i for i in clusts if i ] if not clusts : raise IPyradError except ( IOError , IPyradError ) : LOGGER . debug ( "skipping empty chunk - {}" . format ( handle ) ) return 0 highindels = 0 try : aligned = persistent_popen_align3 ( clusts , 200 , is_gbs ) except Exception as inst : LOGGER . debug ( "Error in handle - {} - {}" . format ( handle , inst ) ) aligned = [ ] refined = [ ] for clust in aligned : filtered = aligned_indel_filter ( clust , max_internal_indels ) if not filtered : refined . append ( clust ) else : highindels += 1 if refined : outhandle = handle . rsplit ( "." , 1 ) [ 0 ] + ".aligned" with open ( outhandle , 'wb' ) as outfile : outfile . write ( "\n//\n//\n" . join ( refined ) + "\n" ) log_level = logging . getLevelName ( LOGGER . getEffectiveLevel ( ) ) if not log_level == "DEBUG" : os . remove ( handle ) return highindels
9760	def resources ( ctx , job , gpu ) : def get_experiment_resources ( ) : try : message_handler = Printer . gpu_resources if gpu else Printer . resources PolyaxonClient ( ) . experiment . resources ( user , project_name , _experiment , message_handler = message_handler ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get resources for experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) def get_experiment_job_resources ( ) : try : message_handler = Printer . gpu_resources if gpu else Printer . resources PolyaxonClient ( ) . experiment_job . resources ( user , project_name , _experiment , _job , message_handler = message_handler ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get resources for job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) if job : _job = get_experiment_job_or_local ( job ) get_experiment_job_resources ( ) else : get_experiment_resources ( )
8022	def cast ( cls , fx_spot , domestic_curve = None , foreign_curve = None ) : assert domestic_curve . origin == foreign_curve . origin return cls ( fx_spot , domestic_curve = domestic_curve , foreign_curve = foreign_curve )
6247	def get_program ( self , label : str ) -> moderngl . Program : return self . _project . get_program ( label )
13096	def watch ( self ) : wm = pyinotify . WatchManager ( ) self . notifier = pyinotify . Notifier ( wm , default_proc_fun = self . callback ) wm . add_watch ( self . directory , pyinotify . ALL_EVENTS ) try : self . notifier . loop ( ) except ( KeyboardInterrupt , AttributeError ) : print_notification ( "Stopping" ) finally : self . notifier . stop ( ) self . terminate_processes ( )
4357	def remove_namespace ( self , namespace ) : if namespace in self . active_ns : del self . active_ns [ namespace ] if len ( self . active_ns ) == 0 and self . connected : self . kill ( detach = True )
570	def _handleModelRunnerException ( jobID , modelID , jobsDAO , experimentDir , logger , e ) : msg = StringIO . StringIO ( ) print >> msg , "Exception occurred while running model %s: %r (%s)" % ( modelID , e , type ( e ) ) traceback . print_exc ( None , msg ) completionReason = jobsDAO . CMPL_REASON_ERROR completionMsg = msg . getvalue ( ) logger . error ( completionMsg ) if type ( e ) is not InvalidConnectionException : jobsDAO . modelUpdateResults ( modelID , results = None , numRecords = 0 ) if type ( e ) == JobFailException : workerCmpReason = jobsDAO . jobGetFields ( jobID , [ 'workerCompletionReason' ] ) [ 0 ] if workerCmpReason == ClientJobsDAO . CMPL_REASON_SUCCESS : jobsDAO . jobSetFields ( jobID , fields = dict ( cancel = True , workerCompletionReason = ClientJobsDAO . CMPL_REASON_ERROR , workerCompletionMsg = ": " . join ( str ( i ) for i in e . args ) ) , useConnectionID = False , ignoreUnchanged = True ) return ( completionReason , completionMsg )
9285	def close ( self ) : self . _connected = False self . buf = b'' if self . sock is not None : self . sock . close ( )
9222	def reverse_guard ( lst ) : rev = { '<' : '>=' , '>' : '=<' , '>=' : '<' , '=<' : '>' } return [ rev [ l ] if l in rev else l for l in lst ]
7086	def _single_true ( iterable ) : iterator = iter ( iterable ) has_true = any ( iterator ) has_another_true = any ( iterator ) return has_true and not has_another_true
9124	def belanno ( keyword : str , file : TextIO ) : directory = get_data_dir ( keyword ) obo_url = f'http://purl.obolibrary.org/obo/{keyword}.obo' obo_path = os . path . join ( directory , f'{keyword}.obo' ) obo_cache_path = os . path . join ( directory , f'{keyword}.obo.pickle' ) obo_getter = make_obo_getter ( obo_url , obo_path , preparsed_path = obo_cache_path ) graph = obo_getter ( ) convert_obo_graph_to_belanno ( graph , file = file , )
7659	def append_columns ( self , columns ) : self . append_records ( [ dict ( time = t , duration = d , value = v , confidence = c ) for ( t , d , v , c ) in six . moves . zip ( columns [ 'time' ] , columns [ 'duration' ] , columns [ 'value' ] , columns [ 'confidence' ] ) ] )
7705	def remove_item ( self , jid ) : if jid not in self . _jids : raise KeyError ( jid ) index = self . _jids [ jid ] for i in range ( index , len ( self . _jids ) ) : self . _jids [ self . _items [ i ] . jid ] -= 1 del self . _jids [ jid ] del self . _items [ index ]
585	def _deleteRangeFromKNN ( self , start = 0 , end = None ) : classifier = self . htm_prediction_model . _getAnomalyClassifier ( ) knn = classifier . getSelf ( ) . _knn prototype_idx = numpy . array ( classifier . getSelf ( ) . getParameter ( 'categoryRecencyList' ) ) if end is None : end = prototype_idx . max ( ) + 1 idsIdxToDelete = numpy . logical_and ( prototype_idx >= start , prototype_idx < end ) idsToDelete = prototype_idx [ idsIdxToDelete ] nProtos = knn . _numPatterns knn . removeIds ( idsToDelete . tolist ( ) ) assert knn . _numPatterns == nProtos - len ( idsToDelete )
8385	def write_main ( argv ) : if len ( argv ) != 1 : print ( "Please provide the name of a file to write." ) return 1 filename = argv [ 0 ] resource_name = "files/" + filename tweaks_name = amend_filename ( filename , "_tweaks" ) if not pkg_resources . resource_exists ( "edx_lint" , resource_name ) : print ( u"Don't have file %r to write." % filename ) return 2 if os . path . exists ( filename ) : print ( u"Checking existing copy of %s" % filename ) tef = TamperEvidentFile ( filename ) if not tef . validate ( ) : bak_name = amend_filename ( filename , "_backup" ) print ( u"Your copy of %s seems to have been edited, renaming it to %s" % ( filename , bak_name ) ) if os . path . exists ( bak_name ) : print ( u"A previous %s exists, deleting it" % bak_name ) os . remove ( bak_name ) os . rename ( filename , bak_name ) print ( u"Reading edx_lint/files/%s" % filename ) cfg = configparser . RawConfigParser ( ) resource_string = pkg_resources . resource_string ( "edx_lint" , resource_name ) . decode ( "utf8" ) if six . PY2 : cfg . readfp ( cStringIO ( resource_string ) , resource_name ) else : cfg . read_string ( resource_string , resource_name ) if os . path . exists ( tweaks_name ) : print ( u"Applying local tweaks from %s" % tweaks_name ) cfg_tweaks = configparser . RawConfigParser ( ) cfg_tweaks . read ( [ tweaks_name ] ) merge_configs ( cfg , cfg_tweaks ) print ( u"Writing %s" % filename ) output_text = cStringIO ( ) output_text . write ( WARNING_HEADER . format ( filename = filename , tweaks_name = tweaks_name ) ) cfg . write ( output_text ) out_tef = TamperEvidentFile ( filename ) if six . PY2 : output_bytes = output_text . getvalue ( ) else : output_bytes = output_text . getvalue ( ) . encode ( "utf8" ) out_tef . write ( output_bytes ) return 0
12881	def next ( self ) : self . index += 1 t = self . peek ( ) if not self . depth : self . _cut ( ) return t
2631	def _status ( self ) : job_id_list = ' ' . join ( self . resources . keys ( ) ) cmd = "condor_q {0} -af:jr JobStatus" . format ( job_id_list ) retcode , stdout , stderr = super ( ) . execute_wait ( cmd ) for line in stdout . strip ( ) . split ( '\n' ) : parts = line . split ( ) job_id = parts [ 0 ] status = translate_table . get ( parts [ 1 ] , 'UNKNOWN' ) self . resources [ job_id ] [ 'status' ] = status
4623	def _derive_checksum ( self , s ) : checksum = hashlib . sha256 ( bytes ( s , "ascii" ) ) . hexdigest ( ) return checksum [ : 4 ]
1685	def RepositoryName ( self ) : r fullname = self . FullName ( ) if os . path . exists ( fullname ) : project_dir = os . path . dirname ( fullname ) if _repository : repo = FileInfo ( _repository ) . FullName ( ) root_dir = project_dir while os . path . exists ( root_dir ) : if os . path . normcase ( root_dir ) == os . path . normcase ( repo ) : return os . path . relpath ( fullname , root_dir ) . replace ( '\\' , '/' ) one_up_dir = os . path . dirname ( root_dir ) if one_up_dir == root_dir : break root_dir = one_up_dir if os . path . exists ( os . path . join ( project_dir , ".svn" ) ) : root_dir = project_dir one_up_dir = os . path . dirname ( root_dir ) while os . path . exists ( os . path . join ( one_up_dir , ".svn" ) ) : root_dir = os . path . dirname ( root_dir ) one_up_dir = os . path . dirname ( one_up_dir ) prefix = os . path . commonprefix ( [ root_dir , project_dir ] ) return fullname [ len ( prefix ) + 1 : ] root_dir = current_dir = os . path . dirname ( fullname ) while current_dir != os . path . dirname ( current_dir ) : if ( os . path . exists ( os . path . join ( current_dir , ".git" ) ) or os . path . exists ( os . path . join ( current_dir , ".hg" ) ) or os . path . exists ( os . path . join ( current_dir , ".svn" ) ) ) : root_dir = current_dir current_dir = os . path . dirname ( current_dir ) if ( os . path . exists ( os . path . join ( root_dir , ".git" ) ) or os . path . exists ( os . path . join ( root_dir , ".hg" ) ) or os . path . exists ( os . path . join ( root_dir , ".svn" ) ) ) : prefix = os . path . commonprefix ( [ root_dir , project_dir ] ) return fullname [ len ( prefix ) + 1 : ] return fullname
4092	def addSearchers ( self , * searchers ) : self . _searchers . extend ( searchers ) debug . logger & debug . flagCompiler and debug . logger ( 'current compiled MIBs location(s): %s' % ', ' . join ( [ str ( x ) for x in self . _searchers ] ) ) return self
2517	def p_file_comment ( self , f_term , predicate ) : try : for _ , _ , comment in self . graph . triples ( ( f_term , predicate , None ) ) : self . builder . set_file_comment ( self . doc , six . text_type ( comment ) ) except CardinalityError : self . more_than_one_error ( 'file comment' )
7403	def above ( self , ref ) : if not self . _valid_ordering_reference ( ref ) : raise ValueError ( "%r can only be moved above instances of %r which %s equals %r." % ( self , self . __class__ , self . order_with_respect_to , self . _get_order_with_respect_to ( ) ) ) if self . order == ref . order : return if self . order > ref . order : o = ref . order else : o = self . get_ordering_queryset ( ) . filter ( order__lt = ref . order ) . aggregate ( Max ( 'order' ) ) . get ( 'order__max' ) or 0 self . to ( o )
292	def plot_rolling_sharpe ( returns , factor_returns = None , rolling_window = APPROX_BDAYS_PER_MONTH * 6 , legend_loc = 'best' , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) y_axis_formatter = FuncFormatter ( utils . two_dec_places ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) rolling_sharpe_ts = timeseries . rolling_sharpe ( returns , rolling_window ) rolling_sharpe_ts . plot ( alpha = .7 , lw = 3 , color = 'orangered' , ax = ax , ** kwargs ) if factor_returns is not None : rolling_sharpe_ts_factor = timeseries . rolling_sharpe ( factor_returns , rolling_window ) rolling_sharpe_ts_factor . plot ( alpha = .7 , lw = 3 , color = 'grey' , ax = ax , ** kwargs ) ax . set_title ( 'Rolling Sharpe ratio (6-month)' ) ax . axhline ( rolling_sharpe_ts . mean ( ) , color = 'steelblue' , linestyle = '--' , lw = 3 ) ax . axhline ( 0.0 , color = 'black' , linestyle = '-' , lw = 3 ) ax . set_ylabel ( 'Sharpe ratio' ) ax . set_xlabel ( '' ) if factor_returns is None : ax . legend ( [ 'Sharpe' , 'Average' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) else : ax . legend ( [ 'Sharpe' , 'Benchmark Sharpe' , 'Average' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) return ax
7733	def make_kick_request ( self , nick , reason ) : self . clear_muc_child ( ) self . muc_child = MucAdminQuery ( parent = self . xmlnode ) item = MucItem ( "none" , "none" , nick = nick , reason = reason ) self . muc_child . add_item ( item ) return self . muc_child
5382	def _build_pipeline_request ( self , task_view ) : job_metadata = task_view . job_metadata job_params = task_view . job_params job_resources = task_view . job_resources task_metadata = task_view . task_descriptors [ 0 ] . task_metadata task_params = task_view . task_descriptors [ 0 ] . task_params task_resources = task_view . task_descriptors [ 0 ] . task_resources script = task_view . job_metadata [ 'script' ] reserved_labels = google_base . build_pipeline_labels ( job_metadata , task_metadata , task_id_pattern = 'task-%d' ) pipeline = _Pipelines . build_pipeline ( project = self . _project , zones = job_resources . zones , min_cores = job_resources . min_cores , min_ram = job_resources . min_ram , disk_size = job_resources . disk_size , boot_disk_size = job_resources . boot_disk_size , preemptible = job_resources . preemptible , accelerator_type = job_resources . accelerator_type , accelerator_count = job_resources . accelerator_count , image = job_resources . image , script_name = script . name , envs = job_params [ 'envs' ] | task_params [ 'envs' ] , inputs = job_params [ 'inputs' ] | task_params [ 'inputs' ] , outputs = job_params [ 'outputs' ] | task_params [ 'outputs' ] , pipeline_name = job_metadata [ 'pipeline-name' ] ) logging_uri = task_resources . logging_path . uri scopes = job_resources . scopes or google_base . DEFAULT_SCOPES pipeline . update ( _Pipelines . build_pipeline_args ( self . _project , script . value , job_params , task_params , reserved_labels , job_resources . preemptible , logging_uri , scopes , job_resources . keep_alive ) ) return pipeline
4428	async def _now ( self , ctx ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) song = 'Nothing' if player . current : position = lavalink . Utils . format_time ( player . position ) if player . current . stream : duration = '🔴 LIVE' else : duration = lavalink . Utils . format_time ( player . current . duration ) song = f'**[{player.current.title}]({player.current.uri})**\n({position}/{duration})' embed = discord . Embed ( color = discord . Color . blurple ( ) , title = 'Now Playing' , description = song ) await ctx . send ( embed = embed )
5239	def market_open ( self , session , mins ) -> Session : if session not in self . exch : return SessNA start_time = self . exch [ session ] [ 0 ] return Session ( start_time , shift_time ( start_time , int ( mins ) ) )
11858	def make_factor ( var , e , bn ) : node = bn . variable_node ( var ) vars = [ X for X in [ var ] + node . parents if X not in e ] cpt = dict ( ( event_values ( e1 , vars ) , node . p ( e1 [ var ] , e1 ) ) for e1 in all_events ( vars , bn , e ) ) return Factor ( vars , cpt )
1285	def footnote_item ( self , key , text ) : back = ( '<a href="#fnref-%s" class="footnote">&#8617;</a>' ) % escape ( key ) text = text . rstrip ( ) if text . endswith ( '</p>' ) : text = re . sub ( r'<\/p>$' , r'%s</p>' % back , text ) else : text = '%s<p>%s</p>' % ( text , back ) html = '<li id="fn-%s">%s</li>\n' % ( escape ( key ) , text ) return html
3362	def to_yaml ( model , sort = False , ** kwargs ) : obj = model_to_dict ( model , sort = sort ) obj [ "version" ] = YAML_SPEC return yaml . dump ( obj , ** kwargs )
1038	def end ( self ) : return Range ( self . source_buffer , self . end_pos , self . end_pos , expanded_from = self . expanded_from )
12409	def package_info ( cls , package ) : if package not in cls . package_info_cache : package_json_url = 'https://pypi.python.org/pypi/%s/json' % package try : logging . getLogger ( 'requests' ) . setLevel ( logging . WARN ) response = requests . get ( package_json_url ) response . raise_for_status ( ) cls . package_info_cache [ package ] = simplejson . loads ( response . text ) except Exception as e : log . debug ( 'Could not get package info from %s: %s' , package_json_url , e ) cls . package_info_cache [ package ] = None return cls . package_info_cache [ package ]
9651	def check_shastore_version ( from_store , settings ) : sprint = settings [ "sprint" ] error = settings [ "error" ] sprint ( "checking .shastore version for potential incompatibilities" , level = "verbose" ) if not from_store or 'sake version' not in from_store : errmes = [ "Since you've used this project last, a new version of " , "sake was installed that introduced backwards incompatible" , " changes. Run 'sake clean', and rebuild before continuing\n" ] errmes = " " . join ( errmes ) error ( errmes ) sys . exit ( 1 )
3107	def _retrieve_info ( self , http ) : if self . invalid : info = _metadata . get_service_account_info ( http , service_account = self . service_account_email or 'default' ) self . invalid = False self . service_account_email = info [ 'email' ] self . scopes = info [ 'scopes' ]
10110	def write ( self , _force = False , _exists_ok = False , ** items ) : if self . fname and self . fname . exists ( ) : raise ValueError ( 'db file already exists, use force=True to overwrite' ) with self . connection ( ) as db : for table in self . tables : db . execute ( table . sql ( translate = self . translate ) ) db . execute ( 'PRAGMA foreign_keys = ON;' ) db . commit ( ) refs = defaultdict ( list ) for t in self . tables : if t . name not in items : continue rows , keys = [ ] , [ ] cols = { c . name : c for c in t . columns } for i , row in enumerate ( items [ t . name ] ) : pk = row [ t . primary_key [ 0 ] ] if t . primary_key and len ( t . primary_key ) == 1 else None values = [ ] for k , v in row . items ( ) : if k in t . many_to_many : assert pk at = t . many_to_many [ k ] atkey = tuple ( [ at . name ] + [ c . name for c in at . columns ] ) for vv in v : fkey , context = self . association_table_context ( t , k , vv ) refs [ atkey ] . append ( ( pk , fkey , context ) ) else : col = cols [ k ] if isinstance ( v , list ) : v = ( col . separator or ';' ) . join ( col . convert ( vv ) for vv in v ) else : v = col . convert ( v ) if v is not None else None if i == 0 : keys . append ( col . name ) values . append ( v ) rows . append ( tuple ( values ) ) insert ( db , self . translate , t . name , keys , * rows ) for atkey , rows in refs . items ( ) : insert ( db , self . translate , atkey [ 0 ] , atkey [ 1 : ] , * rows ) db . commit ( )
3652	def run ( self ) : while self . _base . is_running : if self . _worker : self . _worker ( ) time . sleep ( self . _sleep_duration )
8571	def get_nic ( self , datacenter_id , server_id , nic_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/servers/%s/nics/%s?depth=%s' % ( datacenter_id , server_id , nic_id , str ( depth ) ) ) return response
5071	def get_configuration_value_for_site ( site , key , default = None ) : if hasattr ( site , 'configuration' ) : return site . configuration . get_value ( key , default ) return default
4562	def recurse ( desc , pre = 'pre_recursion' , post = None , python_path = None ) : def call ( f , desc ) : if isinstance ( f , str ) : f = getattr ( datatype , f , None ) return f and f ( desc ) desc = load . load_if_filename ( desc ) or desc desc = construct . to_type_constructor ( desc , python_path ) datatype = desc . get ( 'datatype' ) desc = call ( pre , desc ) or desc for child_name in getattr ( datatype , 'CHILDREN' , [ ] ) : child = desc . get ( child_name ) if child : is_plural = child_name . endswith ( 's' ) remove_s = is_plural and child_name != 'drivers' cname = child_name [ : - 1 ] if remove_s else child_name new_path = python_path or ( 'bibliopixel.' + cname ) if is_plural : if isinstance ( child , ( dict , str ) ) : child = [ child ] for i , c in enumerate ( child ) : child [ i ] = recurse ( c , pre , post , new_path ) desc [ child_name ] = child else : desc [ child_name ] = recurse ( child , pre , post , new_path ) d = call ( post , desc ) return desc if d is None else d
13867	def truncate ( when , unit , week_start = mon ) : if is_datetime ( when ) : if unit == millisecond : return when . replace ( microsecond = int ( round ( when . microsecond / 1000.0 ) ) * 1000 ) elif unit == second : return when . replace ( microsecond = 0 ) elif unit == minute : return when . replace ( second = 0 , microsecond = 0 ) elif unit == hour : return when . replace ( minute = 0 , second = 0 , microsecond = 0 ) elif unit == day : return when . replace ( hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) elif unit == week : weekday = prevweekday ( when , week_start ) return when . replace ( year = weekday . year , month = weekday . month , day = weekday . day , hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) elif unit == month : return when . replace ( day = 1 , hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) elif unit == year : return when . replace ( month = 1 , day = 1 , hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) elif is_date ( when ) : if unit == week : return prevweekday ( when , week_start ) elif unit == month : return when . replace ( day = 1 ) elif unit == year : return when . replace ( month = 1 , day = 1 ) elif is_time ( when ) : if unit == millisecond : return when . replace ( microsecond = int ( when . microsecond / 1000.0 ) * 1000 ) elif unit == second : return when . replace ( microsecond = 0 ) elif unit == minute : return when . replace ( second = 0 , microsecond = 0 ) return when
10476	def _queueMouseButton ( self , coord , mouseButton , modFlags , clickCount = 1 , dest_coord = None ) : mouseButtons = { Quartz . kCGMouseButtonLeft : 'LeftMouse' , Quartz . kCGMouseButtonRight : 'RightMouse' , } if mouseButton not in mouseButtons : raise ValueError ( 'Mouse button given not recognized' ) eventButtonDown = getattr ( Quartz , 'kCGEvent%sDown' % mouseButtons [ mouseButton ] ) eventButtonUp = getattr ( Quartz , 'kCGEvent%sUp' % mouseButtons [ mouseButton ] ) eventButtonDragged = getattr ( Quartz , 'kCGEvent%sDragged' % mouseButtons [ mouseButton ] ) buttonDown = Quartz . CGEventCreateMouseEvent ( None , eventButtonDown , coord , mouseButton ) Quartz . CGEventSetFlags ( buttonDown , modFlags ) Quartz . CGEventSetIntegerValueField ( buttonDown , Quartz . kCGMouseEventClickState , int ( clickCount ) ) if dest_coord : buttonDragged = Quartz . CGEventCreateMouseEvent ( None , eventButtonDragged , dest_coord , mouseButton ) Quartz . CGEventSetFlags ( buttonDragged , modFlags ) buttonUp = Quartz . CGEventCreateMouseEvent ( None , eventButtonUp , dest_coord , mouseButton ) else : buttonUp = Quartz . CGEventCreateMouseEvent ( None , eventButtonUp , coord , mouseButton ) Quartz . CGEventSetFlags ( buttonUp , modFlags ) Quartz . CGEventSetIntegerValueField ( buttonUp , Quartz . kCGMouseEventClickState , int ( clickCount ) ) self . _queueEvent ( Quartz . CGEventPost , ( Quartz . kCGSessionEventTap , buttonDown ) ) if dest_coord : self . _queueEvent ( Quartz . CGEventPost , ( Quartz . kCGHIDEventTap , buttonDragged ) ) self . _queueEvent ( Quartz . CGEventPost , ( Quartz . kCGSessionEventTap , buttonUp ) )
6308	def load_resource_module ( self ) : try : name = '{}.{}' . format ( self . name , 'dependencies' ) self . dependencies_module = importlib . import_module ( name ) except ModuleNotFoundError as err : raise EffectError ( ( "Effect package '{}' has no 'dependencies' module or the module has errors. " "Forwarded error from importlib: {}" ) . format ( self . name , err ) ) try : self . resources = getattr ( self . dependencies_module , 'resources' ) except AttributeError : raise EffectError ( "Effect dependencies module '{}' has no 'resources' attribute" . format ( name ) ) if not isinstance ( self . resources , list ) : raise EffectError ( "Effect dependencies module '{}': 'resources' is of type {} instead of a list" . format ( name , type ( self . resources ) ) ) try : self . effect_packages = getattr ( self . dependencies_module , 'effect_packages' ) except AttributeError : raise EffectError ( "Effect dependencies module '{}' has 'effect_packages' attribute" . format ( name ) ) if not isinstance ( self . effect_packages , list ) : raise EffectError ( "Effect dependencies module '{}': 'effect_packages' is of type {} instead of a list" . format ( name , type ( self . effects ) ) )
40	def add ( self , * args , ** kwargs ) : idx = self . _next_idx super ( ) . add ( * args , ** kwargs ) self . _it_sum [ idx ] = self . _max_priority ** self . _alpha self . _it_min [ idx ] = self . _max_priority ** self . _alpha
496	def _classifyState ( self , state ) : if state . ROWID < self . getParameter ( 'trainRecords' ) : if not state . setByUser : state . anomalyLabel = [ ] self . _deleteRecordsFromKNN ( [ state ] ) return label = KNNAnomalyClassifierRegion . AUTO_THRESHOLD_CLASSIFIED_LABEL autoLabel = label + KNNAnomalyClassifierRegion . AUTO_TAG newCategory = self . _recomputeRecordFromKNN ( state ) labelList = self . _categoryToLabelList ( newCategory ) if state . setByUser : if label in state . anomalyLabel : state . anomalyLabel . remove ( label ) if autoLabel in state . anomalyLabel : state . anomalyLabel . remove ( autoLabel ) labelList . extend ( state . anomalyLabel ) if state . anomalyScore >= self . getParameter ( 'anomalyThreshold' ) : labelList . append ( label ) elif label in labelList : ind = labelList . index ( label ) labelList [ ind ] = autoLabel labelList = list ( set ( labelList ) ) if label in labelList and autoLabel in labelList : labelList . remove ( autoLabel ) if state . anomalyLabel == labelList : return state . anomalyLabel = labelList if state . anomalyLabel == [ ] : self . _deleteRecordsFromKNN ( [ state ] ) else : self . _addRecordToKNN ( state )
8823	def start_rpc_listeners ( self ) : self . _setup_rpc ( ) if not self . endpoints : return [ ] self . conn = n_rpc . create_connection ( ) self . conn . create_consumer ( self . topic , self . endpoints , fanout = False ) return self . conn . consume_in_threads ( )
11102	def make_zip_archive ( self , dst = None , filters = all_true , compress = True , overwrite = False , makedirs = False , verbose = False ) : self . assert_exists ( ) if dst is None : dst = self . _auto_zip_archive_dst ( ) else : dst = self . change ( new_abspath = dst ) if not dst . basename . lower ( ) . endswith ( ".zip" ) : raise ValueError ( "zip archive name has to be endswith '.zip'!" ) if dst . exists ( ) : if not overwrite : raise IOError ( "'%s' already exists!" % dst ) if compress : compression = ZIP_DEFLATED else : compression = ZIP_STORED if not dst . parent . exists ( ) : if makedirs : os . makedirs ( dst . parent . abspath ) if verbose : msg = "Making zip archive for '%s' ..." % self print ( msg ) current_dir = os . getcwd ( ) if self . is_dir ( ) : total_size = 0 selected = list ( ) for p in self . glob ( "**/*" ) : if filters ( p ) : selected . append ( p ) total_size += p . size if verbose : msg = "Got {} files, total size is {}, compressing ..." . format ( len ( selected ) , repr_data_size ( total_size ) , ) print ( msg ) with ZipFile ( dst . abspath , "w" , compression ) as f : os . chdir ( self . abspath ) for p in selected : relpath = p . relative_to ( self ) . __str__ ( ) f . write ( relpath ) elif self . is_file ( ) : with ZipFile ( dst . abspath , "w" , compression ) as f : os . chdir ( self . parent . abspath ) f . write ( self . basename ) os . chdir ( current_dir ) if verbose : msg = "Complete! Archive size is {}." . format ( dst . size_in_text ) print ( msg )
2576	def _add_input_deps ( self , executor , args , kwargs ) : if executor == 'data_manager' : return args , kwargs inputs = kwargs . get ( 'inputs' , [ ] ) for idx , f in enumerate ( inputs ) : if isinstance ( f , File ) and f . is_remote ( ) : inputs [ idx ] = self . data_manager . stage_in ( f , executor ) for kwarg , f in kwargs . items ( ) : if isinstance ( f , File ) and f . is_remote ( ) : kwargs [ kwarg ] = self . data_manager . stage_in ( f , executor ) newargs = list ( args ) for idx , f in enumerate ( newargs ) : if isinstance ( f , File ) and f . is_remote ( ) : newargs [ idx ] = self . data_manager . stage_in ( f , executor ) return tuple ( newargs ) , kwargs
9216	def file ( self , filename ) : with open ( filename ) as f : self . lexer . input ( f . read ( ) ) return self
12759	def load_csv ( self , filename , start_frame = 10 , max_frames = int ( 1e300 ) ) : import pandas as pd compression = None if filename . endswith ( '.gz' ) : compression = 'gzip' df = pd . read_csv ( filename , compression = compression ) . set_index ( 'time' ) . fillna ( - 1 ) assert self . world . dt == pd . Series ( df . index ) . diff ( ) . mean ( ) markers = [ ] for c in df . columns : m = re . match ( r'^marker\d\d-(.*)-c$' , c ) if m : markers . append ( m . group ( 1 ) ) self . channels = self . _map_labels_to_channels ( markers ) cols = [ c for c in df . columns if re . match ( r'^marker\d\d-.*-[xyzc]$' , c ) ] self . data = df [ cols ] . values . reshape ( ( len ( df ) , len ( markers ) , 4 ) ) [ start_frame : ] self . data [ : , : , [ 1 , 2 ] ] = self . data [ : , : , [ 2 , 1 ] ] logging . info ( '%s: loaded marker data %s' , filename , self . data . shape ) self . process_data ( ) self . create_bodies ( )
8033	def find_dupes ( paths , exact = False , ignores = None , min_size = 0 ) : groups = { '' : getPaths ( paths , ignores ) } groups = groupBy ( groups , sizeClassifier , 'sizes' , min_size = min_size ) groups = groupBy ( groups , hashClassifier , 'header hashes' , limit = HEAD_SIZE ) if exact : groups = groupBy ( groups , groupByContent , fun_desc = 'contents' ) else : groups = groupBy ( groups , hashClassifier , fun_desc = 'hashes' ) return groups
8137	def brightness ( self , value = 1.0 ) : b = ImageEnhance . Brightness ( self . img ) self . img = b . enhance ( value )
10866	def update ( self , params , values ) : params = listify ( params ) values = listify ( values ) for i , p in enumerate ( params ) : if ( p [ - 2 : ] == '-a' ) and ( values [ i ] < 0 ) : values [ i ] = 0.0 super ( PlatonicSpheresCollection , self ) . update ( params , values )
3541	def do_apply ( mutation_pk , dict_synonyms , backup ) : filename , mutation_id = filename_and_mutation_id_from_pk ( int ( mutation_pk ) ) update_line_numbers ( filename ) context = Context ( mutation_id = mutation_id , filename = filename , dict_synonyms = dict_synonyms , ) mutate_file ( backup = backup , context = context , ) if context . number_of_performed_mutations == 0 : raise RuntimeError ( 'No mutations performed.' )
5708	def process_request ( self , request ) : try : session = request . session except AttributeError : raise ImproperlyConfigured ( 'django-lockdown requires the Django ' 'sessions framework' ) if settings . ENABLED is False : return None if self . remote_addr_exceptions : remote_addr_exceptions = self . remote_addr_exceptions else : remote_addr_exceptions = settings . REMOTE_ADDR_EXCEPTIONS if remote_addr_exceptions : trusted_proxies = self . trusted_proxies or settings . TRUSTED_PROXIES remote_addr = request . META . get ( 'REMOTE_ADDR' ) if remote_addr in remote_addr_exceptions : return None if remote_addr in trusted_proxies : x_forwarded_for = request . META . get ( 'HTTP_X_FORWARDED_FOR' ) if x_forwarded_for : remote_addr = x_forwarded_for . split ( ',' ) [ - 1 ] . strip ( ) if remote_addr in remote_addr_exceptions : return None if self . url_exceptions : url_exceptions = compile_url_exceptions ( self . url_exceptions ) else : url_exceptions = compile_url_exceptions ( settings . URL_EXCEPTIONS ) for pattern in url_exceptions : if pattern . search ( request . path ) : return None try : resolved_path = resolve ( request . path ) except Resolver404 : pass else : if resolved_path . func in settings . VIEW_EXCEPTIONS : return None if self . until_date : until_date = self . until_date else : until_date = settings . UNTIL_DATE if self . after_date : after_date = self . after_date else : after_date = settings . AFTER_DATE if until_date or after_date : locked_date = False if until_date and datetime . datetime . now ( ) < until_date : locked_date = True if after_date and datetime . datetime . now ( ) > after_date : locked_date = True if not locked_date : return None form_data = request . POST if request . method == 'POST' else None if self . form : form_class = self . form else : form_class = get_lockdown_form ( settings . FORM ) form = form_class ( data = form_data , ** self . form_kwargs ) authorized = False token = session . get ( self . session_key ) if hasattr ( form , 'authenticate' ) : if form . authenticate ( token ) : authorized = True elif token is True : authorized = True if authorized and self . logout_key and self . logout_key in request . GET : if self . session_key in session : del session [ self . session_key ] querystring = request . GET . copy ( ) del querystring [ self . logout_key ] return self . redirect ( request ) if authorized : return None if form . is_valid ( ) : if hasattr ( form , 'generate_token' ) : token = form . generate_token ( ) else : token = True session [ self . session_key ] = token return self . redirect ( request ) page_data = { 'until_date' : until_date , 'after_date' : after_date } if not hasattr ( form , 'show_form' ) or form . show_form ( ) : page_data [ 'form' ] = form if self . extra_context : page_data . update ( self . extra_context ) return render ( request , 'lockdown/form.html' , page_data )
6507	def excerpt ( self ) : if "content" not in self . _results_fields : return None match_phrases = [ self . _match_phrase ] if six . PY2 : separate_phrases = [ phrase . decode ( 'utf-8' ) for phrase in shlex . split ( self . _match_phrase . encode ( 'utf-8' ) ) ] else : separate_phrases = [ phrase for phrase in shlex . split ( self . _match_phrase ) ] if len ( separate_phrases ) > 1 : match_phrases . extend ( separate_phrases ) else : match_phrases = separate_phrases matches = SearchResultProcessor . find_matches ( SearchResultProcessor . strings_in_dictionary ( self . _results_fields [ "content" ] ) , match_phrases , DESIRED_EXCERPT_LENGTH ) excerpt_text = ELLIPSIS . join ( matches ) for match_word in match_phrases : excerpt_text = SearchResultProcessor . decorate_matches ( excerpt_text , match_word ) return excerpt_text
4415	def add_at ( self , index : int , requester : int , track : dict ) : self . queue . insert ( min ( index , len ( self . queue ) - 1 ) , AudioTrack ( ) . build ( track , requester ) )
6167	def to_bin ( data , width ) : data_str = bin ( data & ( 2 ** width - 1 ) ) [ 2 : ] . zfill ( width ) return [ int ( x ) for x in tuple ( data_str ) ]
11641	def yaml_get_data ( filename ) : with open ( filename , 'rb' ) as fd : yaml_data = yaml . load ( fd ) return yaml_data return False
2119	def _parent_filter ( self , parent , relationship , ** kwargs ) : if parent is None or relationship is None : return { } parent_filter_kwargs = { } query_params = ( ( self . _reverse_rel_name ( relationship ) , parent ) , ) parent_filter_kwargs [ 'query' ] = query_params if kwargs . get ( 'workflow_job_template' , None ) is None : parent_data = self . read ( pk = parent ) [ 'results' ] [ 0 ] parent_filter_kwargs [ 'workflow_job_template' ] = parent_data [ 'workflow_job_template' ] return parent_filter_kwargs
5346	def compose_mailing_lists ( projects , data ) : for p in [ project for project in data if len ( data [ project ] [ 'mailing_lists' ] ) > 0 ] : if 'mailing_lists' not in projects [ p ] : projects [ p ] [ 'mailing_lists' ] = [ ] urls = [ url [ 'url' ] . replace ( 'mailto:' , '' ) for url in data [ p ] [ 'mailing_lists' ] if url [ 'url' ] not in projects [ p ] [ 'mailing_lists' ] ] projects [ p ] [ 'mailing_lists' ] += urls for p in [ project for project in data if len ( data [ project ] [ 'dev_list' ] ) > 0 ] : if 'mailing_lists' not in projects [ p ] : projects [ p ] [ 'mailing_lists' ] = [ ] mailing_list = data [ p ] [ 'dev_list' ] [ 'url' ] . replace ( 'mailto:' , '' ) projects [ p ] [ 'mailing_lists' ] . append ( mailing_list ) return projects
7215	def list ( self ) : r = self . gbdx_connection . get ( self . _base_url ) raise_for_status ( r ) return r . json ( ) [ 'tasks' ]
11118	def get_parent_directory_info ( self , relativePath ) : relativePath = os . path . normpath ( relativePath ) if relativePath in ( '' , '.' ) : return self , "relativePath is empty pointing to the repostitory itself." parentDirPath , _ = os . path . split ( relativePath ) return self . get_directory_info ( parentDirPath )
12611	def search_by_eid ( self , table_name , eid ) : elem = self . table ( table_name ) . get ( eid = eid ) if elem is None : raise KeyError ( 'Could not find {} with eid {}.' . format ( table_name , eid ) ) return elem
1469	def process_tick ( self , tup ) : curtime = int ( time . time ( ) ) window_info = WindowContext ( curtime - self . window_duration , curtime ) self . processWindow ( window_info , list ( self . current_tuples ) ) for tup in self . current_tuples : self . ack ( tup ) self . current_tuples . clear ( )
3514	def chartbeat_top ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return ChartbeatTopNode ( )
8394	def main ( argv = None ) : if argv is None : argv = sys . argv [ 1 : ] if not argv or argv [ 0 ] == "help" : show_help ( ) return 0 elif argv [ 0 ] == "check" : return check_main ( argv [ 1 : ] ) elif argv [ 0 ] == "list" : return list_main ( argv [ 1 : ] ) elif argv [ 0 ] == "write" : return write_main ( argv [ 1 : ] ) else : print ( u"Don't understand {!r}" . format ( " " . join ( argv ) ) ) show_help ( ) return 1
11711	def add_logged_in_session ( self , response = None ) : if not response : response = self . get ( 'go/api/pipelines.xml' ) self . _set_session_cookie ( response ) if not self . _session_id : raise AuthenticationFailed ( 'No session id extracted from request.' ) response = self . get ( 'go/pipelines' ) match = re . search ( r'name="authenticity_token".+?value="([^"]+)' , response . read ( ) . decode ( 'utf-8' ) ) if match : self . _authenticity_token = match . group ( 1 ) else : raise AuthenticationFailed ( 'Authenticity token not found on page' )
2949	def execute ( self , task , script , ** kwargs ) : locals ( ) . update ( kwargs ) exec ( script )
52	def on ( self , image ) : shape = normalize_shape ( image ) if shape [ 0 : 2 ] == self . shape [ 0 : 2 ] : return self . deepcopy ( ) else : keypoints = [ kp . project ( self . shape , shape ) for kp in self . keypoints ] return self . deepcopy ( keypoints , shape )
9546	def add_record_check ( self , record_check , modulus = 1 ) : assert callable ( record_check ) , 'record check must be a callable function' t = record_check , modulus self . _record_checks . append ( t )
4949	def _transform_item ( self , content_metadata_item ) : content_metadata_type = content_metadata_item [ 'content_type' ] transformed_item = { } for integrated_channel_schema_key , edx_data_schema_key in self . DATA_TRANSFORM_MAPPING . items ( ) : transformer = ( getattr ( self , 'transform_{content_type}_{edx_data_schema_key}' . format ( content_type = content_metadata_type , edx_data_schema_key = edx_data_schema_key ) , None ) or getattr ( self , 'transform_{edx_data_schema_key}' . format ( edx_data_schema_key = edx_data_schema_key ) , None ) ) if transformer : transformed_item [ integrated_channel_schema_key ] = transformer ( content_metadata_item ) else : try : transformed_item [ integrated_channel_schema_key ] = content_metadata_item [ edx_data_schema_key ] except KeyError : LOGGER . exception ( 'Failed to transform content metadata item field [%s] for [%s]: [%s]' , edx_data_schema_key , self . enterprise_customer . name , content_metadata_item , ) return transformed_item
11657	def fit ( self , X , y = None ) : X = check_array ( X , copy = self . copy , dtype = [ np . float64 , np . float32 , np . float16 , np . float128 ] ) feature_range = self . feature_range if feature_range [ 0 ] >= feature_range [ 1 ] : raise ValueError ( "Minimum of desired feature range must be smaller" " than maximum. Got %s." % str ( feature_range ) ) if self . fit_feature_range is not None : fit_feature_range = self . fit_feature_range if fit_feature_range [ 0 ] >= fit_feature_range [ 1 ] : raise ValueError ( "Minimum of desired (fit) feature range must " "be smaller than maximum. Got %s." % str ( feature_range ) ) if ( fit_feature_range [ 0 ] < feature_range [ 0 ] or fit_feature_range [ 1 ] > feature_range [ 1 ] ) : raise ValueError ( "fit_feature_range must be a subset of " "feature_range. Got %s, fit %s." % ( str ( feature_range ) , str ( fit_feature_range ) ) ) feature_range = fit_feature_range data_min = np . min ( X , axis = 0 ) data_range = np . max ( X , axis = 0 ) - data_min data_range [ data_range == 0.0 ] = 1.0 self . scale_ = ( feature_range [ 1 ] - feature_range [ 0 ] ) / data_range self . min_ = feature_range [ 0 ] - data_min * self . scale_ self . data_range = data_range self . data_min = data_min return self
4632	def child ( self , offset256 ) : a = bytes ( self ) + offset256 s = hashlib . sha256 ( a ) . digest ( ) return self . add ( s )
7604	def get_player_chests ( self , tag : crtag , timeout : int = None ) : url = self . api . PLAYER + '/' + tag + '/upcomingchests' return self . _get_model ( url , timeout = timeout )
5719	def _convert_path ( path , name ) : table = os . path . splitext ( path ) [ 0 ] table = table . replace ( os . path . sep , '__' ) if name is not None : table = ' ' . join ( [ table , name ] ) table = re . sub ( '[^0-9a-zA-Z_]+' , '_' , table ) table = table . lower ( ) return table
13717	def request ( self , batch , attempt = 0 ) : try : q = self . api . new_queue ( ) for msg in batch : q . add ( msg [ 'event' ] , msg [ 'value' ] , source = msg [ 'source' ] ) q . submit ( ) except : if attempt > self . retries : raise self . request ( batch , attempt + 1 )
7815	def run ( self ) : if self . args . roster_cache and os . path . exists ( self . args . roster_cache ) : logging . info ( u"Loading roster from {0!r}" . format ( self . args . roster_cache ) ) try : self . client . roster_client . load_roster ( self . args . roster_cache ) except ( IOError , ValueError ) , err : logging . error ( u"Could not load the roster: {0!r}" . format ( err ) ) self . client . connect ( ) self . client . run ( )
8990	def first_consumed_mesh ( self ) : for instruction in self . instructions : if instruction . consumes_meshes ( ) : return instruction . first_consumed_mesh raise IndexError ( "{} consumes no meshes" . format ( self ) )
12345	def stitch ( self , folder = None ) : debug ( 'stitching ' + self . __str__ ( ) ) if not folder : folder = self . path macros = [ ] files = [ ] for well in self . wells : f , m = stitch_macro ( well , folder ) macros . extend ( m ) files . extend ( f ) chopped_arguments = zip ( chop ( macros , _pools ) , chop ( files , _pools ) ) chopped_filenames = Parallel ( n_jobs = _pools ) ( delayed ( fijibin . macro . run ) ( macro = arg [ 0 ] , output_files = arg [ 1 ] ) for arg in chopped_arguments ) return [ f for list_ in chopped_filenames for f in list_ ]
6171	def freq_resp ( self , mode = 'dB' , fs = 8000 , ylim = [ - 100 , 2 ] ) : iir_d . freqz_resp_cas_list ( [ self . sos ] , mode , fs = fs ) pylab . grid ( ) pylab . ylim ( ylim )
12982	def file ( file_object , start_on = None , ignore = ( ) , use_short = True , ** queries ) : return string ( file_object . read ( ) , start_on = start_on , ignore = ignore , use_short = use_short , ** queries )
8599	def get_share ( self , group_id , resource_id , depth = 1 ) : response = self . _perform_request ( '/um/groups/%s/shares/%s?depth=%s' % ( group_id , resource_id , str ( depth ) ) ) return response
2202	def ensure_app_cache_dir ( appname , * args ) : from ubelt import util_path dpath = get_app_cache_dir ( appname , * args ) util_path . ensuredir ( dpath ) return dpath
8323	def isList ( l ) : return hasattr ( l , '__iter__' ) or ( type ( l ) in ( types . ListType , types . TupleType ) )
4110	def ac2poly ( data ) : a , e , _c = LEVINSON ( data ) a = numpy . insert ( a , 0 , 1 ) return a , e
6733	def str_to_list ( s ) : if s is None : return [ ] elif isinstance ( s , ( tuple , list ) ) : return s elif not isinstance ( s , six . string_types ) : raise NotImplementedError ( 'Unknown type: %s' % type ( s ) ) return [ _ . strip ( ) . lower ( ) for _ in ( s or '' ) . split ( ',' ) if _ . strip ( ) ]
12878	def many_until1 ( these , term ) : first = [ these ( ) ] these_results , term_result = many_until ( these , term ) return ( first + these_results , term_result )
8317	def parse_balanced_image ( self , markup ) : opened = 0 closed = 0 for i in range ( len ( markup ) ) : if markup [ i ] == "[" : opened += 1 if markup [ i ] == "]" : closed += 1 if opened == closed : return markup [ : i + 1 ] return markup
8425	def grey_pal ( start = 0.2 , end = 0.8 ) : gamma = 2.2 ends = ( ( 0.0 , start , start ) , ( 1.0 , end , end ) ) cdict = { 'red' : ends , 'green' : ends , 'blue' : ends } grey_cmap = mcolors . LinearSegmentedColormap ( 'grey' , cdict ) def continuous_grey_palette ( n ) : colors = [ ] for x in np . linspace ( start ** gamma , end ** gamma , n ) : x = ( x ** ( 1. / gamma ) - start ) / ( end - start ) colors . append ( mcolors . rgb2hex ( grey_cmap ( x ) ) ) return colors return continuous_grey_palette
4910	def _create_session ( self , scope ) : now = datetime . datetime . utcnow ( ) if self . session is None or self . expires_at is None or now >= self . expires_at : if self . session : self . session . close ( ) oauth_access_token , expires_at = self . _get_oauth_access_token ( self . enterprise_configuration . key , self . enterprise_configuration . secret , self . enterprise_configuration . degreed_user_id , self . enterprise_configuration . degreed_user_password , scope ) session = requests . Session ( ) session . timeout = self . SESSION_TIMEOUT session . headers [ 'Authorization' ] = 'Bearer {}' . format ( oauth_access_token ) session . headers [ 'content-type' ] = 'application/json' self . session = session self . expires_at = expires_at
2214	def repr2 ( data , ** kwargs ) : custom_extensions = kwargs . get ( 'extensions' , None ) _return_info = kwargs . get ( '_return_info' , False ) kwargs [ '_root_info' ] = _rectify_root_info ( kwargs . get ( '_root_info' , None ) ) outstr = None _leaf_info = None if custom_extensions : func = custom_extensions . lookup ( data ) if func is not None : outstr = func ( data , ** kwargs ) if outstr is None : if isinstance ( data , dict ) : outstr , _leaf_info = _format_dict ( data , ** kwargs ) elif isinstance ( data , ( list , tuple , set , frozenset ) ) : outstr , _leaf_info = _format_list ( data , ** kwargs ) if outstr is None : func = _FORMATTER_EXTENSIONS . lookup ( data ) if func is not None : outstr = func ( data , ** kwargs ) else : outstr = _format_object ( data , ** kwargs ) if _return_info : _leaf_info = _rectify_leaf_info ( _leaf_info ) return outstr , _leaf_info else : return outstr
27	def nn ( input , layers_sizes , reuse = None , flatten = False , name = "" ) : for i , size in enumerate ( layers_sizes ) : activation = tf . nn . relu if i < len ( layers_sizes ) - 1 else None input = tf . layers . dense ( inputs = input , units = size , kernel_initializer = tf . contrib . layers . xavier_initializer ( ) , reuse = reuse , name = name + '_' + str ( i ) ) if activation : input = activation ( input ) if flatten : assert layers_sizes [ - 1 ] == 1 input = tf . reshape ( input , [ - 1 ] ) return input
3952	def read_adc ( self , channel , gain = 1 , data_rate = None ) : assert 0 <= channel <= 3 , 'Channel must be a value within 0-3!' return self . _read ( channel + 0x04 , gain , data_rate , ADS1x15_CONFIG_MODE_SINGLE )
6814	def configure ( self ) : print ( 'env.services:' , self . genv . services ) for service in list ( self . genv . services ) : service = service . strip ( ) . upper ( ) funcs = common . service_configurators . get ( service , [ ] ) if funcs : print ( '!' * 80 ) print ( 'Configuring service %s...' % ( service , ) ) for func in funcs : print ( 'Function:' , func ) if not self . dryrun : func ( )
366	def projective_transform_by_points ( x , src , dst , map_args = None , output_shape = None , order = 1 , mode = 'constant' , cval = 0.0 , clip = True , preserve_range = False ) : if map_args is None : map_args = { } if isinstance ( src , list ) : src = np . array ( src ) if isinstance ( dst , list ) : dst = np . array ( dst ) if np . max ( x ) > 1 : x = x / 255 m = transform . ProjectiveTransform ( ) m . estimate ( dst , src ) warped = transform . warp ( x , m , map_args = map_args , output_shape = output_shape , order = order , mode = mode , cval = cval , clip = clip , preserve_range = preserve_range ) return warped
3931	def _auth_with_refresh_token ( session , refresh_token ) : token_request_data = { 'client_id' : OAUTH2_CLIENT_ID , 'client_secret' : OAUTH2_CLIENT_SECRET , 'grant_type' : 'refresh_token' , 'refresh_token' : refresh_token , } res = _make_token_request ( session , token_request_data ) return res [ 'access_token' ]
11730	def pvpc_calc_tcu_cp_feu_d ( df , verbose = True , convert_kwh = True ) : if 'TCU' + TARIFAS [ 0 ] not in df . columns : if convert_kwh : cols_mwh = [ c + t for c in COLS_PVPC for t in TARIFAS if c != 'COF' ] df [ cols_mwh ] = df [ cols_mwh ] . applymap ( lambda x : x / 1000. ) gb_t = df . groupby ( lambda x : TARIFAS [ np . argmax ( [ t in x for t in TARIFAS ] ) ] , axis = 1 ) for k , g in gb_t : if verbose : print ( 'TARIFA {}' . format ( k ) ) print ( g . head ( ) ) df [ 'TCU{}' . format ( k ) ] = g [ k ] - g [ 'TEU{}' . format ( k ) ] cols_cp = [ c + k for c in COLS_PVPC if c not in [ '' , 'COF' , 'TEU' ] ] df [ 'CP{}' . format ( k ) ] = g [ cols_cp ] . sum ( axis = 1 ) cols_k = [ 'TEU' + k , 'TCU' + k , 'COF' + k ] g = df [ cols_k ] . groupby ( 'TEU' + k ) pr = g . apply ( lambda x : x [ 'TCU' + k ] . dot ( x [ 'COF' + k ] ) / x [ 'COF' + k ] . sum ( ) ) pr . name = 'PD_' + k df = df . join ( pr , on = 'TEU' + k , rsuffix = '_r' ) df [ 'PD_' + k ] += df [ 'TEU' + k ] return df
10553	def create_helpingmaterial ( project_id , info , media_url = None , file_path = None ) : try : helping = dict ( project_id = project_id , info = info , media_url = None , ) if file_path : files = { 'file' : open ( file_path , 'rb' ) } payload = { 'project_id' : project_id } res = _pybossa_req ( 'post' , 'helpingmaterial' , payload = payload , files = files ) else : res = _pybossa_req ( 'post' , 'helpingmaterial' , payload = helping ) if res . get ( 'id' ) : return HelpingMaterial ( res ) else : return res except : raise
9144	def drop ( connection , skip ) : for idx , name , manager in _iterate_managers ( connection , skip ) : click . secho ( f'dropping {name}' , fg = 'cyan' , bold = True ) manager . drop_all ( )
12030	def comments_load ( self ) : self . comment_times , self . comment_sweeps , self . comment_tags = [ ] , [ ] , [ ] self . comments = 0 self . comment_text = "" try : self . comment_tags = list ( self . ABFblock . segments [ 0 ] . eventarrays [ 0 ] . annotations [ 'comments' ] ) self . comment_times = list ( self . ABFblock . segments [ 0 ] . eventarrays [ 0 ] . times / self . trace . itemsize ) self . comment_sweeps = list ( self . comment_times ) except : for events in self . ABFblock . segments [ 0 ] . events : self . comment_tags = events . annotations [ 'comments' ] . tolist ( ) self . comment_times = np . array ( events . times . magnitude / self . trace . itemsize ) self . comment_sweeps = self . comment_times / self . sweepInterval for i , c in enumerate ( self . comment_tags ) : self . comment_tags [ i ] = c . decode ( "utf-8" )
9853	def centers ( self ) : for idx in numpy . ndindex ( self . grid . shape ) : yield self . delta * numpy . array ( idx ) + self . origin
9668	def _make_package ( args ) : from lingpy . sequence . sound_classes import token2class from lingpy . data import Model columns = [ 'LATEX' , 'FEATURES' , 'SOUND' , 'IMAGE' , 'COUNT' , 'NOTE' ] bipa = TranscriptionSystem ( 'bipa' ) for src , rows in args . repos . iter_sources ( type = 'td' ) : args . log . info ( 'TranscriptionData {0} ...' . format ( src [ 'NAME' ] ) ) uritemplate = URITemplate ( src [ 'URITEMPLATE' ] ) if src [ 'URITEMPLATE' ] else None out = [ [ 'BIPA_GRAPHEME' , 'CLTS_NAME' , 'GENERATED' , 'EXPLICIT' , 'GRAPHEME' , 'URL' ] + columns ] graphemes = set ( ) for row in rows : if row [ 'GRAPHEME' ] in graphemes : args . log . warn ( 'skipping duplicate grapheme: {0}' . format ( row [ 'GRAPHEME' ] ) ) continue graphemes . add ( row [ 'GRAPHEME' ] ) if not row [ 'BIPA' ] : bipa_sound = bipa [ row [ 'GRAPHEME' ] ] explicit = '' else : bipa_sound = bipa [ row [ 'BIPA' ] ] explicit = '+' generated = '+' if bipa_sound . generated else '' if is_valid_sound ( bipa_sound , bipa ) : bipa_grapheme = bipa_sound . s bipa_name = bipa_sound . name else : bipa_grapheme , bipa_name = '<NA>' , '<NA>' url = uritemplate . expand ( ** row ) if uritemplate else row . get ( 'URL' , '' ) out . append ( [ bipa_grapheme , bipa_name , generated , explicit , row [ 'GRAPHEME' ] , url ] + [ row . get ( c , '' ) for c in columns ] ) found = len ( [ o for o in out if o [ 0 ] != '<NA>' ] ) args . log . info ( '... {0} of {1} graphemes found ({2:.0f}%)' . format ( found , len ( out ) , found / len ( out ) * 100 ) ) with UnicodeWriter ( pkg_path ( 'transcriptiondata' , '{0}.tsv' . format ( src [ 'NAME' ] ) ) , delimiter = '\t' ) as writer : writer . writerows ( out ) count = 0 with UnicodeWriter ( pkg_path ( 'soundclasses' , 'lingpy.tsv' ) , delimiter = '\t' ) as writer : writer . writerow ( [ 'CLTS_NAME' , 'BIPA_GRAPHEME' ] + SOUNDCLASS_SYSTEMS ) for grapheme , sound in sorted ( bipa . sounds . items ( ) ) : if not sound . alias : writer . writerow ( [ sound . name , grapheme ] + [ token2class ( grapheme , Model ( cls ) ) for cls in SOUNDCLASS_SYSTEMS ] ) count += 1 args . log . info ( 'SoundClasses: {0} written to file.' . format ( count ) )
13239	def intervals ( self , range_start = datetime . datetime . min , range_end = datetime . datetime . max ) : current_period = None max_continuous_days = 60 range_start = self . to_timezone ( range_start ) range_end = self . to_timezone ( range_end ) for period in self . _daily_periods ( range_start . date ( ) , range_end . date ( ) ) : if period . end < range_start or period . start > range_end : continue if current_period is None : current_period = period else : if ( ( ( period . start < current_period . end ) or ( period . start - current_period . end ) <= datetime . timedelta ( minutes = 1 ) ) and ( current_period . end - current_period . start ) < datetime . timedelta ( days = max_continuous_days ) ) : current_period = Period ( current_period . start , period . end ) else : yield current_period current_period = period if current_period : yield current_period
9680	def config ( self ) : config = [ ] data = { } self . cnxn . xfer ( [ 0x3C ] ) sleep ( 10e-3 ) for i in range ( 256 ) : resp = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] config . append ( resp ) for i in range ( 0 , 15 ) : data [ "Bin Boundary {0}" . format ( i ) ] = self . _16bit_unsigned ( config [ 2 * i ] , config [ 2 * i + 1 ] ) for i in range ( 0 , 16 ) : data [ "BPV {0}" . format ( i ) ] = self . _calculate_float ( config [ 4 * i + 32 : 4 * i + 36 ] ) for i in range ( 0 , 16 ) : data [ "BPD {0}" . format ( i ) ] = self . _calculate_float ( config [ 4 * i + 96 : 4 * i + 100 ] ) for i in range ( 0 , 16 ) : data [ "BSVW {0}" . format ( i ) ] = self . _calculate_float ( config [ 4 * i + 160 : 4 * i + 164 ] ) data [ "GSC" ] = self . _calculate_float ( config [ 224 : 228 ] ) data [ "SFR" ] = self . _calculate_float ( config [ 228 : 232 ] ) data [ "LaserDAC" ] = config [ 232 ] data [ "FanDAC" ] = config [ 233 ] if self . firmware [ 'major' ] > 15. : data [ 'TOF_SFR' ] = config [ 234 ] sleep ( 0.1 ) return data
7000	def _runpf_worker ( task ) : ( lcfile , outdir , timecols , magcols , errcols , lcformat , lcformatdir , pfmethods , pfkwargs , getblssnr , sigclip , nworkers , minobservations , excludeprocessed ) = task if os . path . exists ( lcfile ) : pfresult = runpf ( lcfile , outdir , timecols = timecols , magcols = magcols , errcols = errcols , lcformat = lcformat , lcformatdir = lcformatdir , pfmethods = pfmethods , pfkwargs = pfkwargs , getblssnr = getblssnr , sigclip = sigclip , nworkers = nworkers , minobservations = minobservations , excludeprocessed = excludeprocessed ) return pfresult else : LOGERROR ( 'LC does not exist for requested file %s' % lcfile ) return None
1634	def CheckPosixThreading ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] for single_thread_func , multithread_safe_func , pattern in _THREADING_LIST : if Search ( pattern , line ) : error ( filename , linenum , 'runtime/threadsafe_fn' , 2 , 'Consider using ' + multithread_safe_func + '...) instead of ' + single_thread_func + '...) for improved thread safety.' )
8078	def arrow ( self , x , y , width , type = NORMAL , draw = True , ** kwargs ) : path = self . BezierPath ( ** kwargs ) if type == self . NORMAL : head = width * .4 tail = width * .2 path . moveto ( x , y ) path . lineto ( x - head , y + head ) path . lineto ( x - head , y + tail ) path . lineto ( x - width , y + tail ) path . lineto ( x - width , y - tail ) path . lineto ( x - head , y - tail ) path . lineto ( x - head , y - head ) path . lineto ( x , y ) elif type == self . FORTYFIVE : head = .3 tail = 1 + head path . moveto ( x , y ) path . lineto ( x , y + width * ( 1 - head ) ) path . lineto ( x - width * head , y + width ) path . lineto ( x - width * head , y + width * tail * .4 ) path . lineto ( x - width * tail * .6 , y + width ) path . lineto ( x - width , y + width * tail * .6 ) path . lineto ( x - width * tail * .4 , y + width * head ) path . lineto ( x - width , y + width * head ) path . lineto ( x - width * ( 1 - head ) , y ) path . lineto ( x , y ) else : raise NameError ( _ ( "arrow: available types for arrow() are NORMAL and FORTYFIVE\n" ) ) if draw : path . draw ( ) return path
3312	def do_MKCOL ( self , environ , start_response ) : path = environ [ "PATH_INFO" ] provider = self . _davProvider if util . get_content_length ( environ ) != 0 : self . _fail ( HTTP_MEDIATYPE_NOT_SUPPORTED , "The server does not handle any body content." , ) if environ . setdefault ( "HTTP_DEPTH" , "0" ) != "0" : self . _fail ( HTTP_BAD_REQUEST , "Depth must be '0'." ) if provider . exists ( path , environ ) : self . _fail ( HTTP_METHOD_NOT_ALLOWED , "MKCOL can only be executed on an unmapped URL." , ) parentRes = provider . get_resource_inst ( util . get_uri_parent ( path ) , environ ) if not parentRes or not parentRes . is_collection : self . _fail ( HTTP_CONFLICT , "Parent must be an existing collection." ) self . _check_write_permission ( parentRes , "0" , environ ) parentRes . create_collection ( util . get_uri_name ( path ) ) return util . send_status_response ( environ , start_response , HTTP_CREATED )
8513	def fit_and_score_estimator ( estimator , parameters , cv , X , y = None , scoring = None , iid = True , n_jobs = 1 , verbose = 1 , pre_dispatch = '2*n_jobs' ) : scorer = check_scoring ( estimator , scoring = scoring ) n_samples = num_samples ( X ) X , y = check_arrays ( X , y , allow_lists = True , sparse_format = 'csr' , allow_nans = True ) if y is not None : if len ( y ) != n_samples : raise ValueError ( 'Target variable (y) has a different number ' 'of samples (%i) than data (X: %i samples)' % ( len ( y ) , n_samples ) ) cv = check_cv ( cv = cv , y = y , classifier = is_classifier ( estimator ) ) out = Parallel ( n_jobs = n_jobs , verbose = verbose , pre_dispatch = pre_dispatch ) ( delayed ( _fit_and_score ) ( clone ( estimator ) , X , y , scorer , train , test , verbose , parameters , fit_params = None ) for train , test in cv . split ( X , y ) ) assert len ( out ) == cv . n_splits train_scores , test_scores = [ ] , [ ] n_train_samples , n_test_samples = [ ] , [ ] for test_score , n_test , train_score , n_train , _ in out : train_scores . append ( train_score ) test_scores . append ( test_score ) n_test_samples . append ( n_test ) n_train_samples . append ( n_train ) train_scores , test_scores = map ( list , check_arrays ( train_scores , test_scores , warn_nans = True , replace_nans = True ) ) if iid : if verbose > 0 and is_msmbuilder_estimator ( estimator ) : print ( '[CV] Using MSMBuilder API n_samples averaging' ) print ( '[CV] n_train_samples: %s' % str ( n_train_samples ) ) print ( '[CV] n_test_samples: %s' % str ( n_test_samples ) ) mean_test_score = np . average ( test_scores , weights = n_test_samples ) mean_train_score = np . average ( train_scores , weights = n_train_samples ) else : mean_test_score = np . average ( test_scores ) mean_train_score = np . average ( train_scores ) grid_scores = { 'mean_test_score' : mean_test_score , 'test_scores' : test_scores , 'mean_train_score' : mean_train_score , 'train_scores' : train_scores , 'n_test_samples' : n_test_samples , 'n_train_samples' : n_train_samples } return grid_scores
330	def model_returns_normal ( data , samples = 500 , progressbar = True ) : with pm . Model ( ) as model : mu = pm . Normal ( 'mean returns' , mu = 0 , sd = .01 , testval = data . mean ( ) ) sigma = pm . HalfCauchy ( 'volatility' , beta = 1 , testval = data . std ( ) ) returns = pm . Normal ( 'returns' , mu = mu , sd = sigma , observed = data ) pm . Deterministic ( 'annual volatility' , returns . distribution . variance ** .5 * np . sqrt ( 252 ) ) pm . Deterministic ( 'sharpe' , returns . distribution . mean / returns . distribution . variance ** .5 * np . sqrt ( 252 ) ) trace = pm . sample ( samples , progressbar = progressbar ) return model , trace
7862	def handle_tls_connected_event ( self , event ) : if self . settings [ "tls_verify_peer" ] : valid = self . settings [ "tls_verify_callback" ] ( event . stream , event . peer_certificate ) if not valid : raise SSLError ( "Certificate verification failed" ) event . stream . tls_established = True with event . stream . lock : event . stream . _restart_stream ( )
13067	def make_parents ( self , collection , lang = None ) : return [ { "id" : member . id , "label" : str ( member . get_label ( lang ) ) , "model" : str ( member . model ) , "type" : str ( member . type ) , "size" : member . size } for member in collection . parents if member . get_label ( ) ]
13153	def dict_cursor ( func ) : @ wraps ( func ) def wrapper ( cls , * args , ** kwargs ) : with ( yield from cls . get_cursor ( _CursorType . DICT ) ) as c : return ( yield from func ( cls , c , * args , ** kwargs ) ) return wrapper
6563	def load_cnf ( fp ) : fp = iter ( fp ) csp = ConstraintSatisfactionProblem ( dimod . BINARY ) num_clauses = num_variables = 0 problem_pattern = re . compile ( _PROBLEM_REGEX ) for line in fp : matches = problem_pattern . findall ( line ) if matches : if len ( matches ) > 1 : raise ValueError nv , nc = matches [ 0 ] num_variables , num_clauses = int ( nv ) , int ( nc ) break clause_pattern = re . compile ( _CLAUSE_REGEX ) for line in fp : if clause_pattern . match ( line ) is not None : clause = [ int ( v ) for v in line . split ( ' ' ) [ : - 1 ] ] variables = [ abs ( v ) for v in clause ] f = _cnf_or ( clause ) csp . add_constraint ( f , variables ) for v in range ( 1 , num_variables + 1 ) : csp . add_variable ( v ) for v in csp . variables : if v > num_variables : msg = ( "given .cnf file's header defines variables [1, {}] and {} clauses " "but constraints a reference to variable {}" ) . format ( num_variables , num_clauses , v ) raise ValueError ( msg ) if len ( csp ) != num_clauses : msg = ( "given .cnf file's header defines {} " "clauses but the file contains {}" ) . format ( num_clauses , len ( csp ) ) raise ValueError ( msg ) return csp
3108	def locked_get ( self ) : query = { self . key_name : self . key_value } entities = self . model_class . objects . filter ( ** query ) if len ( entities ) > 0 : credential = getattr ( entities [ 0 ] , self . property_name ) if getattr ( credential , 'set_store' , None ) is not None : credential . set_store ( self ) return credential else : return None
2295	def predict_proba ( self , a , b , ** kwargs ) : estimators = { 'entropy' : lambda x , y : eval_entropy ( y ) - eval_entropy ( x ) , 'integral' : integral_approx_estimator } ref_measures = { 'gaussian' : lambda x : standard_scale . fit_transform ( x . reshape ( ( - 1 , 1 ) ) ) , 'uniform' : lambda x : min_max_scale . fit_transform ( x . reshape ( ( - 1 , 1 ) ) ) , 'None' : lambda x : x } ref_measure = ref_measures [ kwargs . get ( 'refMeasure' , 'gaussian' ) ] estimator = estimators [ kwargs . get ( 'estimator' , 'entropy' ) ] a = ref_measure ( a ) b = ref_measure ( b ) return estimator ( a , b )
7749	def process_iq ( self , stanza ) : typ = stanza . stanza_type if typ in ( "result" , "error" ) : return self . _process_iq_response ( stanza ) if typ not in ( "get" , "set" ) : raise BadRequestProtocolError ( "Bad <iq/> type" ) logger . debug ( "Handling <iq type='{0}'> stanza: {1!r}" . format ( stanza , typ ) ) payload = stanza . get_payload ( None ) logger . debug ( " payload: {0!r}" . format ( payload ) ) if not payload : raise BadRequestProtocolError ( "<iq/> stanza with no child element" ) handler = self . _get_iq_handler ( typ , payload ) if not handler : payload = stanza . get_payload ( None , specialize = True ) logger . debug ( " specialized payload: {0!r}" . format ( payload ) ) if not isinstance ( payload , XMLPayload ) : handler = self . _get_iq_handler ( typ , payload ) if handler : response = handler ( stanza ) self . _process_handler_result ( response ) return True else : raise ServiceUnavailableProtocolError ( "Not implemented" )
2127	def populate_resource_columns ( item_dict ) : item_dict [ 'type' ] = item_dict [ 'name' ] if len ( item_dict [ 'summary_fields' ] ) == 0 : item_dict [ 'resource_name' ] = None item_dict [ 'resource_type' ] = None else : sf = item_dict [ 'summary_fields' ] item_dict [ 'resource_name' ] = sf . get ( 'resource_name' , '[unknown]' ) item_dict [ 'resource_type' ] = sf . get ( 'resource_type' , '[unknown]' )
6627	def _raiseUnavailableFor401 ( message ) : def __raiseUnavailableFor401 ( fn ) : def wrapped ( * args , ** kwargs ) : try : return fn ( * args , ** kwargs ) except requests . exceptions . HTTPError as e : if e . response . status_code == requests . codes . unauthorized : raise access_common . Unavailable ( message ) else : raise return wrapped return __raiseUnavailableFor401
5755	def get_regressions ( package_descriptors , targets , building_repo_data , testing_repo_data , main_repo_data ) : regressions = { } for package_descriptor in package_descriptors . values ( ) : pkg_name = package_descriptor . pkg_name debian_pkg_name = package_descriptor . debian_pkg_name regressions [ pkg_name ] = { } for target in targets : regressions [ pkg_name ] [ target ] = False main_version = main_repo_data . get ( target , { } ) . get ( debian_pkg_name , None ) if main_version is not None : main_ver_loose = LooseVersion ( main_version ) for repo_data in [ building_repo_data , testing_repo_data ] : version = repo_data . get ( target , { } ) . get ( debian_pkg_name , None ) if not version or main_ver_loose > LooseVersion ( version ) : regressions [ pkg_name ] [ target ] = True return regressions
997	def printState ( self , aState ) : def formatRow ( var , i ) : s = '' for c in range ( self . numberOfCols ) : if c > 0 and c % 10 == 0 : s += ' ' s += str ( var [ c , i ] ) s += ' ' return s for i in xrange ( self . cellsPerColumn ) : print formatRow ( aState , i )
9231	def fetch_events_async ( self , issues , tag_name ) : if not issues : return issues max_simultaneous_requests = self . options . max_simultaneous_requests verbose = self . options . verbose gh = self . github user = self . options . user repo = self . options . project self . events_cnt = 0 if verbose : print ( "fetching events for {} {}... " . format ( len ( issues ) , tag_name ) ) def worker ( issue ) : page = 1 issue [ 'events' ] = [ ] while page > 0 : rc , data = gh . repos [ user ] [ repo ] . issues [ issue [ 'number' ] ] . events . get ( page = page , per_page = PER_PAGE_NUMBER ) if rc == 200 : issue [ 'events' ] . extend ( data ) self . events_cnt += len ( data ) else : self . raise_GitHubError ( rc , data , gh . getheaders ( ) ) page = NextPage ( gh ) threads = [ ] cnt = len ( issues ) for i in range ( 0 , ( cnt // max_simultaneous_requests ) + 1 ) : for j in range ( max_simultaneous_requests ) : idx = i * max_simultaneous_requests + j if idx == cnt : break t = threading . Thread ( target = worker , args = ( issues [ idx ] , ) ) threads . append ( t ) t . start ( ) if verbose > 2 : print ( "." , end = "" ) if not idx % PER_PAGE_NUMBER : print ( "" ) for t in threads : t . join ( ) if verbose > 2 : print ( "." )
1790	def IDIV ( cpu , src ) : reg_name_h = { 8 : 'AH' , 16 : 'DX' , 32 : 'EDX' , 64 : 'RDX' } [ src . size ] reg_name_l = { 8 : 'AL' , 16 : 'AX' , 32 : 'EAX' , 64 : 'RAX' } [ src . size ] dividend = Operators . CONCAT ( src . size * 2 , cpu . read_register ( reg_name_h ) , cpu . read_register ( reg_name_l ) ) divisor = src . read ( ) if isinstance ( divisor , int ) and divisor == 0 : raise DivideByZeroError ( ) dst_size = src . size * 2 divisor = Operators . SEXTEND ( divisor , src . size , dst_size ) mask = ( 1 << dst_size ) - 1 sign_mask = 1 << ( dst_size - 1 ) dividend_sign = ( dividend & sign_mask ) != 0 divisor_sign = ( divisor & sign_mask ) != 0 if isinstance ( divisor , int ) : if divisor_sign : divisor = ( ( ~ divisor ) + 1 ) & mask divisor = - divisor if isinstance ( dividend , int ) : if dividend_sign : dividend = ( ( ~ dividend ) + 1 ) & mask dividend = - dividend quotient = Operators . SDIV ( dividend , divisor ) if ( isinstance ( dividend , int ) and isinstance ( dividend , int ) ) : remainder = dividend - ( quotient * divisor ) else : remainder = Operators . SREM ( dividend , divisor ) cpu . write_register ( reg_name_l , Operators . EXTRACT ( quotient , 0 , src . size ) ) cpu . write_register ( reg_name_h , Operators . EXTRACT ( remainder , 0 , src . size ) )
1889	def min ( self , constraints , X : BitVec , M = 10000 ) : assert isinstance ( X , BitVec ) return self . optimize ( constraints , X , 'minimize' , M )
4841	def get_program_by_uuid ( self , program_uuid ) : return self . _load_data ( self . PROGRAMS_ENDPOINT , resource_id = program_uuid , default = None )
4373	def create ( ) : name = request . form . get ( "name" ) if name : room , created = get_or_create ( ChatRoom , name = name ) return redirect ( url_for ( 'room' , slug = room . slug ) ) return redirect ( url_for ( 'rooms' ) )
7380	async def run ( self , * args , data ) : cmd = self . _get ( data . text ) try : if cmd is not None : command = self [ cmd ] ( * args , data = data ) return await peony . utils . execute ( command ) except : fmt = "Error occurred while running function {cmd}:" peony . utils . log_error ( fmt . format ( cmd = cmd ) )
10281	def get_peripheral_successor_edges ( graph : BELGraph , subgraph : BELGraph ) -> EdgeIterator : for u in subgraph : for _ , v , k in graph . out_edges ( u , keys = True ) : if v not in subgraph : yield u , v , k
2183	def existing_versions ( self ) : import glob pattern = join ( self . dpath , self . fname + '_*' + self . ext ) for fname in glob . iglob ( pattern ) : data_fpath = join ( self . dpath , fname ) yield data_fpath
10771	def contour ( self , level ) : if not isinstance ( level , numbers . Number ) : raise TypeError ( ( "'_level' must be of type 'numbers.Number' but is " "'{:s}'" ) . format ( type ( level ) ) ) vertices = self . _contour_generator . create_contour ( level ) return self . formatter ( level , vertices )
4812	def tokenize ( text , custom_dict = None ) : global TOKENIZER if not TOKENIZER : TOKENIZER = DeepcutTokenizer ( ) return TOKENIZER . tokenize ( text , custom_dict = custom_dict )
6386	def _sb_r1 ( self , term , r1_prefixes = None ) : vowel_found = False if hasattr ( r1_prefixes , '__iter__' ) : for prefix in r1_prefixes : if term [ : len ( prefix ) ] == prefix : return len ( prefix ) for i in range ( len ( term ) ) : if not vowel_found and term [ i ] in self . _vowels : vowel_found = True elif vowel_found and term [ i ] not in self . _vowels : return i + 1 return len ( term )
6442	def _cond_k ( self , word , suffix_len ) : return ( len ( word ) - suffix_len >= 3 ) and ( word [ - suffix_len - 1 ] in { 'i' , 'l' } or ( word [ - suffix_len - 3 ] == 'u' and word [ - suffix_len - 1 ] == 'e' ) )
2844	def enable_FTDI_driver ( ) : logger . debug ( 'Enabling FTDI driver.' ) if sys . platform == 'darwin' : logger . debug ( 'Detected Mac OSX' ) _check_running_as_root ( ) subprocess . check_call ( 'kextload -b com.apple.driver.AppleUSBFTDI' , shell = True ) subprocess . check_call ( 'kextload /System/Library/Extensions/FTDIUSBSerialDriver.kext' , shell = True ) elif sys . platform . startswith ( 'linux' ) : logger . debug ( 'Detected Linux' ) _check_running_as_root ( ) subprocess . check_call ( 'modprobe -q ftdi_sio' , shell = True ) subprocess . check_call ( 'modprobe -q usbserial' , shell = True )
13805	def merge_ordered ( ordereds : typing . Iterable [ typing . Any ] ) -> typing . Iterable [ typing . Any ] : seen_set = set ( ) add_seen = seen_set . add return reversed ( tuple ( map ( lambda obj : add_seen ( obj ) or obj , filterfalse ( seen_set . __contains__ , chain . from_iterable ( map ( reversed , reversed ( ordereds ) ) ) , ) , ) ) )
13255	def time ( self , t ) : _time = arrow . get ( t ) . format ( 'YYYY-MM-DDTHH:mm:ss' ) self . _time = datetime . datetime . strptime ( _time , '%Y-%m-%dT%H:%M:%S' )
7199	def get_chip ( self , coordinates , catid , chip_type = 'PAN' , chip_format = 'TIF' , filename = 'chip.tif' ) : def t2s1 ( t ) : return str ( t ) . strip ( '(,)' ) . replace ( ',' , '' ) def t2s2 ( t ) : return str ( t ) . strip ( '(,)' ) . replace ( ' ' , '' ) if len ( coordinates ) != 4 : print ( 'Wrong coordinate entry' ) return False W , S , E , N = coordinates box = ( ( W , S ) , ( W , N ) , ( E , N ) , ( E , S ) , ( W , S ) ) box_wkt = 'POLYGON ((' + ',' . join ( [ t2s1 ( corner ) for corner in box ] ) + '))' results = self . get_images_by_catid_and_aoi ( catid = catid , aoi_wkt = box_wkt ) description = self . describe_images ( results ) pan_id , ms_id , num_bands = None , None , 0 for catid , images in description . items ( ) : for partnum , part in images [ 'parts' ] . items ( ) : if 'PAN' in part . keys ( ) : pan_id = part [ 'PAN' ] [ 'id' ] bucket = part [ 'PAN' ] [ 'bucket' ] if 'WORLDVIEW_8_BAND' in part . keys ( ) : ms_id = part [ 'WORLDVIEW_8_BAND' ] [ 'id' ] num_bands = 8 bucket = part [ 'WORLDVIEW_8_BAND' ] [ 'bucket' ] elif 'RGBN' in part . keys ( ) : ms_id = part [ 'RGBN' ] [ 'id' ] num_bands = 4 bucket = part [ 'RGBN' ] [ 'bucket' ] band_str = '' if chip_type == 'PAN' : band_str = pan_id + '?bands=0' elif chip_type == 'MS' : band_str = ms_id + '?' elif chip_type == 'PS' : if num_bands == 8 : band_str = ms_id + '?bands=4,2,1&panId=' + pan_id elif num_bands == 4 : band_str = ms_id + '?bands=0,1,2&panId=' + pan_id location_str = '&upperLeft={}&lowerRight={}' . format ( t2s2 ( ( W , N ) ) , t2s2 ( ( E , S ) ) ) service_url = 'https://idaho.geobigdata.io/v1/chip/bbox/' + bucket + '/' url = service_url + band_str + location_str url += '&format=' + chip_format + '&token=' + self . gbdx_connection . access_token r = requests . get ( url ) if r . status_code == 200 : with open ( filename , 'wb' ) as f : f . write ( r . content ) return True else : print ( 'Cannot download chip' ) return False
2405	def update_prompt ( self , prompt_text ) : if ( isinstance ( prompt_text , basestring ) ) : self . _prompt = util_functions . sub_chars ( prompt_text ) ret = self . _prompt else : raise util_functions . InputError ( prompt_text , "Invalid prompt. Need to enter a string value." ) return ret
2603	def client_file ( self ) : return os . path . join ( self . ipython_dir , 'profile_{0}' . format ( self . profile ) , 'security/ipcontroller-client.json' )
2314	def anm_score ( self , x , y ) : gp = GaussianProcessRegressor ( ) . fit ( x , y ) y_predict = gp . predict ( x ) indepscore = normalized_hsic ( y_predict - y , x ) return indepscore
5844	def get_data_view ( self , data_view_id ) : url = routes . get_data_view ( data_view_id ) response = self . _get ( url ) . json ( ) result = response [ "data" ] [ "data_view" ] datasets_list = [ ] for dataset in result [ "datasets" ] : datasets_list . append ( Dataset ( name = dataset [ "name" ] , id = dataset [ "id" ] , description = dataset [ "description" ] ) ) columns_list = [ ] for column in result [ "columns" ] : columns_list . append ( ColumnFactory . from_dict ( column ) ) return DataView ( view_id = data_view_id , name = result [ "name" ] , description = result [ "description" ] , datasets = datasets_list , columns = columns_list , )
7340	async def get_media_metadata ( data , path = None ) : if isinstance ( data , bytes ) : media_type = await get_type ( data , path ) else : raise TypeError ( "get_metadata input must be a bytes" ) media_category = get_category ( media_type ) _logger . info ( "media_type: %s, media_category: %s" % ( media_type , media_category ) ) return media_type , media_category
11862	def weighted_sample ( bn , e ) : w = 1 event = dict ( e ) for node in bn . nodes : Xi = node . variable if Xi in e : w *= node . p ( e [ Xi ] , event ) else : event [ Xi ] = node . sample ( event ) return event , w
10665	def stoichiometry_coefficient ( compound , element ) : stoichiometry = parse_compound ( compound . strip ( ) ) . count ( ) return stoichiometry [ element ]
7845	def get_items ( self ) : ret = [ ] l = self . xpath_ctxt . xpathEval ( "d:item" ) if l is not None : for i in l : ret . append ( DiscoItem ( self , i ) ) return ret
2797	def transfer ( self , new_region_slug ) : return self . get_data ( "images/%s/actions/" % self . id , type = POST , params = { "type" : "transfer" , "region" : new_region_slug } )
13208	def _parse_abstract ( self ) : command = LatexCommand ( 'setDocAbstract' , { 'name' : 'abstract' , 'required' : True , 'bracket' : '{' } ) try : parsed = next ( command . parse ( self . _tex ) ) except StopIteration : self . _logger . warning ( 'lsstdoc has no abstract' ) self . _abstract = None return try : content = parsed [ 'abstract' ] except KeyError : self . _logger . warning ( 'lsstdoc has no abstract' ) self . _abstract = None return content = content . strip ( ) self . _abstract = content
1966	def syscall ( self ) : index = self . _syscall_abi . syscall_number ( ) try : table = getattr ( linux_syscalls , self . current . machine ) name = table . get ( index , None ) implementation = getattr ( self , name ) except ( AttributeError , KeyError ) : if name is not None : raise SyscallNotImplemented ( index , name ) else : raise Exception ( f"Bad syscall index, {index}" ) return self . _syscall_abi . invoke ( implementation )
9369	def legal_ogrn ( ) : ogrn = "" . join ( map ( str , [ random . randint ( 1 , 9 ) for _ in range ( 12 ) ] ) ) ogrn += str ( ( int ( ogrn ) % 11 % 10 ) ) return ogrn
7091	def _cpinfo_key_worker ( task ) : cpfile , keyspeclist = task keystoget = [ x [ 0 ] for x in keyspeclist ] nonesubs = [ x [ - 2 ] for x in keyspeclist ] nansubs = [ x [ - 1 ] for x in keyspeclist ] for i , k in enumerate ( keystoget ) : thisk = k . split ( '.' ) if sys . version_info [ : 2 ] < ( 3 , 4 ) : thisk = [ ( int ( x ) if x . isdigit ( ) else x ) for x in thisk ] else : thisk = [ ( int ( x ) if x . isdecimal ( ) else x ) for x in thisk ] keystoget [ i ] = thisk keystoget . insert ( 0 , [ 'objectid' ] ) nonesubs . insert ( 0 , '' ) nansubs . insert ( 0 , '' ) vals = checkplot_infokey_worker ( ( cpfile , keystoget ) ) for val , nonesub , nansub , valind in zip ( vals , nonesubs , nansubs , range ( len ( vals ) ) ) : if val is None : outval = nonesub elif isinstance ( val , float ) and not np . isfinite ( val ) : outval = nansub elif isinstance ( val , ( list , tuple ) ) : outval = ', ' . join ( val ) else : outval = val vals [ valind ] = outval return vals
4260	def load_exif ( album ) : if not hasattr ( album . gallery , "exifCache" ) : _restore_cache ( album . gallery ) cache = album . gallery . exifCache for media in album . medias : if media . type == "image" : key = os . path . join ( media . path , media . filename ) if key in cache : media . exif = cache [ key ]
2643	def filepath ( self ) : if hasattr ( self , 'local_path' ) : return self . local_path if self . scheme in [ 'ftp' , 'http' , 'https' , 'globus' ] : return self . filename elif self . scheme in [ 'file' ] : return self . path else : raise Exception ( 'Cannot return filepath for unknown scheme {}' . format ( self . scheme ) )
9020	def _connect_rows ( self , connections ) : for connection in connections : from_row_id = self . _to_id ( connection [ FROM ] [ ID ] ) from_row = self . _id_cache [ from_row_id ] from_row_start_index = connection [ FROM ] . get ( START , DEFAULT_START ) from_row_number_of_possible_meshes = from_row . number_of_produced_meshes - from_row_start_index to_row_id = self . _to_id ( connection [ TO ] [ ID ] ) to_row = self . _id_cache [ to_row_id ] to_row_start_index = connection [ TO ] . get ( START , DEFAULT_START ) to_row_number_of_possible_meshes = to_row . number_of_consumed_meshes - to_row_start_index meshes = min ( from_row_number_of_possible_meshes , to_row_number_of_possible_meshes ) number_of_meshes = connection . get ( MESHES , meshes ) from_row_stop_index = from_row_start_index + number_of_meshes to_row_stop_index = to_row_start_index + number_of_meshes assert 0 <= from_row_start_index <= from_row_stop_index produced_meshes = from_row . produced_meshes [ from_row_start_index : from_row_stop_index ] assert 0 <= to_row_start_index <= to_row_stop_index consumed_meshes = to_row . consumed_meshes [ to_row_start_index : to_row_stop_index ] assert len ( produced_meshes ) == len ( consumed_meshes ) mesh_pairs = zip ( produced_meshes , consumed_meshes ) for produced_mesh , consumed_mesh in mesh_pairs : produced_mesh . connect_to ( consumed_mesh )
1144	def init ( self ) : "Initialize the message-digest and set all fields to zero." self . length = 0 self . input = [ ] self . H0 = 0x67452301 self . H1 = 0xEFCDAB89 self . H2 = 0x98BADCFE self . H3 = 0x10325476 self . H4 = 0xC3D2E1F0
6941	def _double_inverted_gaussian ( x , amp1 , loc1 , std1 , amp2 , loc2 , std2 ) : gaussian1 = - _gaussian ( x , amp1 , loc1 , std1 ) gaussian2 = - _gaussian ( x , amp2 , loc2 , std2 ) return gaussian1 + gaussian2
10774	def get_settings_path ( settings_module ) : cwd = os . getcwd ( ) settings_filename = '%s.py' % ( settings_module . split ( '.' ) [ - 1 ] ) while cwd : if settings_filename in os . listdir ( cwd ) : break cwd = os . path . split ( cwd ) [ 0 ] if os . name == 'nt' and NT_ROOT . match ( cwd ) : return None elif cwd == '/' : return None return cwd
12348	def stitch_coordinates ( self , well_row = 0 , well_column = 0 ) : well = [ w for w in self . wells if attribute ( w , 'u' ) == well_column and attribute ( w , 'v' ) == well_row ] if len ( well ) == 1 : well = well [ 0 ] tile = os . path . join ( well , 'TileConfiguration.registered.txt' ) with open ( tile ) as f : data = [ x . strip ( ) for l in f . readlines ( ) if l [ 0 : 7 ] == 'image--' for x in l . split ( ';' ) ] coordinates = ( ast . literal_eval ( x ) for x in data [ 2 : : 3 ] ) coordinates = sum ( coordinates , ( ) ) attr = tuple ( attributes ( x ) for x in data [ 0 : : 3 ] ) return coordinates [ 0 : : 2 ] , coordinates [ 1 : : 2 ] , attr else : print ( 'leicaexperiment stitch_coordinates' '({}, {}) Well not found' . format ( well_row , well_column ) )
1491	def get_serializer ( context ) : cluster_config = context . get_cluster_config ( ) serializer_clsname = cluster_config . get ( constants . TOPOLOGY_SERIALIZER_CLASSNAME , None ) if serializer_clsname is None : return PythonSerializer ( ) else : try : topo_pex_path = context . get_topology_pex_path ( ) pex_loader . load_pex ( topo_pex_path ) serializer_cls = pex_loader . import_and_get_class ( topo_pex_path , serializer_clsname ) serializer = serializer_cls ( ) return serializer except Exception as e : raise RuntimeError ( "Error with loading custom serializer class: %s, with error message: %s" % ( serializer_clsname , str ( e ) ) )
13357	def moyennes_glissantes ( df , sur = 8 , rep = 0.75 ) : return pd . rolling_mean ( df , window = sur , min_periods = rep * sur )
13747	def get_item ( self , hash_key , start = 0 , extra_attrs = None ) : table = self . get_table ( ) try : item = table . get_item ( hash_key = hash_key ) except DynamoDBKeyNotFoundError : item = None if item is None : item = self . create_item ( hash_key = hash_key , start = start , extra_attrs = extra_attrs , ) return item
8734	def divide_timedelta ( td1 , td2 ) : try : return td1 / td2 except TypeError : return td1 . total_seconds ( ) / td2 . total_seconds ( )
2167	def list_resource_commands ( self ) : resource_path = os . path . abspath ( os . path . join ( os . path . dirname ( __file__ ) , os . pardir , 'resources' ) ) answer = set ( [ ] ) for _ , name , _ in pkgutil . iter_modules ( [ resource_path ] ) : res = tower_cli . get_resource ( name ) if not getattr ( res , 'internal' , False ) : answer . add ( name ) return sorted ( answer )
2324	def predict ( self , x , * args , ** kwargs ) : if len ( args ) > 0 : if type ( args [ 0 ] ) == nx . Graph or type ( args [ 0 ] ) == nx . DiGraph : return self . orient_graph ( x , * args , ** kwargs ) else : return self . predict_proba ( x , * args , ** kwargs ) elif type ( x ) == DataFrame : return self . predict_dataset ( x , * args , ** kwargs ) elif type ( x ) == Series : return self . predict_proba ( x . iloc [ 0 ] , x . iloc [ 1 ] , * args , ** kwargs )
3619	def register ( self , model , index_cls = AlgoliaIndex , auto_indexing = None ) : if self . is_registered ( model ) : raise RegistrationError ( '{} is already registered with Algolia engine' . format ( model ) ) if not issubclass ( index_cls , AlgoliaIndex ) : raise RegistrationError ( '{} should be a subclass of AlgoliaIndex' . format ( index_cls ) ) index_obj = index_cls ( model , self . client , self . __settings ) self . __registered_models [ model ] = index_obj if ( isinstance ( auto_indexing , bool ) and auto_indexing ) or self . __auto_indexing : post_save . connect ( self . __post_save_receiver , model ) pre_delete . connect ( self . __pre_delete_receiver , model ) logger . info ( 'REGISTER %s' , model )
11530	def __get_rev ( self , key , version , ** kwa ) : if '_doc' in kwa : doc = kwa [ '_doc' ] else : if type ( version ) is int : if version == 0 : order = pymongo . ASCENDING elif version == - 1 : order = pymongo . DESCENDING doc = self . _collection . find_one ( { 'k' : key } , sort = [ [ 'd' , order ] ] ) elif type ( version ) is datetime : ver = self . __round_time ( version ) doc = self . _collection . find_one ( { 'k' : key , 'd' : ver } ) if doc is None : raise KeyError ( 'Supplied key `{0}` or version `{1}` does not exist' . format ( key , str ( version ) ) ) coded_val = doc [ 'v' ] return pickle . loads ( coded_val )
10849	def set_verbosity ( self , verbosity = 'vvv' , handlers = None ) : self . verbosity = sanitize ( verbosity ) self . set_level ( v2l [ verbosity ] , handlers = handlers ) self . set_formatter ( v2f [ verbosity ] , handlers = handlers )
13226	async def process_ltd_doc ( session , github_api_token , ltd_product_url , mongo_collection = None ) : logger = logging . getLogger ( __name__ ) ltd_product_data = await get_ltd_product ( session , url = ltd_product_url ) product_name = ltd_product_data [ 'slug' ] doc_handle_match = DOCUMENT_HANDLE_PATTERN . match ( product_name ) if doc_handle_match is None : logger . debug ( '%s is not a document repo' , product_name ) return try : return await process_sphinx_technote ( session , github_api_token , ltd_product_data , mongo_collection = mongo_collection ) except NotSphinxTechnoteError : logger . debug ( '%s is not a Sphinx-based technote.' , product_name ) except Exception : logger . exception ( 'Unexpected error trying to process %s' , product_name ) return try : return await process_lander_page ( session , github_api_token , ltd_product_data , mongo_collection = mongo_collection ) except NotLanderPageError : logger . debug ( '%s is not a Lander page with a metadata.jsonld file.' , product_name ) except Exception : logger . exception ( 'Unexpected error trying to process %s' , product_name ) return
12855	def subtree ( events ) : stack = 0 for obj in events : if obj [ 'type' ] == ENTER : stack += 1 elif obj [ 'type' ] == EXIT : if stack == 0 : break stack -= 1 yield obj
9293	def db_value ( self , value ) : value = self . transform_value ( value ) return self . hhash . encrypt ( value , salt_size = self . salt_size , rounds = self . rounds )
2937	def deserialize_data ( self , workflow , start_node ) : name = start_node . getAttribute ( 'name' ) value = start_node . getAttribute ( 'value' ) return name , value
10556	def update_helping_material ( helpingmaterial ) : try : helpingmaterial_id = helpingmaterial . id helpingmaterial = _forbidden_attributes ( helpingmaterial ) res = _pybossa_req ( 'put' , 'helpingmaterial' , helpingmaterial_id , payload = helpingmaterial . data ) if res . get ( 'id' ) : return HelpingMaterial ( res ) else : return res except : raise
2330	def computeGaussKernel ( x ) : xnorm = np . power ( euclidean_distances ( x , x ) , 2 ) return np . exp ( - xnorm / ( 2.0 ) )
5086	def has_implicit_access_to_catalog ( user , obj ) : request = get_request_or_stub ( ) decoded_jwt = get_decoded_jwt_from_request ( request ) return request_user_has_implicit_access_via_jwt ( decoded_jwt , ENTERPRISE_CATALOG_ADMIN_ROLE , obj )
8110	def search_news ( q , start = 0 , wait = 10 , asynchronous = False , cached = False ) : service = GOOGLE_NEWS return GoogleSearch ( q , start , service , "" , wait , asynchronous , cached )
11242	def indent_css ( f , output ) : line_count = get_line_count ( f ) f = open ( f , 'r+' ) output = open ( output , 'r+' ) for line in range ( line_count ) : string = f . readline ( ) . rstrip ( ) if len ( string ) > 0 : if string [ - 1 ] == ";" : output . write ( " " + string + "\n" ) else : output . write ( string + "\n" ) output . close ( ) f . close ( )
5840	def check_predict_status ( self , view_id , predict_request_id ) : failure_message = "Get status on predict failed" bare_response = self . _get_success_json ( self . _get ( 'v1/data_views/' + str ( view_id ) + '/predict/' + str ( predict_request_id ) + '/status' , None , failure_message = failure_message ) ) result = bare_response [ "data" ] return result
4680	def getAccountsFromPublicKey ( self , pub ) : names = self . rpc . get_key_references ( [ str ( pub ) ] ) [ 0 ] for name in names : yield name
2778	def remove_forwarding_rules ( self , forwarding_rules ) : rules_dict = [ rule . __dict__ for rule in forwarding_rules ] return self . get_data ( "load_balancers/%s/forwarding_rules/" % self . id , type = DELETE , params = { "forwarding_rules" : rules_dict } )
6080	def convergence_of_galaxies_from_grid ( grid , galaxies ) : if galaxies : return sum ( map ( lambda g : g . convergence_from_grid ( grid ) , galaxies ) ) else : return np . full ( ( grid . shape [ 0 ] ) , 0.0 )
1697	def reduce_by_window ( self , window_config , reduce_function ) : from heronpy . streamlet . impl . reducebywindowbolt import ReduceByWindowStreamlet reduce_streamlet = ReduceByWindowStreamlet ( window_config , reduce_function , self ) self . _add_child ( reduce_streamlet ) return reduce_streamlet
3804	def calculate ( self , T , method ) : r if method == GHARAGHEIZI_G : kg = Gharagheizi_gas ( T , self . MW , self . Tb , self . Pc , self . omega ) elif method == DIPPR_9B : Cvgm = self . Cvgm ( T ) if hasattr ( self . Cvgm , '__call__' ) else self . Cvgm mug = self . mug ( T ) if hasattr ( self . mug , '__call__' ) else self . mug kg = DIPPR9B ( T , self . MW , Cvgm , mug , self . Tc ) elif method == CHUNG : Cvgm = self . Cvgm ( T ) if hasattr ( self . Cvgm , '__call__' ) else self . Cvgm mug = self . mug ( T ) if hasattr ( self . mug , '__call__' ) else self . mug kg = Chung ( T , self . MW , self . Tc , self . omega , Cvgm , mug ) elif method == ELI_HANLEY : Cvgm = self . Cvgm ( T ) if hasattr ( self . Cvgm , '__call__' ) else self . Cvgm kg = eli_hanley ( T , self . MW , self . Tc , self . Vc , self . Zc , self . omega , Cvgm ) elif method == EUCKEN_MOD : Cvgm = self . Cvgm ( T ) if hasattr ( self . Cvgm , '__call__' ) else self . Cvgm mug = self . mug ( T ) if hasattr ( self . mug , '__call__' ) else self . mug kg = Eucken_modified ( self . MW , Cvgm , mug ) elif method == EUCKEN : Cvgm = self . Cvgm ( T ) if hasattr ( self . Cvgm , '__call__' ) else self . Cvgm mug = self . mug ( T ) if hasattr ( self . mug , '__call__' ) else self . mug kg = Eucken ( self . MW , Cvgm , mug ) elif method == DIPPR_PERRY_8E : kg = EQ102 ( T , * self . Perrys2_314_coeffs ) elif method == VDI_PPDS : kg = horner ( self . VDI_PPDS_coeffs , T ) elif method == BAHADORI_G : kg = Bahadori_gas ( T , self . MW ) elif method == COOLPROP : kg = CoolProp_T_dependent_property ( T , self . CASRN , 'L' , 'g' ) elif method in self . tabular_data : kg = self . interpolate ( T , method ) return kg
6732	def add_class_methods_as_module_level_functions_for_fabric ( instance , module_name , method_name , module_alias = None ) : import imp from . decorators import task_or_dryrun module_obj = sys . modules [ module_name ] module_alias = re . sub ( '[^a-zA-Z0-9]+' , '' , module_alias or '' ) method_obj = getattr ( instance , method_name ) if not method_name . startswith ( '_' ) : func = getattr ( instance , method_name ) if not hasattr ( func , 'is_task_or_dryrun' ) : func = task_or_dryrun ( func ) if module_name == module_alias or ( module_name . startswith ( 'satchels.' ) and module_name . endswith ( module_alias ) ) : setattr ( module_obj , method_name , func ) else : _module_obj = module_obj module_obj = create_module ( module_alias ) setattr ( module_obj , method_name , func ) post_import_modules . add ( module_alias ) fabric_name = '%s.%s' % ( module_alias or module_name , method_name ) func . wrapped . __func__ . fabric_name = fabric_name return func
12798	def _fetch ( self , method , url = None , post_data = None , parse_data = True , key = None , parameters = None , listener = None , full_return = False ) : headers = self . get_headers ( ) headers [ "Content-Type" ] = "application/json" handlers = [ ] debuglevel = int ( self . _settings [ "debug" ] ) handlers . append ( urllib2 . HTTPHandler ( debuglevel = debuglevel ) ) if hasattr ( httplib , "HTTPS" ) : handlers . append ( urllib2 . HTTPSHandler ( debuglevel = debuglevel ) ) handlers . append ( urllib2 . HTTPCookieProcessor ( cookielib . CookieJar ( ) ) ) password_url = self . _get_password_url ( ) if password_url and "Authorization" not in headers : pwd_manager = urllib2 . HTTPPasswordMgrWithDefaultRealm ( ) pwd_manager . add_password ( None , password_url , self . _settings [ "user" ] , self . _settings [ "password" ] ) handlers . append ( HTTPBasicAuthHandler ( pwd_manager ) ) opener = urllib2 . build_opener ( * handlers ) if post_data is not None : post_data = json . dumps ( post_data ) uri = self . _url ( url , parameters ) request = RESTRequest ( uri , method = method , headers = headers ) if post_data is not None : request . add_data ( post_data ) response = None try : response = opener . open ( request ) body = response . read ( ) if password_url and password_url not in self . _settings [ "authorizations" ] and request . has_header ( "Authorization" ) : self . _settings [ "authorizations" ] [ password_url ] = request . get_header ( "Authorization" ) except urllib2 . HTTPError as e : if e . code == 401 : raise AuthenticationError ( "Access denied while trying to access %s" % uri ) elif e . code == 404 : raise ConnectionError ( "URL not found: %s" % uri ) else : raise except urllib2 . URLError as e : raise ConnectionError ( "Error while fetching from %s: %s" % ( uri , e ) ) finally : if response : response . close ( ) opener . close ( ) data = None if parse_data : if not key : key = string . split ( url , "/" ) [ 0 ] data = self . parse ( body , key ) if full_return : info = response . info ( ) if response else None status = int ( string . split ( info [ "status" ] ) [ 0 ] ) if ( info and "status" in info ) else None return { "success" : ( status >= 200 and status < 300 ) , "data" : data , "info" : info , "body" : body } return data
11910	def bump_version ( version , which = None ) : try : parts = [ int ( n ) for n in version . split ( '.' ) ] except ValueError : fail ( 'Current version is not numeric' ) if len ( parts ) != 3 : fail ( 'Current version is not semantic versioning' ) PARTS = { 'major' : 0 , 'minor' : 1 , 'patch' : 2 } index = PARTS [ which ] if which in PARTS else 2 before , middle , after = parts [ : index ] , parts [ index ] , parts [ index + 1 : ] middle += 1 return '.' . join ( str ( n ) for n in before + [ middle ] + after )
10610	def _calculate_H ( self , T ) : if self . isCoal : return self . _calculate_Hfr_coal ( T ) H = 0.0 for compound in self . material . compounds : index = self . material . get_compound_index ( compound ) dH = thermo . H ( compound , T , self . _compound_masses [ index ] ) H = H + dH return H
11290	def load_dict ( self , source , namespace = '' ) : for key , value in source . items ( ) : if isinstance ( key , str ) : nskey = ( namespace + '.' + key ) . strip ( '.' ) if isinstance ( value , dict ) : self . load_dict ( value , namespace = nskey ) else : self [ nskey ] = value else : raise TypeError ( 'Key has type %r (not a string)' % type ( key ) ) return self
4369	def send ( self , message , json = False , callback = None ) : pkt = dict ( type = "message" , data = message , endpoint = self . ns_name ) if json : pkt [ 'type' ] = "json" if callback : pkt [ 'ack' ] = True pkt [ 'id' ] = msgid = self . socket . _get_next_msgid ( ) self . socket . _save_ack_callback ( msgid , callback ) self . socket . send_packet ( pkt )
4991	def post ( self , request , * args , ** kwargs ) : enterprise_customer_uuid , course_run_id , course_key , program_uuid = RouterView . get_path_variables ( ** kwargs ) enterprise_customer = get_enterprise_customer_or_404 ( enterprise_customer_uuid ) if course_key : context_data = get_global_context ( request , enterprise_customer ) try : kwargs [ 'course_id' ] = RouterView . get_course_run_id ( request . user , enterprise_customer , course_key ) except Http404 : error_code = 'ENTRV001' log_message = ( 'Could not find course run with id {course_run_id} ' 'for course key {course_key} and ' 'for enterprise_customer_uuid {enterprise_customer_uuid} ' 'and program {program_uuid}. ' 'Returned error code {error_code} to user {userid}' . format ( course_key = course_key , course_run_id = course_run_id , enterprise_customer_uuid = enterprise_customer_uuid , error_code = error_code , userid = request . user . id , program_uuid = program_uuid , ) ) return render_page_with_error_code_message ( request , context_data , error_code , log_message ) return self . redirect ( request , * args , ** kwargs )
2352	def root ( self ) : if self . _root is None and self . _root_locator is not None : return self . page . find_element ( * self . _root_locator ) return self . _root
5769	def _advapi32_load_key ( key_object , key_info , container ) : key_type = 'public' if isinstance ( key_info , keys . PublicKeyInfo ) else 'private' algo = key_info . algorithm if algo == 'rsa' : provider = Advapi32Const . MS_ENH_RSA_AES_PROV else : provider = Advapi32Const . MS_ENH_DSS_DH_PROV context_handle = None key_handle = None try : context_handle = open_context_handle ( provider , verify_only = key_type == 'public' ) blob = _advapi32_create_blob ( key_info , key_type , algo ) buffer_ = buffer_from_bytes ( blob ) key_handle_pointer = new ( advapi32 , 'HCRYPTKEY *' ) res = advapi32 . CryptImportKey ( context_handle , buffer_ , len ( blob ) , null ( ) , 0 , key_handle_pointer ) handle_error ( res ) key_handle = unwrap ( key_handle_pointer ) output = container ( key_handle , key_object ) output . context_handle = context_handle if algo == 'rsa' : ex_blob = _advapi32_create_blob ( key_info , key_type , algo , signing = False ) ex_buffer = buffer_from_bytes ( ex_blob ) ex_key_handle_pointer = new ( advapi32 , 'HCRYPTKEY *' ) res = advapi32 . CryptImportKey ( context_handle , ex_buffer , len ( ex_blob ) , null ( ) , 0 , ex_key_handle_pointer ) handle_error ( res ) output . ex_key_handle = unwrap ( ex_key_handle_pointer ) return output except ( Exception ) : if key_handle : advapi32 . CryptDestroyKey ( key_handle ) if context_handle : close_context_handle ( context_handle ) raise
13639	def bind ( mod_path , with_path = None ) : if with_path : if os . path . isdir ( with_path ) : sys . path . insert ( 0 , with_path ) else : sys . path . insert ( 0 , with_path . rsplit ( '/' , 2 ) [ 0 ] ) pass mod = importlib . import_module ( mod_path ) settings = Settings ( ) for v in dir ( mod ) : if v [ 0 ] == '_' or type ( getattr ( mod , v ) ) . __name__ == 'module' : continue setattr ( settings , v , getattr ( mod , v ) ) pass Settings . _path = mod_path Settings . _wrapped = settings return settings
13472	def clean ( self ) : cleaned = super ( EventForm , self ) . clean ( ) if Event . objects . filter ( name = cleaned [ 'name' ] , start_date = cleaned [ 'start_date' ] ) . count ( ) : raise forms . ValidationError ( u'This event appears to be in the database already.' ) return cleaned
21	def boolean_flag ( parser , name , default = False , help = None ) : dest = name . replace ( '-' , '_' ) parser . add_argument ( "--" + name , action = "store_true" , default = default , dest = dest , help = help ) parser . add_argument ( "--no-" + name , action = "store_false" , dest = dest )
840	def closestTrainingPattern ( self , inputPattern , cat ) : dist = self . _getDistances ( inputPattern ) sorted = dist . argsort ( ) for patIdx in sorted : patternCat = self . _categoryList [ patIdx ] if patternCat == cat : if self . useSparseMemory : closestPattern = self . _Memory . getRow ( int ( patIdx ) ) else : closestPattern = self . _M [ patIdx ] return closestPattern return None
7929	def _start_thread ( self ) : with self . lock : if self . threads and self . queue . empty ( ) : return if len ( self . threads ) >= self . max_threads : return thread_n = self . last_thread_n + 1 self . last_thread_n = thread_n thread = threading . Thread ( target = self . _run , name = "{0!r} #{1}" . format ( self , thread_n ) , args = ( thread_n , ) ) self . threads . append ( thread ) thread . daemon = True thread . start ( )
6602	def collect_result ( self , package_index ) : result_fullpath = self . result_fullpath ( package_index ) try : with gzip . open ( result_fullpath , 'rb' ) as f : result = pickle . load ( f ) except Exception as e : logger = logging . getLogger ( __name__ ) logger . warning ( e ) return None return result
7511	def select_samples ( dbsamples , samples , pidx = None ) : samples = [ i . name for i in samples ] if pidx : sidx = [ list ( dbsamples [ pidx ] ) . index ( i ) for i in samples ] else : sidx = [ list ( dbsamples ) . index ( i ) for i in samples ] sidx . sort ( ) return sidx
3069	def wrap_http_for_jwt_access ( credentials , http ) : orig_request_method = http . request wrap_http_for_auth ( credentials , http ) authenticated_request_method = http . request def new_request ( uri , method = 'GET' , body = None , headers = None , redirections = httplib2 . DEFAULT_MAX_REDIRECTS , connection_type = None ) : if 'aud' in credentials . _kwargs : if ( credentials . access_token is None or credentials . access_token_expired ) : credentials . refresh ( None ) return request ( authenticated_request_method , uri , method , body , headers , redirections , connection_type ) else : headers = _initialize_headers ( headers ) _apply_user_agent ( headers , credentials . user_agent ) uri_root = uri . split ( '?' , 1 ) [ 0 ] token , unused_expiry = credentials . _create_token ( { 'aud' : uri_root } ) headers [ 'Authorization' ] = 'Bearer ' + token return request ( orig_request_method , uri , method , body , clean_headers ( headers ) , redirections , connection_type ) http . request = new_request http . request . credentials = credentials
2964	def insert_rule ( self , chain , src = None , dest = None , target = None ) : if not chain : raise ValueError ( "Invalid chain" ) if not target : raise ValueError ( "Invalid target" ) if not ( src or dest ) : raise ValueError ( "Need src, dest, or both" ) args = [ "-I" , chain ] if src : args += [ "-s" , src ] if dest : args += [ "-d" , dest ] args += [ "-j" , target ] self . call ( * args )
1811	def SETNAE ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . CF , 1 , 0 ) )
3073	def _load_config ( self , client_secrets_file , client_id , client_secret ) : if client_id and client_secret : self . client_id , self . client_secret = client_id , client_secret return if client_secrets_file : self . _load_client_secrets ( client_secrets_file ) return if 'GOOGLE_OAUTH2_CLIENT_SECRETS_FILE' in self . app . config : self . _load_client_secrets ( self . app . config [ 'GOOGLE_OAUTH2_CLIENT_SECRETS_FILE' ] ) return try : self . client_id , self . client_secret = ( self . app . config [ 'GOOGLE_OAUTH2_CLIENT_ID' ] , self . app . config [ 'GOOGLE_OAUTH2_CLIENT_SECRET' ] ) except KeyError : raise ValueError ( 'OAuth2 configuration could not be found. Either specify the ' 'client_secrets_file or client_id and client_secret or set ' 'the app configuration variables ' 'GOOGLE_OAUTH2_CLIENT_SECRETS_FILE or ' 'GOOGLE_OAUTH2_CLIENT_ID and GOOGLE_OAUTH2_CLIENT_SECRET.' )
3446	def add_mip_obj ( model ) : if len ( model . variables ) > 1e4 : LOGGER . warning ( "the MIP version of minimal media is extremely slow for" " models that large :(" ) exchange_rxns = find_boundary_types ( model , "exchange" ) big_m = max ( abs ( b ) for r in exchange_rxns for b in r . bounds ) prob = model . problem coefs = { } to_add = [ ] for rxn in exchange_rxns : export = len ( rxn . reactants ) == 1 indicator = prob . Variable ( "ind_" + rxn . id , lb = 0 , ub = 1 , type = "binary" ) if export : vrv = rxn . reverse_variable indicator_const = prob . Constraint ( vrv - indicator * big_m , ub = 0 , name = "ind_constraint_" + rxn . id ) else : vfw = rxn . forward_variable indicator_const = prob . Constraint ( vfw - indicator * big_m , ub = 0 , name = "ind_constraint_" + rxn . id ) to_add . extend ( [ indicator , indicator_const ] ) coefs [ indicator ] = 1 model . add_cons_vars ( to_add ) model . solver . update ( ) model . objective . set_linear_coefficients ( coefs ) model . objective . direction = "min"
9944	def link_file ( self , path , prefixed_path , source_storage ) : if prefixed_path in self . symlinked_files : return self . log ( "Skipping '%s' (already linked earlier)" % path ) if not self . delete_file ( path , prefixed_path , source_storage ) : return source_path = source_storage . path ( path ) if self . dry_run : self . log ( "Pretending to link '%s'" % source_path , level = 1 ) else : self . log ( "Linking '%s'" % source_path , level = 1 ) full_path = self . storage . path ( prefixed_path ) try : os . makedirs ( os . path . dirname ( full_path ) ) except OSError : pass try : if os . path . lexists ( full_path ) : os . unlink ( full_path ) os . symlink ( source_path , full_path ) except AttributeError : import platform raise CommandError ( "Symlinking is not supported by Python %s." % platform . python_version ( ) ) except NotImplementedError : import platform raise CommandError ( "Symlinking is not supported in this " "platform (%s)." % platform . platform ( ) ) except OSError as e : raise CommandError ( e ) if prefixed_path not in self . symlinked_files : self . symlinked_files . append ( prefixed_path )
10401	def get_final_score ( self ) -> float : if not self . done_chomping ( ) : raise ValueError ( 'algorithm has not yet completed' ) return self . graph . nodes [ self . target_node ] [ self . tag ]
11374	def license_is_oa ( license ) : for oal in OA_LICENSES : if re . search ( oal , license ) : return True return False
7190	def new ( n , prefix = None ) : if isinstance ( n , Leaf ) : return Leaf ( n . type , n . value , prefix = n . prefix if prefix is None else prefix ) n . parent = None if prefix is not None : n . prefix = prefix return n
5423	def _wait_after ( provider , job_ids , poll_interval , stop_on_failure ) : job_ids_to_check = { j for j in job_ids if j != dsub_util . NO_JOB } error_messages = [ ] while job_ids_to_check and ( not error_messages or not stop_on_failure ) : print ( 'Waiting for: %s.' % ( ', ' . join ( job_ids_to_check ) ) ) jobs_left = _wait_for_any_job ( provider , job_ids_to_check , poll_interval ) jobs_completed = job_ids_to_check . difference ( jobs_left ) tasks_completed = provider . lookup_job_tasks ( { '*' } , job_ids = jobs_completed ) dominant_job_tasks = _dominant_task_for_jobs ( tasks_completed ) if len ( dominant_job_tasks ) != len ( jobs_completed ) : jobs_found = dsub_util . tasks_to_job_ids ( dominant_job_tasks ) jobs_not_found = jobs_completed . difference ( jobs_found ) for j in jobs_not_found : error = '%s: not found' % j print_error ( ' %s' % error ) error_messages += [ error ] for t in dominant_job_tasks : job_id = t . get_field ( 'job-id' ) status = t . get_field ( 'task-status' ) print ( ' %s: %s' % ( str ( job_id ) , str ( status ) ) ) if status in [ 'FAILURE' , 'CANCELED' ] : error_messages += [ provider . get_tasks_completion_messages ( [ t ] ) ] job_ids_to_check = jobs_left return error_messages
12248	def sync ( self , * buckets ) : if buckets : for _bucket in buckets : for key in mimicdb . backend . smembers ( tpl . bucket % _bucket ) : mimicdb . backend . delete ( tpl . key % ( _bucket , key ) ) mimicdb . backend . delete ( tpl . bucket % _bucket ) bucket = self . get_bucket ( _bucket , force = True ) for key in bucket . list ( force = True ) : mimicdb . backend . sadd ( tpl . bucket % bucket . name , key . name ) mimicdb . backend . hmset ( tpl . key % ( bucket . name , key . name ) , dict ( size = key . size , md5 = key . etag . strip ( '"' ) ) ) else : for bucket in mimicdb . backend . smembers ( tpl . connection ) : for key in mimicdb . backend . smembers ( tpl . bucket % bucket ) : mimicdb . backend . delete ( tpl . key % ( bucket , key ) ) mimicdb . backend . delete ( tpl . bucket % bucket ) for bucket in self . get_all_buckets ( force = True ) : for key in bucket . list ( force = True ) : mimicdb . backend . sadd ( tpl . bucket % bucket . name , key . name ) mimicdb . backend . hmset ( tpl . key % ( bucket . name , key . name ) , dict ( size = key . size , md5 = key . etag . strip ( '"' ) ) )
1066	def getheader ( self , name , default = None ) : return self . dict . get ( name . lower ( ) , default )
947	def _printAvailableCheckpoints ( experimentDir ) : checkpointParentDir = getCheckpointParentDir ( experimentDir ) if not os . path . exists ( checkpointParentDir ) : print "No available checkpoints." return checkpointDirs = [ x for x in os . listdir ( checkpointParentDir ) if _isCheckpointDir ( os . path . join ( checkpointParentDir , x ) ) ] if not checkpointDirs : print "No available checkpoints." return print "Available checkpoints:" checkpointList = [ _checkpointLabelFromCheckpointDir ( x ) for x in checkpointDirs ] for checkpoint in sorted ( checkpointList ) : print "\t" , checkpoint print print "To start from a checkpoint:" print " python run_opf_experiment.py experiment --load <CHECKPOINT>" print "For example, to start from the checkpoint \"MyCheckpoint\":" print " python run_opf_experiment.py experiment --load MyCheckpoint"
2890	def create_task ( self ) : return self . spec_class ( self . spec , self . get_task_spec_name ( ) , lane = self . get_lane ( ) , description = self . node . get ( 'name' , None ) )
6821	def configure_modsecurity ( self ) : r = self . local_renderer if r . env . modsecurity_enabled and not self . last_manifest . modsecurity_enabled : self . install_packages ( ) fn = self . render_to_file ( 'apache/apache_modsecurity.template.conf' ) r . put ( local_path = fn , remote_path = '/etc/modsecurity/modsecurity.conf' , use_sudo = True ) r . env . modsecurity_download_filename = '/tmp/owasp-modsecurity-crs.tar.gz' r . sudo ( 'cd /tmp; wget --output-document={apache_modsecurity_download_filename} {apache_modsecurity_download_url}' ) r . env . modsecurity_download_top = r . sudo ( "cd /tmp; " "tar tzf %(apache_modsecurity_download_filename)s | sed -e 's@/.*@@' | uniq" % self . genv ) r . sudo ( 'cd /tmp; tar -zxvf %(apache_modsecurity_download_filename)s' % self . genv ) r . sudo ( 'cd /tmp; cp -R %(apache_modsecurity_download_top)s/* /etc/modsecurity/' % self . genv ) r . sudo ( 'mv /etc/modsecurity/modsecurity_crs_10_setup.conf.example /etc/modsecurity/modsecurity_crs_10_setup.conf' ) r . sudo ( 'rm -f /etc/modsecurity/activated_rules/*' ) r . sudo ( 'cd /etc/modsecurity/base_rules; ' 'for f in * ; do ln -s /etc/modsecurity/base_rules/$f /etc/modsecurity/activated_rules/$f ; done' ) r . sudo ( 'cd /etc/modsecurity/optional_rules; ' 'for f in * ; do ln -s /etc/modsecurity/optional_rules/$f /etc/modsecurity/activated_rules/$f ; done' ) r . env . httpd_conf_append . append ( 'Include "/etc/modsecurity/activated_rules/*.conf"' ) self . enable_mod ( 'evasive' ) self . enable_mod ( 'headers' ) elif not self . env . modsecurity_enabled and self . last_manifest . modsecurity_enabled : self . disable_mod ( 'modsecurity' )
8520	def plot_3 ( data , ss , * args ) : if len ( data ) <= 1 : warnings . warn ( "Only one datapoint. Could not compute t-SNE embedding." ) return None scores = np . array ( [ d [ 'mean_test_score' ] for d in data ] ) warped = np . array ( [ ss . point_to_unit ( d [ 'parameters' ] ) for d in data ] ) X = TSNE ( n_components = 2 ) . fit_transform ( warped ) e_scores = np . exp ( scores ) mine , maxe = np . min ( e_scores ) , np . max ( e_scores ) color = ( e_scores - mine ) / ( maxe - mine ) mapped_colors = list ( map ( rgb2hex , cm . get_cmap ( 'RdBu_r' ) ( color ) ) ) p = bk . figure ( title = 't-SNE (unsupervised)' , tools = TOOLS ) df_params = nonconstant_parameters ( data ) df_params [ 'score' ] = scores df_params [ 'x' ] = X [ : , 0 ] df_params [ 'y' ] = X [ : , 1 ] df_params [ 'color' ] = mapped_colors df_params [ 'radius' ] = 1 p . circle ( x = 'x' , y = 'y' , color = 'color' , radius = 'radius' , source = ColumnDataSource ( data = df_params ) , fill_alpha = 0.6 , line_color = None ) cp = p hover = cp . select ( dict ( type = HoverTool ) ) format_tt = [ ( s , '@%s' % s ) for s in df_params . columns ] hover . tooltips = OrderedDict ( [ ( "index" , "$index" ) ] + format_tt ) xax , yax = p . axis xax . axis_label = 't-SNE coord 1' yax . axis_label = 't-SNE coord 2' return p
12066	def comments ( abf , minutes = False ) : if not len ( abf . commentTimes ) : return for i in range ( len ( abf . commentTimes ) ) : t , c = abf . commentTimes [ i ] , abf . commentTags [ i ] if minutes : t = t / 60 pylab . axvline ( t , lw = 1 , color = 'r' , ls = "--" , alpha = .5 ) X1 , X2 , Y1 , Y2 = pylab . axis ( ) Y2 = Y2 - abs ( Y2 - Y1 ) * .02 pylab . text ( t , Y2 , c , size = 8 , color = 'r' , rotation = 'vertical' , ha = 'right' , va = 'top' , weight = 'bold' , alpha = .5 ) if minutes : pylab . xlabel ( "minutes" ) else : pylab . xlabel ( "seconds" )
7740	def hold_exception ( method ) : @ functools . wraps ( method ) def wrapper ( self , * args , ** kwargs ) : try : return method ( self , * args , ** kwargs ) except Exception : if self . exc_info : raise if not self . _stack : logger . debug ( '@hold_exception wrapped method {0!r} called' ' from outside of the main loop' . format ( method ) ) raise self . exc_info = sys . exc_info ( ) logger . debug ( u"exception in glib main loop callback:" , exc_info = self . exc_info ) main_loop = self . _stack [ - 1 ] if main_loop is not None : main_loop . quit ( ) return False return wrapper
5921	def strip_fit ( self , ** kwargs ) : kwargs . setdefault ( 'fit' , 'rot+trans' ) kw_fit = { } for k in ( 'xy' , 'fit' , 'fitgroup' , 'input' ) : if k in kwargs : kw_fit [ k ] = kwargs . pop ( k ) kwargs [ 'input' ] = kwargs . pop ( 'strip_input' , [ 'Protein' ] ) kwargs [ 'force' ] = kw_fit [ 'force' ] = kwargs . pop ( 'force' , self . force ) paths = self . strip_water ( ** kwargs ) transformer_nowater = self . nowater [ paths [ 'xtc' ] ] return transformer_nowater . fit ( ** kw_fit )
4996	def default_content_filter ( sender , instance , ** kwargs ) : if kwargs [ 'created' ] and not instance . content_filter : instance . content_filter = get_default_catalog_content_filter ( ) instance . save ( )
2630	def scale_in ( self , blocks = 0 , machines = 0 , strategy = None ) : count = 0 instances = self . client . servers . list ( ) for instance in instances [ 0 : machines ] : print ( "Deleting : " , instance ) instance . delete ( ) count += 1 return count
5229	def _load_yaml_ ( file_name ) : if not os . path . exists ( file_name ) : return dict ( ) with open ( file_name , 'r' , encoding = 'utf-8' ) as fp : return YAML ( ) . load ( stream = fp )
1101	def context_diff ( a , b , fromfile = '' , tofile = '' , fromfiledate = '' , tofiledate = '' , n = 3 , lineterm = '\n' ) : r prefix = dict ( insert = '+ ' , delete = '- ' , replace = '! ' , equal = ' ' ) started = False for group in SequenceMatcher ( None , a , b ) . get_grouped_opcodes ( n ) : if not started : started = True fromdate = '\t%s' % ( fromfiledate ) if fromfiledate else '' todate = '\t%s' % ( tofiledate ) if tofiledate else '' yield '*** %s%s%s' % ( fromfile , fromdate , lineterm ) yield '--- %s%s%s' % ( tofile , todate , lineterm ) first , last = group [ 0 ] , group [ - 1 ] yield '***************' + lineterm file1_range = _format_range_context ( first [ 1 ] , last [ 2 ] ) yield '*** %s ****%s' % ( file1_range , lineterm ) if any ( tag in ( 'replace' , 'delete' ) for tag , _ , _ , _ , _ in group ) : for tag , i1 , i2 , _ , _ in group : if tag != 'insert' : for line in a [ i1 : i2 ] : yield prefix [ tag ] + line file2_range = _format_range_context ( first [ 3 ] , last [ 4 ] ) yield '--- %s ----%s' % ( file2_range , lineterm ) if any ( tag in ( 'replace' , 'insert' ) for tag , _ , _ , _ , _ in group ) : for tag , _ , _ , j1 , j2 in group : if tag != 'delete' : for line in b [ j1 : j2 ] : yield prefix [ tag ] + line
2898	def get_task ( self , id ) : tasks = [ task for task in self . get_tasks ( ) if task . id == id ] return tasks [ 0 ] if len ( tasks ) == 1 else None
3861	def add_event ( self , event_ ) : conv_event = self . _wrap_event ( event_ ) if conv_event . id_ not in self . _events_dict : self . _events . append ( conv_event ) self . _events_dict [ conv_event . id_ ] = conv_event else : logger . info ( 'Conversation %s ignoring duplicate event %s' , self . id_ , conv_event . id_ ) return None return conv_event
12674	def subset ( * args ) : if args and isinstance ( args [ 0 ] , dataframe . DataFrame ) : return args [ 0 ] . subset ( * args [ 1 : ] ) elif not args : raise ValueError ( "No arguments provided" ) else : return pipeable . Pipeable ( pipeable . PipingMethod . SUBSET , * args )
4555	def genVector ( width , height , x_mult = 1 , y_mult = 1 ) : center_x = ( width - 1 ) / 2 center_y = ( height - 1 ) / 2 def length ( x , y ) : dx = math . pow ( x - center_x , 2 * x_mult ) dy = math . pow ( y - center_y , 2 * y_mult ) return int ( math . sqrt ( dx + dy ) ) return [ [ length ( x , y ) for x in range ( width ) ] for y in range ( height ) ]
10438	def startprocessmonitor ( self , process_name , interval = 2 ) : if process_name in self . _process_stats : self . _process_stats [ process_name ] . stop ( ) self . _process_stats [ process_name ] = ProcessStats ( process_name , interval ) self . _process_stats [ process_name ] . start ( ) return 1
7434	def _bufcountlines ( filename , gzipped ) : if gzipped : fin = gzip . open ( filename ) else : fin = open ( filename ) nlines = 0 buf_size = 1024 * 1024 read_f = fin . read buf = read_f ( buf_size ) while buf : nlines += buf . count ( '\n' ) buf = read_f ( buf_size ) fin . close ( ) return nlines
9594	def execute_script ( self , script , * args ) : return self . _execute ( Command . EXECUTE_SCRIPT , { 'script' : script , 'args' : list ( args ) } )
7133	def add_icon ( icon_data , dest ) : with open ( os . path . join ( dest , "icon.png" ) , "wb" ) as f : f . write ( icon_data )
10	def save_policy ( self , path ) : with open ( path , 'wb' ) as f : pickle . dump ( self . policy , f )
4842	def get_program_type_by_slug ( self , slug ) : return self . _load_data ( self . PROGRAM_TYPES_ENDPOINT , resource_id = slug , default = None , )
1656	def IsOutOfLineMethodDefinition ( clean_lines , linenum ) : for i in xrange ( linenum , max ( - 1 , linenum - 10 ) , - 1 ) : if Match ( r'^([^()]*\w+)\(' , clean_lines . elided [ i ] ) : return Match ( r'^[^()]*\w+::\w+\(' , clean_lines . elided [ i ] ) is not None return False
7385	def group_theta ( self , group ) : for i , g in enumerate ( self . nodes . keys ( ) ) : if g == group : break return i * self . major_angle
838	def infer ( self , inputPattern , computeScores = True , overCategories = True , partitionId = None ) : sparsity = 0.0 if self . minSparsity > 0.0 : sparsity = ( float ( len ( inputPattern . nonzero ( ) [ 0 ] ) ) / len ( inputPattern ) ) if len ( self . _categoryList ) == 0 or sparsity < self . minSparsity : winner = None inferenceResult = numpy . zeros ( 1 ) dist = numpy . ones ( 1 ) categoryDist = numpy . ones ( 1 ) else : maxCategoryIdx = max ( self . _categoryList ) inferenceResult = numpy . zeros ( maxCategoryIdx + 1 ) dist = self . _getDistances ( inputPattern , partitionId = partitionId ) validVectorCount = len ( self . _categoryList ) - self . _categoryList . count ( - 1 ) if self . exact : exactMatches = numpy . where ( dist < 0.00001 ) [ 0 ] if len ( exactMatches ) > 0 : for i in exactMatches [ : min ( self . k , validVectorCount ) ] : inferenceResult [ self . _categoryList [ i ] ] += 1.0 else : sorted = dist . argsort ( ) for j in sorted [ : min ( self . k , validVectorCount ) ] : inferenceResult [ self . _categoryList [ j ] ] += 1.0 if inferenceResult . any ( ) : winner = inferenceResult . argmax ( ) inferenceResult /= inferenceResult . sum ( ) else : winner = None categoryDist = min_score_per_category ( maxCategoryIdx , self . _categoryList , dist ) categoryDist . clip ( 0 , 1.0 , categoryDist ) if self . verbosity >= 1 : print "%s infer:" % ( g_debugPrefix ) print " active inputs:" , _labeledInput ( inputPattern , cellsPerCol = self . cellsPerCol ) print " winner category:" , winner print " pct neighbors of each category:" , inferenceResult print " dist of each prototype:" , dist print " dist of each category:" , categoryDist result = ( winner , inferenceResult , dist , categoryDist ) return result
4668	def encrypt ( privkey , passphrase ) : if isinstance ( privkey , str ) : privkey = PrivateKey ( privkey ) else : privkey = PrivateKey ( repr ( privkey ) ) privkeyhex = repr ( privkey ) addr = format ( privkey . bitcoin . address , "BTC" ) a = _bytes ( addr ) salt = hashlib . sha256 ( hashlib . sha256 ( a ) . digest ( ) ) . digest ( ) [ 0 : 4 ] if SCRYPT_MODULE == "scrypt" : key = scrypt . hash ( passphrase , salt , 16384 , 8 , 8 ) elif SCRYPT_MODULE == "pylibscrypt" : key = scrypt . scrypt ( bytes ( passphrase , "utf-8" ) , salt , 16384 , 8 , 8 ) else : raise ValueError ( "No scrypt module loaded" ) ( derived_half1 , derived_half2 ) = ( key [ : 32 ] , key [ 32 : ] ) aes = AES . new ( derived_half2 , AES . MODE_ECB ) encrypted_half1 = _encrypt_xor ( privkeyhex [ : 32 ] , derived_half1 [ : 16 ] , aes ) encrypted_half2 = _encrypt_xor ( privkeyhex [ 32 : ] , derived_half1 [ 16 : ] , aes ) " flag byte is forced 0xc0 because Graphene only uses compressed keys " payload = b"\x01" + b"\x42" + b"\xc0" + salt + encrypted_half1 + encrypted_half2 " Checksum " checksum = hashlib . sha256 ( hashlib . sha256 ( payload ) . digest ( ) ) . digest ( ) [ : 4 ] privatkey = hexlify ( payload + checksum ) . decode ( "ascii" ) return Base58 ( privatkey )
877	def agitate ( self ) : for ( varName , var ) in self . permuteVars . iteritems ( ) : var . agitate ( ) self . newPosition ( )
5255	def disassemble_one ( bytecode , pc = 0 , fork = DEFAULT_FORK ) : instruction_table = instruction_tables [ fork ] if isinstance ( bytecode , bytes ) : bytecode = bytearray ( bytecode ) if isinstance ( bytecode , str ) : bytecode = bytearray ( bytecode . encode ( 'latin-1' ) ) bytecode = iter ( bytecode ) try : opcode = next ( bytecode ) except StopIteration : return assert isinstance ( opcode , int ) instruction = copy . copy ( instruction_table . get ( opcode , None ) ) if instruction is None : instruction = Instruction ( opcode , 'INVALID' , 0 , 0 , 0 , 0 , 'Unspecified invalid instruction.' ) instruction . pc = pc try : if instruction . has_operand : instruction . parse_operand ( bytecode ) except ParseError : instruction = None finally : return instruction
11966	def _bin_to_dec ( ip , check = True ) : if check and not is_bin ( ip ) : raise ValueError ( '_bin_to_dec: invalid IP: "%s"' % ip ) if isinstance ( ip , int ) : ip = str ( ip ) return int ( str ( ip ) , 2 )
2957	def _get_blockade_id_from_cwd ( self , cwd = None ) : if not cwd : cwd = os . getcwd ( ) parent_dir = os . path . abspath ( cwd ) basename = os . path . basename ( parent_dir ) . lower ( ) blockade_id = re . sub ( r"[^a-z0-9]" , "" , basename ) if not blockade_id : blockade_id = "default" return blockade_id
11635	def oauth2_access_parser ( self , raw_access ) : parsed_access = json . loads ( raw_access . content . decode ( 'utf-8' ) ) self . access_token = parsed_access [ 'access_token' ] self . token_type = parsed_access [ 'token_type' ] self . refresh_token = parsed_access [ 'refresh_token' ] self . guid = parsed_access [ 'xoauth_yahoo_guid' ] credentials = { 'access_token' : self . access_token , 'token_type' : self . token_type , 'refresh_token' : self . refresh_token , 'guid' : self . guid } return credentials
11364	def run_shell_command ( commands , ** kwargs ) : p = subprocess . Popen ( commands , stdout = subprocess . PIPE , stderr = subprocess . PIPE , ** kwargs ) output , error = p . communicate ( ) return p . returncode , output , error
9450	def cancel_scheduled_hangup ( self , call_params ) : path = '/' + self . api_version + '/CancelScheduledHangup/' method = 'POST' return self . request ( path , method , call_params )
449	def batch_normalization ( x , mean , variance , offset , scale , variance_epsilon , data_format , name = None ) : with ops . name_scope ( name , 'batchnorm' , [ x , mean , variance , scale , offset ] ) : inv = math_ops . rsqrt ( variance + variance_epsilon ) if scale is not None : inv *= scale a = math_ops . cast ( inv , x . dtype ) b = math_ops . cast ( offset - mean * inv if offset is not None else - mean * inv , x . dtype ) df = { 'channels_first' : 'NCHW' , 'channels_last' : 'NHWC' } return _bias_add ( _bias_scale ( x , a , df [ data_format ] ) , b , df [ data_format ] )
4912	def contains_content_items ( self , request , pk , course_run_ids , program_uuids ) : enterprise_customer = self . get_object ( ) course_run_ids = [ unquote ( quote_plus ( course_run_id ) ) for course_run_id in course_run_ids ] contains_content_items = False for catalog in enterprise_customer . enterprise_customer_catalogs . all ( ) : contains_course_runs = not course_run_ids or catalog . contains_courses ( course_run_ids ) contains_program_uuids = not program_uuids or catalog . contains_programs ( program_uuids ) if contains_course_runs and contains_program_uuids : contains_content_items = True break return Response ( { 'contains_content_items' : contains_content_items } )
10573	def get_local_songs ( filepaths , include_filters = None , exclude_filters = None , all_includes = False , all_excludes = False , exclude_patterns = None , max_depth = float ( 'inf' ) ) : logger . info ( "Loading local songs..." ) supported_filepaths = get_supported_filepaths ( filepaths , SUPPORTED_SONG_FORMATS , max_depth = max_depth ) included_songs , excluded_songs = exclude_filepaths ( supported_filepaths , exclude_patterns = exclude_patterns ) matched_songs , filtered_songs = filter_local_songs ( included_songs , include_filters = include_filters , exclude_filters = exclude_filters , all_includes = all_includes , all_excludes = all_excludes ) logger . info ( "Excluded {0} local songs" . format ( len ( excluded_songs ) ) ) logger . info ( "Filtered {0} local songs" . format ( len ( filtered_songs ) ) ) logger . info ( "Loaded {0} local songs" . format ( len ( matched_songs ) ) ) return matched_songs , filtered_songs , excluded_songs
10174	def set_bookmark ( self ) : def _success_date ( ) : bookmark = { 'date' : self . new_bookmark or datetime . datetime . utcnow ( ) . strftime ( self . doc_id_suffix ) } yield dict ( _index = self . last_index_written , _type = self . bookmark_doc_type , _source = bookmark ) if self . last_index_written : bulk ( self . client , _success_date ( ) , stats_only = True )
184	def coords_almost_equals ( self , other , max_distance = 1e-6 , points_per_edge = 8 ) : if isinstance ( other , LineString ) : pass elif isinstance ( other , tuple ) : other = LineString ( [ other ] ) else : other = LineString ( other ) if len ( self . coords ) == 0 and len ( other . coords ) == 0 : return True elif 0 in [ len ( self . coords ) , len ( other . coords ) ] : return False self_subd = self . subdivide ( points_per_edge ) other_subd = other . subdivide ( points_per_edge ) dist_self2other = self_subd . compute_pointwise_distances ( other_subd ) dist_other2self = other_subd . compute_pointwise_distances ( self_subd ) dist = max ( np . max ( dist_self2other ) , np . max ( dist_other2self ) ) return dist < max_distance
1471	def setup ( executor ) : def signal_handler ( signal_to_handle , frame ) : Log . info ( 'signal_handler invoked with signal %s' , signal_to_handle ) executor . stop_state_manager_watches ( ) sys . exit ( signal_to_handle ) def cleanup ( ) : Log . info ( 'Executor terminated; exiting all process in executor.' ) for pid in executor . processes_to_monitor . keys ( ) : os . kill ( pid , signal . SIGTERM ) time . sleep ( 5 ) os . killpg ( 0 , signal . SIGTERM ) shardid = executor . shard log . configure ( logfile = 'heron-executor-%s.stdout' % shardid ) pid = os . getpid ( ) sid = os . getsid ( pid ) if pid <> sid : Log . info ( 'Set up process group; executor becomes leader' ) os . setpgrp ( ) Log . info ( 'Register the SIGTERM signal handler' ) signal . signal ( signal . SIGTERM , signal_handler ) Log . info ( 'Register the atexit clean up' ) atexit . register ( cleanup )
9002	def _register_instruction_in_defs ( self , instruction ) : type_ = instruction . type color_ = instruction . color instruction_to_svg_dict = self . _instruction_to_svg . instruction_to_svg_dict instruction_id = "{}:{}" . format ( type_ , color_ ) defs_id = instruction_id + ":defs" if instruction_id not in self . _instruction_type_color_to_symbol : svg_dict = instruction_to_svg_dict ( instruction ) self . _compute_scale ( instruction_id , svg_dict ) symbol = self . _make_definition ( svg_dict , instruction_id ) self . _instruction_type_color_to_symbol [ defs_id ] = symbol [ DEFINITION_HOLDER ] . pop ( "defs" , { } ) self . _instruction_type_color_to_symbol [ instruction_id ] = symbol return instruction_id
4701	def env ( ) : if cij . ssh . env ( ) : cij . err ( "cij.lnvm.env: invalid SSH environment" ) return 1 lnvm = cij . env_to_dict ( PREFIX , REQUIRED ) nvme = cij . env_to_dict ( "NVME" , [ "DEV_NAME" ] ) if "BGN" not in lnvm . keys ( ) : cij . err ( "cij.lnvm.env: invalid LNVM_BGN" ) return 1 if "END" not in lnvm . keys ( ) : cij . err ( "cij.lnvm.env: invalid LNVM_END" ) return 1 if "DEV_TYPE" not in lnvm . keys ( ) : cij . err ( "cij.lnvm.env: invalid LNVM_DEV_TYPE" ) return 1 lnvm [ "DEV_NAME" ] = "%sb%03de%03d" % ( nvme [ "DEV_NAME" ] , int ( lnvm [ "BGN" ] ) , int ( lnvm [ "END" ] ) ) lnvm [ "DEV_PATH" ] = "/dev/%s" % lnvm [ "DEV_NAME" ] cij . env_export ( PREFIX , EXPORTED , lnvm ) return 0
7515	def enter_singles ( iloc , pnames , snppad , edg , aseqs , asnps , smask , samplecov , locuscov , start ) : seq = aseqs [ iloc , : , edg [ 0 ] : edg [ 1 ] + 1 ] snp = asnps [ iloc , edg [ 0 ] : edg [ 1 ] + 1 , ] nalln = np . all ( seq == "N" , axis = 1 ) nsidx = nalln + smask samplecov = samplecov + np . invert ( nsidx ) . astype ( np . int32 ) idx = np . sum ( np . invert ( nsidx ) . astype ( np . int32 ) ) locuscov [ idx ] += 1 seq = seq [ ~ nsidx , ] names = pnames [ ~ nsidx ] outstr = "\n" . join ( [ name + s . tostring ( ) for name , s in zip ( names , seq ) ] ) snpstring = [ "-" if snp [ i , 0 ] else "*" if snp [ i , 1 ] else " " for i in range ( len ( snp ) ) ] outstr += "\n" + snppad + "" . join ( snpstring ) + "|{}|" . format ( iloc + start ) return outstr , samplecov , locuscov
8016	async def receive_json ( self , content , ** kwargs ) : if isinstance ( content , dict ) and "stream" in content and "payload" in content : steam_name = content [ "stream" ] payload = content [ "payload" ] if steam_name not in self . applications_accepting_frames : raise ValueError ( "Invalid multiplexed frame received (stream not mapped)" ) await self . send_upstream ( message = { "type" : "websocket.receive" , "text" : await self . encode_json ( payload ) } , stream_name = steam_name ) return else : raise ValueError ( "Invalid multiplexed **frame received (no channel/payload key)" )
4468	def deserialize ( encoded , ** kwargs ) : params = jsonpickle . decode ( encoded , ** kwargs ) return __reconstruct ( params )
12974	def compat_convertHashedIndexes ( self , fetchAll = True ) : saver = IndexedRedisSave ( self . mdl ) if fetchAll is True : objs = self . all ( ) saver . compat_convertHashedIndexes ( objs ) else : didWarnOnce = False pks = self . getPrimaryKeys ( ) for pk in pks : obj = self . get ( pk ) if not obj : if didWarnOnce is False : sys . stderr . write ( 'WARNING(once)! An object (type=%s , pk=%d) disappered while ' 'running compat_convertHashedIndexes! This probably means an application ' 'is using the model while converting indexes. This is a very BAD IDEA (tm).' ) didWarnOnce = True continue saver . compat_convertHashedIndexes ( [ obj ] )
13779	def FindMessageTypeByName ( self , full_name ) : full_name = _NormalizeFullyQualifiedName ( full_name ) if full_name not in self . _descriptors : self . FindFileContainingSymbol ( full_name ) return self . _descriptors [ full_name ]
6583	def _post_start ( self ) : flags = fcntl . fcntl ( self . _process . stdout , fcntl . F_GETFL ) fcntl . fcntl ( self . _process . stdout , fcntl . F_SETFL , flags | os . O_NONBLOCK )
8007	def make_error_response ( self , cond ) : if self . stanza_type == "error" : raise ValueError ( "Errors may not be generated in response" " to errors" ) stanza = Presence ( stanza_type = "error" , from_jid = self . from_jid , to_jid = self . to_jid , stanza_id = self . stanza_id , status = self . _status , show = self . _show , priority = self . _priority , error_cond = cond ) if self . _payload is None : self . decode_payload ( ) for payload in self . _payload : stanza . add_payload ( payload ) return stanza
3816	async def _add_channel_services ( self ) : logger . info ( 'Adding channel services...' ) services = [ "babel" , "babel_presence_last_seen" ] map_list = [ dict ( p = json . dumps ( { "3" : { "1" : { "1" : service } } } ) ) for service in services ] await self . _channel . send_maps ( map_list ) logger . info ( 'Channel services added' )
8972	def connect_to ( self , other_mesh ) : other_mesh . disconnect ( ) self . disconnect ( ) self . _connect_to ( other_mesh )
9606	def check_unused_args ( self , used_args , args , kwargs ) : for k , v in kwargs . items ( ) : if k in used_args : self . _used_kwargs . update ( { k : v } ) else : self . _unused_kwargs . update ( { k : v } )
1278	def markdown ( text , escape = True , ** kwargs ) : return Markdown ( escape = escape , ** kwargs ) ( text )
4417	async def play_now ( self , requester : int , track : dict ) : self . add_next ( requester , track ) await self . play ( ignore_shuffle = True )
1613	def IsErrorSuppressedByNolint ( category , linenum ) : return ( _global_error_suppressions . get ( category , False ) or linenum in _error_suppressions . get ( category , set ( ) ) or linenum in _error_suppressions . get ( None , set ( ) ) )
3455	def weight ( self ) : try : return sum ( [ count * elements_and_molecular_weights [ element ] for element , count in self . elements . items ( ) ] ) except KeyError as e : warn ( "The element %s does not appear in the periodic table" % e )
7345	async def call_on_response ( self , data ) : since_id = self . kwargs . get ( self . param , 0 ) + 1 if self . fill_gaps : if data [ - 1 ] [ 'id' ] != since_id : max_id = data [ - 1 ] [ 'id' ] - 1 responses = with_max_id ( self . request ( ** self . kwargs , max_id = max_id ) ) async for tweets in responses : data . extend ( tweets ) if data [ - 1 ] [ 'id' ] == self . last_id : data = data [ : - 1 ] if not data and not self . force : raise StopAsyncIteration await self . set_param ( data )
6162	def MPSK_bb ( N_symb , Ns , M , pulse = 'rect' , alpha = 0.25 , MM = 6 ) : data = np . random . randint ( 0 , M , N_symb ) xs = np . exp ( 1j * 2 * np . pi / M * data ) x = np . hstack ( ( xs . reshape ( N_symb , 1 ) , np . zeros ( ( N_symb , int ( Ns ) - 1 ) ) ) ) x = x . flatten ( ) if pulse . lower ( ) == 'rect' : b = np . ones ( int ( Ns ) ) elif pulse . lower ( ) == 'rc' : b = rc_imp ( Ns , alpha , MM ) elif pulse . lower ( ) == 'src' : b = sqrt_rc_imp ( Ns , alpha , MM ) else : raise ValueError ( 'pulse type must be rec, rc, or src' ) x = signal . lfilter ( b , 1 , x ) if M == 4 : x = x * np . exp ( 1j * np . pi / 4 ) return x , b / float ( Ns ) , data
6440	def dist_euclidean ( src , tar , qval = 2 , alphabet = None ) : return Euclidean ( ) . dist ( src , tar , qval , alphabet )
7087	def get_epochs_given_midtimes_and_period ( t_mid , period , err_t_mid = None , t0_fixed = None , t0_percentile = None , verbose = False ) : kwargarr = np . array ( [ isinstance ( err_t_mid , np . ndarray ) , t0_fixed , t0_percentile ] ) if not _single_true ( kwargarr ) and not np . all ( ~ kwargarr . astype ( bool ) ) : raise AssertionError ( 'can have at most one of err_t_mid, t0_fixed, t0_percentile' ) t_mid = t_mid [ np . isfinite ( t_mid ) ] N_midtimes = len ( t_mid ) if t0_fixed : t0 = t0_fixed elif isinstance ( err_t_mid , np . ndarray ) : t0_avg = np . average ( t_mid , weights = 1 / err_t_mid ** 2 ) t0_options = np . arange ( min ( t_mid ) , max ( t_mid ) + period , period ) t0 = t0_options [ np . argmin ( np . abs ( t0_options - t0_avg ) ) ] else : if not t0_percentile : if N_midtimes % 2 == 1 : t0 = np . median ( t_mid ) else : t0 = t_mid [ int ( N_midtimes / 2 ) ] else : t0 = np . sort ( t_mid ) [ int ( N_midtimes * t0_percentile / 100 ) ] epoch = ( t_mid - t0 ) / period int_epoch = np . round ( epoch , 0 ) if verbose : LOGINFO ( 'epochs before rounding' ) LOGINFO ( '\n{:s}' . format ( repr ( epoch ) ) ) LOGINFO ( 'epochs after rounding' ) LOGINFO ( '\n{:s}' . format ( repr ( int_epoch ) ) ) return int_epoch , t0
5112	def animate ( self , out = None , t = None , line_kwargs = None , scatter_kwargs = None , ** kwargs ) : if not self . _initialized : msg = ( "Network has not been initialized. " "Call '.initialize()' first." ) raise QueueingToolError ( msg ) if not HAS_MATPLOTLIB : msg = "Matplotlib is necessary to animate a simulation." raise ImportError ( msg ) self . _update_all_colors ( ) kwargs . setdefault ( 'bgcolor' , self . colors [ 'bgcolor' ] ) fig = plt . figure ( figsize = kwargs . get ( 'figsize' , ( 7 , 7 ) ) ) ax = fig . gca ( ) mpl_kwargs = { 'line_kwargs' : line_kwargs , 'scatter_kwargs' : scatter_kwargs , 'pos' : kwargs . get ( 'pos' ) } line_args , scat_args = self . g . lines_scatter_args ( ** mpl_kwargs ) lines = LineCollection ( ** line_args ) lines = ax . add_collection ( lines ) scatt = ax . scatter ( ** scat_args ) t = np . infty if t is None else t now = self . _t def update ( frame_number ) : if t is not None : if self . _t > now + t : return False self . _simulate_next_event ( slow = True ) lines . set_color ( line_args [ 'colors' ] ) scatt . set_edgecolors ( scat_args [ 'edgecolors' ] ) scatt . set_facecolor ( scat_args [ 'c' ] ) if hasattr ( ax , 'set_facecolor' ) : ax . set_facecolor ( kwargs [ 'bgcolor' ] ) else : ax . set_axis_bgcolor ( kwargs [ 'bgcolor' ] ) ax . get_xaxis ( ) . set_visible ( False ) ax . get_yaxis ( ) . set_visible ( False ) animation_args = { 'fargs' : None , 'event_source' : None , 'init_func' : None , 'frames' : None , 'blit' : False , 'interval' : 10 , 'repeat' : None , 'func' : update , 'repeat_delay' : None , 'fig' : fig , 'save_count' : None , } for key , value in kwargs . items ( ) : if key in animation_args : animation_args [ key ] = value animation = FuncAnimation ( ** animation_args ) if 'filename' not in kwargs : plt . ioff ( ) plt . show ( ) else : save_args = { 'filename' : None , 'writer' : None , 'fps' : None , 'dpi' : None , 'codec' : None , 'bitrate' : None , 'extra_args' : None , 'metadata' : None , 'extra_anim' : None , 'savefig_kwargs' : None } for key , value in kwargs . items ( ) : if key in save_args : save_args [ key ] = value animation . save ( ** save_args )
1730	def match ( self , string , pos ) : return self . pat . match ( string , int ( pos ) )
7264	def validate ( method ) : name_error = 'configuration option "{}" is not supported' @ functools . wraps ( method ) def validator ( self , name , * args ) : if name not in self . allowed_opts : raise ValueError ( name_error . format ( name ) ) return method ( self , name , * args ) return validator
735	def sort ( filename , key , outputFile , fields = None , watermark = 1024 * 1024 * 100 ) : if fields is not None : assert set ( key ) . issubset ( set ( [ f [ 0 ] for f in fields ] ) ) with FileRecordStream ( filename ) as f : if fields : fieldNames = [ ff [ 0 ] for ff in fields ] indices = [ f . getFieldNames ( ) . index ( name ) for name in fieldNames ] assert len ( indices ) == len ( fields ) else : fileds = f . getFields ( ) fieldNames = f . getFieldNames ( ) indices = None key = [ fieldNames . index ( name ) for name in key ] chunk = 0 records = [ ] for i , r in enumerate ( f ) : if indices : temp = [ ] for i in indices : temp . append ( r [ i ] ) r = temp records . append ( r ) available_memory = psutil . avail_phymem ( ) if available_memory < watermark : _sortChunk ( records , key , chunk , fields ) records = [ ] chunk += 1 if len ( records ) > 0 : _sortChunk ( records , key , chunk , fields ) chunk += 1 _mergeFiles ( key , chunk , outputFile , fields )
6010	def load_poisson_noise_map ( poisson_noise_map_path , poisson_noise_map_hdu , pixel_scale , convert_poisson_noise_map_from_weight_map , convert_poisson_noise_map_from_inverse_noise_map , poisson_noise_map_from_image , image , exposure_time_map , convert_from_electrons , gain , convert_from_adus ) : poisson_noise_map_options = sum ( [ convert_poisson_noise_map_from_weight_map , convert_poisson_noise_map_from_inverse_noise_map , poisson_noise_map_from_image ] ) if poisson_noise_map_options == 0 and poisson_noise_map_path is not None : return PoissonNoiseMap . from_fits_with_pixel_scale ( file_path = poisson_noise_map_path , hdu = poisson_noise_map_hdu , pixel_scale = pixel_scale ) elif poisson_noise_map_from_image : if not ( convert_from_electrons or convert_from_adus ) and exposure_time_map is None : raise exc . DataException ( 'Cannot compute the Poisson noise-map from the image if an ' 'exposure-time (or exposure time map) is not supplied to convert to adus' ) if convert_from_adus and gain is None : raise exc . DataException ( 'Cannot compute the Poisson noise-map from the image if a' 'gain is not supplied to convert from adus' ) return PoissonNoiseMap . from_image_and_exposure_time_map ( pixel_scale = pixel_scale , image = image , exposure_time_map = exposure_time_map , convert_from_electrons = convert_from_electrons , gain = gain , convert_from_adus = convert_from_adus ) elif convert_poisson_noise_map_from_weight_map and poisson_noise_map_path is not None : weight_map = Array . from_fits ( file_path = poisson_noise_map_path , hdu = poisson_noise_map_hdu ) return PoissonNoiseMap . from_weight_map ( weight_map = weight_map , pixel_scale = pixel_scale ) elif convert_poisson_noise_map_from_inverse_noise_map and poisson_noise_map_path is not None : inverse_noise_map = Array . from_fits ( file_path = poisson_noise_map_path , hdu = poisson_noise_map_hdu ) return PoissonNoiseMap . from_inverse_noise_map ( inverse_noise_map = inverse_noise_map , pixel_scale = pixel_scale ) else : return None
10210	def _is_root ( ) : import os import ctypes try : return os . geteuid ( ) == 0 except AttributeError : return ctypes . windll . shell32 . IsUserAnAdmin ( ) != 0 return False
12013	def generate_panel ( self , img ) : plt . figure ( figsize = ( 14 , 6 ) ) ax = plt . gca ( ) fig = plt . gcf ( ) plt . subplot ( 122 ) data_save = np . zeros_like ( self . postcard ) self . roll_best = np . zeros ( ( 4 , 2 ) ) for i in range ( 4 ) : g = np . where ( self . qs == i ) [ 0 ] wh = np . where ( self . times [ g ] > 54947 ) self . roll_best [ i ] = self . do_rolltest ( g , wh ) self . do_photometry ( ) for i in range ( 4 ) : g = np . where ( self . qs == i ) [ 0 ] plt . errorbar ( self . times [ g ] , self . obs_flux [ g ] , yerr = self . flux_uncert [ i ] , fmt = fmt [ i ] ) plt . xlabel ( 'Time' , fontsize = 20 ) plt . ylabel ( 'Relative Flux' , fontsize = 20 ) plt . subplot ( 121 ) implot = plt . imshow ( img , interpolation = 'nearest' , cmap = 'gray' , vmin = 98000 * 52 , vmax = 104000 * 52 ) cid = fig . canvas . mpl_connect ( 'button_press_event' , self . onclick ) plt . show ( block = True )
1536	def get_heron_options_from_env ( ) : heron_options_raw = os . environ . get ( "HERON_OPTIONS" ) if heron_options_raw is None : raise RuntimeError ( "HERON_OPTIONS environment variable not found" ) options = { } for option_line in heron_options_raw . replace ( "%%%%" , " " ) . split ( ',' ) : key , sep , value = option_line . partition ( "=" ) if sep : options [ key ] = value else : raise ValueError ( "Invalid HERON_OPTIONS part %r" % option_line ) return options
2889	def parse_node ( self ) : try : self . task = self . create_task ( ) self . task . documentation = self . parser . _parse_documentation ( self . node , xpath = self . xpath , task_parser = self ) boundary_event_nodes = self . process_xpath ( './/bpmn:boundaryEvent[@attachedToRef="%s"]' % self . get_id ( ) ) if boundary_event_nodes : parent_task = _BoundaryEventParent ( self . spec , '%s.BoundaryEventParent' % self . get_id ( ) , self . task , lane = self . task . lane ) self . process_parser . parsed_nodes [ self . node . get ( 'id' ) ] = parent_task parent_task . connect_outgoing ( self . task , '%s.FromBoundaryEventParent' % self . get_id ( ) , None , None ) for boundary_event in boundary_event_nodes : b = self . process_parser . parse_node ( boundary_event ) parent_task . connect_outgoing ( b , '%s.FromBoundaryEventParent' % boundary_event . get ( 'id' ) , None , None ) else : self . process_parser . parsed_nodes [ self . node . get ( 'id' ) ] = self . task children = [ ] outgoing = self . process_xpath ( './/bpmn:sequenceFlow[@sourceRef="%s"]' % self . get_id ( ) ) if len ( outgoing ) > 1 and not self . handles_multiple_outgoing ( ) : raise ValidationException ( 'Multiple outgoing flows are not supported for ' 'tasks of type' , node = self . node , filename = self . process_parser . filename ) for sequence_flow in outgoing : target_ref = sequence_flow . get ( 'targetRef' ) target_node = one ( self . process_xpath ( './/*[@id="%s"]' % target_ref ) ) c = self . process_parser . parse_node ( target_node ) children . append ( ( c , target_node , sequence_flow ) ) if children : default_outgoing = self . node . get ( 'default' ) if not default_outgoing : ( c , target_node , sequence_flow ) = children [ 0 ] default_outgoing = sequence_flow . get ( 'id' ) for ( c , target_node , sequence_flow ) in children : self . connect_outgoing ( c , target_node , sequence_flow , sequence_flow . get ( 'id' ) == default_outgoing ) return parent_task if boundary_event_nodes else self . task except ValidationException : raise except Exception as ex : exc_info = sys . exc_info ( ) tb = "" . join ( traceback . format_exception ( exc_info [ 0 ] , exc_info [ 1 ] , exc_info [ 2 ] ) ) LOG . error ( "%r\n%s" , ex , tb ) raise ValidationException ( "%r" % ( ex ) , node = self . node , filename = self . process_parser . filename )
12937	def depricated_name ( newmethod ) : def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , ** kwargs ) : warnings . simplefilter ( 'always' , DeprecationWarning ) warnings . warn ( "Function {} is depricated, please use {} instead." . format ( func . __name__ , newmethod ) , category = DeprecationWarning , stacklevel = 2 ) warnings . simplefilter ( 'default' , DeprecationWarning ) return func ( * args , ** kwargs ) return wrapper return decorator
1425	def initialize ( self , config , context ) : self . logger . info ( "Initializing PulsarSpout with the following" ) self . logger . info ( "Component-specific config: \n%s" % str ( config ) ) self . logger . info ( "Context: \n%s" % str ( context ) ) self . emit_count = 0 self . ack_count = 0 self . fail_count = 0 if not PulsarSpout . serviceUrl in config or not PulsarSpout . topicName in config : self . logger . fatal ( "Need to specify both serviceUrl and topicName" ) self . pulsar_cluster = str ( config [ PulsarSpout . serviceUrl ] ) self . topic = str ( config [ PulsarSpout . topicName ] ) mode = config [ api_constants . TOPOLOGY_RELIABILITY_MODE ] if mode == api_constants . TopologyReliabilityMode . ATLEAST_ONCE : self . acking_timeout = 1000 * int ( config [ api_constants . TOPOLOGY_MESSAGE_TIMEOUT_SECS ] ) else : self . acking_timeout = 30000 if PulsarSpout . receiveTimeoutMs in config : self . receive_timeout_ms = config [ PulsarSpout . receiveTimeoutMs ] else : self . receive_timeout_ms = 10 if PulsarSpout . deserializer in config : self . deserializer = config [ PulsarSpout . deserializer ] if not callable ( self . deserializer ) : self . logger . fatal ( "Pulsar Message Deserializer needs to be callable" ) else : self . deserializer = self . default_deserializer self . logConfFileName = GenerateLogConfig ( context ) self . logger . info ( "Generated LogConf at %s" % self . logConfFileName ) self . client = pulsar . Client ( self . pulsar_cluster , log_conf_file_path = self . logConfFileName ) self . logger . info ( "Setup Client with cluster %s" % self . pulsar_cluster ) try : self . consumer = self . client . subscribe ( self . topic , context . get_topology_name ( ) , consumer_type = pulsar . ConsumerType . Failover , unacked_messages_timeout_ms = self . acking_timeout ) except Exception as e : self . logger . fatal ( "Pulsar client subscription failed: %s" % str ( e ) ) self . logger . info ( "Subscribed to topic %s" % self . topic )
420	def save_validation_log ( self , ** kwargs ) : self . _fill_project_info ( kwargs ) kwargs . update ( { 'time' : datetime . utcnow ( ) } ) _result = self . db . ValidLog . insert_one ( kwargs ) _log = self . _print_dict ( kwargs ) logging . info ( "[Database] valid log: " + _log )
13871	def CanonicalPath ( path ) : path = os . path . normpath ( path ) path = os . path . abspath ( path ) path = os . path . normcase ( path ) return path
7690	def sonify ( annotation , sr = 22050 , duration = None , ** kwargs ) : length = None if duration is None : duration = annotation . duration if duration is not None : length = int ( duration * sr ) if annotation . namespace in SONIFY_MAPPING : ann = coerce_annotation ( annotation , annotation . namespace ) return SONIFY_MAPPING [ annotation . namespace ] ( ann , sr = sr , length = length , ** kwargs ) for namespace , func in six . iteritems ( SONIFY_MAPPING ) : try : ann = coerce_annotation ( annotation , namespace ) return func ( ann , sr = sr , length = length , ** kwargs ) except NamespaceError : pass raise NamespaceError ( 'Unable to sonify annotation of namespace="{:s}"' . format ( annotation . namespace ) )
13037	def overview ( ) : doc = Host ( ) search = doc . search ( ) search . aggs . bucket ( 'tag_count' , 'terms' , field = 'tags' , order = { '_count' : 'desc' } , size = 100 ) response = search . execute ( ) print_line ( "{0:<25} {1}" . format ( 'Tag' , 'Count' ) ) print_line ( "-" * 30 ) for entry in response . aggregations . tag_count . buckets : print_line ( "{0:<25} {1}" . format ( entry . key , entry . doc_count ) )
7313	def process_request ( self , request ) : if not request : return if not db_loaded : load_db ( ) tz = request . session . get ( 'django_timezone' ) if not tz : tz = timezone . get_default_timezone ( ) client_ip = get_ip_address_from_request ( request ) ip_addrs = client_ip . split ( ',' ) for ip in ip_addrs : if is_valid_ip ( ip ) and not is_local_ip ( ip ) : if ':' in ip : tz = db_v6 . time_zone_by_addr ( ip ) break else : tz = db . time_zone_by_addr ( ip ) break if tz : timezone . activate ( tz ) request . session [ 'django_timezone' ] = str ( tz ) if getattr ( settings , 'AUTH_USER_MODEL' , None ) and getattr ( request , 'user' , None ) : detected_timezone . send ( sender = get_user_model ( ) , instance = request . user , timezone = tz ) else : timezone . deactivate ( )
9562	def _apply_skips ( self , i , r , summarize = False , report_unexpected_exceptions = True , context = None ) : for skip in self . _skips : try : result = skip ( r ) if result is True : yield True except Exception as e : if report_unexpected_exceptions : p = { 'code' : UNEXPECTED_EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED_EXCEPTION ] % ( e . __class__ . __name__ , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( skip . __name__ , skip . __doc__ ) if context is not None : p [ 'context' ] = context yield p
8368	def _parse ( self ) : p1 = "\[.*?\](.*?)\[\/.*?\]" p2 = "\[(.*?)\]" self . links = [ ] for p in ( p1 , p2 ) : for link in re . findall ( p , self . description ) : self . links . append ( link ) self . description = re . sub ( p , "\\1" , self . description ) self . description = self . description . strip ( )
6031	def unmasked_blurred_image_from_psf_and_unmasked_image ( self , psf , unmasked_image_1d ) : blurred_image_1d = self . regular . convolve_array_1d_with_psf ( padded_array_1d = unmasked_image_1d , psf = psf ) return self . regular . scaled_array_2d_from_array_1d ( array_1d = blurred_image_1d )
2805	def convert_elementwise_add ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting elementwise_add ...' ) if 'broadcast' in params : model0 = layers [ inputs [ 0 ] ] model1 = layers [ inputs [ 1 ] ] if names == 'short' : tf_name = 'A' + random_string ( 7 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) def target_layer ( x ) : layer = tf . add ( x [ 0 ] , x [ 1 ] ) return layer lambda_layer = keras . layers . Lambda ( target_layer , name = tf_name ) layers [ scope_name ] = lambda_layer ( [ layers [ inputs [ 0 ] ] , layers [ inputs [ 1 ] ] ] ) else : model0 = layers [ inputs [ 0 ] ] model1 = layers [ inputs [ 1 ] ] if names == 'short' : tf_name = 'A' + random_string ( 7 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) add = keras . layers . Add ( name = tf_name ) layers [ scope_name ] = add ( [ model0 , model1 ] )
9937	def list ( self , ignore_patterns ) : for storage in six . itervalues ( self . storages ) : if storage . exists ( '' ) : for path in utils . get_files ( storage , ignore_patterns ) : yield path , storage
1571	def submit_tar ( cl_args , unknown_args , tmp_dir ) : topology_file = cl_args [ 'topology-file-name' ] java_defines = cl_args [ 'topology_main_jvm_property' ] main_class = cl_args [ 'topology-class-name' ] res = execute . heron_tar ( main_class , topology_file , tuple ( unknown_args ) , tmp_dir , java_defines ) result . render ( res ) if not result . is_successful ( res ) : err_context = ( "Failed to create topology definition " "file when executing class '%s' of file '%s'" ) % ( main_class , topology_file ) res . add_context ( err_context ) return res return launch_topologies ( cl_args , topology_file , tmp_dir )
4246	def id_by_name ( self , hostname ) : addr = self . _gethostbyname ( hostname ) return self . id_by_addr ( addr )
266	def format_asset ( asset ) : try : import zipline . assets except ImportError : return asset if isinstance ( asset , zipline . assets . Asset ) : return asset . symbol else : return asset
2083	def launch ( self , workflow_job_template = None , monitor = False , wait = False , timeout = None , extra_vars = None , ** kwargs ) : if extra_vars is not None and len ( extra_vars ) > 0 : kwargs [ 'extra_vars' ] = parser . process_extra_vars ( extra_vars ) debug . log ( 'Launching the workflow job.' , header = 'details' ) self . _pop_none ( kwargs ) post_response = client . post ( 'workflow_job_templates/{0}/launch/' . format ( workflow_job_template ) , data = kwargs ) . json ( ) workflow_job_id = post_response [ 'id' ] post_response [ 'changed' ] = True if monitor : return self . monitor ( workflow_job_id , timeout = timeout ) elif wait : return self . wait ( workflow_job_id , timeout = timeout ) return post_response
5602	def create_app ( mapchete_files = None , zoom = None , bounds = None , single_input_file = None , mode = "continue" , debug = None ) : from flask import Flask , render_template_string app = Flask ( __name__ ) mapchete_processes = { os . path . splitext ( os . path . basename ( mapchete_file ) ) [ 0 ] : mapchete . open ( mapchete_file , zoom = zoom , bounds = bounds , single_input_file = single_input_file , mode = mode , with_cache = True , debug = debug ) for mapchete_file in mapchete_files } mp = next ( iter ( mapchete_processes . values ( ) ) ) pyramid_type = mp . config . process_pyramid . grid pyramid_srid = mp . config . process_pyramid . crs . to_epsg ( ) process_bounds = "," . join ( [ str ( i ) for i in mp . config . bounds_at_zoom ( ) ] ) grid = "g" if pyramid_srid == 3857 else "WGS84" web_pyramid = BufferedTilePyramid ( pyramid_type ) @ app . route ( '/' , methods = [ 'GET' ] ) def index ( ) : return render_template_string ( pkgutil . get_data ( 'mapchete.static' , 'index.html' ) . decode ( "utf-8" ) , srid = pyramid_srid , process_bounds = process_bounds , is_mercator = ( pyramid_srid == 3857 ) , process_names = mapchete_processes . keys ( ) ) @ app . route ( "/" . join ( [ "" , "wmts_simple" , "1.0.0" , "<string:mp_name>" , "default" , grid , "<int:zoom>" , "<int:row>" , "<int:col>.<string:file_ext>" ] ) , methods = [ 'GET' ] ) def get ( mp_name , zoom , row , col , file_ext ) : logger . debug ( "received tile (%s, %s, %s) for process %s" , zoom , row , col , mp_name ) return _tile_response ( mapchete_processes [ mp_name ] , web_pyramid . tile ( zoom , row , col ) , debug ) return app
9132	def count ( cls , session : Optional [ Session ] = None ) -> int : if session is None : session = _make_session ( ) count = session . query ( cls ) . count ( ) session . close ( ) return count
13190	def json_doc_to_xml ( json_obj , lang = 'en' , custom_namespace = None ) : if 'meta' not in json_obj : raise Exception ( "This function requires a conforming Open511 JSON document with a 'meta' section." ) json_obj = dict ( json_obj ) meta = json_obj . pop ( 'meta' ) elem = get_base_open511_element ( lang = lang , version = meta . pop ( 'version' ) ) pagination = json_obj . pop ( 'pagination' , None ) json_struct_to_xml ( json_obj , elem , custom_namespace = custom_namespace ) if pagination : elem . append ( json_struct_to_xml ( pagination , 'pagination' , custom_namespace = custom_namespace ) ) json_struct_to_xml ( meta , elem ) return elem
2434	def set_lics_list_ver ( self , doc , value ) : if not self . lics_list_ver_set : self . lics_list_ver_set = True vers = version . Version . from_str ( value ) if vers is not None : doc . creation_info . license_list_version = vers return True else : raise SPDXValueError ( 'CreationInfo::LicenseListVersion' ) else : raise CardinalityError ( 'CreationInfo::LicenseListVersion' )
11549	def main ( ) : usage = "Usage: %prog PATH_TO_PACKAGE" parser = optparse . OptionParser ( usage = usage ) parser . add_option ( "-v" , "--verbose" , action = "store_true" , dest = "verbose" , default = False , help = "Show debug output" ) parser . add_option ( "-d" , "--output-dir" , action = "store" , type = "string" , dest = "output_dir" , default = '' , help = "" ) parser . add_option ( "-t" , "--test-args" , action = "store" , type = "string" , dest = "test_args" , default = '' , help = ( "Pass argument on to bin/test. Quote the argument, " + "for instance \"-t '-m somemodule'\"." ) ) ( options , args ) = parser . parse_args ( ) if options . verbose : log_level = logging . DEBUG else : log_level = logging . INFO logging . basicConfig ( level = log_level , format = "%(levelname)s: %(message)s" ) curdir = os . getcwd ( ) testbinary = os . path . join ( curdir , 'bin' , 'test' ) if not os . path . exists ( testbinary ) : raise RuntimeError ( "Test command doesn't exist: %s" % testbinary ) coveragebinary = os . path . join ( curdir , 'bin' , 'coverage' ) if not os . path . exists ( coveragebinary ) : logger . debug ( "Trying globally installed coverage command." ) coveragebinary = 'coverage' logger . info ( "Running tests in coverage mode (can take a long time)" ) parts = [ coveragebinary , 'run' , testbinary ] if options . test_args : parts . append ( options . test_args ) system ( " " . join ( parts ) ) logger . debug ( "Creating coverage reports..." ) if options . output_dir : coverage_dir = options . output_dir open_in_browser = False else : coverage_dir = 'htmlcov' open_in_browser = True system ( "%s html --directory=%s" % ( coveragebinary , coverage_dir ) ) logger . info ( "Wrote coverage files to %s" , coverage_dir ) if open_in_browser : index_file = os . path . abspath ( os . path . join ( coverage_dir , 'index.html' ) ) logger . debug ( "About to open %s in your webbrowser." , index_file ) webbrowser . open ( 'file://' + index_file ) logger . info ( "Opened reports in your browser." )
10925	def reset ( self , new_damping = None ) : self . _num_iter = 0 self . _inner_run_counter = 0 self . _J_update_counter = self . update_J_frequency self . _fresh_JTJ = False self . _has_run = False if new_damping is not None : self . damping = np . array ( new_damping ) . astype ( 'float' ) self . _set_err_paramvals ( )
3976	def _expand_libs_in_apps ( specs ) : for app_name , app_spec in specs [ 'apps' ] . iteritems ( ) : if 'depends' in app_spec and 'libs' in app_spec [ 'depends' ] : app_spec [ 'depends' ] [ 'libs' ] = _get_dependent ( 'libs' , app_name , specs , 'apps' )
13329	def remove ( path ) : r = cpenv . resolve ( path ) if isinstance ( r . resolved [ 0 ] , cpenv . VirtualEnvironment ) : EnvironmentCache . discard ( r . resolved [ 0 ] ) EnvironmentCache . save ( )
7706	def load_roster ( self , source ) : try : tree = ElementTree . parse ( source ) except ElementTree . ParseError , err : raise ValueError ( "Invalid roster format: {0}" . format ( err ) ) roster = Roster . from_xml ( tree . getroot ( ) ) for item in roster : item . verify_roster_result ( True ) self . roster = roster
4364	def encode ( data , json_dumps = default_json_dumps ) : payload = '' msg = str ( MSG_TYPES [ data [ 'type' ] ] ) if msg in [ '0' , '1' ] : msg += '::' + data [ 'endpoint' ] if 'qs' in data and data [ 'qs' ] != '' : msg += ':' + data [ 'qs' ] elif msg == '2' : msg += '::' elif msg in [ '3' , '4' , '5' ] : if msg == '3' : payload = data [ 'data' ] if msg == '4' : payload = json_dumps ( data [ 'data' ] ) if msg == '5' : d = { } d [ 'name' ] = data [ 'name' ] if 'args' in data and data [ 'args' ] != [ ] : d [ 'args' ] = data [ 'args' ] payload = json_dumps ( d ) if 'id' in data : msg += ':' + str ( data [ 'id' ] ) if data [ 'ack' ] == 'data' : msg += '+' msg += ':' else : msg += '::' if 'endpoint' not in data : data [ 'endpoint' ] = '' if payload != '' : msg += data [ 'endpoint' ] + ':' + payload else : msg += data [ 'endpoint' ] elif msg == '6' : msg += '::' + data . get ( 'endpoint' , '' ) + ':' + str ( data [ 'ackId' ] ) if 'args' in data and data [ 'args' ] != [ ] : msg += '+' + json_dumps ( data [ 'args' ] ) elif msg == '7' : msg += ':::' if 'reason' in data and data [ 'reason' ] != '' : msg += str ( ERROR_REASONS [ data [ 'reason' ] ] ) if 'advice' in data and data [ 'advice' ] != '' : msg += '+' + str ( ERROR_ADVICES [ data [ 'advice' ] ] ) msg += data [ 'endpoint' ] elif msg == '8' : msg += '::' return msg
2622	def spin_up_instance ( self , command , job_name ) : command = Template ( template_string ) . substitute ( jobname = job_name , user_script = command , linger = str ( self . linger ) . lower ( ) , worker_init = self . worker_init ) instance_type = self . instance_type subnet = self . sn_ids [ 0 ] ami_id = self . image_id total_instances = len ( self . instances ) if float ( self . spot_max_bid ) > 0 : spot_options = { 'MarketType' : 'spot' , 'SpotOptions' : { 'MaxPrice' : str ( self . spot_max_bid ) , 'SpotInstanceType' : 'one-time' , 'InstanceInterruptionBehavior' : 'terminate' } } else : spot_options = { } if total_instances > self . max_nodes : logger . warn ( "Exceeded instance limit ({}). Cannot continue\n" . format ( self . max_nodes ) ) return [ None ] try : tag_spec = [ { "ResourceType" : "instance" , "Tags" : [ { 'Key' : 'Name' , 'Value' : job_name } ] } ] instance = self . ec2 . create_instances ( MinCount = 1 , MaxCount = 1 , InstanceType = instance_type , ImageId = ami_id , KeyName = self . key_name , SubnetId = subnet , SecurityGroupIds = [ self . sg_id ] , TagSpecifications = tag_spec , InstanceMarketOptions = spot_options , InstanceInitiatedShutdownBehavior = 'terminate' , IamInstanceProfile = { 'Arn' : self . iam_instance_profile_arn } , UserData = command ) except ClientError as e : print ( e ) logger . error ( e . response ) return [ None ] except Exception as e : logger . error ( "Request for EC2 resources failed : {0}" . format ( e ) ) return [ None ] self . instances . append ( instance [ 0 ] . id ) logger . info ( "Started up 1 instance {} . Instance type:{}" . format ( instance [ 0 ] . id , instance_type ) ) return instance
13260	def main ( argv = None , white_list = None , load_yaz_extension = True ) : assert argv is None or isinstance ( argv , list ) , type ( argv ) assert white_list is None or isinstance ( white_list , list ) , type ( white_list ) assert isinstance ( load_yaz_extension , bool ) , type ( load_yaz_extension ) argv = sys . argv if argv is None else argv assert len ( argv ) > 0 , len ( argv ) if load_yaz_extension : load ( "~/.yaz" , "yaz_extension" ) parser = Parser ( prog = argv [ 0 ] ) parser . add_task_tree ( get_task_tree ( white_list ) ) task , kwargs = parser . parse_arguments ( argv ) if task : try : result = task ( ** kwargs ) if isinstance ( result , bool ) : code = 0 if result else 1 output = None elif isinstance ( result , int ) : code = result % 256 output = None else : code = 0 output = result except Error as error : code = error . return_code output = error else : code = 1 output = parser . format_help ( ) . rstrip ( ) if output is not None : print ( output ) sys . exit ( code )
11138	def __clean_before_after ( self , stateBefore , stateAfter , keepNoneEmptyDirectory = True ) : errors = [ ] afterDict = { } [ afterDict . setdefault ( list ( aitem ) [ 0 ] , [ ] ) . append ( aitem ) for aitem in stateAfter ] for bitem in reversed ( stateBefore ) : relaPath = list ( bitem ) [ 0 ] basename = os . path . basename ( relaPath ) btype = bitem [ relaPath ] [ 'type' ] alist = afterDict . get ( relaPath , [ ] ) aitem = [ a for a in alist if a [ relaPath ] [ 'type' ] == btype ] if len ( aitem ) > 1 : errors . append ( "Multiple '%s' of type '%s' where found in '%s', this should never had happened. Please report issue" % ( basename , btype , relaPath ) ) continue if not len ( aitem ) : removeDirs = [ ] removeFiles = [ ] if btype == 'dir' : if not len ( relaPath ) : errors . append ( "Removing main repository directory is not allowed" ) continue removeDirs . append ( os . path . join ( self . __path , relaPath ) ) removeFiles . append ( os . path . join ( self . __path , relaPath , self . __dirInfo ) ) removeFiles . append ( os . path . join ( self . __path , relaPath , self . __dirLock ) ) elif btype == 'file' : removeFiles . append ( os . path . join ( self . __path , relaPath ) ) removeFiles . append ( os . path . join ( self . __path , relaPath , self . __fileInfo % basename ) ) removeFiles . append ( os . path . join ( self . __path , relaPath , self . __fileLock % basename ) ) else : removeDirs . append ( os . path . join ( self . __path , relaPath ) ) removeFiles . append ( os . path . join ( self . __path , relaPath , self . __fileInfo % basename ) ) for fpath in removeFiles : if os . path . isfile ( fpath ) : try : os . remove ( fpath ) except Exception as err : errors . append ( "Unable to clean file '%s' (%s)" % ( fpath , str ( err ) ) ) for dpath in removeDirs : if os . path . isdir ( dpath ) : if keepNoneEmptyDirectory or not len ( os . listdir ( dpath ) ) : try : shutil . rmtree ( dpath ) except Exception as err : errors . append ( "Unable to clean directory '%s' (%s)" % ( fpath , str ( err ) ) ) return len ( errors ) == 0 , errors
7543	def chunk_clusters ( data , sample ) : num = 0 optim = int ( ( sample . stats . clusters_total // data . cpus ) + ( sample . stats . clusters_total % data . cpus ) ) chunkslist = [ ] with gzip . open ( sample . files . clusters , 'rb' ) as clusters : pairdealer = itertools . izip ( * [ iter ( clusters ) ] * 2 ) done = 0 while not done : done , chunk = clustdealer ( pairdealer , optim ) chunkhandle = os . path . join ( data . dirs . clusts , "tmp_" + str ( sample . name ) + "." + str ( num * optim ) ) if chunk : chunkslist . append ( ( optim , chunkhandle ) ) with open ( chunkhandle , 'wb' ) as outchunk : outchunk . write ( "//\n//\n" . join ( chunk ) + "//\n//\n" ) num += 1 return chunkslist
2201	def ensure_app_config_dir ( appname , * args ) : from ubelt import util_path dpath = get_app_config_dir ( appname , * args ) util_path . ensuredir ( dpath ) return dpath
4821	def redirect_if_blocked ( course_run_ids , user = None , ip_address = None , url = None ) : for course_run_id in course_run_ids : redirect_url = embargo_api . redirect_if_blocked ( CourseKey . from_string ( course_run_id ) , user = user , ip_address = ip_address , url = url ) if redirect_url : return redirect_url
6260	def calc_global_bbox ( self , view_matrix , bbox_min , bbox_max ) : if self . matrix is not None : view_matrix = matrix44 . multiply ( self . matrix , view_matrix ) if self . mesh : bbox_min , bbox_max = self . mesh . calc_global_bbox ( view_matrix , bbox_min , bbox_max ) for child in self . children : bbox_min , bbox_max = child . calc_global_bbox ( view_matrix , bbox_min , bbox_max ) return bbox_min , bbox_max
11976	def _sub ( self , other ) : if isinstance ( other , self . __class__ ) : sub = self . _ip_dec - other . _ip_dec if isinstance ( other , int ) : sub = self . _ip_dec - other else : other = self . __class__ ( other ) sub = self . _ip_dec - other . _ip_dec return sub
10540	def delete_category ( category_id ) : try : res = _pybossa_req ( 'delete' , 'category' , category_id ) if type ( res ) . __name__ == 'bool' : return True else : return res except : raise
8006	def handle_read ( self ) : with self . _lock : logger . debug ( "handle_read()" ) if self . _socket is None : return while True : try : sock , address = self . _socket . accept ( ) except socket . error , err : if err . args [ 0 ] in BLOCKING_ERRORS : break else : raise logger . debug ( "Accepted connection from: {0!r}" . format ( address ) ) self . _target ( sock , address )
1183	def push_new_context ( self , pattern_offset ) : child_context = _MatchContext ( self . state , self . pattern_codes [ self . code_position + pattern_offset : ] ) self . state . context_stack . append ( child_context ) return child_context
862	def isTemporal ( inferenceType ) : if InferenceType . __temporalInferenceTypes is None : InferenceType . __temporalInferenceTypes = set ( [ InferenceType . TemporalNextStep , InferenceType . TemporalClassification , InferenceType . TemporalAnomaly , InferenceType . TemporalMultiStep , InferenceType . NontemporalMultiStep ] ) return inferenceType in InferenceType . __temporalInferenceTypes
4243	def _get_region ( self , ipnum ) : region_code = None country_code = None seek_country = self . _seek_country ( ipnum ) def get_region_code ( offset ) : region1 = chr ( offset // 26 + 65 ) region2 = chr ( offset % 26 + 65 ) return '' . join ( [ region1 , region2 ] ) if self . _databaseType == const . REGION_EDITION_REV0 : seek_region = seek_country - const . STATE_BEGIN_REV0 if seek_region >= 1000 : country_code = 'US' region_code = get_region_code ( seek_region - 1000 ) else : country_code = const . COUNTRY_CODES [ seek_region ] elif self . _databaseType == const . REGION_EDITION_REV1 : seek_region = seek_country - const . STATE_BEGIN_REV1 if seek_region < const . US_OFFSET : pass elif seek_region < const . CANADA_OFFSET : country_code = 'US' region_code = get_region_code ( seek_region - const . US_OFFSET ) elif seek_region < const . WORLD_OFFSET : country_code = 'CA' region_code = get_region_code ( seek_region - const . CANADA_OFFSET ) else : index = ( seek_region - const . WORLD_OFFSET ) // const . FIPS_RANGE if index < len ( const . COUNTRY_CODES ) : country_code = const . COUNTRY_CODES [ index ] elif self . _databaseType in const . CITY_EDITIONS : rec = self . _get_record ( ipnum ) region_code = rec . get ( 'region_code' ) country_code = rec . get ( 'country_code' ) return { 'country_code' : country_code , 'region_code' : region_code }
7819	def dispatch ( self , block = False , timeout = None ) : logger . debug ( " dispatching..." ) try : event = self . queue . get ( block , timeout ) except Queue . Empty : logger . debug ( " queue empty" ) return None try : logger . debug ( " event: {0!r}" . format ( event ) ) if event is QUIT : return QUIT handlers = list ( self . _handler_map [ None ] ) klass = event . __class__ if klass in self . _handler_map : handlers += self . _handler_map [ klass ] logger . debug ( " handlers: {0!r}" . format ( handlers ) ) handlers . sort ( key = lambda x : x [ 0 ] ) for dummy , handler in handlers : logger . debug ( u" passing the event to: {0!r}" . format ( handler ) ) result = handler ( event ) if isinstance ( result , Event ) : self . queue . put ( result ) elif result and event is not QUIT : return event return event finally : self . queue . task_done ( )
2581	def _load_checkpoints ( self , checkpointDirs ) : memo_lookup_table = { } for checkpoint_dir in checkpointDirs : logger . info ( "Loading checkpoints from {}" . format ( checkpoint_dir ) ) checkpoint_file = os . path . join ( checkpoint_dir , 'tasks.pkl' ) try : with open ( checkpoint_file , 'rb' ) as f : while True : try : data = pickle . load ( f ) memo_fu = Future ( ) if data [ 'exception' ] : memo_fu . set_exception ( data [ 'exception' ] ) else : memo_fu . set_result ( data [ 'result' ] ) memo_lookup_table [ data [ 'hash' ] ] = memo_fu except EOFError : break except FileNotFoundError : reason = "Checkpoint file was not found: {}" . format ( checkpoint_file ) logger . error ( reason ) raise BadCheckpoint ( reason ) except Exception : reason = "Failed to load checkpoint: {}" . format ( checkpoint_file ) logger . error ( reason ) raise BadCheckpoint ( reason ) logger . info ( "Completed loading checkpoint:{0} with {1} tasks" . format ( checkpoint_file , len ( memo_lookup_table . keys ( ) ) ) ) return memo_lookup_table
824	def mostLikely ( self , pred ) : if len ( pred ) == 1 : return pred . keys ( ) [ 0 ] mostLikelyOutcome = None maxProbability = 0 for prediction , probability in pred . items ( ) : if probability > maxProbability : mostLikelyOutcome = prediction maxProbability = probability return mostLikelyOutcome
5872	def serialize_organization ( organization ) : return { 'id' : organization . id , 'name' : organization . name , 'short_name' : organization . short_name , 'description' : organization . description , 'logo' : organization . logo }
5864	def remove_organization_course ( organization , course_key ) : _validate_organization_data ( organization ) _validate_course_key ( course_key ) return data . delete_organization_course ( course_key = course_key , organization = organization )
11536	def map_pin ( self , abstract_pin_id , physical_pin_id ) : if physical_pin_id : self . _pin_mapping [ abstract_pin_id ] = physical_pin_id else : self . _pin_mapping . pop ( abstract_pin_id , None )
8091	def graph_background ( s ) : if s . background == None : s . _ctx . background ( None ) else : s . _ctx . background ( s . background ) if s . depth : try : clr = colors . color ( s . background ) . darker ( 0.2 ) p = s . _ctx . rect ( 0 , 0 , s . _ctx . WIDTH , s . _ctx . HEIGHT , draw = False ) colors . gradientfill ( p , clr , clr . lighter ( 0.35 ) ) colors . shadow ( dx = 0 , dy = 0 , blur = 2 , alpha = 0.935 , clr = s . background ) except : pass
4649	def json ( self ) : if not self . _is_constructed ( ) or self . _is_require_reconstruction ( ) : self . constructTx ( ) return dict ( self )
9475	def add_node ( self , label ) : try : n = self . _nodes [ label ] except KeyError : n = Node ( ) n [ 'label' ] = label self . _nodes [ label ] = n return n
12618	def get_shape ( img ) : if hasattr ( img , 'shape' ) : shape = img . shape else : shape = img . get_data ( ) . shape return shape
6303	def get_package ( self , name ) -> 'EffectPackage' : name , cls_name = parse_package_string ( name ) try : return self . package_map [ name ] except KeyError : raise EffectError ( "No package '{}' registered" . format ( name ) )
7113	def predict ( self , X ) : x = X if not isinstance ( X , list ) : x = [ X ] y = self . estimator . predict ( x ) y = [ item [ 0 ] for item in y ] y = [ self . _remove_prefix ( label ) for label in y ] if not isinstance ( X , list ) : y = y [ 0 ] return y
8628	def create_project ( session , title , description , currency , budget , jobs ) : project_data = { 'title' : title , 'description' : description , 'currency' : currency , 'budget' : budget , 'jobs' : jobs } response = make_post_request ( session , 'projects' , json_data = project_data ) json_data = response . json ( ) if response . status_code == 200 : project_data = json_data [ 'result' ] p = Project ( project_data ) p . url = urljoin ( session . url , 'projects/%s' % p . seo_url ) return p else : raise ProjectNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] , )
9321	def _validate_api_root ( self ) : if not self . _title : msg = "No 'title' in API Root for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if not self . _versions : msg = "No 'versions' in API Root for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . _max_content_length is None : msg = "No 'max_content_length' in API Root for request '{}'" raise ValidationError ( msg . format ( self . url ) )
2584	def get_tasks ( self , count ) : tasks = [ ] for i in range ( 0 , count ) : try : x = self . pending_task_queue . get ( block = False ) except queue . Empty : break else : tasks . append ( x ) return tasks
8420	def same_log10_order_of_magnitude ( x , delta = 0.1 ) : dmin = np . log10 ( np . min ( x ) * ( 1 - delta ) ) dmax = np . log10 ( np . max ( x ) * ( 1 + delta ) ) return np . floor ( dmin ) == np . floor ( dmax )
11659	def inverse_transform ( self , X ) : X = check_array ( X , copy = self . copy ) X -= self . min_ X /= self . scale_ return X
11764	def compute_utility ( self , board , move , player ) : "If X wins with this move, return 1; if O return -1; else return 0." if ( self . k_in_row ( board , move , player , ( 0 , 1 ) ) or self . k_in_row ( board , move , player , ( 1 , 0 ) ) or self . k_in_row ( board , move , player , ( 1 , - 1 ) ) or self . k_in_row ( board , move , player , ( 1 , 1 ) ) ) : return if_ ( player == 'X' , + 1 , - 1 ) else : return 0
6502	def strings_in_dictionary ( dictionary ) : strings = [ value for value in six . itervalues ( dictionary ) if not isinstance ( value , dict ) ] for child_dict in [ dv for dv in six . itervalues ( dictionary ) if isinstance ( dv , dict ) ] : strings . extend ( SearchResultProcessor . strings_in_dictionary ( child_dict ) ) return strings
9457	def sound_touch ( self , call_params ) : path = '/' + self . api_version + '/SoundTouch/' method = 'POST' return self . request ( path , method , call_params )
6979	def filter_kepler_lcdict ( lcdict , filterflags = True , nanfilter = 'sap,pdc' , timestoignore = None ) : cols = lcdict [ 'columns' ] if filterflags : nbefore = lcdict [ 'time' ] . size filterind = lcdict [ 'sap_quality' ] == 0 for col in cols : if '.' in col : key , subkey = col . split ( '.' ) lcdict [ key ] [ subkey ] = lcdict [ key ] [ subkey ] [ filterind ] else : lcdict [ col ] = lcdict [ col ] [ filterind ] nafter = lcdict [ 'time' ] . size LOGINFO ( 'applied quality flag filter, ndet before = %s, ndet after = %s' % ( nbefore , nafter ) ) if nanfilter and nanfilter == 'sap,pdc' : notnanind = ( npisfinite ( lcdict [ 'sap' ] [ 'sap_flux' ] ) & npisfinite ( lcdict [ 'pdc' ] [ 'pdcsap_flux' ] ) & npisfinite ( lcdict [ 'time' ] ) ) elif nanfilter and nanfilter == 'sap' : notnanind = ( npisfinite ( lcdict [ 'sap' ] [ 'sap_flux' ] ) & npisfinite ( lcdict [ 'time' ] ) ) elif nanfilter and nanfilter == 'pdc' : notnanind = ( npisfinite ( lcdict [ 'pdc' ] [ 'pdcsap_flux' ] ) & npisfinite ( lcdict [ 'time' ] ) ) if nanfilter : nbefore = lcdict [ 'time' ] . size for col in cols : if '.' in col : key , subkey = col . split ( '.' ) lcdict [ key ] [ subkey ] = lcdict [ key ] [ subkey ] [ notnanind ] else : lcdict [ col ] = lcdict [ col ] [ notnanind ] nafter = lcdict [ 'time' ] . size LOGINFO ( 'removed nans, ndet before = %s, ndet after = %s' % ( nbefore , nafter ) ) if ( timestoignore and isinstance ( timestoignore , list ) and len ( timestoignore ) > 0 ) : exclind = npfull_like ( lcdict [ 'time' ] , True , dtype = np . bool_ ) nbefore = exclind . size for ignoretime in timestoignore : time0 , time1 = ignoretime [ 0 ] , ignoretime [ 1 ] thismask = ~ ( ( lcdict [ 'time' ] >= time0 ) & ( lcdict [ 'time' ] <= time1 ) ) exclind = exclind & thismask for col in cols : if '.' in col : key , subkey = col . split ( '.' ) lcdict [ key ] [ subkey ] = lcdict [ key ] [ subkey ] [ exclind ] else : lcdict [ col ] = lcdict [ col ] [ exclind ] nafter = lcdict [ 'time' ] . size LOGINFO ( 'removed timestoignore, ndet before = %s, ndet after = %s' % ( nbefore , nafter ) ) return lcdict
9522	def merge_to_one_seq ( infile , outfile , seqname = 'union' ) : seq_reader = sequences . file_reader ( infile ) seqs = [ ] for seq in seq_reader : seqs . append ( copy . copy ( seq ) ) new_seq = '' . join ( [ seq . seq for seq in seqs ] ) if type ( seqs [ 0 ] ) == sequences . Fastq : new_qual = '' . join ( [ seq . qual for seq in seqs ] ) seqs [ : ] = [ ] merged = sequences . Fastq ( seqname , new_seq , new_qual ) else : merged = sequences . Fasta ( seqname , new_seq ) seqs [ : ] = [ ] f = utils . open_file_write ( outfile ) print ( merged , file = f ) utils . close ( f )
5780	def _obtain_credentials ( self ) : protocol_values = { 'SSLv3' : Secur32Const . SP_PROT_SSL3_CLIENT , 'TLSv1' : Secur32Const . SP_PROT_TLS1_CLIENT , 'TLSv1.1' : Secur32Const . SP_PROT_TLS1_1_CLIENT , 'TLSv1.2' : Secur32Const . SP_PROT_TLS1_2_CLIENT , } protocol_bit_mask = 0 for key , value in protocol_values . items ( ) : if key in self . _protocols : protocol_bit_mask |= value algs = [ Secur32Const . CALG_AES_128 , Secur32Const . CALG_AES_256 , Secur32Const . CALG_3DES , Secur32Const . CALG_SHA1 , Secur32Const . CALG_ECDHE , Secur32Const . CALG_DH_EPHEM , Secur32Const . CALG_RSA_KEYX , Secur32Const . CALG_RSA_SIGN , Secur32Const . CALG_ECDSA , Secur32Const . CALG_DSS_SIGN , ] if 'TLSv1.2' in self . _protocols : algs . extend ( [ Secur32Const . CALG_SHA512 , Secur32Const . CALG_SHA384 , Secur32Const . CALG_SHA256 , ] ) alg_array = new ( secur32 , 'ALG_ID[%s]' % len ( algs ) ) for index , alg in enumerate ( algs ) : alg_array [ index ] = alg flags = Secur32Const . SCH_USE_STRONG_CRYPTO | Secur32Const . SCH_CRED_NO_DEFAULT_CREDS if not self . _manual_validation and not self . _extra_trust_roots : flags |= Secur32Const . SCH_CRED_AUTO_CRED_VALIDATION else : flags |= Secur32Const . SCH_CRED_MANUAL_CRED_VALIDATION schannel_cred_pointer = struct ( secur32 , 'SCHANNEL_CRED' ) schannel_cred = unwrap ( schannel_cred_pointer ) schannel_cred . dwVersion = Secur32Const . SCHANNEL_CRED_VERSION schannel_cred . cCreds = 0 schannel_cred . paCred = null ( ) schannel_cred . hRootStore = null ( ) schannel_cred . cMappers = 0 schannel_cred . aphMappers = null ( ) schannel_cred . cSupportedAlgs = len ( alg_array ) schannel_cred . palgSupportedAlgs = alg_array schannel_cred . grbitEnabledProtocols = protocol_bit_mask schannel_cred . dwMinimumCipherStrength = 0 schannel_cred . dwMaximumCipherStrength = 0 schannel_cred . dwSessionLifespan = 0 schannel_cred . dwFlags = flags schannel_cred . dwCredFormat = 0 cred_handle_pointer = new ( secur32 , 'CredHandle *' ) result = secur32 . AcquireCredentialsHandleW ( null ( ) , Secur32Const . UNISP_NAME , Secur32Const . SECPKG_CRED_OUTBOUND , null ( ) , schannel_cred_pointer , null ( ) , null ( ) , cred_handle_pointer , null ( ) ) handle_error ( result ) self . _credentials_handle = cred_handle_pointer
11399	def update_keywords ( self ) : for field in record_get_field_instances ( self . record , '653' , ind1 = '1' ) : subs = field_get_subfields ( field ) new_subs = [ ] if 'a' in subs : for val in subs [ 'a' ] : new_subs . extend ( [ ( '9' , 'author' ) , ( 'a' , val ) ] ) new_field = create_field ( subfields = new_subs , ind1 = '1' ) record_replace_field ( self . record , '653' , new_field , field_position_global = field [ 4 ] )
11620	def to_utf8 ( y ) : out = [ ] for x in y : if x < 0x080 : out . append ( x ) elif x < 0x0800 : out . append ( ( x >> 6 ) | 0xC0 ) out . append ( ( x & 0x3F ) | 0x80 ) elif x < 0x10000 : out . append ( ( x >> 12 ) | 0xE0 ) out . append ( ( ( x >> 6 ) & 0x3F ) | 0x80 ) out . append ( ( x & 0x3F ) | 0x80 ) else : out . append ( ( x >> 18 ) | 0xF0 ) out . append ( ( x >> 12 ) & 0x3F ) out . append ( ( ( x >> 6 ) & 0x3F ) | 0x80 ) out . append ( ( x & 0x3F ) | 0x80 ) return '' . join ( map ( chr , out ) )
4208	def lpc ( x , N = None ) : m = len ( x ) if N is None : N = m - 1 elif N > m - 1 : x . resize ( N + 1 ) X = fft ( x , 2 ** nextpow2 ( 2. * len ( x ) - 1 ) ) R = real ( ifft ( abs ( X ) ** 2 ) ) R = R / ( m - 1. ) a , e , ref = LEVINSON ( R , N ) return a , e
3136	def get ( self , ** queryparams ) : return self . _mc_client . _get ( url = self . _build_path ( ) , ** queryparams )
1370	def get_subparser ( parser , command ) : subparsers_actions = [ action for action in parser . _actions if isinstance ( action , argparse . _SubParsersAction ) ] for subparsers_action in subparsers_actions : for choice , subparser in subparsers_action . choices . items ( ) : if choice == command : return subparser return None
12493	def check_array ( array , accept_sparse = None , dtype = None , order = None , copy = False , force_all_finite = True , ensure_2d = True , allow_nd = False ) : if isinstance ( accept_sparse , str ) : accept_sparse = [ accept_sparse ] if sp . issparse ( array ) : array = _ensure_sparse_format ( array , accept_sparse , dtype , order , copy , force_all_finite ) else : if ensure_2d : array = np . atleast_2d ( array ) array = np . array ( array , dtype = dtype , order = order , copy = copy ) if not allow_nd and array . ndim >= 3 : raise ValueError ( "Found array with dim %d. Expected <= 2" % array . ndim ) if force_all_finite : _assert_all_finite ( array ) return array
1839	def JNE ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , False == cpu . ZF , target . read ( ) , cpu . PC )
12175	def plotAllSweeps ( abfFile ) : r = io . AxonIO ( filename = abfFile ) bl = r . read_block ( lazy = False , cascade = True ) print ( abfFile + "\nplotting %d sweeps..." % len ( bl . segments ) ) plt . figure ( figsize = ( 12 , 10 ) ) plt . title ( abfFile ) for sweep in range ( len ( bl . segments ) ) : trace = bl . segments [ sweep ] . analogsignals [ 0 ] plt . plot ( trace . times - trace . times [ 0 ] , trace . magnitude , alpha = .5 ) plt . ylabel ( trace . dimensionality ) plt . xlabel ( "seconds" ) plt . show ( ) plt . close ( )
5481	def retry_api_check ( exception ) : if isinstance ( exception , apiclient . errors . HttpError ) : if exception . resp . status in TRANSIENT_HTTP_ERROR_CODES : _print_error ( 'Retrying...' ) return True if isinstance ( exception , socket . error ) : if exception . errno in TRANSIENT_SOCKET_ERROR_CODES : _print_error ( 'Retrying...' ) return True if isinstance ( exception , oauth2client . client . AccessTokenRefreshError ) : _print_error ( 'Retrying...' ) return True if isinstance ( exception , SSLError ) : _print_error ( 'Retrying...' ) return True if isinstance ( exception , ServerNotFoundError ) : _print_error ( 'Retrying...' ) return True return False
2715	def create ( self , ** kwargs ) : for attr in kwargs . keys ( ) : setattr ( self , attr , kwargs [ attr ] ) params = { "name" : self . name } output = self . get_data ( "tags" , type = "POST" , params = params ) if output : self . name = output [ 'tag' ] [ 'name' ] self . resources = output [ 'tag' ] [ 'resources' ]
7901	def configure_room ( self , form ) : if form . type == "cancel" : return None elif form . type != "submit" : raise ValueError ( "A 'submit' form required to configure a room" ) iq = Iq ( to_jid = self . room_jid . bare ( ) , stanza_type = "set" ) query = iq . new_query ( MUC_OWNER_NS , "query" ) form . as_xml ( query ) self . manager . stream . set_response_handlers ( iq , self . process_configuration_success , self . process_configuration_error ) self . manager . stream . send ( iq ) return iq . get_id ( )
2430	def set_spdx_doc_uri ( self , doc , spdx_doc_uri ) : if validations . validate_doc_namespace ( spdx_doc_uri ) : doc . ext_document_references [ - 1 ] . spdx_document_uri = spdx_doc_uri else : raise SPDXValueError ( 'Document::ExternalDocumentRef' )
4907	def _sync_content_metadata ( self , serialized_data , http_method ) : try : status_code , response_body = getattr ( self , '_' + http_method ) ( urljoin ( self . enterprise_configuration . degreed_base_url , self . global_degreed_config . course_api_path ) , serialized_data , self . CONTENT_PROVIDER_SCOPE ) except requests . exceptions . RequestException as exc : raise ClientError ( 'DegreedAPIClient request failed: {error} {message}' . format ( error = exc . __class__ . __name__ , message = str ( exc ) ) ) if status_code >= 400 : raise ClientError ( 'DegreedAPIClient request failed with status {status_code}: {message}' . format ( status_code = status_code , message = response_body ) )
3522	def _hashable_bytes ( data ) : if isinstance ( data , bytes ) : return data elif isinstance ( data , str ) : return data . encode ( 'ascii' ) else : raise TypeError ( data )
3393	def undelete_model_genes ( cobra_model ) : if cobra_model . _trimmed_genes is not None : for x in cobra_model . _trimmed_genes : x . functional = True if cobra_model . _trimmed_reactions is not None : for the_reaction , ( lower_bound , upper_bound ) in cobra_model . _trimmed_reactions . items ( ) : the_reaction . lower_bound = lower_bound the_reaction . upper_bound = upper_bound cobra_model . _trimmed_genes = [ ] cobra_model . _trimmed_reactions = { } cobra_model . _trimmed = False
6711	def disk ( self ) : r = self . local_renderer r . run ( r . env . disk_usage_command )
11179	def authorize_url ( self ) : auth_url = OAUTH_ROOT + '/authorize' params = { 'client_id' : self . client_id , 'redirect_uri' : self . redirect_uri , } return "{}?{}" . format ( auth_url , urlencode ( params ) )
4405	def parse_line ( line , document = None ) : result = re . match ( line_pattern , line ) if result : _ , lineno , offset , severity , msg = result . groups ( ) lineno = int ( lineno or 1 ) offset = int ( offset or 0 ) errno = 2 if severity == 'error' : errno = 1 diag = { 'source' : 'mypy' , 'range' : { 'start' : { 'line' : lineno - 1 , 'character' : offset } , 'end' : { 'line' : lineno - 1 , 'character' : offset + 1 } } , 'message' : msg , 'severity' : errno } if document : word = document . word_at_position ( diag [ 'range' ] [ 'start' ] ) if word : diag [ 'range' ] [ 'end' ] [ 'character' ] = ( diag [ 'range' ] [ 'start' ] [ 'character' ] + len ( word ) ) return diag
8843	def unindent ( self ) : if self . tab_always_indent : cursor = self . editor . textCursor ( ) if not cursor . hasSelection ( ) : cursor . select ( cursor . LineUnderCursor ) self . unindent_selection ( cursor ) else : super ( PyIndenterMode , self ) . unindent ( )
9119	def dropbox_form ( request ) : from briefkasten import generate_post_token token = generate_post_token ( secret = request . registry . settings [ 'post_secret' ] ) return dict ( action = request . route_url ( 'dropbox_form_submit' , token = token ) , fileupload_url = request . route_url ( 'dropbox_fileupload' , token = token ) , ** defaults ( request ) )
1507	def print_cluster_info ( cl_args ) : parsed_roles = read_and_parse_roles ( cl_args ) masters = list ( parsed_roles [ Role . MASTERS ] ) slaves = list ( parsed_roles [ Role . SLAVES ] ) zookeepers = list ( parsed_roles [ Role . ZOOKEEPERS ] ) cluster = list ( parsed_roles [ Role . CLUSTER ] ) info = OrderedDict ( ) info [ 'numNodes' ] = len ( cluster ) info [ 'nodes' ] = cluster roles = OrderedDict ( ) roles [ 'masters' ] = masters roles [ 'slaves' ] = slaves roles [ 'zookeepers' ] = zookeepers urls = OrderedDict ( ) urls [ 'serviceUrl' ] = get_service_url ( cl_args ) urls [ 'heronUi' ] = get_heron_ui_url ( cl_args ) urls [ 'heronTracker' ] = get_heron_tracker_url ( cl_args ) info [ 'roles' ] = roles info [ 'urls' ] = urls print json . dumps ( info , indent = 2 )
13452	def imgmin ( self ) : if not hasattr ( self , '_imgmin' ) : imgmin = _np . min ( self . images [ 0 ] ) for img in self . images : imin = _np . min ( img ) if imin > imgmin : imgmin = imin self . _imgmin = imgmin return _np . min ( self . image )
3233	def list_rules ( client = None , ** kwargs ) : result = client . list_rules ( ** kwargs ) if not result . get ( "Rules" ) : result . update ( { "Rules" : [ ] } ) return result
8703	def download_file ( self , filename ) : res = self . __exchange ( 'send("{filename}")' . format ( filename = filename ) ) if ( 'unexpected' in res ) or ( 'stdin' in res ) : log . error ( 'Unexpected error downloading file: %s' , res ) raise Exception ( 'Unexpected error downloading file' ) self . __write ( 'C' ) sent_filename = self . __expect ( NUL ) . strip ( ) log . info ( 'receiveing ' + sent_filename ) self . __write ( ACK , True ) buf = '' data = '' chunk , buf = self . __read_chunk ( buf ) while chunk != '' : self . __write ( ACK , True ) data = data + chunk chunk , buf = self . __read_chunk ( buf ) return data
9678	def read_info_string ( self ) : infostring = [ ] self . cnxn . xfer ( [ 0x3F ] ) sleep ( 9e-3 ) for i in range ( 60 ) : resp = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] infostring . append ( chr ( resp ) ) sleep ( 0.1 ) return '' . join ( infostring )
12484	def get_subdict ( adict , path , sep = os . sep ) : return reduce ( adict . __class__ . get , [ p for p in op . split ( sep ) if p ] , adict )
1067	def getheaders ( self , name ) : result = [ ] current = '' have_header = 0 for s in self . getallmatchingheaders ( name ) : if s [ 0 ] . isspace ( ) : if current : current = "%s\n %s" % ( current , s . strip ( ) ) else : current = s . strip ( ) else : if have_header : result . append ( current ) current = s [ s . find ( ":" ) + 1 : ] . strip ( ) have_header = 1 if have_header : result . append ( current ) return result
8638	def revoke_project_bid ( session , bid_id ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } bid_data = { 'action' : 'revoke' } endpoint = 'bids/{}' . format ( bid_id ) response = make_put_request ( session , endpoint , headers = headers , params_data = bid_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : json_data = response . json ( ) raise BidNotRevokedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
701	def firstNonFullGeneration ( self , swarmId , minNumParticles ) : if not swarmId in self . _swarmNumParticlesPerGeneration : return None numPsPerGen = self . _swarmNumParticlesPerGeneration [ swarmId ] numPsPerGen = numpy . array ( numPsPerGen ) firstNonFull = numpy . where ( numPsPerGen < minNumParticles ) [ 0 ] if len ( firstNonFull ) == 0 : return len ( numPsPerGen ) else : return firstNonFull [ 0 ]
1201	def reset ( self ) : self . level . reset ( ) return self . level . observations ( ) [ self . state_attribute ]
13256	def as_dict ( self ) : entry_dict = { } entry_dict [ 'UUID' ] = self . uuid entry_dict [ 'Creation Date' ] = self . time entry_dict [ 'Time Zone' ] = self . tz if self . tags : entry_dict [ 'Tags' ] = self . tags entry_dict [ 'Entry Text' ] = self . text entry_dict [ 'Starred' ] = self . starred entry_dict [ 'Location' ] = self . location return entry_dict
4068	def update_items ( self , payload ) : to_send = [ self . check_items ( [ p ] ) [ 0 ] for p in payload ] headers = { } headers . update ( self . default_headers ( ) ) for chunk in chunks ( to_send , 50 ) : req = requests . post ( url = self . endpoint + "/{t}/{u}/items/" . format ( t = self . library_type , u = self . library_id ) , headers = headers , data = json . dumps ( chunk ) , ) self . request = req try : req . raise_for_status ( ) except requests . exceptions . HTTPError : error_handler ( req ) return True
7129	def main ( source , force , name , quiet , verbose , destination , add_to_dash , add_to_global , icon , index_page , enable_js , online_redirect_url , parser , ) : try : logging . config . dictConfig ( create_log_config ( verbose = verbose , quiet = quiet ) ) except ValueError as e : click . secho ( e . args [ 0 ] , fg = "red" ) raise SystemExit ( 1 ) if icon : icon_data = icon . read ( ) if not icon_data . startswith ( PNG_HEADER ) : log . error ( '"{}" is not a valid PNG image.' . format ( click . format_filename ( icon . name ) ) ) raise SystemExit ( 1 ) else : icon_data = None source , dest , name = setup_paths ( source , destination , name = name , add_to_global = add_to_global , force = force , ) if parser is None : parser = parsers . get_doctype ( source ) if parser is None : log . error ( '"{}" does not contain a known documentation format.' . format ( click . format_filename ( source ) ) ) raise SystemExit ( errno . EINVAL ) docset = prepare_docset ( source , dest , name , index_page , enable_js , online_redirect_url ) doc_parser = parser ( doc_path = docset . docs ) log . info ( ( "Converting " + click . style ( "{parser_name}" , bold = True ) + ' docs from "{src}" to "{dst}".' ) . format ( parser_name = parser . name , src = click . format_filename ( source , shorten = True ) , dst = click . format_filename ( dest ) , ) ) with docset . db_conn : log . info ( "Parsing documentation..." ) toc = patch_anchors ( doc_parser , show_progressbar = not quiet ) for entry in doc_parser . parse ( ) : docset . db_conn . execute ( "INSERT INTO searchIndex VALUES (NULL, ?, ?, ?)" , entry . as_tuple ( ) , ) toc . send ( entry ) count = docset . db_conn . execute ( "SELECT COUNT(1) FROM searchIndex" ) . fetchone ( ) [ 0 ] log . info ( ( "Added " + click . style ( "{count:,}" , fg = "green" if count > 0 else "red" ) + " index entries." ) . format ( count = count ) ) toc . close ( ) if icon_data : add_icon ( icon_data , dest ) if add_to_dash or add_to_global : log . info ( "Adding to Dash.app..." ) os . system ( 'open -a dash "{}"' . format ( dest ) )
1320	def GetTopLevelControl ( self ) -> 'Control' : handle = self . NativeWindowHandle if handle : topHandle = GetAncestor ( handle , GAFlag . Root ) if topHandle : if topHandle == handle : return self else : return ControlFromHandle ( topHandle ) else : pass else : control = self while True : control = control . GetParentControl ( ) handle = control . NativeWindowHandle if handle : topHandle = GetAncestor ( handle , GAFlag . Root ) return ControlFromHandle ( topHandle )
2052	def LDRD ( cpu , dest1 , dest2 , src , offset = None ) : assert dest1 . type == 'register' assert dest2 . type == 'register' assert src . type == 'memory' mem1 = cpu . read_int ( src . address ( ) , 32 ) mem2 = cpu . read_int ( src . address ( ) + 4 , 32 ) writeback = cpu . _compute_writeback ( src , offset ) dest1 . write ( mem1 ) dest2 . write ( mem2 ) cpu . _cs_hack_ldr_str_writeback ( src , offset , writeback )
8141	def translate ( self , x , y ) : self . x = x self . y = y
2587	def start ( self ) : start = time . time ( ) self . _kill_event = threading . Event ( ) self . procs = { } for worker_id in range ( self . worker_count ) : p = multiprocessing . Process ( target = worker , args = ( worker_id , self . uid , self . pending_task_queue , self . pending_result_queue , self . ready_worker_queue , ) ) p . start ( ) self . procs [ worker_id ] = p logger . debug ( "Manager synced with workers" ) self . _task_puller_thread = threading . Thread ( target = self . pull_tasks , args = ( self . _kill_event , ) ) self . _result_pusher_thread = threading . Thread ( target = self . push_results , args = ( self . _kill_event , ) ) self . _task_puller_thread . start ( ) self . _result_pusher_thread . start ( ) logger . info ( "Loop start" ) self . _kill_event . wait ( ) logger . critical ( "[MAIN] Received kill event, terminating worker processes" ) self . _task_puller_thread . join ( ) self . _result_pusher_thread . join ( ) for proc_id in self . procs : self . procs [ proc_id ] . terminate ( ) logger . critical ( "Terminating worker {}:{}" . format ( self . procs [ proc_id ] , self . procs [ proc_id ] . is_alive ( ) ) ) self . procs [ proc_id ] . join ( ) logger . debug ( "Worker:{} joined successfully" . format ( self . procs [ proc_id ] ) ) self . task_incoming . close ( ) self . result_outgoing . close ( ) self . context . term ( ) delta = time . time ( ) - start logger . info ( "process_worker_pool ran for {} seconds" . format ( delta ) ) return
5907	def edit_txt ( filename , substitutions , newname = None ) : if newname is None : newname = filename _substitutions = [ { 'lRE' : re . compile ( str ( lRE ) ) , 'sRE' : re . compile ( str ( sRE ) ) , 'repl' : repl } for lRE , sRE , repl in substitutions if repl is not None ] with tempfile . TemporaryFile ( ) as target : with open ( filename , 'rb' ) as src : logger . info ( "editing txt = {0!r} ({1:d} substitutions)" . format ( filename , len ( substitutions ) ) ) for line in src : line = line . decode ( "utf-8" ) keep_line = True for subst in _substitutions : m = subst [ 'lRE' ] . match ( line ) if m : logger . debug ( 'match: ' + line . rstrip ( ) ) if subst [ 'repl' ] is False : keep_line = False else : line = subst [ 'sRE' ] . sub ( str ( subst [ 'repl' ] ) , line ) logger . debug ( 'replaced: ' + line . rstrip ( ) ) if keep_line : target . write ( line . encode ( 'utf-8' ) ) else : logger . debug ( "Deleting line %r" , line ) target . seek ( 0 ) with open ( newname , 'wb' ) as final : shutil . copyfileobj ( target , final ) logger . info ( "edited txt = {newname!r}" . format ( ** vars ( ) ) )
3271	def resolution_millis ( self ) : if self . resolution is None or not isinstance ( self . resolution , basestring ) : return self . resolution val , mult = self . resolution . split ( ' ' ) return int ( float ( val ) * self . _multipier ( mult ) * 1000 )
4406	async def connect ( self ) : await self . _lavalink . bot . wait_until_ready ( ) if self . _ws and self . _ws . open : log . debug ( 'WebSocket still open, closing...' ) await self . _ws . close ( ) user_id = self . _lavalink . bot . user . id shard_count = self . _lavalink . bot . shard_count or self . _shards headers = { 'Authorization' : self . _password , 'Num-Shards' : shard_count , 'User-Id' : str ( user_id ) } log . debug ( 'Preparing to connect to Lavalink' ) log . debug ( ' with URI: {}' . format ( self . _uri ) ) log . debug ( ' with headers: {}' . format ( str ( headers ) ) ) log . info ( 'Connecting to Lavalink...' ) try : self . _ws = await websockets . connect ( self . _uri , loop = self . _loop , extra_headers = headers ) except OSError as error : log . exception ( 'Failed to connect to Lavalink: {}' . format ( str ( error ) ) ) else : log . info ( 'Connected to Lavalink!' ) self . _loop . create_task ( self . listen ( ) ) version = self . _ws . response_headers . get ( 'Lavalink-Major-Version' , 2 ) try : self . _lavalink . _server_version = int ( version ) except ValueError : self . _lavalink . _server_version = 2 log . info ( 'Lavalink server version is {}' . format ( version ) ) if self . _queue : log . info ( 'Replaying {} queued events...' . format ( len ( self . _queue ) ) ) for task in self . _queue : await self . send ( ** task )
8688	def delete ( self , key_name ) : self . db . remove ( Query ( ) . name == key_name ) return self . get ( key_name ) == { }
9125	def _store_helper ( model : Action , session : Optional [ Session ] = None ) -> None : if session is None : session = _make_session ( ) session . add ( model ) session . commit ( ) session . close ( )
4857	def deprecated ( extra ) : def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , ** kwargs ) : message = 'You called the deprecated function `{function}`. {extra}' . format ( function = func . __name__ , extra = extra ) frame = inspect . currentframe ( ) . f_back warnings . warn_explicit ( message , category = DeprecationWarning , filename = inspect . getfile ( frame . f_code ) , lineno = frame . f_lineno ) return func ( * args , ** kwargs ) return wrapper return decorator
5745	def fsplit ( file_to_split ) : dirname = file_to_split + '_splitted' if not os . path . exists ( dirname ) : os . mkdir ( dirname ) part_file_size = os . path . getsize ( file_to_split ) / number_of_files + 1 splitted_files = [ ] with open ( file_to_split , "r" ) as f : number = 0 actual = 0 while 1 : prec = actual f . seek ( part_file_size , os . SEEK_CUR ) s = f . readline ( ) if len ( s ) == 0 : s = f . readline ( ) while len ( s ) != 0 and s != separator : s = f . readline ( ) actual = f . tell ( ) new_file = os . path . join ( dirname , str ( number ) ) with open ( file_to_split , "r" ) as temp : temp . seek ( prec ) copy = temp . read ( actual - prec ) open ( new_file , 'w' ) . write ( copy ) splitted_files . append ( new_file ) number += 1 if len ( s ) == 0 : break return splitted_files
6132	def toJSON ( self ) : return { "id" : self . id , "compile" : self . compile , "position" : self . position , "version" : self . version }
878	def newPosition ( self , whichVars = None ) : globalBestPosition = None if self . _hsObj . _speculativeParticles : genIdx = self . genIdx else : genIdx = self . genIdx - 1 if genIdx >= 0 : ( bestModelId , _ ) = self . _resultsDB . bestModelIdAndErrScore ( self . swarmId , genIdx ) if bestModelId is not None : ( particleState , _ , _ , _ , _ ) = self . _resultsDB . getParticleInfo ( bestModelId ) globalBestPosition = Particle . getPositionFromState ( particleState ) for ( varName , var ) in self . permuteVars . iteritems ( ) : if whichVars is not None and varName not in whichVars : continue if globalBestPosition is None : var . newPosition ( None , self . _rng ) else : var . newPosition ( globalBestPosition [ varName ] , self . _rng ) position = self . getPosition ( ) if self . logger . getEffectiveLevel ( ) <= logging . DEBUG : msg = StringIO . StringIO ( ) print >> msg , "New particle position: \n%s" % ( pprint . pformat ( position , indent = 4 ) ) print >> msg , "Particle variables:" for ( varName , var ) in self . permuteVars . iteritems ( ) : print >> msg , " %s: %s" % ( varName , str ( var ) ) self . logger . debug ( msg . getvalue ( ) ) msg . close ( ) return position
2855	def setup ( self , pin , mode ) : self . _setup_pin ( pin , mode ) self . mpsse_write_gpio ( )
6375	def stem ( self , word ) : word = normalize ( 'NFC' , text_type ( word . lower ( ) ) ) word = word . translate ( self . _umlauts ) wlen = len ( word ) - 1 if wlen > 3 : if wlen > 5 : if word [ - 3 : ] == 'nen' : return word [ : - 3 ] if wlen > 4 : if word [ - 2 : ] in { 'en' , 'se' , 'es' , 'er' } : return word [ : - 2 ] if word [ - 1 ] in { 'e' , 'n' , 'r' , 's' } : return word [ : - 1 ] return word
5093	def refresh_maps ( self ) : for robot in self . robots : resp2 = ( requests . get ( urljoin ( self . ENDPOINT , 'users/me/robots/{}/maps' . format ( robot . serial ) ) , headers = self . _headers ) ) resp2 . raise_for_status ( ) self . _maps . update ( { robot . serial : resp2 . json ( ) } )
6752	def get_tasks ( self ) : tasks = set ( self . tasks ) for _name in dir ( self ) : if isinstance ( getattr ( type ( self ) , _name , None ) , property ) : continue attr = getattr ( self , _name ) if hasattr ( attr , '__call__' ) and getattr ( attr , 'is_task' , False ) : tasks . add ( _name ) return sorted ( tasks )
12407	def cons ( collection , value ) : if isinstance ( value , collections . Mapping ) : if collection is None : collection = { } collection . update ( ** value ) elif isinstance ( value , six . string_types ) : if collection is None : collection = [ ] collection . append ( value ) elif isinstance ( value , collections . Iterable ) : if collection is None : collection = [ ] collection . extend ( value ) else : if collection is None : collection = [ ] collection . append ( value ) return collection
9324	def refresh_collections ( self , accept = MEDIA_TYPE_TAXII_V20 ) : url = self . url + "collections/" response = self . _conn . get ( url , headers = { "Accept" : accept } ) self . _collections = [ ] for item in response . get ( "collections" , [ ] ) : collection_url = url + item [ "id" ] + "/" collection = Collection ( collection_url , conn = self . _conn , collection_info = item ) self . _collections . append ( collection ) self . _loaded_collections = True
7084	def _make_magseries_plot ( axes , stimes , smags , serrs , magsarefluxes = False , ms = 2.0 ) : scaledplottime = stimes - npmin ( stimes ) axes . plot ( scaledplottime , smags , marker = 'o' , ms = ms , ls = 'None' , mew = 0 , color = 'green' , rasterized = True ) if not magsarefluxes : plot_ylim = axes . get_ylim ( ) axes . set_ylim ( ( plot_ylim [ 1 ] , plot_ylim [ 0 ] ) ) axes . set_xlim ( ( npmin ( scaledplottime ) - 1.0 , npmax ( scaledplottime ) + 1.0 ) ) axes . grid ( color = '#a9a9a9' , alpha = 0.9 , zorder = 0 , linewidth = 1.0 , linestyle = ':' ) plot_xlabel = 'JD - %.3f' % npmin ( stimes ) if magsarefluxes : plot_ylabel = 'flux' else : plot_ylabel = 'magnitude' axes . set_xlabel ( plot_xlabel ) axes . set_ylabel ( plot_ylabel ) axes . get_yaxis ( ) . get_major_formatter ( ) . set_useOffset ( False ) axes . get_xaxis ( ) . get_major_formatter ( ) . set_useOffset ( False )
12680	def get_formatted_messages ( self , formats , label , context ) : format_templates = { } for fmt in formats : if fmt . endswith ( ".txt" ) : context . autoescape = False format_templates [ fmt ] = render_to_string ( ( "notification/%s/%s" % ( label , fmt ) , "notification/%s" % fmt ) , context_instance = context ) return format_templates
12458	def iterkeys ( data , ** kwargs ) : return iter ( data . keys ( ** kwargs ) ) if IS_PY3 else data . iterkeys ( ** kwargs )
3948	def _timezone_format ( value ) : return timezone . make_aware ( value , timezone . get_current_timezone ( ) ) if getattr ( settings , 'USE_TZ' , False ) else value
10833	def query_by_admin ( cls , admin ) : return cls . query . filter_by ( admin_type = resolve_admin_type ( admin ) , admin_id = admin . get_id ( ) )
3639	def squad ( self , squad_id = 0 , persona_id = None ) : method = 'GET' url = 'squad/%s/user/%s' % ( squad_id , persona_id or self . persona_id ) events = [ self . pin . event ( 'page_view' , 'Hub - Squads' ) ] self . pin . send ( events ) rc = self . __request__ ( method , url ) events = [ self . pin . event ( 'page_view' , 'Squad Details' ) , self . pin . event ( 'page_view' , 'Squads - Squad Overview' ) ] self . pin . send ( events ) return [ itemParse ( i ) for i in rc . get ( 'players' , ( ) ) ]
12647	def set_auth ( pem = None , cert = None , key = None , aad = False ) : if any ( [ cert , key ] ) and pem : raise ValueError ( 'Cannot specify both pem and cert or key' ) if any ( [ cert , key ] ) and not all ( [ cert , key ] ) : raise ValueError ( 'Must specify both cert and key' ) if pem : set_config_value ( 'security' , 'pem' ) set_config_value ( 'pem_path' , pem ) elif cert or key : set_config_value ( 'security' , 'cert' ) set_config_value ( 'cert_path' , cert ) set_config_value ( 'key_path' , key ) elif aad : set_config_value ( 'security' , 'aad' ) else : set_config_value ( 'security' , 'none' )
11292	def consume_json ( request ) : client = OEmbedConsumer ( ) urls = request . GET . getlist ( 'urls' ) width = request . GET . get ( 'width' ) height = request . GET . get ( 'height' ) template_dir = request . GET . get ( 'template_dir' ) output = { } ctx = RequestContext ( request ) for url in urls : try : provider = oembed . site . provider_for_url ( url ) except OEmbedMissingEndpoint : oembeds = None rendered = None else : oembeds = url rendered = client . parse_text ( url , width , height , context = ctx , template_dir = template_dir ) output [ url ] = { 'oembeds' : oembeds , 'rendered' : rendered , } return HttpResponse ( simplejson . dumps ( output ) , mimetype = 'application/json' )
4645	def delete ( self , key ) : query = ( "DELETE FROM {} WHERE {}=?" . format ( self . __tablename__ , self . __key__ ) , ( key , ) , ) connection = sqlite3 . connect ( self . sqlite_file ) cursor = connection . cursor ( ) cursor . execute ( * query ) connection . commit ( )
1456	def valid_path ( path ) : if path . endswith ( '*' ) : Log . debug ( 'Checking classpath entry suffix as directory: %s' , path [ : - 1 ] ) if os . path . isdir ( path [ : - 1 ] ) : return True return False Log . debug ( 'Checking classpath entry as directory: %s' , path ) if os . path . isdir ( path ) : return True else : Log . debug ( 'Checking classpath entry as file: %s' , path ) if os . path . isfile ( path ) : return True return False
5601	def serve ( mapchete_file , port = None , internal_cache = None , zoom = None , bounds = None , overwrite = False , readonly = False , memory = False , input_file = None , debug = False , logfile = None ) : app = create_app ( mapchete_files = [ mapchete_file ] , zoom = zoom , bounds = bounds , single_input_file = input_file , mode = _get_mode ( memory , readonly , overwrite ) , debug = debug ) if os . environ . get ( "MAPCHETE_TEST" ) == "TRUE" : logger . debug ( "don't run flask app, MAPCHETE_TEST environment detected" ) else : app . run ( threaded = True , debug = True , port = port , host = '0.0.0.0' , extra_files = [ mapchete_file ] )
271	def estimate_intraday ( returns , positions , transactions , EOD_hour = 23 ) : txn_val = transactions . copy ( ) txn_val . index . names = [ 'date' ] txn_val [ 'value' ] = txn_val . amount * txn_val . price txn_val = txn_val . reset_index ( ) . pivot_table ( index = 'date' , values = 'value' , columns = 'symbol' ) . replace ( np . nan , 0 ) txn_val [ 'date' ] = txn_val . index . date txn_val = txn_val . groupby ( 'date' ) . cumsum ( ) txn_val [ 'exposure' ] = txn_val . abs ( ) . sum ( axis = 1 ) condition = ( txn_val [ 'exposure' ] == txn_val . groupby ( pd . TimeGrouper ( '24H' ) ) [ 'exposure' ] . transform ( max ) ) txn_val = txn_val [ condition ] . drop ( 'exposure' , axis = 1 ) txn_val [ 'cash' ] = - txn_val . sum ( axis = 1 ) positions_shifted = positions . copy ( ) . shift ( 1 ) . fillna ( 0 ) starting_capital = positions . iloc [ 0 ] . sum ( ) / ( 1 + returns [ 0 ] ) positions_shifted . cash [ 0 ] = starting_capital txn_val . index = txn_val . index . normalize ( ) corrected_positions = positions_shifted . add ( txn_val , fill_value = 0 ) corrected_positions . index . name = 'period_close' corrected_positions . columns . name = 'sid' return corrected_positions
2220	def lookup ( self , data ) : for func in self . lazy_init : func ( ) for type , func in self . func_registry . items ( ) : if isinstance ( data , type ) : return func
12396	def get_method ( self , * args , ** kwargs ) : for method in self . gen_methods ( * args , ** kwargs ) : return method msg = 'No method was found for %r on %r.' raise self . DispatchError ( msg % ( ( args , kwargs ) , self . inst ) )
13196	def ensure_format ( doc , format ) : assert format in ( 'xml' , 'json' ) if getattr ( doc , 'tag' , None ) == 'open511' : if format == 'json' : return xml_to_json ( doc ) elif isinstance ( doc , dict ) and 'meta' in doc : if format == 'xml' : return json_doc_to_xml ( doc ) else : raise ValueError ( "Unrecognized input document" ) return doc
9042	def eigh ( self ) : from numpy . linalg import svd if self . _cache [ "eig" ] is not None : return self . _cache [ "eig" ] U , S = svd ( self . L ) [ : 2 ] S *= S S += self . _epsilon self . _cache [ "eig" ] = S , U return self . _cache [ "eig" ]
10428	def get_pmids ( graph : BELGraph , output : TextIO ) : for pmid in get_pubmed_identifiers ( graph ) : click . echo ( pmid , file = output )
9788	def bookmark ( ctx , username ) : ctx . obj = ctx . obj or { } ctx . obj [ 'username' ] = username
3821	async def delete_conversation ( self , delete_conversation_request ) : response = hangouts_pb2 . DeleteConversationResponse ( ) await self . _pb_request ( 'conversations/deleteconversation' , delete_conversation_request , response ) return response
13624	def Integer ( value , base = 10 , encoding = None ) : try : return int ( Text ( value , encoding ) , base ) except ( TypeError , ValueError ) : return None
11667	def linear ( Ks , dim , num_q , rhos , nus ) : r return _get_linear ( Ks , dim ) ( num_q , rhos , nus )
12430	def create_nginx_config ( self ) : cfg = '# nginx config for {0}\n' . format ( self . _project_name ) if not self . _shared_hosting : if self . _user : cfg += 'user {0};\n' . format ( self . _user ) cfg += 'worker_processes 1;\nerror_log {0}-errors.log;\n\pid {1}_ nginx.pid;\n\n' . format ( os . path . join ( self . _log_dir , self . _project_name ) , os . path . join ( self . _var_dir , self . _project_name ) ) cfg += 'events {\n\tworker_connections 32;\n}\n\n' cfg += 'http {\n' if self . _include_mimetypes : cfg += '\tinclude mime.types;\n' cfg += '\tdefault_type application/octet-stream;\n' cfg += '\tclient_max_body_size 1G;\n' cfg += '\tproxy_max_temp_file_size 0;\n' cfg += '\tproxy_buffering off;\n' cfg += '\taccess_log {0}-access.log;\n' . format ( os . path . join ( self . _log_dir , self . _project_name ) ) cfg += '\tsendfile on;\n' cfg += '\tkeepalive_timeout 65;\n' cfg += '\tserver {\n' cfg += '\t\tlisten 0.0.0.0:{0};\n' . format ( self . _port ) if self . _server_name : cfg += '\t\tserver_name {0};\n' . format ( self . _server_name ) cfg += '\t\tlocation / {\n' cfg += '\t\t\tuwsgi_pass unix:///{0}.sock;\n' . format ( os . path . join ( self . _var_dir , self . _project_name ) ) cfg += '\t\t\tinclude uwsgi_params;\n' cfg += '\t\t}\n\n' cfg += '\t\terror_page 500 502 503 504 /50x.html;\n' cfg += '\t\tlocation = /50x.html {\n' cfg += '\t\t\troot html;\n' cfg += '\t\t}\n' cfg += '\t}\n' if not self . _shared_hosting : cfg += '}\n' f = open ( self . _nginx_config , 'w' ) f . write ( cfg ) f . close ( )
7952	def wait_for_writability ( self ) : with self . lock : while True : if self . _state in ( "closing" , "closed" , "aborted" ) : return False if self . _socket and bool ( self . _write_queue ) : return True self . _write_queue_cond . wait ( ) return False
9677	def calculate_bin_boundary ( self , bb ) : return min ( enumerate ( OPC_LOOKUP ) , key = lambda x : abs ( x [ 1 ] - bb ) ) [ 0 ]
2025	def SGT ( self , a , b ) : s0 , s1 = to_signed ( a ) , to_signed ( b ) return Operators . ITEBV ( 256 , s0 > s1 , 1 , 0 )
8736	def construct_datetime ( cls , * args , ** kwargs ) : if len ( args ) == 1 : arg = args [ 0 ] method = cls . __get_dt_constructor ( type ( arg ) . __module__ , type ( arg ) . __name__ , ) result = method ( arg ) try : result = result . replace ( tzinfo = kwargs . pop ( 'tzinfo' ) ) except KeyError : pass if kwargs : first_key = kwargs . keys ( ) [ 0 ] tmpl = ( "{first_key} is an invalid keyword " "argument for this function." ) raise TypeError ( tmpl . format ( ** locals ( ) ) ) else : result = datetime . datetime ( * args , ** kwargs ) return result
11339	def set_target_fahrenheit ( self , fahrenheit , mode = config . SCHEDULE_HOLD ) : temperature = fahrenheit_to_nuheat ( fahrenheit ) self . set_target_temperature ( temperature , mode )
235	def compute_cap_exposures ( positions , caps ) : long_exposures = [ ] short_exposures = [ ] gross_exposures = [ ] net_exposures = [ ] positions_wo_cash = positions . drop ( 'cash' , axis = 'columns' ) tot_gross_exposure = positions_wo_cash . abs ( ) . sum ( axis = 'columns' ) tot_long_exposure = positions_wo_cash [ positions_wo_cash > 0 ] . sum ( axis = 'columns' ) tot_short_exposure = positions_wo_cash [ positions_wo_cash < 0 ] . abs ( ) . sum ( axis = 'columns' ) for bucket_name , boundaries in CAP_BUCKETS . items ( ) : in_bucket = positions_wo_cash [ ( caps >= boundaries [ 0 ] ) & ( caps <= boundaries [ 1 ] ) ] gross_bucket = in_bucket . abs ( ) . sum ( axis = 'columns' ) . divide ( tot_gross_exposure ) long_bucket = in_bucket [ in_bucket > 0 ] . sum ( axis = 'columns' ) . divide ( tot_long_exposure ) short_bucket = in_bucket [ in_bucket < 0 ] . sum ( axis = 'columns' ) . divide ( tot_short_exposure ) net_bucket = long_bucket . subtract ( short_bucket ) gross_exposures . append ( gross_bucket ) long_exposures . append ( long_bucket ) short_exposures . append ( short_bucket ) net_exposures . append ( net_bucket ) return long_exposures , short_exposures , gross_exposures , net_exposures
12896	def get_volume_steps ( self ) : if not self . __volume_steps : self . __volume_steps = yield from self . handle_int ( self . API . get ( 'volume_steps' ) ) return self . __volume_steps
9812	def revoke ( username ) : try : PolyaxonClient ( ) . user . revoke_superuser ( username ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not revoke superuser role from user `{}`.' . format ( username ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Superuser role was revoked successfully from user `{}`." . format ( username ) )
11856	def extender ( self , edge ) : "See what edges can be extended by this edge." ( j , k , B , _ , _ ) = edge for ( i , j , A , alpha , B1b ) in self . chart [ j ] : if B1b and B == B1b [ 0 ] : self . add_edge ( [ i , k , A , alpha + [ edge ] , B1b [ 1 : ] ] )
1075	def _days_in_month ( year , month ) : "year, month -> number of days in that month in that year." assert 1 <= month <= 12 , month if month == 2 and _is_leap ( year ) : return 29 return _DAYS_IN_MONTH [ month ]
610	def _generateFileFromTemplates ( templateFileNames , outputFilePath , replacementDict ) : installPath = os . path . dirname ( __file__ ) outputFile = open ( outputFilePath , "w" ) outputLines = [ ] inputLines = [ ] firstFile = True for templateFileName in templateFileNames : if not firstFile : inputLines . extend ( [ os . linesep ] * 2 ) firstFile = False inputFilePath = os . path . join ( installPath , templateFileName ) inputFile = open ( inputFilePath ) inputLines . extend ( inputFile . readlines ( ) ) inputFile . close ( ) print "Writing " , len ( inputLines ) , "lines..." for line in inputLines : tempLine = line for k , v in replacementDict . iteritems ( ) : if v is None : v = "None" tempLine = re . sub ( k , v , tempLine ) outputFile . write ( tempLine ) outputFile . close ( )
1825	def LEAVE ( cpu ) : cpu . STACK = cpu . FRAME cpu . FRAME = cpu . pop ( cpu . address_bit_size )
3598	def reviews ( self , packageName , filterByDevice = False , sort = 2 , nb_results = None , offset = None ) : path = REVIEWS_URL + "?doc={}&sort={}" . format ( requests . utils . quote ( packageName ) , sort ) if nb_results is not None : path += "&n={}" . format ( nb_results ) if offset is not None : path += "&o={}" . format ( offset ) if filterByDevice : path += "&dfil=1" data = self . executeRequestApi2 ( path ) output = [ ] for review in data . payload . reviewResponse . getResponse . review : output . append ( utils . parseProtobufObj ( review ) ) return output
8799	def update_group_states_for_vifs ( self , vifs , ack ) : vif_keys = [ self . vif_key ( vif . device_id , vif . mac_address ) for vif in vifs ] self . set_fields ( vif_keys , SECURITY_GROUP_ACK , ack )
6892	def serial_starfeatures ( lclist , outdir , lc_catalog_pickle , neighbor_radius_arcsec , maxobjects = None , deredden = True , custom_bandpasses = None , lcformat = 'hat-sql' , lcformatdir = None ) : if not os . path . exists ( outdir ) : os . makedirs ( outdir ) if maxobjects : lclist = lclist [ : maxobjects ] with open ( lc_catalog_pickle , 'rb' ) as infd : kdt_dict = pickle . load ( infd ) kdt = kdt_dict [ 'kdtree' ] objlist = kdt_dict [ 'objects' ] [ 'objectid' ] objlcfl = kdt_dict [ 'objects' ] [ 'lcfname' ] tasks = [ ( x , outdir , kdt , objlist , objlcfl , neighbor_radius_arcsec , deredden , custom_bandpasses , lcformat , lcformatdir ) for x in lclist ] for task in tqdm ( tasks ) : result = _starfeatures_worker ( task ) return result
8583	def get_attached_volume ( self , datacenter_id , server_id , volume_id ) : response = self . _perform_request ( '/datacenters/%s/servers/%s/volumes/%s' % ( datacenter_id , server_id , volume_id ) ) return response
139	def to_bounding_box ( self ) : from imgaug . augmentables . bbs import BoundingBox xx = self . xx yy = self . yy return BoundingBox ( x1 = min ( xx ) , x2 = max ( xx ) , y1 = min ( yy ) , y2 = max ( yy ) , label = self . label )
4141	def _arburg2 ( X , order ) : x = np . array ( X ) N = len ( x ) if order <= 0. : raise ValueError ( "order must be > 0" ) rho = sum ( abs ( x ) ** 2. ) / N den = rho * 2. * N ef = np . zeros ( N , dtype = complex ) eb = np . zeros ( N , dtype = complex ) for j in range ( 0 , N ) : ef [ j ] = x [ j ] eb [ j ] = x [ j ] a = np . zeros ( 1 , dtype = complex ) a [ 0 ] = 1 ref = np . zeros ( order , dtype = complex ) temp = 1. E = np . zeros ( order + 1 ) E [ 0 ] = rho for m in range ( 0 , order ) : efp = ef [ 1 : ] ebp = eb [ 0 : - 1 ] num = - 2. * np . dot ( ebp . conj ( ) . transpose ( ) , efp ) den = np . dot ( efp . conj ( ) . transpose ( ) , efp ) den += np . dot ( ebp , ebp . conj ( ) . transpose ( ) ) ref [ m ] = num / den ef = efp + ref [ m ] * ebp eb = ebp + ref [ m ] . conj ( ) . transpose ( ) * efp a . resize ( len ( a ) + 1 ) a = a + ref [ m ] * np . flipud ( a ) . conjugate ( ) E [ m + 1 ] = ( 1 - ref [ m ] . conj ( ) . transpose ( ) * ref [ m ] ) * E [ m ] return a , E [ - 1 ] , ref
4861	def validate_username ( self , value ) : try : user = User . objects . get ( username = value ) except User . DoesNotExist : raise serializers . ValidationError ( "User does not exist" ) try : enterprise_customer_user = models . EnterpriseCustomerUser . objects . get ( user_id = user . pk ) except models . EnterpriseCustomerUser . DoesNotExist : raise serializers . ValidationError ( "User has no EnterpriseCustomerUser" ) self . enterprise_customer_user = enterprise_customer_user return value
7012	def lcdict_to_pickle ( lcdict , outfile = None ) : if not outfile and lcdict [ 'objectid' ] : outfile = '%s-hplc.pkl' % lcdict [ 'objectid' ] elif not outfile and not lcdict [ 'objectid' ] : outfile = 'hplc.pkl' with open ( outfile , 'wb' ) as outfd : pickle . dump ( lcdict , outfd , protocol = pickle . HIGHEST_PROTOCOL ) if os . path . exists ( outfile ) : LOGINFO ( 'lcdict for object: %s -> %s OK' % ( lcdict [ 'objectid' ] , outfile ) ) return outfile else : LOGERROR ( 'could not make a pickle for this lcdict!' ) return None
13305	def foex ( a , b ) : return ( np . sum ( a > b , dtype = float ) / len ( a ) - 0.5 ) * 100
2477	def add_lic_xref ( self , doc , ref ) : if self . has_extr_lic ( doc ) : self . extr_lic ( doc ) . add_xref ( ref ) return True else : raise OrderError ( 'ExtractedLicense::CrossRef' )
11857	def settings ( request ) : settings = Setting . objects . all ( ) . as_dict ( default = '' ) context = { 'SETTINGS' : settings , } return context
5718	def pull_datapackage ( descriptor , name , backend , ** backend_options ) : warnings . warn ( 'Functions "push/pull_datapackage" are deprecated. ' 'Please use "Package" class' , UserWarning ) datapackage_name = name plugin = import_module ( 'jsontableschema.plugins.%s' % backend ) storage = plugin . Storage ( ** backend_options ) resources = [ ] for table in storage . buckets : schema = storage . describe ( table ) base = os . path . dirname ( descriptor ) path , name = _restore_path ( table ) fullpath = os . path . join ( base , path ) helpers . ensure_dir ( fullpath ) with io . open ( fullpath , 'wb' ) as file : model = Schema ( deepcopy ( schema ) ) data = storage . iter ( table ) writer = csv . writer ( file , encoding = 'utf-8' ) writer . writerow ( model . headers ) for row in data : writer . writerow ( row ) resource = { 'schema' : schema , 'path' : path } if name is not None : resource [ 'name' ] = name resources . append ( resource ) mode = 'w' encoding = 'utf-8' if six . PY2 : mode = 'wb' encoding = None resources = _restore_resources ( resources ) helpers . ensure_dir ( descriptor ) with io . open ( descriptor , mode = mode , encoding = encoding ) as file : descriptor = { 'name' : datapackage_name , 'resources' : resources , } json . dump ( descriptor , file , indent = 4 ) return storage
6787	def preview ( self , components = None , ask = 0 ) : ask = int ( ask ) self . init ( ) component_order , plan_funcs = self . get_component_funcs ( components = components ) print ( '\n%i changes found for host %s.\n' % ( len ( component_order ) , self . genv . host_string ) ) if component_order and plan_funcs : if self . verbose : print ( 'These components have changed:\n' ) for component in sorted ( component_order ) : print ( ( ' ' * 4 ) + component ) print ( 'Deployment plan for host %s:\n' % self . genv . host_string ) for func_name , _ in plan_funcs : print ( success_str ( ( ' ' * 4 ) + func_name ) ) if component_order : print ( ) if ask and self . genv . host_string == self . genv . hosts [ - 1 ] : if component_order : if not raw_input ( 'Begin deployment? [yn] ' ) . strip ( ) . lower ( ) . startswith ( 'y' ) : sys . exit ( 0 ) else : sys . exit ( 0 )
5489	def from_file ( cls , file ) : if not os . path . exists ( file ) : raise ValueError ( "Config file not found." ) try : config_parser = configparser . ConfigParser ( ) config_parser . read ( file ) configuration = cls ( file , config_parser ) if not configuration . check_config_sanity ( ) : raise ValueError ( "Error in config file." ) else : return configuration except configparser . Error : raise ValueError ( "Config file is invalid." )
6614	def receive_one ( self ) : if not self . isopen : logger = logging . getLogger ( __name__ ) logger . warning ( 'the drop box is not open' ) return return self . dropbox . receive_one ( )
12413	def serialize ( self , data , format = None ) : return self . _resource . serialize ( data , response = self , format = format )
4341	def repeat ( self , count = 1 ) : if not isinstance ( count , int ) or count < 1 : raise ValueError ( "count must be a postive integer." ) effect_args = [ 'repeat' , '{}' . format ( count ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'repeat' )
2262	def find_duplicates ( items , k = 2 , key = None ) : duplicates = defaultdict ( list ) if key is None : for count , item in enumerate ( items ) : duplicates [ item ] . append ( count ) else : for count , item in enumerate ( items ) : duplicates [ key ( item ) ] . append ( count ) for key in list ( duplicates . keys ( ) ) : if len ( duplicates [ key ] ) < k : del duplicates [ key ] duplicates = dict ( duplicates ) return duplicates
741	def radiusForSpeed ( self , speed ) : overlap = 1.5 coordinatesPerTimestep = speed * self . timestep / self . scale radius = int ( round ( float ( coordinatesPerTimestep ) / 2 * overlap ) ) minRadius = int ( math . ceil ( ( math . sqrt ( self . w ) - 1 ) / 2 ) ) return max ( radius , minRadius )
9535	def get_complete_version ( version = None ) : if version is None : from django_cryptography import VERSION as version else : assert len ( version ) == 5 assert version [ 3 ] in ( 'alpha' , 'beta' , 'rc' , 'final' ) return version
7553	def nworker ( data , chunk ) : oldlimit = set_mkl_thread_limit ( 1 ) with h5py . File ( data . database . input , 'r' ) as io5 : seqview = io5 [ "bootsarr" ] [ : ] maparr = io5 [ "bootsmap" ] [ : , 0 ] smps = io5 [ "quartets" ] [ chunk : chunk + data . _chunksize ] nall_mask = seqview [ : ] == 78 rquartets = np . zeros ( ( smps . shape [ 0 ] , 4 ) , dtype = np . uint16 ) rinvariants = np . zeros ( ( smps . shape [ 0 ] , 16 , 16 ) , dtype = np . uint16 ) for idx in xrange ( smps . shape [ 0 ] ) : sidx = smps [ idx ] seqs = seqview [ sidx ] nmask = np . any ( nall_mask [ sidx ] , axis = 0 ) nmask += np . all ( seqs == seqs [ 0 ] , axis = 0 ) bidx , invar = calculate ( seqs , maparr , nmask , TESTS ) rquartets [ idx ] = smps [ idx ] [ bidx ] rinvariants [ idx ] = invar set_mkl_thread_limit ( oldlimit ) return rquartets , rinvariants
10348	def export_namespace ( graph , namespace , directory = None , cacheable = False ) : directory = os . getcwd ( ) if directory is None else directory path = os . path . join ( directory , '{}.belns' . format ( namespace ) ) with open ( path , 'w' ) as file : log . info ( 'Outputting to %s' , path ) right_names = get_names_by_namespace ( graph , namespace ) log . info ( 'Graph has %d correct names in %s' , len ( right_names ) , namespace ) wrong_names = get_incorrect_names_by_namespace ( graph , namespace ) log . info ( 'Graph has %d incorrect names in %s' , len ( right_names ) , namespace ) undefined_ns_names = get_undefined_namespace_names ( graph , namespace ) log . info ( 'Graph has %d names in missing namespace %s' , len ( right_names ) , namespace ) names = ( right_names | wrong_names | undefined_ns_names ) if 0 == len ( names ) : log . warning ( '%s is empty' , namespace ) write_namespace ( namespace_name = namespace , namespace_keyword = namespace , namespace_domain = 'Other' , author_name = graph . authors , author_contact = graph . contact , citation_name = graph . name , values = names , cacheable = cacheable , file = file )
10397	def remove_random_edge_until_has_leaves ( self ) -> None : while True : leaves = set ( self . iter_leaves ( ) ) if leaves : return self . remove_random_edge ( )
12605	def _to_string ( data ) : sdata = data . copy ( ) for k , v in data . items ( ) : if isinstance ( v , datetime ) : sdata [ k ] = timestamp_to_date_str ( v ) elif not isinstance ( v , ( string_types , float , int ) ) : sdata [ k ] = str ( v ) return sdata
6914	def generate_rrab_lightcurve ( times , mags = None , errs = None , paramdists = { 'period' : sps . uniform ( loc = 0.45 , scale = 0.35 ) , 'fourierorder' : [ 8 , 11 ] , 'amplitude' : sps . uniform ( loc = 0.4 , scale = 0.5 ) , 'phioffset' : np . pi , } , magsarefluxes = False ) : modeldict = generate_sinusoidal_lightcurve ( times , mags = mags , errs = errs , paramdists = paramdists , magsarefluxes = magsarefluxes ) modeldict [ 'vartype' ] = 'RRab' return modeldict
4319	def _parse_stat ( stat_output ) : lines = stat_output . split ( '\n' ) stat_dict = { } for line in lines : split_line = line . split ( ':' ) if len ( split_line ) == 2 : key = split_line [ 0 ] val = split_line [ 1 ] . strip ( ' ' ) try : val = float ( val ) except ValueError : val = None stat_dict [ key ] = val return stat_dict
405	def pixel_wise_softmax ( x , name = 'pixel_wise_softmax' ) : with tf . name_scope ( name ) : return tf . nn . softmax ( x )
1137	def commonprefix ( m ) : "Given a list of pathnames, returns the longest common leading component" if not m : return '' s1 = min ( m ) s2 = max ( m ) for i , c in enumerate ( s1 ) : if c != s2 [ i ] : return s1 [ : i ] return s1
7970	def _add_timeout_handler ( self , handler ) : self . timeout_handlers . append ( handler ) if self . event_thread is None : return self . _run_timeout_threads ( handler )
5937	def transform_args ( self , * args , ** kwargs ) : options = [ ] for option , value in kwargs . items ( ) : if not option . startswith ( '-' ) : if len ( option ) == 1 : option = '-' + option else : option = '--' + option if value is True : options . append ( option ) continue elif value is False : raise ValueError ( 'A False value is ambiguous for option {0!r}' . format ( option ) ) if option [ : 2 ] == '--' : options . append ( option + '=' + str ( value ) ) else : options . extend ( ( option , str ( value ) ) ) return options + list ( args )
12298	def discover_all_plugins ( self ) : for v in pkg_resources . iter_entry_points ( 'dgit.plugins' ) : m = v . load ( ) m . setup ( self )
360	def load_folder_list ( path = "" ) : return [ os . path . join ( path , o ) for o in os . listdir ( path ) if os . path . isdir ( os . path . join ( path , o ) ) ]
11727	def ppdict ( dict_to_print , br = '\n' , html = False , key_align = 'l' , sort_keys = True , key_preffix = '' , key_suffix = '' , value_prefix = '' , value_suffix = '' , left_margin = 3 , indent = 2 ) : if dict_to_print : if sort_keys : dic = dict_to_print . copy ( ) keys = list ( dic . keys ( ) ) keys . sort ( ) dict_to_print = OrderedDict ( ) for k in keys : dict_to_print [ k ] = dic [ k ] tmp = [ '{' ] ks = [ type ( x ) == str and "'%s'" % x or x for x in dict_to_print . keys ( ) ] vs = [ type ( x ) == str and "'%s'" % x or x for x in dict_to_print . values ( ) ] max_key_len = max ( [ len ( str ( x ) ) for x in ks ] ) for i in range ( len ( ks ) ) : k = { 1 : str ( ks [ i ] ) . ljust ( max_key_len ) , key_align == 'r' : str ( ks [ i ] ) . rjust ( max_key_len ) } [ 1 ] v = vs [ i ] tmp . append ( ' ' * indent + '{}{}{}:{}{}{},' . format ( key_preffix , k , key_suffix , value_prefix , v , value_suffix ) ) tmp [ - 1 ] = tmp [ - 1 ] [ : - 1 ] tmp . append ( '}' ) if left_margin : tmp = [ ' ' * left_margin + x for x in tmp ] if html : return '<code>{}</code>' . format ( br . join ( tmp ) . replace ( ' ' , '&nbsp;' ) ) else : return br . join ( tmp ) else : return '{}'
757	def _allow_new_attributes ( f ) : def decorated ( self , * args , ** kw ) : if not hasattr ( self , '_canAddAttributes' ) : self . __dict__ [ '_canAddAttributes' ] = 1 else : self . _canAddAttributes += 1 assert self . _canAddAttributes >= 1 count = self . _canAddAttributes f ( self , * args , ** kw ) if hasattr ( self , '_canAddAttributes' ) : self . _canAddAttributes -= 1 else : self . _canAddAttributes = count - 1 assert self . _canAddAttributes >= 0 if self . _canAddAttributes == 0 : del self . _canAddAttributes decorated . __doc__ = f . __doc__ decorated . __name__ = f . __name__ return decorated
5121	def reset_colors ( self ) : for k , e in enumerate ( self . g . edges ( ) ) : self . g . set_ep ( e , 'edge_color' , self . edge2queue [ k ] . colors [ 'edge_color' ] ) for v in self . g . nodes ( ) : self . g . set_vp ( v , 'vertex_fill_color' , self . colors [ 'vertex_fill_color' ] )
1350	def write_json_response ( self , response ) : self . write ( tornado . escape . json_encode ( response ) ) self . set_header ( "Content-Type" , "application/json" )
12048	def determineProtocol ( fname ) : f = open ( fname , 'rb' ) raw = f . read ( 5000 ) f . close ( ) protoComment = "unknown" if b"SWHLab4[" in raw : protoComment = raw . split ( b"SWHLab4[" ) [ 1 ] . split ( b"]" , 1 ) [ 0 ] elif b"SWH[" in raw : protoComment = raw . split ( b"SWH[" ) [ 1 ] . split ( b"]" , 1 ) [ 0 ] else : protoComment = "?" if not type ( protoComment ) is str : protoComment = protoComment . decode ( "utf-8" ) return protoComment
10346	def merge_namespaces ( input_locations , output_path , namespace_name , namespace_keyword , namespace_domain , author_name , citation_name , namespace_description = None , namespace_species = None , namespace_version = None , namespace_query_url = None , namespace_created = None , author_contact = None , author_copyright = None , citation_description = None , citation_url = None , citation_version = None , citation_date = None , case_sensitive = True , delimiter = '|' , cacheable = True , functions = None , value_prefix = '' , sort_key = None , check_keywords = True ) : results = get_merged_namespace_names ( input_locations , check_keywords = check_keywords ) with open ( output_path , 'w' ) as file : write_namespace ( namespace_name = namespace_name , namespace_keyword = namespace_keyword , namespace_domain = namespace_domain , author_name = author_name , citation_name = citation_name , values = results , namespace_species = namespace_species , namespace_description = namespace_description , namespace_query_url = namespace_query_url , namespace_version = namespace_version , namespace_created = namespace_created , author_contact = author_contact , author_copyright = author_copyright , citation_description = citation_description , citation_url = citation_url , citation_version = citation_version , citation_date = citation_date , case_sensitive = case_sensitive , delimiter = delimiter , cacheable = cacheable , functions = functions , value_prefix = value_prefix , sort_key = sort_key , file = file )
13784	def get_tm_session ( session_factory , transaction_manager ) : dbsession = session_factory ( ) zope . sqlalchemy . register ( dbsession , transaction_manager = transaction_manager ) return dbsession
3537	def crazy_egg ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return CrazyEggNode ( )
10185	def _events_config ( self ) : result = { } for ep in iter_entry_points ( group = self . entry_point_group_events ) : for cfg in ep . load ( ) ( ) : if cfg [ 'event_type' ] not in self . enabled_events : continue elif cfg [ 'event_type' ] in result : raise DuplicateEventError ( 'Duplicate event {0} in entry point ' '{1}' . format ( cfg [ 'event_type' ] , ep . name ) ) cfg . update ( self . enabled_events [ cfg [ 'event_type' ] ] or { } ) result [ cfg [ 'event_type' ] ] = cfg return result
4988	def eligible_for_direct_audit_enrollment ( self , request , enterprise_customer , resource_id , course_key = None ) : course_identifier = course_key if course_key else resource_id return request . GET . get ( 'audit' ) and request . path == self . COURSE_ENROLLMENT_VIEW_URL . format ( enterprise_customer . uuid , course_identifier ) and enterprise_customer . catalog_contains_course ( resource_id ) and EnrollmentApiClient ( ) . has_course_mode ( resource_id , 'audit' )
12635	def levenshtein_analysis ( self , field_weights = None ) : if field_weights is None : if not isinstance ( self . field_weights , dict ) : raise ValueError ( 'Expected a dict for `field_weights` parameter, ' 'got {}' . format ( type ( self . field_weights ) ) ) key_dicoms = list ( self . dicom_groups . keys ( ) ) file_dists = calculate_file_distances ( key_dicoms , field_weights , self . _dist_method_cls ) return file_dists
9990	def get_object ( self , name ) : parts = name . split ( "." ) child = parts . pop ( 0 ) if parts : return self . spaces [ child ] . get_object ( "." . join ( parts ) ) else : return self . _namespace_impl [ child ]
1214	def _int_to_pos ( self , flat_position ) : return flat_position % self . env . action_space . screen_shape [ 0 ] , flat_position % self . env . action_space . screen_shape [ 1 ]
13034	def write_triples ( filename , triples , delimiter = DEFAULT_DELIMITER , triple_order = "hrt" ) : with open ( filename , 'w' ) as f : for t in triples : line = t . serialize ( delimiter , triple_order ) f . write ( line + "\n" )
11519	def perform_upload ( self , upload_token , filename , ** kwargs ) : parameters = dict ( ) parameters [ 'uploadtoken' ] = upload_token parameters [ 'filename' ] = filename try : create_additional_revision = kwargs [ 'create_additional_revision' ] except KeyError : create_additional_revision = False if not create_additional_revision : parameters [ 'revision' ] = 'head' optional_keys = [ 'mode' , 'folderid' , 'item_id' , 'itemid' , 'revision' ] for key in optional_keys : if key in kwargs : if key == 'item_id' : parameters [ 'itemid' ] = kwargs [ key ] continue if key == 'folder_id' : parameters [ 'folderid' ] = kwargs [ key ] continue parameters [ key ] = kwargs [ key ] file_payload = open ( kwargs . get ( 'filepath' , filename ) , 'rb' ) parameters [ 'length' ] = os . fstat ( file_payload . fileno ( ) ) . st_size response = self . request ( 'midas.upload.perform' , parameters , file_payload ) return response
8053	def listener ( self , sock , * args ) : conn , addr = sock . accept ( ) f = conn . makefile ( conn ) self . shell = ShoebotCmd ( self . bot , stdin = f , stdout = f , intro = INTRO ) print ( _ ( "Connected" ) ) GObject . io_add_watch ( conn , GObject . IO_IN , self . handler ) if self . shell . intro : self . shell . stdout . write ( str ( self . shell . intro ) + "\n" ) self . shell . stdout . flush ( ) return True
7171	def train_subprocess ( self , * args , ** kwargs ) : ret = call ( [ sys . executable , '-m' , 'padatious' , 'train' , self . cache_dir , '-d' , json . dumps ( self . serialized_args ) , '-a' , json . dumps ( args ) , '-k' , json . dumps ( kwargs ) , ] ) if ret == 2 : raise TypeError ( 'Invalid train arguments: {} {}' . format ( args , kwargs ) ) data = self . serialized_args self . clear ( ) self . apply_training_args ( data ) self . padaos . compile ( ) if ret == 0 : self . must_train = False return True elif ret == 10 : return False else : raise ValueError ( 'Training failed and returned code: {}' . format ( ret ) )
1357	def get_argument_topology ( self ) : try : topology = self . get_argument ( constants . PARAM_TOPOLOGY ) return topology except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
425	def delete_tasks ( self , ** kwargs ) : self . _fill_project_info ( kwargs ) self . db . Task . delete_many ( kwargs ) logging . info ( "[Database] Delete Task SUCCESS" )
11076	def load_user_rights ( self , user ) : if user . username in self . admins : user . is_admin = True elif not hasattr ( user , 'is_admin' ) : user . is_admin = False
12506	def signed_session ( self , session = None ) : from sfctl . config import ( aad_metadata , aad_cache ) if session : session = super ( AdalAuthentication , self ) . signed_session ( session ) else : session = super ( AdalAuthentication , self ) . signed_session ( ) if self . no_verify : session . verify = False authority_uri , cluster_id , client_id = aad_metadata ( ) existing_token , existing_cache = aad_cache ( ) context = adal . AuthenticationContext ( authority_uri , cache = existing_cache ) new_token = context . acquire_token ( cluster_id , existing_token [ 'userId' ] , client_id ) header = "{} {}" . format ( "Bearer" , new_token [ 'accessToken' ] ) session . headers [ 'Authorization' ] = header return session
12258	def sdcone ( x , rho ) : U , V = np . linalg . eigh ( x ) return V . dot ( np . diag ( np . maximum ( U , 0 ) ) . dot ( V . T ) )
1098	def get_close_matches ( word , possibilities , n = 3 , cutoff = 0.6 ) : if not n > 0 : raise ValueError ( "n must be > 0: %r" % ( n , ) ) if not 0.0 <= cutoff <= 1.0 : raise ValueError ( "cutoff must be in [0.0, 1.0]: %r" % ( cutoff , ) ) result = [ ] s = SequenceMatcher ( ) s . set_seq2 ( word ) for x in possibilities : s . set_seq1 ( x ) if s . real_quick_ratio ( ) >= cutoff and s . quick_ratio ( ) >= cutoff and s . ratio ( ) >= cutoff : result . append ( ( s . ratio ( ) , x ) ) result = heapq . nlargest ( n , result ) return [ x for score , x in result ]
4213	def pass_from_pipe ( cls ) : is_pipe = not sys . stdin . isatty ( ) return is_pipe and cls . strip_last_newline ( sys . stdin . read ( ) )
2999	def sectorPerformanceDF ( token = '' , version = '' ) : df = pd . DataFrame ( sectorPerformance ( token , version ) ) _toDatetime ( df ) _reindex ( df , 'name' ) return df
7061	def sqs_delete_queue ( queue_url , client = None ) : if not client : client = boto3 . client ( 'sqs' ) try : client . delete_queue ( QueueUrl = queue_url ) return True except Exception as e : LOGEXCEPTION ( 'could not delete the specified queue: %s' % ( queue_url , ) ) return False
10190	def tell ( self , message , sender = no_sender ) : if sender is not no_sender and not isinstance ( sender , ActorRef ) : raise ValueError ( "Sender must be actor reference" ) self . _cell . send_message ( message , sender )
10829	def accept ( self ) : with db . session . begin_nested ( ) : self . state = MembershipState . ACTIVE db . session . merge ( self )
6270	def swap_buffers ( self ) : if not self . window . context : return self . frames += 1 self . window . flip ( ) self . window . dispatch_events ( )
5287	def construct_formset ( self ) : formset = super ( InlineFormSetFactory , self ) . construct_formset ( ) formset . model = self . inline_model return formset
13730	def value_to_bool ( config_val , evar ) : if not config_val : return False if config_val . strip ( ) . lower ( ) == 'true' : return True else : return False
3010	def _get_scopes ( self ) : if _credentials_from_request ( self . request ) : return ( self . _scopes | _credentials_from_request ( self . request ) . scopes ) else : return self . _scopes
6204	def populations_diff_coeff ( particles , populations ) : D_counts = particles . diffusion_coeff_counts if len ( D_counts ) == 1 : pop_sizes = [ pop . stop - pop . start for pop in populations ] assert D_counts [ 0 ] [ 1 ] >= sum ( pop_sizes ) D_counts = [ ( D_counts [ 0 ] [ 0 ] , ps ) for ps in pop_sizes ] D_list = [ ] D_pop_start = 0 for pop , ( D , counts ) in zip ( populations , D_counts ) : D_list . append ( D ) assert pop . start >= D_pop_start assert pop . stop <= D_pop_start + counts D_pop_start += counts return D_list
13603	def error_message ( self , message , fh = None , prefix = "[error]:" , suffix = "..." ) : msg = prefix + message + suffix fh = fh or sys . stderr if fh is sys . stderr : termcolor . cprint ( msg , color = "red" ) else : fh . write ( msg ) pass
7670	def add ( self , jam , on_conflict = 'fail' ) : if on_conflict not in [ 'overwrite' , 'fail' , 'ignore' ] : raise ParameterError ( "on_conflict='{}' is not in ['fail', " "'overwrite', 'ignore']." . format ( on_conflict ) ) if not self . file_metadata == jam . file_metadata : if on_conflict == 'overwrite' : self . file_metadata = jam . file_metadata elif on_conflict == 'fail' : raise JamsError ( "Metadata conflict! " "Resolve manually or force-overwrite it." ) self . annotations . extend ( jam . annotations ) self . sandbox . update ( ** jam . sandbox )
6819	def sync_media ( self , sync_set = None , clean = 0 , iter_local_paths = 0 ) : self . genv . SITE = self . genv . SITE or self . genv . default_site r = self . local_renderer clean = int ( clean ) self . vprint ( 'Getting site data for %s...' % self . genv . SITE ) self . set_site_specifics ( self . genv . SITE ) sync_sets = r . env . sync_sets if sync_set : sync_sets = [ sync_set ] ret_paths = [ ] for _sync_set in sync_sets : for paths in r . env . sync_sets [ _sync_set ] : r . env . sync_local_path = os . path . abspath ( paths [ 'local_path' ] % self . genv ) if paths [ 'local_path' ] . endswith ( '/' ) and not r . env . sync_local_path . endswith ( '/' ) : r . env . sync_local_path += '/' if iter_local_paths : ret_paths . append ( r . env . sync_local_path ) continue r . env . sync_remote_path = paths [ 'remote_path' ] % self . genv if clean : r . sudo ( 'rm -Rf {apache_sync_remote_path}' ) print ( 'Syncing %s to %s...' % ( r . env . sync_local_path , r . env . sync_remote_path ) ) r . env . tmp_chmod = paths . get ( 'chmod' , r . env . chmod ) r . sudo ( 'mkdir -p {apache_sync_remote_path}' ) r . sudo ( 'chmod -R {apache_tmp_chmod} {apache_sync_remote_path}' ) r . local ( 'rsync -rvz --progress --recursive --no-p --no-g ' '--rsh "ssh -o StrictHostKeyChecking=no -i {key_filename}" {apache_sync_local_path} {user}@{host_string}:{apache_sync_remote_path}' ) r . sudo ( 'chown -R {apache_web_user}:{apache_web_group} {apache_sync_remote_path}' ) if iter_local_paths : return ret_paths
9945	def copy_file ( self , path , prefixed_path , source_storage ) : if prefixed_path in self . copied_files : return self . log ( "Skipping '%s' (already copied earlier)" % path ) if not self . delete_file ( path , prefixed_path , source_storage ) : return source_path = source_storage . path ( path ) if self . dry_run : self . log ( "Pretending to copy '%s'" % source_path , level = 1 ) else : self . log ( "Copying '%s'" % source_path , level = 1 ) with source_storage . open ( path ) as source_file : self . storage . save ( prefixed_path , source_file ) self . copied_files . append ( prefixed_path )
236	def plot_cap_exposures_net ( net_exposures , ax = None ) : if ax is None : ax = plt . gca ( ) color_list = plt . cm . gist_rainbow ( np . linspace ( 0 , 1 , 5 ) ) cap_names = CAP_BUCKETS . keys ( ) for i in range ( len ( net_exposures ) ) : ax . plot ( net_exposures [ i ] , color = color_list [ i ] , alpha = 0.8 , label = cap_names [ i ] ) ax . axhline ( 0 , color = 'k' , linestyle = '-' ) ax . set ( title = 'Net exposure to market caps' , ylabel = 'Proportion of net exposure \n in market cap buckets' ) return ax
5453	def _remove_empty_items ( d , required ) : new_dict = { } for k , v in d . items ( ) : if k in required : new_dict [ k ] = v elif isinstance ( v , int ) or v : new_dict [ k ] = v return new_dict
11513	def move_item ( self , token , item_id , src_folder_id , dest_folder_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'id' ] = item_id parameters [ 'srcfolderid' ] = src_folder_id parameters [ 'dstfolderid' ] = dest_folder_id response = self . request ( 'midas.item.move' , parameters ) return response
5206	def format_earning ( data : pd . DataFrame , header : pd . DataFrame ) -> pd . DataFrame : if data . dropna ( subset = [ 'value' ] ) . empty : return pd . DataFrame ( ) res = pd . concat ( [ grp . loc [ : , [ 'value' ] ] . set_index ( header . value ) for _ , grp in data . groupby ( data . position ) ] , axis = 1 ) res . index . name = None res . columns = res . iloc [ 0 ] res = res . iloc [ 1 : ] . transpose ( ) . reset_index ( ) . apply ( pd . to_numeric , downcast = 'float' , errors = 'ignore' ) res . rename ( columns = lambda vv : '_' . join ( vv . lower ( ) . split ( ) ) . replace ( 'fy_' , 'fy' ) , inplace = True , ) years = res . columns [ res . columns . str . startswith ( 'fy' ) ] lvl_1 = res . level == 1 for yr in years : res . loc [ : , yr ] = res . loc [ : , yr ] . round ( 1 ) pct = f'{yr}_pct' res . loc [ : , pct ] = 0. res . loc [ lvl_1 , pct ] = res . loc [ lvl_1 , pct ] . astype ( float ) . round ( 1 ) res . loc [ lvl_1 , pct ] = res . loc [ lvl_1 , yr ] / res . loc [ lvl_1 , yr ] . sum ( ) * 100 sub_pct = [ ] for _ , snap in res [ : : - 1 ] . iterrows ( ) : if snap . level > 2 : continue if snap . level == 1 : if len ( sub_pct ) == 0 : continue sub = pd . concat ( sub_pct , axis = 1 ) . transpose ( ) res . loc [ sub . index , pct ] = res . loc [ sub . index , yr ] / res . loc [ sub . index , yr ] . sum ( ) * 100 sub_pct = [ ] if snap . level == 2 : sub_pct . append ( snap ) res . set_index ( 'segment_name' , inplace = True ) res . index . name = None return res
433	def draw_boxes_and_labels_to_image ( image , classes , coords , scores , classes_list , is_center = True , is_rescale = True , save_name = None ) : if len ( coords ) != len ( classes ) : raise AssertionError ( "number of coordinates and classes are equal" ) if len ( scores ) > 0 and len ( scores ) != len ( classes ) : raise AssertionError ( "number of scores and classes are equal" ) image = image . copy ( ) imh , imw = image . shape [ 0 : 2 ] thick = int ( ( imh + imw ) // 430 ) for i , _v in enumerate ( coords ) : if is_center : x , y , x2 , y2 = tl . prepro . obj_box_coord_centroid_to_upleft_butright ( coords [ i ] ) else : x , y , x2 , y2 = coords [ i ] if is_rescale : x , y , x2 , y2 = tl . prepro . obj_box_coord_scale_to_pixelunit ( [ x , y , x2 , y2 ] , ( imh , imw ) ) cv2 . rectangle ( image , ( int ( x ) , int ( y ) ) , ( int ( x2 ) , int ( y2 ) ) , [ 0 , 255 , 0 ] , thick ) cv2 . putText ( image , classes_list [ classes [ i ] ] + ( ( " %.2f" % ( scores [ i ] ) ) if ( len ( scores ) != 0 ) else " " ) , ( int ( x ) , int ( y ) ) , 0 , 1.5e-3 * imh , [ 0 , 0 , 256 ] , int ( thick / 2 ) + 1 ) if save_name is not None : save_image ( image , save_name ) return image
10654	def prepare_to_run ( self , clock , period_count ) : self . period_count = period_count self . _exec_year_end_datetime = clock . get_datetime_at_period_ix ( period_count ) self . _prev_year_end_datetime = clock . start_datetime self . _curr_year_end_datetime = clock . start_datetime + relativedelta ( years = 1 ) del self . gl . transactions [ : ] for c in self . components : c . prepare_to_run ( clock , period_count ) self . negative_income_tax_total = 0
5955	def find_executables ( path ) : execs = [ ] for exe in os . listdir ( path ) : fullexe = os . path . join ( path , exe ) if ( os . access ( fullexe , os . X_OK ) and not os . path . isdir ( fullexe ) and exe not in [ 'GMXRC' , 'GMXRC.bash' , 'GMXRC.csh' , 'GMXRC.zsh' , 'demux.pl' , 'xplor2gmx.pl' ] ) : execs . append ( exe ) return execs
5070	def format_price ( price , currency = '$' ) : if int ( price ) == price : return '{}{}' . format ( currency , int ( price ) ) return '{}{:0.2f}' . format ( currency , price )
9995	def del_attr ( self , name ) : if name in self . namespace : if name in self . cells : self . del_cells ( name ) elif name in self . spaces : self . del_space ( name ) elif name in self . refs : self . del_ref ( name ) else : raise RuntimeError ( "Must not happen" ) else : raise KeyError ( "'%s' not found in Space '%s'" % ( name , self . name ) )
3718	def estimate ( self ) : self . mul ( 300 ) self . Cpig ( 300 ) estimates = { 'Tb' : self . Tb ( self . counts ) , 'Tm' : self . Tm ( self . counts ) , 'Tc' : self . Tc ( self . counts , self . Tb_estimated ) , 'Pc' : self . Pc ( self . counts , self . atom_count ) , 'Vc' : self . Vc ( self . counts ) , 'Hf' : self . Hf ( self . counts ) , 'Gf' : self . Gf ( self . counts ) , 'Hfus' : self . Hfus ( self . counts ) , 'Hvap' : self . Hvap ( self . counts ) , 'mul' : self . mul , 'mul_coeffs' : self . calculated_mul_coeffs , 'Cpig' : self . Cpig , 'Cpig_coeffs' : self . calculated_Cpig_coeffs } return estimates
3300	def make_sub_element ( parent , tag , nsmap = None ) : if use_lxml : return etree . SubElement ( parent , tag , nsmap = nsmap ) return etree . SubElement ( parent , tag )
12436	def traverse ( cls , request , params = None ) : result = cls . parse ( request . path ) if result is None : return cls , { } elif not result : raise http . exceptions . NotFound ( ) resource , data , rest = result if params : data . update ( params ) if resource is None : return cls , data if data . get ( 'path' ) is not None : request . path = data . pop ( 'path' ) elif rest is not None : request . path = rest result = resource . traverse ( request , params = data ) return result
8049	def run ( self ) : if self . err is not None : assert self . source is None msg = "%s%03i %s" % ( rst_prefix , rst_fail_load , "Failed to load file: %s" % self . err , ) yield 0 , 0 , msg , type ( self ) module = [ ] try : module = parse ( StringIO ( self . source ) , self . filename ) except SyntaxError as err : msg = "%s%03i %s" % ( rst_prefix , rst_fail_parse , "Failed to parse file: %s" % err , ) yield 0 , 0 , msg , type ( self ) module = [ ] except AllError : msg = "%s%03i %s" % ( rst_prefix , rst_fail_all , "Failed to parse __all__ entry." , ) yield 0 , 0 , msg , type ( self ) module = [ ] for definition in module : if not definition . docstring : continue try : unindented = trim ( dequote_docstring ( definition . docstring ) ) rst_errors = list ( rst_lint . lint ( unindented ) ) except Exception as err : msg = "%s%03i %s" % ( rst_prefix , rst_fail_lint , "Failed to lint docstring: %s - %s" % ( definition . name , err ) , ) yield definition . start , 0 , msg , type ( self ) continue for rst_error in rst_errors : if rst_error . level <= 1 : continue msg = rst_error . message . split ( "\n" , 1 ) [ 0 ] code = code_mapping ( rst_error . level , msg ) assert code < 100 , code code += 100 * rst_error . level msg = "%s%03i %s" % ( rst_prefix , code , msg ) yield definition . start + rst_error . line , 0 , msg , type ( self )
138	def to_shapely_line_string ( self , closed = False , interpolate = 0 ) : return _convert_points_to_shapely_line_string ( self . exterior , closed = closed , interpolate = interpolate )
12146	def analyzeSingle ( abfFname ) : assert os . path . exists ( abfFname ) and abfFname . endswith ( ".abf" ) ABFfolder , ABFfname = os . path . split ( abfFname ) abfID = os . path . splitext ( ABFfname ) [ 0 ] IN = INDEX ( ABFfolder ) IN . analyzeABF ( abfID ) IN . scan ( ) IN . html_single_basic ( [ abfID ] , overwrite = True ) IN . html_single_plot ( [ abfID ] , overwrite = True ) IN . scan ( ) IN . html_index ( ) return
1398	def extract_scheduler_location ( self , topology ) : schedulerLocation = { "name" : None , "http_endpoint" : None , "job_page_link" : None , } if topology . scheduler_location : schedulerLocation [ "name" ] = topology . scheduler_location . topology_name schedulerLocation [ "http_endpoint" ] = topology . scheduler_location . http_endpoint schedulerLocation [ "job_page_link" ] = topology . scheduler_location . job_page_link [ 0 ] if len ( topology . scheduler_location . job_page_link ) > 0 else "" return schedulerLocation
12901	def set_sleep ( self , value = False ) : return ( yield from self . handle_set ( self . API . get ( 'sleep' ) , int ( value ) ) )
10755	def iso_name_increment ( name , is_dir = False , max_length = 8 ) : if not is_dir and '.' in name : name , ext = name . rsplit ( '.' ) ext = '.{}' . format ( ext ) else : ext = '' for position , char in reversed ( list ( enumerate ( name ) ) ) : if char not in string . digits : break base , tag = name [ : position + 1 ] , name [ position + 1 : ] tag = str ( int ( tag or 0 ) + 1 ) if len ( tag ) + len ( base ) > max_length : base = base [ : max_length - len ( tag ) ] return '' . join ( [ base , tag , ext ] )
7874	def get_payload ( self , payload_class , payload_key = None , specialize = False ) : if self . _payload is None : self . decode_payload ( ) if payload_class is None : if self . _payload : payload = self . _payload [ 0 ] if specialize and isinstance ( payload , XMLPayload ) : klass = payload_class_for_element_name ( payload . element . tag ) if klass is not XMLPayload : payload = klass . from_xml ( payload . element ) self . _payload [ 0 ] = payload return payload else : return None elements = payload_class . _pyxmpp_payload_element_name for i , payload in enumerate ( self . _payload ) : if isinstance ( payload , XMLPayload ) : if payload_class is not XMLPayload : if payload . xml_element_name not in elements : continue payload = payload_class . from_xml ( payload . element ) elif not isinstance ( payload , payload_class ) : continue if payload_key is not None and payload_key != payload . handler_key ( ) : continue self . _payload [ i ] = payload return payload return None
6872	def given_lc_get_transit_tmids_tstarts_tends ( time , flux , err_flux , blsfit_savpath = None , trapfit_savpath = None , magsarefluxes = True , nworkers = 1 , sigclip = None , extra_maskfrac = 0.03 ) : endp = 1.05 * ( np . nanmax ( time ) - np . nanmin ( time ) ) / 2 blsdict = kbls . bls_parallel_pfind ( time , flux , err_flux , magsarefluxes = magsarefluxes , startp = 0.1 , endp = endp , maxtransitduration = 0.3 , nworkers = nworkers , sigclip = sigclip ) blsd = kbls . bls_stats_singleperiod ( time , flux , err_flux , blsdict [ 'bestperiod' ] , magsarefluxes = True , sigclip = sigclip , perioddeltapercent = 5 ) if blsfit_savpath : make_fit_plot ( blsd [ 'phases' ] , blsd [ 'phasedmags' ] , None , blsd [ 'blsmodel' ] , blsd [ 'period' ] , blsd [ 'epoch' ] , blsd [ 'epoch' ] , blsfit_savpath , magsarefluxes = magsarefluxes ) ingduration_guess = blsd [ 'transitduration' ] * 0.2 transitparams = [ blsd [ 'period' ] , blsd [ 'epoch' ] , blsd [ 'transitdepth' ] , blsd [ 'transitduration' ] , ingduration_guess ] if trapfit_savpath : trapd = traptransit_fit_magseries ( time , flux , err_flux , transitparams , magsarefluxes = magsarefluxes , sigclip = sigclip , plotfit = trapfit_savpath ) tmids , t_starts , t_ends = get_transit_times ( blsd , time , extra_maskfrac , trapd = trapd ) return tmids , t_starts , t_ends
8245	def aggregated ( cache = DEFAULT_CACHE ) : global _aggregated_name , _aggregated_dict if _aggregated_name != cache : _aggregated_name = cache _aggregated_dict = { } for path in glob ( os . path . join ( cache , "*" ) ) : if os . path . isdir ( path ) : p = os . path . basename ( path ) _aggregated_dict [ p ] = glob ( os . path . join ( path , "*" ) ) _aggregated_dict [ p ] = [ os . path . basename ( f ) [ : - 4 ] for f in _aggregated_dict [ p ] ] return _aggregated_dict
11870	def color_from_rgb ( red , green , blue ) : r = min ( red , 255 ) g = min ( green , 255 ) b = min ( blue , 255 ) if r > 1 or g > 1 or b > 1 : r = r / 255.0 g = g / 255.0 b = b / 255.0 return color_from_hls ( * rgb_to_hls ( r , g , b ) )
1859	def CMPS ( cpu , dest , src ) : src_reg = { 8 : 'SI' , 32 : 'ESI' , 64 : 'RSI' } [ cpu . address_bit_size ] dest_reg = { 8 : 'DI' , 32 : 'EDI' , 64 : 'RDI' } [ cpu . address_bit_size ] base , _ , ty = cpu . get_descriptor ( cpu . DS ) src_addr = cpu . read_register ( src_reg ) + base dest_addr = cpu . read_register ( dest_reg ) + base size = dest . size arg1 = cpu . read_int ( dest_addr , size ) arg0 = cpu . read_int ( src_addr , size ) res = ( arg0 - arg1 ) & ( ( 1 << size ) - 1 ) cpu . _calculate_CMP_flags ( size , res , arg0 , arg1 ) increment = Operators . ITEBV ( cpu . address_bit_size , cpu . DF , - size // 8 , size // 8 ) cpu . write_register ( src_reg , cpu . read_register ( src_reg ) + increment ) cpu . write_register ( dest_reg , cpu . read_register ( dest_reg ) + increment )
3771	def mixing_logarithmic ( fracs , props ) : r if not none_and_length_check ( [ fracs , props ] ) : return None return exp ( sum ( frac * log ( prop ) for frac , prop in zip ( fracs , props ) ) )
9270	def get_temp_tag_for_repo_creation ( self ) : tag_date = self . tag_times_dict . get ( REPO_CREATED_TAG_NAME , None ) if not tag_date : tag_name , tag_date = self . fetcher . fetch_repo_creation_date ( ) self . tag_times_dict [ tag_name ] = timestring_to_datetime ( tag_date ) return REPO_CREATED_TAG_NAME
4724	def main ( conf ) : fpath = yml_fpath ( conf [ "OUTPUT" ] ) if os . path . exists ( fpath ) : cij . err ( "main:FAILED { fpath: %r }, exists" % fpath ) return 1 trun = trun_setup ( conf ) if not trun : return 1 trun_to_file ( trun ) trun_emph ( trun ) tr_err = 0 tr_ent_err = trun_enter ( trun ) for tsuite in ( ts for ts in trun [ "testsuites" ] if not tr_ent_err ) : ts_err = 0 ts_ent_err = tsuite_enter ( trun , tsuite ) for tcase in ( tc for tc in tsuite [ "testcases" ] if not ts_ent_err ) : tc_err = tcase_enter ( trun , tsuite , tcase ) if not tc_err : tc_err += script_run ( trun , tcase ) tc_err += tcase_exit ( trun , tsuite , tcase ) tcase [ "status" ] = "FAIL" if tc_err else "PASS" trun [ "progress" ] [ tcase [ "status" ] ] += 1 trun [ "progress" ] [ "UNKN" ] -= 1 ts_err += tc_err trun_to_file ( trun ) if not ts_ent_err : ts_err += tsuite_exit ( trun , tsuite ) ts_err += ts_ent_err tr_err += ts_err tsuite [ "status" ] = "FAIL" if ts_err else "PASS" cij . emph ( "rnr:tsuite %r" % tsuite [ "status" ] , tsuite [ "status" ] != "PASS" ) if not tr_ent_err : trun_exit ( trun ) tr_err += tr_ent_err trun [ "status" ] = "FAIL" if tr_err else "PASS" trun [ "stamp" ] [ "end" ] = int ( time . time ( ) ) + 1 trun_to_file ( trun ) cij . emph ( "rnr:main:progress %r" % trun [ "progress" ] ) cij . emph ( "rnr:main:trun %r" % trun [ "status" ] , trun [ "status" ] != "PASS" ) return trun [ "progress" ] [ "UNKN" ] + trun [ "progress" ] [ "FAIL" ]
2759	def get_all_load_balancers ( self ) : data = self . get_data ( "load_balancers" ) load_balancers = list ( ) for jsoned in data [ 'load_balancers' ] : load_balancer = LoadBalancer ( ** jsoned ) load_balancer . token = self . token load_balancer . health_check = HealthCheck ( ** jsoned [ 'health_check' ] ) load_balancer . sticky_sessions = StickySesions ( ** jsoned [ 'sticky_sessions' ] ) forwarding_rules = list ( ) for rule in jsoned [ 'forwarding_rules' ] : forwarding_rules . append ( ForwardingRule ( ** rule ) ) load_balancer . forwarding_rules = forwarding_rules load_balancers . append ( load_balancer ) return load_balancers
10243	def count_citation_years ( graph : BELGraph ) -> typing . Counter [ int ] : result = defaultdict ( set ) for _ , _ , data in graph . edges ( data = True ) : if CITATION not in data or CITATION_DATE not in data [ CITATION ] : continue try : dt = _ensure_datetime ( data [ CITATION ] [ CITATION_DATE ] ) result [ dt . year ] . add ( ( data [ CITATION ] [ CITATION_TYPE ] , data [ CITATION ] [ CITATION_REFERENCE ] ) ) except Exception : continue return count_dict_values ( result )
9183	def lookup_document_pointer ( ident_hash , cursor ) : id , version = split_ident_hash ( ident_hash , split_version = True ) stmt = "SELECT name FROM modules WHERE uuid = %s" args = [ id ] if version and version [ 0 ] is not None : operator = version [ 1 ] is None and 'is' or '=' stmt += " AND (major_version = %s AND minor_version {} %s)" . format ( operator ) args . extend ( version ) cursor . execute ( stmt , args ) try : title = cursor . fetchone ( ) [ 0 ] except TypeError : raise DocumentLookupError ( ) else : metadata = { 'title' : title } return cnxepub . DocumentPointer ( ident_hash , metadata )
3930	def _get_authorization_code ( session , credentials_prompt ) : browser = Browser ( session , OAUTH2_LOGIN_URL ) email = credentials_prompt . get_email ( ) browser . submit_form ( FORM_SELECTOR , { EMAIL_SELECTOR : email } ) password = credentials_prompt . get_password ( ) browser . submit_form ( FORM_SELECTOR , { PASSWORD_SELECTOR : password } ) if browser . has_selector ( TOTP_CHALLENGE_SELECTOR ) : browser . submit_form ( TOTP_CHALLENGE_SELECTOR , { } ) elif browser . has_selector ( PHONE_CHALLENGE_SELECTOR ) : browser . submit_form ( PHONE_CHALLENGE_SELECTOR , { } ) if browser . has_selector ( VERIFICATION_FORM_SELECTOR ) : if browser . has_selector ( TOTP_CODE_SELECTOR ) : input_selector = TOTP_CODE_SELECTOR elif browser . has_selector ( PHONE_CODE_SELECTOR ) : input_selector = PHONE_CODE_SELECTOR else : raise GoogleAuthError ( 'Unknown verification code input' ) verfification_code = credentials_prompt . get_verification_code ( ) browser . submit_form ( VERIFICATION_FORM_SELECTOR , { input_selector : verfification_code } ) try : return browser . get_cookie ( 'oauth_code' ) except KeyError : raise GoogleAuthError ( 'Authorization code cookie not found' )
5920	def fit ( self , xy = False , ** kwargs ) : kwargs . setdefault ( 's' , self . tpr ) kwargs . setdefault ( 'n' , self . ndx ) kwargs [ 'f' ] = self . xtc force = kwargs . pop ( 'force' , self . force ) if xy : fitmode = 'rotxy+transxy' kwargs . pop ( 'fit' , None ) infix_default = '_fitxy' else : fitmode = kwargs . pop ( 'fit' , 'rot+trans' ) infix_default = '_fit' dt = kwargs . get ( 'dt' ) if dt : infix_default += '_dt{0:d}ps' . format ( int ( dt ) ) kwargs . setdefault ( 'o' , self . outfile ( self . infix_filename ( None , self . xtc , infix_default , 'xtc' ) ) ) fitgroup = kwargs . pop ( 'fitgroup' , 'backbone' ) kwargs . setdefault ( 'input' , [ fitgroup , "system" ] ) if kwargs . get ( 'center' , False ) : logger . warn ( "Transformer.fit(): center=%(center)r used: centering should not be combined with fitting." , kwargs ) if len ( kwargs [ 'inputs' ] ) != 3 : logger . error ( "If you insist on centering you must provide three groups in the 'input' kwarg: (center, fit, output)" ) raise ValuError ( "Insufficient index groups for centering,fitting,output" ) logger . info ( "Fitting trajectory %r to with xy=%r..." , kwargs [ 'f' ] , xy ) logger . info ( "Fitting on index group %(fitgroup)r" , vars ( ) ) with utilities . in_dir ( self . dirname ) : if self . check_file_exists ( kwargs [ 'o' ] , resolve = "indicate" , force = force ) : logger . warn ( "File %r exists; force regenerating it with force=True." , kwargs [ 'o' ] ) else : gromacs . trjconv ( fit = fitmode , ** kwargs ) logger . info ( "Fitted trajectory (fitmode=%s): %r." , fitmode , kwargs [ 'o' ] ) return { 'tpr' : self . rp ( kwargs [ 's' ] ) , 'xtc' : self . rp ( kwargs [ 'o' ] ) }
3950	def _read ( self , mux , gain , data_rate , mode ) : config = ADS1x15_CONFIG_OS_SINGLE config |= ( mux & 0x07 ) << ADS1x15_CONFIG_MUX_OFFSET if gain not in ADS1x15_CONFIG_GAIN : raise ValueError ( 'Gain must be one of: 2/3, 1, 2, 4, 8, 16' ) config |= ADS1x15_CONFIG_GAIN [ gain ] config |= mode if data_rate is None : data_rate = self . _data_rate_default ( ) config |= self . _data_rate_config ( data_rate ) config |= ADS1x15_CONFIG_COMP_QUE_DISABLE self . _device . writeList ( ADS1x15_POINTER_CONFIG , [ ( config >> 8 ) & 0xFF , config & 0xFF ] ) time . sleep ( 1.0 / data_rate + 0.0001 ) result = self . _device . readList ( ADS1x15_POINTER_CONVERSION , 2 ) return self . _conversion_value ( result [ 1 ] , result [ 0 ] )
5363	def stdout ( self ) : if self . _streaming : stdout = [ ] while not self . __stdout . empty ( ) : try : line = self . __stdout . get_nowait ( ) stdout . append ( line ) except : pass else : stdout = self . __stdout return stdout
1154	def remove ( self , value ) : if value not in self : raise KeyError ( value ) self . discard ( value )
9909	def set_primary ( self ) : query = EmailAddress . objects . filter ( is_primary = True , user = self . user ) query = query . exclude ( pk = self . pk ) with transaction . atomic ( ) : query . update ( is_primary = False ) self . is_primary = True self . save ( ) logger . info ( "Set %s as the primary email address for %s." , self . email , self . user , )
7512	def padnames ( names ) : longname_len = max ( len ( i ) for i in names ) padding = 5 pnames = [ name + " " * ( longname_len - len ( name ) + padding ) for name in names ] snppad = "//" + " " * ( longname_len - 2 + padding ) return np . array ( pnames ) , snppad
2651	def monitor_wrapper ( f , task_id , monitoring_hub_url , run_id , sleep_dur ) : def wrapped ( * args , ** kwargs ) : p = Process ( target = monitor , args = ( os . getpid ( ) , task_id , monitoring_hub_url , run_id , sleep_dur ) ) p . start ( ) try : return f ( * args , ** kwargs ) finally : p . terminate ( ) p . join ( ) return wrapped
5773	def dsa_sign ( private_key , data , hash_algorithm ) : if private_key . algorithm != 'dsa' : raise ValueError ( 'The key specified is not a DSA private key' ) return _sign ( private_key , data , hash_algorithm )
1885	def solve_buffer ( self , addr , nbytes , constrain = False ) : buffer = self . cpu . read_bytes ( addr , nbytes ) result = [ ] with self . _constraints as temp_cs : cs_to_use = self . constraints if constrain else temp_cs for c in buffer : result . append ( self . _solver . get_value ( cs_to_use , c ) ) cs_to_use . add ( c == result [ - 1 ] ) return result
7768	def _stream_authorized ( self , event ) : with self . lock : if event . stream != self . stream : return self . me = event . stream . me self . peer = event . stream . peer presence = self . settings [ u"initial_presence" ] if presence : self . send ( presence )
9672	def iteration ( self ) : i = 0 conv = np . inf old_conv = - np . inf conv_list = [ ] m = self . original if isinstance ( self . original , pd . DataFrame ) : ipfn_method = self . ipfn_df elif isinstance ( self . original , np . ndarray ) : ipfn_method = self . ipfn_np self . original = self . original . astype ( 'float64' ) else : print ( 'Data input instance not recognized' ) sys . exit ( 0 ) while ( ( i <= self . max_itr and conv > self . conv_rate ) and ( i <= self . max_itr and abs ( conv - old_conv ) > self . rate_tolerance ) ) : old_conv = conv m , conv = ipfn_method ( m , self . aggregates , self . dimensions , self . weight_col ) conv_list . append ( conv ) i += 1 converged = 1 if i <= self . max_itr : if not conv > self . conv_rate : print ( 'ipfn converged: convergence_rate below threshold' ) elif not abs ( conv - old_conv ) > self . rate_tolerance : print ( 'ipfn converged: convergence_rate not updating or below rate_tolerance' ) else : print ( 'Maximum iterations reached' ) converged = 0 if self . verbose == 0 : return m elif self . verbose == 1 : return m , converged elif self . verbose == 2 : return m , converged , pd . DataFrame ( { 'iteration' : range ( i ) , 'conv' : conv_list } ) . set_index ( 'iteration' ) else : print ( 'wrong verbose input, return None' ) sys . exit ( 0 )
6139	def get_is_sim_running ( self ) : sim_info = self . simulation_info ( ) try : progress_info = sim_info [ 'simulation_info_progress' ] ret = progress_info [ 'simulation_progress_is_running' ] except KeyError : ret = False return ret
12151	def html_single_basic ( self , abfID , launch = False , overwrite = False ) : if type ( abfID ) is str : abfID = [ abfID ] for thisABFid in cm . abfSort ( abfID ) : parentID = cm . parent ( self . groups , thisABFid ) saveAs = os . path . abspath ( "%s/%s_basic.html" % ( self . folder2 , parentID ) ) if overwrite is False and os . path . basename ( saveAs ) in self . files2 : continue filesByType = cm . filesByType ( self . groupFiles [ parentID ] ) html = "" html += '<div style="background-color: #DDDDDD;">' html += '<span class="title">summary of data from: %s</span></br>' % parentID html += '<code>%s</code>' % os . path . abspath ( self . folder1 + "/" + parentID + ".abf" ) html += '</div>' catOrder = [ "experiment" , "plot" , "tif" , "other" ] categories = cm . list_order_by ( filesByType . keys ( ) , catOrder ) for category in [ x for x in categories if len ( filesByType [ x ] ) ] : if category == 'experiment' : html += "<h3>Experimental Data:</h3>" elif category == 'plot' : html += "<h3>Intrinsic Properties:</h3>" elif category == 'tif' : html += "<h3>Micrographs:</h3>" elif category == 'other' : html += "<h3>Additional Files:</h3>" else : html += "<h3>????:</h3>" for fname in filesByType [ category ] : html += self . htmlFor ( fname ) html += '<br>' * 3 print ( "creating" , saveAs , '...' ) style . save ( html , saveAs , launch = launch )
6867	def _pkl_magseries_plot ( stimes , smags , serrs , plotdpi = 100 , magsarefluxes = False ) : scaledplottime = stimes - npmin ( stimes ) magseriesfig = plt . figure ( figsize = ( 7.5 , 4.8 ) , dpi = plotdpi ) plt . plot ( scaledplottime , smags , marker = 'o' , ms = 2.0 , ls = 'None' , mew = 0 , color = 'green' , rasterized = True ) if not magsarefluxes : plot_ylim = plt . ylim ( ) plt . ylim ( ( plot_ylim [ 1 ] , plot_ylim [ 0 ] ) ) plt . xlim ( ( npmin ( scaledplottime ) - 2.0 , npmax ( scaledplottime ) + 2.0 ) ) plt . grid ( color = '#a9a9a9' , alpha = 0.9 , zorder = 0 , linewidth = 1.0 , linestyle = ':' ) plot_xlabel = 'JD - %.3f' % npmin ( stimes ) if magsarefluxes : plot_ylabel = 'flux' else : plot_ylabel = 'magnitude' plt . xlabel ( plot_xlabel ) plt . ylabel ( plot_ylabel ) plt . gca ( ) . get_yaxis ( ) . get_major_formatter ( ) . set_useOffset ( False ) plt . gca ( ) . get_xaxis ( ) . get_major_formatter ( ) . set_useOffset ( False ) magseriespng = StrIO ( ) magseriesfig . savefig ( magseriespng , pad_inches = 0.05 , format = 'png' ) plt . close ( ) magseriespng . seek ( 0 ) magseriesb64 = base64 . b64encode ( magseriespng . read ( ) ) magseriespng . close ( ) checkplotdict = { 'magseries' : { 'plot' : magseriesb64 , 'times' : stimes , 'mags' : smags , 'errs' : serrs } } return checkplotdict
3432	def add_groups ( self , group_list ) : def existing_filter ( group ) : if group . id in self . groups : LOGGER . warning ( "Ignoring group '%s' since it already exists." , group . id ) return False return True if isinstance ( group_list , string_types ) or hasattr ( group_list , "id" ) : warn ( "need to pass in a list" ) group_list = [ group_list ] pruned = DictList ( filter ( existing_filter , group_list ) ) for group in pruned : group . _model = self for member in group . members : if isinstance ( member , Metabolite ) : if member not in self . metabolites : self . add_metabolites ( [ member ] ) if isinstance ( member , Reaction ) : if member not in self . reactions : self . add_reactions ( [ member ] ) self . groups += [ group ]
6581	def play ( self , song ) : self . _callbacks . play ( song ) self . _load_track ( song ) time . sleep ( 2 ) while True : try : self . _callbacks . pre_poll ( ) self . _ensure_started ( ) self . _loop_hook ( ) readers , _ , _ = select . select ( self . _get_select_readers ( ) , [ ] , [ ] , 1 ) for handle in readers : if handle . fileno ( ) == self . _control_fd : self . _callbacks . input ( handle . readline ( ) . strip ( ) , song ) else : value = self . _read_from_process ( handle ) if self . _player_stopped ( value ) : return finally : self . _callbacks . post_poll ( )
201	def pad_to_aspect_ratio ( self , aspect_ratio , mode = "constant" , cval = 0.0 , return_pad_amounts = False ) : arr_padded , pad_amounts = ia . pad_to_aspect_ratio ( self . arr , aspect_ratio = aspect_ratio , mode = mode , cval = cval , return_pad_amounts = True ) segmap = SegmentationMapOnImage ( arr_padded , shape = self . shape ) segmap . input_was = self . input_was if return_pad_amounts : return segmap , pad_amounts else : return segmap
5357	def _add_to_conf ( self , new_conf ) : for section in new_conf : if section not in self . conf : self . conf [ section ] = new_conf [ section ] else : for param in new_conf [ section ] : self . conf [ section ] [ param ] = new_conf [ section ] [ param ]
7011	def plot_periodbase_lsp ( lspinfo , outfile = None , plotdpi = 100 ) : if isinstance ( lspinfo , str ) and os . path . exists ( lspinfo ) : LOGINFO ( 'loading LSP info from pickle %s' % lspinfo ) with open ( lspinfo , 'rb' ) as infd : lspinfo = pickle . load ( infd ) try : periods = lspinfo [ 'periods' ] lspvals = lspinfo [ 'lspvals' ] bestperiod = lspinfo [ 'bestperiod' ] lspmethod = lspinfo [ 'method' ] plt . plot ( periods , lspvals ) plt . xscale ( 'log' , basex = 10 ) plt . xlabel ( 'Period [days]' ) plt . ylabel ( PLOTYLABELS [ lspmethod ] ) plottitle = '%s best period: %.6f d' % ( METHODSHORTLABELS [ lspmethod ] , bestperiod ) plt . title ( plottitle ) for bestperiod , bestpeak in zip ( lspinfo [ 'nbestperiods' ] , lspinfo [ 'nbestlspvals' ] ) : plt . annotate ( '%.6f' % bestperiod , xy = ( bestperiod , bestpeak ) , xycoords = 'data' , xytext = ( 0.0 , 25.0 ) , textcoords = 'offset points' , arrowprops = dict ( arrowstyle = "->" ) , fontsize = 'x-small' ) plt . grid ( color = '#a9a9a9' , alpha = 0.9 , zorder = 0 , linewidth = 1.0 , linestyle = ':' ) if outfile and isinstance ( outfile , str ) : if outfile . endswith ( '.png' ) : plt . savefig ( outfile , bbox_inches = 'tight' , dpi = plotdpi ) else : plt . savefig ( outfile , bbox_inches = 'tight' ) plt . close ( ) return os . path . abspath ( outfile ) elif dispok : plt . show ( ) plt . close ( ) return else : LOGWARNING ( 'no output file specified and no $DISPLAY set, ' 'saving to lsp-plot.png in current directory' ) outfile = 'lsp-plot.png' plt . savefig ( outfile , bbox_inches = 'tight' , dpi = plotdpi ) plt . close ( ) return os . path . abspath ( outfile ) except Exception as e : LOGEXCEPTION ( 'could not plot this LSP, appears to be empty' ) return
11704	def set_inherited_traits ( self , egg_donor , sperm_donor ) : if type ( egg_donor ) == str : self . reproduce_asexually ( egg_donor , sperm_donor ) else : self . reproduce_sexually ( egg_donor , sperm_donor )
12849	def watch_method ( self , method_name , callback ) : try : method = getattr ( self , method_name ) except AttributeError : raise ApiUsageError ( ) if not isinstance ( method , Token . WatchedMethod ) : setattr ( self , method_name , Token . WatchedMethod ( method ) ) method = getattr ( self , method_name ) method . add_watcher ( callback )
6598	def end ( self ) : results = self . communicationChannel . receive ( ) if self . nruns != len ( results ) : import logging logger = logging . getLogger ( __name__ ) logger . warning ( 'too few results received: {} results received, {} expected' . format ( len ( results ) , self . nruns ) ) return results
9308	def get_canonical_headers ( cls , req , include = None ) : if include is None : include = cls . default_include_headers include = [ x . lower ( ) for x in include ] headers = req . headers . copy ( ) if 'host' not in headers : headers [ 'host' ] = urlparse ( req . url ) . netloc . split ( ':' ) [ 0 ] cano_headers_dict = { } for hdr , val in headers . items ( ) : hdr = hdr . strip ( ) . lower ( ) val = cls . amz_norm_whitespace ( val ) . strip ( ) if ( hdr in include or '*' in include or ( 'x-amz-*' in include and hdr . startswith ( 'x-amz-' ) and not hdr == 'x-amz-client-context' ) ) : vals = cano_headers_dict . setdefault ( hdr , [ ] ) vals . append ( val ) cano_headers = '' signed_headers_list = [ ] for hdr in sorted ( cano_headers_dict ) : vals = cano_headers_dict [ hdr ] val = ',' . join ( sorted ( vals ) ) cano_headers += '{}:{}\n' . format ( hdr , val ) signed_headers_list . append ( hdr ) signed_headers = ';' . join ( signed_headers_list ) return ( cano_headers , signed_headers )
956	def getArgumentDescriptions ( f ) : argspec = inspect . getargspec ( f ) docstring = f . __doc__ descriptions = { } if docstring : lines = docstring . split ( '\n' ) i = 0 while i < len ( lines ) : stripped = lines [ i ] . lstrip ( ) if not stripped : i += 1 continue indentLevel = lines [ i ] . index ( stripped [ 0 ] ) firstWord = stripped . split ( ) [ 0 ] if firstWord . endswith ( ':' ) : firstWord = firstWord [ : - 1 ] if firstWord in argspec . args : argName = firstWord restOfLine = stripped [ len ( firstWord ) + 1 : ] . strip ( ) argLines = [ restOfLine ] i += 1 while i < len ( lines ) : stripped = lines [ i ] . lstrip ( ) if not stripped : break if lines [ i ] . index ( stripped [ 0 ] ) <= indentLevel : break argLines . append ( lines [ i ] . strip ( ) ) i += 1 descriptions [ argName ] = ' ' . join ( argLines ) else : i += 1 args = [ ] if argspec . defaults : defaultCount = len ( argspec . defaults ) else : defaultCount = 0 nonDefaultArgCount = len ( argspec . args ) - defaultCount for i , argName in enumerate ( argspec . args ) : if i >= nonDefaultArgCount : defaultValue = argspec . defaults [ i - nonDefaultArgCount ] args . append ( ( argName , descriptions . get ( argName , "" ) , defaultValue ) ) else : args . append ( ( argName , descriptions . get ( argName , "" ) ) ) return args
13083	def chunk ( self , text , reffs ) : if str ( text . id ) in self . chunker : return self . chunker [ str ( text . id ) ] ( text , reffs ) return self . chunker [ "default" ] ( text , reffs )
6317	def image_data ( image ) : data = image . tobytes ( ) components = len ( data ) // ( image . size [ 0 ] * image . size [ 1 ] ) return components , data
1118	def _MakeParallelBenchmark ( p , work_func , * args ) : def Benchmark ( b ) : e = threading . Event ( ) def Target ( ) : e . wait ( ) for _ in xrange ( b . N / p ) : work_func ( * args ) threads = [ ] for _ in xrange ( p ) : t = threading . Thread ( target = Target ) t . start ( ) threads . append ( t ) b . ResetTimer ( ) e . set ( ) for t in threads : t . join ( ) return Benchmark
7335	async def upload_media ( self , file_ , media_type = None , media_category = None , chunked = None , size_limit = None , ** params ) : if isinstance ( file_ , str ) : url = urlparse ( file_ ) if url . scheme . startswith ( 'http' ) : media = await self . _session . get ( file_ ) else : path = urlparse ( file_ ) . path . strip ( " \"'" ) media = await utils . execute ( open ( path , 'rb' ) ) elif hasattr ( file_ , 'read' ) or isinstance ( file_ , bytes ) : media = file_ else : raise TypeError ( "upload_media input must be a file object or a " "filename or binary data or an aiohttp request" ) media_size = await utils . get_size ( media ) if chunked is not None : size_test = False else : size_test = await self . _size_test ( media_size , size_limit ) if isinstance ( media , aiohttp . ClientResponse ) : media = media . content if chunked or ( size_test and chunked is None ) : args = media , media_size , file_ , media_type , media_category response = await self . _chunked_upload ( * args , ** params ) else : response = await self . upload . media . upload . post ( media = media , ** params ) if not hasattr ( file_ , 'read' ) and not getattr ( media , 'closed' , True ) : media . close ( ) return response
8621	def get_self ( session , user_details = None ) : if user_details : user_details [ 'compact' ] = True response = make_get_request ( session , 'self' , params_data = user_details ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise SelfNotRetrievedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
5348	def compose_bugzilla ( projects , data ) : for p in [ project for project in data if len ( data [ project ] [ 'bugzilla' ] ) > 0 ] : if 'bugzilla' not in projects [ p ] : projects [ p ] [ 'bugzilla' ] = [ ] urls = [ url [ 'query_url' ] for url in data [ p ] [ 'bugzilla' ] if url [ 'query_url' ] not in projects [ p ] [ 'bugzilla' ] ] projects [ p ] [ 'bugzilla' ] += urls return projects
3397	def extend_model ( self , exchange_reactions = False , demand_reactions = True ) : for rxn in self . universal . reactions : rxn . gapfilling_type = 'universal' new_metabolites = self . universal . metabolites . query ( lambda metabolite : metabolite not in self . model . metabolites ) self . model . add_metabolites ( new_metabolites ) existing_exchanges = [ ] for rxn in self . universal . boundary : existing_exchanges = existing_exchanges + [ met . id for met in list ( rxn . metabolites ) ] for met in self . model . metabolites : if exchange_reactions : if met . id not in existing_exchanges : rxn = self . universal . add_boundary ( met , type = 'exchange_smiley' , lb = - 1000 , ub = 0 , reaction_id = 'EX_{}' . format ( met . id ) ) rxn . gapfilling_type = 'exchange' if demand_reactions : rxn = self . universal . add_boundary ( met , type = 'demand_smiley' , lb = 0 , ub = 1000 , reaction_id = 'DM_{}' . format ( met . id ) ) rxn . gapfilling_type = 'demand' new_reactions = self . universal . reactions . query ( lambda reaction : reaction not in self . model . reactions ) self . model . add_reactions ( new_reactions )
9098	def write_bel_namespace ( self , file : TextIO , use_names : bool = False ) -> None : if not self . is_populated ( ) : self . populate ( ) if use_names and not self . has_names : raise ValueError values = ( self . _get_namespace_name_to_encoding ( desc = 'writing names' ) if use_names else self . _get_namespace_identifier_to_encoding ( desc = 'writing identifiers' ) ) write_namespace ( namespace_name = self . _get_namespace_name ( ) , namespace_keyword = self . _get_namespace_keyword ( ) , namespace_query_url = self . identifiers_url , values = values , file = file , )
3047	def _do_retrieve_scopes ( self , http , token ) : logger . info ( 'Refreshing scopes' ) query_params = { 'access_token' : token , 'fields' : 'scope' } token_info_uri = _helpers . update_query_params ( self . token_info_uri , query_params ) resp , content = transport . request ( http , token_info_uri ) content = _helpers . _from_bytes ( content ) if resp . status == http_client . OK : d = json . loads ( content ) self . scopes = set ( _helpers . string_to_scopes ( d . get ( 'scope' , '' ) ) ) else : error_msg = 'Invalid response {0}.' . format ( resp . status ) try : d = json . loads ( content ) if 'error_description' in d : error_msg = d [ 'error_description' ] except ( TypeError , ValueError ) : pass raise Error ( error_msg )
11477	def _create_or_reuse_folder ( local_folder , parent_folder_id , reuse_existing = False ) : local_folder_name = os . path . basename ( local_folder ) folder_id = None if reuse_existing : children = session . communicator . folder_children ( session . token , parent_folder_id ) folders = children [ 'folders' ] for folder in folders : if folder [ 'name' ] == local_folder_name : folder_id = folder [ 'folder_id' ] break if folder_id is None : new_folder = session . communicator . create_folder ( session . token , local_folder_name , parent_folder_id ) folder_id = new_folder [ 'folder_id' ] return folder_id
12119	def headerHTML ( self , fname = None ) : if fname is None : fname = self . fname . replace ( ".abf" , "_header.html" ) html = "<html><body><code>" html += "<h2>abfinfo() for %s.abf</h2>" % self . ID html += self . abfinfo ( ) . replace ( "<" , "&lt;" ) . replace ( ">" , "&gt;" ) . replace ( "\n" , "<br>" ) html += "<h2>Header for %s.abf</h2>" % self . ID html += pprint . pformat ( self . header , indent = 1 ) html = html . replace ( "\n" , '<br>' ) . replace ( " " , "&nbsp;" ) html = html . replace ( r"\x00" , "" ) html += "</code></body></html>" print ( "WRITING HEADER TO:" ) print ( fname ) f = open ( fname , 'w' ) f . write ( html ) f . close ( )
11068	def delete_acl ( self , name ) : if name not in self . _acl : return False del self . _acl [ name ] return True
228	def get_long_short_pos ( positions ) : pos_wo_cash = positions . drop ( 'cash' , axis = 1 ) longs = pos_wo_cash [ pos_wo_cash > 0 ] . sum ( axis = 1 ) . fillna ( 0 ) shorts = pos_wo_cash [ pos_wo_cash < 0 ] . sum ( axis = 1 ) . fillna ( 0 ) cash = positions . cash net_liquidation = longs + shorts + cash df_pos = pd . DataFrame ( { 'long' : longs . divide ( net_liquidation , axis = 'index' ) , 'short' : shorts . divide ( net_liquidation , axis = 'index' ) } ) df_pos [ 'net exposure' ] = df_pos [ 'long' ] + df_pos [ 'short' ] return df_pos
9931	def get_repr ( self , obj , referent = None ) : objtype = type ( obj ) typename = str ( objtype . __module__ ) + "." + objtype . __name__ prettytype = typename . replace ( "__builtin__." , "" ) name = getattr ( obj , "__name__" , "" ) if name : prettytype = "%s %r" % ( prettytype , name ) key = "" if referent : key = self . get_refkey ( obj , referent ) url = reverse ( 'dowser_trace_object' , args = ( typename , id ( obj ) ) ) return ( '<a class="objectid" href="%s">%s</a> ' '<span class="typename">%s</span>%s<br />' '<span class="repr">%s</span>' % ( url , id ( obj ) , prettytype , key , get_repr ( obj , 100 ) ) )
13692	def broadcast_tx ( self , address , amount , secret , secondsecret = None , vendorfield = '' ) : peer = random . choice ( self . PEERS ) park = Park ( peer , 4001 , constants . ARK_NETHASH , '1.1.1' ) return park . transactions ( ) . create ( address , str ( amount ) , vendorfield , secret , secondsecret )
11332	def err ( format_msg , * args , ** kwargs ) : exc_info = kwargs . pop ( "exc_info" , False ) stderr . warning ( str ( format_msg ) . format ( * args , ** kwargs ) , exc_info = exc_info )
7298	def get_form_field_class ( model_field ) : FIELD_MAPPING = { IntField : forms . IntegerField , StringField : forms . CharField , FloatField : forms . FloatField , BooleanField : forms . BooleanField , DateTimeField : forms . DateTimeField , DecimalField : forms . DecimalField , URLField : forms . URLField , EmailField : forms . EmailField } return FIELD_MAPPING . get ( model_field . __class__ , forms . CharField )
5193	def send_select_and_operate_command_set ( self , command_set , callback = asiodnp3 . PrintingCommandCallback . Get ( ) , config = opendnp3 . TaskConfig ( ) . Default ( ) ) : self . master . SelectAndOperate ( command_set , callback , config )
12423	def load ( fp , separator = DEFAULT , index_separator = DEFAULT , cls = dict , list_cls = list ) : converter = None output = cls ( ) arraykeys = set ( ) for line in fp : if converter is None : if isinstance ( line , six . text_type ) : converter = six . u else : converter = six . b default_separator = converter ( '|' ) default_index_separator = converter ( '_' ) newline = converter ( '\n' ) if separator is DEFAULT : separator = default_separator if index_separator is DEFAULT : index_separator = default_index_separator key , value = line . strip ( ) . split ( separator , 1 ) keyparts = key . split ( index_separator ) try : index = int ( keyparts [ - 1 ] ) endwithint = True except ValueError : endwithint = False if len ( keyparts ) > 1 and endwithint : basekey = key . rsplit ( index_separator , 1 ) [ 0 ] if basekey not in arraykeys : arraykeys . add ( basekey ) if basekey in output : if not isinstance ( output [ basekey ] , dict ) : output [ basekey ] = { - 1 : output [ basekey ] } else : output [ basekey ] = { } output [ basekey ] [ index ] = value else : if key in output and isinstance ( output [ key ] , dict ) : output [ key ] [ - 1 ] = value else : output [ key ] = value for key in arraykeys : output [ key ] = list_cls ( pair [ 1 ] for pair in sorted ( six . iteritems ( output [ key ] ) ) ) return output
10478	def _waitFor ( self , timeout , notification , ** kwargs ) : callback = self . _matchOther retelem = None callbackArgs = None callbackKwargs = None if 'callback' in kwargs : callback = kwargs [ 'callback' ] del kwargs [ 'callback' ] if 'args' in kwargs : if not isinstance ( kwargs [ 'args' ] , tuple ) : errStr = 'Notification callback args not given as a tuple' raise TypeError ( errStr ) callbackArgs = kwargs [ 'args' ] del kwargs [ 'args' ] if 'kwargs' in kwargs : if not isinstance ( kwargs [ 'kwargs' ] , dict ) : errStr = 'Notification callback kwargs not given as a dict' raise TypeError ( errStr ) callbackKwargs = kwargs [ 'kwargs' ] del kwargs [ 'kwargs' ] if kwargs : if callbackKwargs : callbackKwargs . update ( kwargs ) else : callbackKwargs = kwargs else : callbackArgs = ( retelem , ) callbackKwargs = kwargs return self . _setNotification ( timeout , notification , callback , callbackArgs , callbackKwargs )
6889	def _parallel_bls_worker ( task ) : try : times , mags , errs = task [ : 3 ] magsarefluxes = task [ 3 ] minfreq , nfreq , stepsize = task [ 4 : 7 ] ndurations , mintransitduration , maxtransitduration = task [ 7 : 10 ] blsobjective , blsmethod , blsoversample = task [ 10 : ] frequencies = minfreq + nparange ( nfreq ) * stepsize periods = 1.0 / frequencies durations = nplinspace ( mintransitduration * periods . min ( ) , maxtransitduration * periods . min ( ) , ndurations ) if magsarefluxes : blsmodel = BoxLeastSquares ( times * u . day , mags * u . dimensionless_unscaled , dy = errs * u . dimensionless_unscaled ) else : blsmodel = BoxLeastSquares ( times * u . day , mags * u . mag , dy = errs * u . mag ) blsresult = blsmodel . power ( periods * u . day , durations * u . day , objective = blsobjective , method = blsmethod , oversample = blsoversample ) return { 'blsresult' : blsresult , 'blsmodel' : blsmodel , 'durations' : durations , 'power' : nparray ( blsresult . power ) } except Exception as e : LOGEXCEPTION ( 'BLS for frequency chunk: (%.6f, %.6f) failed.' % ( frequencies [ 0 ] , frequencies [ - 1 ] ) ) return { 'blsresult' : None , 'blsmodel' : None , 'durations' : durations , 'power' : nparray ( [ npnan for x in range ( nfreq ) ] ) , }
11091	def select_file ( self , filters = all_true , recursive = True ) : for p in self . select ( filters , recursive ) : if p . is_file ( ) : yield p
7765	def disconnect ( self ) : with self . lock : if self . stream : if self . settings [ u"initial_presence" ] : self . send ( Presence ( stanza_type = "unavailable" ) ) self . stream . disconnect ( )
13055	def overview ( ) : search = Service . search ( ) search = search . filter ( "term" , state = 'open' ) search . aggs . bucket ( 'port_count' , 'terms' , field = 'port' , order = { '_count' : 'desc' } , size = 100 ) . metric ( 'unique_count' , 'cardinality' , field = 'address' ) response = search . execute ( ) print_line ( "Port Count" ) print_line ( "---------------" ) for entry in response . aggregations . port_count . buckets : print_line ( "{0:<7} {1}" . format ( entry . key , entry . unique_count . value ) )
1590	def _get_dict_from_config ( topology_config ) : config = { } for kv in topology_config . kvs : if kv . HasField ( "value" ) : assert kv . type == topology_pb2 . ConfigValueType . Value ( "STRING_VALUE" ) if PhysicalPlanHelper . _is_number ( kv . value ) : config [ kv . key ] = PhysicalPlanHelper . _get_number ( kv . value ) elif kv . value . lower ( ) in ( "true" , "false" ) : config [ kv . key ] = True if kv . value . lower ( ) == "true" else False else : config [ kv . key ] = kv . value elif kv . HasField ( "serialized_value" ) and kv . type == topology_pb2 . ConfigValueType . Value ( "PYTHON_SERIALIZED_VALUE" ) : config [ kv . key ] = default_serializer . deserialize ( kv . serialized_value ) else : assert kv . HasField ( "type" ) Log . error ( "Unsupported config <key:value> found: %s, with type: %s" % ( str ( kv ) , str ( kv . type ) ) ) continue return config
8221	def do_toggle_fullscreen ( self , action ) : is_fullscreen = action . get_active ( ) if is_fullscreen : self . fullscreen ( ) else : self . unfullscreen ( )
7840	def get_name ( self ) : var = self . xmlnode . prop ( "name" ) if not var : var = "" return var . decode ( "utf-8" )
10370	def build_edge_data_filter ( annotations : Mapping , partial_match : bool = True ) -> EdgePredicate : @ edge_predicate def annotation_dict_filter ( data : EdgeData ) -> bool : return subdict_matches ( data , annotations , partial_match = partial_match ) return annotation_dict_filter
1179	def split ( self , string , maxsplit = 0 ) : splitlist = [ ] state = _State ( string , 0 , sys . maxint , self . flags ) n = 0 last = state . start while not maxsplit or n < maxsplit : state . reset ( ) state . string_position = state . start if not state . search ( self . _code ) : break if state . start == state . string_position : if last == state . end : break state . start += 1 continue splitlist . append ( string [ last : state . start ] ) if self . groups : match = SRE_Match ( self , state ) splitlist += ( list ( match . groups ( None ) ) ) n += 1 last = state . start = state . string_position splitlist . append ( string [ last : state . end ] ) return splitlist
9641	def _display_details ( var_data ) : meta_keys = ( key for key in list ( var_data . keys ( ) ) if key . startswith ( 'META_' ) ) for key in meta_keys : display_key = key [ 5 : ] . capitalize ( ) pprint ( '{0}: {1}' . format ( display_key , var_data . pop ( key ) ) ) pprint ( var_data )
3747	def calculate ( self , T , P , zs , ws , method ) : r if method == MIXING_LOG_MOLAR : mus = [ i ( T , P ) for i in self . ViscosityLiquids ] return mixing_logarithmic ( zs , mus ) elif method == MIXING_LOG_MASS : mus = [ i ( T , P ) for i in self . ViscosityLiquids ] return mixing_logarithmic ( ws , mus ) elif method == LALIBERTE_MU : ws = list ( ws ) ws . pop ( self . index_w ) return Laliberte_viscosity ( T , ws , self . wCASs ) else : raise Exception ( 'Method not valid' )
1653	def CheckGlobalStatic ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] if linenum + 1 < clean_lines . NumLines ( ) and not Search ( r'[;({]' , line ) : line += clean_lines . elided [ linenum + 1 ] . strip ( ) match = Match ( r'((?:|static +)(?:|const +))(?::*std::)?string( +const)? +' r'([a-zA-Z0-9_:]+)\b(.*)' , line ) if ( match and not Search ( r'\bstring\b(\s+const)?\s*[\*\&]\s*(const\s+)?\w' , line ) and not Search ( r'\boperator\W' , line ) and not Match ( r'\s*(<.*>)?(::[a-zA-Z0-9_]+)*\s*\(([^"]|$)' , match . group ( 4 ) ) ) : if Search ( r'\bconst\b' , line ) : error ( filename , linenum , 'runtime/string' , 4 , 'For a static/global string constant, use a C style string ' 'instead: "%schar%s %s[]".' % ( match . group ( 1 ) , match . group ( 2 ) or '' , match . group ( 3 ) ) ) else : error ( filename , linenum , 'runtime/string' , 4 , 'Static/global string variables are not permitted.' ) if ( Search ( r'\b([A-Za-z0-9_]*_)\(\1\)' , line ) or Search ( r'\b([A-Za-z0-9_]*_)\(CHECK_NOTNULL\(\1\)\)' , line ) ) : error ( filename , linenum , 'runtime/init' , 4 , 'You seem to be initializing a member variable with itself.' )
5717	def push_datapackage ( descriptor , backend , ** backend_options ) : warnings . warn ( 'Functions "push/pull_datapackage" are deprecated. ' 'Please use "Package" class' , UserWarning ) tables = [ ] schemas = [ ] datamap = { } mapping = { } model = Package ( descriptor ) plugin = import_module ( 'jsontableschema.plugins.%s' % backend ) storage = plugin . Storage ( ** backend_options ) for resource in model . resources : if not resource . tabular : continue name = resource . descriptor . get ( 'name' , None ) table = _convert_path ( resource . descriptor [ 'path' ] , name ) schema = resource . descriptor [ 'schema' ] data = resource . table . iter ( keyed = True ) def values ( schema , data ) : for item in data : row = [ ] for field in schema [ 'fields' ] : row . append ( item . get ( field [ 'name' ] , None ) ) yield tuple ( row ) tables . append ( table ) schemas . append ( schema ) datamap [ table ] = values ( schema , data ) if name is not None : mapping [ name ] = table schemas = _convert_schemas ( mapping , schemas ) for table in tables : if table in storage . buckets : storage . delete ( table ) storage . create ( tables , schemas ) for table in storage . buckets : if table in datamap : storage . write ( table , datamap [ table ] ) return storage
10862	def param_particle_rad ( self , ind ) : ind = self . _vps ( listify ( ind ) ) return [ self . _i2p ( i , 'a' ) for i in ind ]
3911	def _on_event ( self , _ ) : self . sort ( key = lambda conv_button : conv_button . last_modified , reverse = True )
592	def _compute ( self , inputs , outputs ) : if self . _sfdr is None : raise RuntimeError ( "Spatial pooler has not been initialized" ) if not self . topDownMode : self . _iterations += 1 buInputVector = inputs [ 'bottomUpIn' ] resetSignal = False if 'resetIn' in inputs : assert len ( inputs [ 'resetIn' ] ) == 1 resetSignal = inputs [ 'resetIn' ] [ 0 ] != 0 rfOutput = self . _doBottomUpCompute ( rfInput = buInputVector . reshape ( ( 1 , buInputVector . size ) ) , resetSignal = resetSignal ) outputs [ 'bottomUpOut' ] [ : ] = rfOutput . flat else : topDownIn = inputs . get ( 'topDownIn' , None ) spatialTopDownOut , temporalTopDownOut = self . _doTopDownInfer ( topDownIn ) outputs [ 'spatialTopDownOut' ] [ : ] = spatialTopDownOut if temporalTopDownOut is not None : outputs [ 'temporalTopDownOut' ] [ : ] = temporalTopDownOut outputs [ 'anomalyScore' ] [ : ] = 0
653	def spDiff ( SP1 , SP2 ) : if ( len ( SP1 . _masterConnectedM ) != len ( SP2 . _masterConnectedM ) ) : print "Connected synapse matrices are different sizes" return False if ( len ( SP1 . _masterPotentialM ) != len ( SP2 . _masterPotentialM ) ) : print "Potential synapse matrices are different sizes" return False if ( len ( SP1 . _masterPermanenceM ) != len ( SP2 . _masterPermanenceM ) ) : print "Permanence matrices are different sizes" return False for i in range ( 0 , len ( SP1 . _masterConnectedM ) ) : connected1 = SP1 . _masterConnectedM [ i ] connected2 = SP2 . _masterConnectedM [ i ] if ( connected1 != connected2 ) : print "Connected Matrices for cell %d different" % ( i ) return False permanences1 = SP1 . _masterPermanenceM [ i ] permanences2 = SP2 . _masterPermanenceM [ i ] if ( permanences1 != permanences2 ) : print "Permanence Matrices for cell %d different" % ( i ) return False potential1 = SP1 . _masterPotentialM [ i ] potential2 = SP2 . _masterPotentialM [ i ] if ( potential1 != potential2 ) : print "Potential Matrices for cell %d different" % ( i ) return False if ( not numpy . array_equal ( SP1 . _firingBoostFactors , SP2 . _firingBoostFactors ) ) : print "Firing boost factors are different between spatial poolers" return False if ( not numpy . array_equal ( SP1 . _dutyCycleAfterInh , SP2 . _dutyCycleAfterInh ) ) : print "Duty cycles after inhibition are different between spatial poolers" return False if ( not numpy . array_equal ( SP1 . _dutyCycleBeforeInh , SP2 . _dutyCycleBeforeInh ) ) : print "Duty cycles before inhibition are different between spatial poolers" return False print ( "Spatial Poolers are equivalent" ) return True
7211	def stderr ( self ) : if not self . id : raise WorkflowError ( 'Workflow is not running. Cannot get stderr.' ) if self . batch_values : raise NotImplementedError ( "Query Each Workflow Id within the Batch Workflow for stderr." ) wf = self . workflow . get ( self . id ) stderr_list = [ ] for task in wf [ 'tasks' ] : stderr_list . append ( { 'id' : task [ 'id' ] , 'taskType' : task [ 'taskType' ] , 'name' : task [ 'name' ] , 'stderr' : self . workflow . get_stderr ( self . id , task [ 'id' ] ) } ) return stderr_list
11754	def parse_definite_clause ( s ) : "Return the antecedents and the consequent of a definite clause." assert is_definite_clause ( s ) if is_symbol ( s . op ) : return [ ] , s else : antecedent , consequent = s . args return conjuncts ( antecedent ) , consequent
3391	def prune_unused_metabolites ( cobra_model ) : output_model = cobra_model . copy ( ) inactive_metabolites = [ m for m in output_model . metabolites if len ( m . reactions ) == 0 ] output_model . remove_metabolites ( inactive_metabolites ) return output_model , inactive_metabolites
9758	def restart ( ctx , copy , file , u ) : config = None update_code = None if file : config = rhea . read ( file ) if u : ctx . invoke ( upload , sync = False ) update_code = True user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) try : if copy : response = PolyaxonClient ( ) . experiment . copy ( user , project_name , _experiment , config = config , update_code = update_code ) Printer . print_success ( 'Experiment was copied with id {}' . format ( response . id ) ) else : response = PolyaxonClient ( ) . experiment . restart ( user , project_name , _experiment , config = config , update_code = update_code ) Printer . print_success ( 'Experiment was restarted with id {}' . format ( response . id ) ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not restart experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 )
9354	def body ( quantity = 2 , separator = '\n\n' , wrap_start = '' , wrap_end = '' , html = False , sentences_quantity = 3 , as_list = False ) : return lorem_ipsum . paragraphs ( quantity = quantity , separator = separator , wrap_start = wrap_start , wrap_end = wrap_end , html = html , sentences_quantity = sentences_quantity , as_list = as_list )
8944	def pushd ( path ) : saved = os . getcwd ( ) os . chdir ( path ) try : yield saved finally : os . chdir ( saved )
12859	def to_date ( self ) : y , m , d = self . to_ymd ( ) return date ( y , m , d )
5724	def verify_valid_gdb_subprocess ( self ) : if not self . gdb_process : raise NoGdbProcessError ( "gdb process is not attached" ) elif self . gdb_process . poll ( ) is not None : raise NoGdbProcessError ( "gdb process has already finished with return code: %s" % str ( self . gdb_process . poll ( ) ) )
5524	def jenks_breaks ( values , nb_class ) : if not isinstance ( values , Iterable ) or isinstance ( values , ( str , bytes ) ) : raise TypeError ( "A sequence of numbers is expected" ) if isinstance ( nb_class , float ) and int ( nb_class ) == nb_class : nb_class = int ( nb_class ) if not isinstance ( nb_class , int ) : raise TypeError ( "Number of class have to be a positive integer: " "expected an instance of 'int' but found {}" . format ( type ( nb_class ) ) ) nb_values = len ( values ) if np and isinstance ( values , np . ndarray ) : values = values [ np . argwhere ( np . isfinite ( values ) ) . reshape ( - 1 ) ] else : values = [ i for i in values if isfinite ( i ) ] if len ( values ) != nb_values : warnings . warn ( 'Invalid values encountered (NaN or Inf) were ignored' ) nb_values = len ( values ) if nb_class >= nb_values or nb_class < 2 : raise ValueError ( "Number of class have to be an integer " "greater than 2 and " "smaller than the number of values to use" ) return jenks . _jenks_breaks ( values , nb_class )
2044	def get_storage_data ( self , storage_address , offset ) : value = self . _world_state [ storage_address ] [ 'storage' ] . get ( offset , 0 ) return simplify ( value )
10944	def reset ( self , new_region_size = None , do_calc_size = True , new_damping = None , new_max_mem = None ) : if new_region_size is not None : self . region_size = new_region_size if new_max_mem != None : self . max_mem = new_max_mem if do_calc_size : self . region_size = calc_particle_group_region_size ( self . state , region_size = self . region_size , max_mem = self . max_mem ) self . stats = [ ] self . particle_groups = separate_particles_into_groups ( self . state , self . region_size , doshift = 'rand' ) if new_damping is not None : self . _kwargs . update ( { 'damping' : new_damping } ) if self . save_J : if len ( self . particle_groups ) > 90 : CLOG . warn ( 'Attempting to create many open files. Consider increasing max_mem and/or region_size to avoid crashes.' ) self . _tempfiles = [ ] self . _has_saved_J = [ ] for a in range ( len ( self . particle_groups ) ) : for _ in [ 'j' , 'tile' ] : self . _tempfiles . append ( tempfile . TemporaryFile ( dir = os . getcwd ( ) ) ) self . _has_saved_J . append ( False )
6363	def dist ( self , src , tar ) : if src == tar : return 0.0 src_comp = self . _rle . encode ( self . _bwt . encode ( src ) ) tar_comp = self . _rle . encode ( self . _bwt . encode ( tar ) ) concat_comp = self . _rle . encode ( self . _bwt . encode ( src + tar ) ) concat_comp2 = self . _rle . encode ( self . _bwt . encode ( tar + src ) ) return ( min ( len ( concat_comp ) , len ( concat_comp2 ) ) - min ( len ( src_comp ) , len ( tar_comp ) ) ) / max ( len ( src_comp ) , len ( tar_comp ) )
5270	def _get_word_start_index ( self , idx ) : i = 0 for _idx in self . word_starts [ 1 : ] : if idx < _idx : return i else : i += 1 return i
7083	def fourier_sinusoidal_residual ( fourierparams , times , mags , errs ) : modelmags , phase , ptimes , pmags , perrs = ( fourier_sinusoidal_func ( fourierparams , times , mags , errs ) ) return ( pmags - modelmags ) / perrs
1274	def from_spec ( spec , kwargs = None ) : memory = util . get_object ( obj = spec , predefined_objects = tensorforce . core . memories . memories , kwargs = kwargs ) assert isinstance ( memory , Memory ) return memory
1883	def new_symbolic_value ( self , nbits , label = None , taint = frozenset ( ) ) : assert nbits in ( 1 , 4 , 8 , 16 , 32 , 64 , 128 , 256 ) avoid_collisions = False if label is None : label = 'val' avoid_collisions = True expr = self . _constraints . new_bitvec ( nbits , name = label , taint = taint , avoid_collisions = avoid_collisions ) self . _input_symbols . append ( expr ) return expr
1754	def write_register ( self , register , value ) : self . _publish ( 'will_write_register' , register , value ) value = self . _regfile . write ( register , value ) self . _publish ( 'did_write_register' , register , value ) return value
5970	def MD_restrained ( dirname = 'MD_POSRES' , ** kwargs ) : logger . info ( "[{dirname!s}] Setting up MD with position restraints..." . format ( ** vars ( ) ) ) kwargs . setdefault ( 'struct' , 'em/em.pdb' ) kwargs . setdefault ( 'qname' , 'PR_GMX' ) kwargs . setdefault ( 'define' , '-DPOSRES' ) kwargs . setdefault ( 'nstxout' , '50000' ) kwargs . setdefault ( 'nstvout' , '50000' ) kwargs . setdefault ( 'nstfout' , '0' ) kwargs . setdefault ( 'nstlog' , '500' ) kwargs . setdefault ( 'nstenergy' , '2500' ) kwargs . setdefault ( 'nstxtcout' , '5000' ) kwargs . setdefault ( 'refcoord_scaling' , 'com' ) kwargs . setdefault ( 'Pcoupl' , "Berendsen" ) new_kwargs = _setup_MD ( dirname , ** kwargs ) new_kwargs . pop ( 'define' , None ) new_kwargs . pop ( 'refcoord_scaling' , None ) new_kwargs . pop ( 'Pcoupl' , None ) return new_kwargs
6506	def process_result ( cls , dictionary , match_phrase , user ) : result_processor = _load_class ( getattr ( settings , "SEARCH_RESULT_PROCESSOR" , None ) , cls ) srp = result_processor ( dictionary , match_phrase ) if srp . should_remove ( user ) : return None try : srp . add_properties ( ) except Exception as ex : log . exception ( "error processing properties for %s - %s: will remove from results" , json . dumps ( dictionary , cls = DjangoJSONEncoder ) , str ( ex ) ) return None return dictionary
3897	def generate_message_doc ( message_descriptor , locations , path , name_prefix = '' ) : prefixed_name = name_prefix + message_descriptor . name print ( make_subsection ( prefixed_name ) ) location = locations [ path ] if location . HasField ( 'leading_comments' ) : print ( textwrap . dedent ( location . leading_comments ) ) row_tuples = [ ] for field_index , field in enumerate ( message_descriptor . field ) : field_location = locations [ path + ( 2 , field_index ) ] if field . type not in [ 11 , 14 ] : type_str = TYPE_TO_STR [ field . type ] else : type_str = make_link ( field . type_name . lstrip ( '.' ) ) row_tuples . append ( ( make_code ( field . name ) , field . number , type_str , LABEL_TO_STR [ field . label ] , textwrap . fill ( get_comment_from_location ( field_location ) , INFINITY ) , ) ) print_table ( ( 'Field' , 'Number' , 'Type' , 'Label' , 'Description' ) , row_tuples ) nested_types = enumerate ( message_descriptor . nested_type ) for index , nested_message_desc in nested_types : generate_message_doc ( nested_message_desc , locations , path + ( 3 , index ) , name_prefix = prefixed_name + '.' ) for index , nested_enum_desc in enumerate ( message_descriptor . enum_type ) : generate_enum_doc ( nested_enum_desc , locations , path + ( 4 , index ) , name_prefix = prefixed_name + '.' )
962	def matchPatterns ( patterns , keys ) : results = [ ] if patterns : for pattern in patterns : prog = re . compile ( pattern ) for key in keys : if prog . match ( key ) : results . append ( key ) else : return None return results
1041	def source_lines ( self ) : return [ self . source_buffer . source_line ( line ) for line in range ( self . line ( ) , self . end ( ) . line ( ) + 1 ) ]
5088	def ecommerce_coupon_url ( self , instance ) : if not instance . entitlement_id : return "N/A" return format_html ( '<a href="{base_url}/coupons/{id}" target="_blank">View coupon "{id}" details</a>' , base_url = settings . ECOMMERCE_PUBLIC_URL_ROOT , id = instance . entitlement_id )
1693	def map ( self , map_function ) : from heronpy . streamlet . impl . mapbolt import MapStreamlet map_streamlet = MapStreamlet ( map_function , self ) self . _add_child ( map_streamlet ) return map_streamlet
10953	def set_model ( self , mdl ) : self . mdl = mdl self . mdl . check_inputs ( self . comps ) for c in self . comps : setattr ( self , '_comp_' + c . category , c )
8725	def at_time ( cls , at , target ) : at = cls . _from_timestamp ( at ) cmd = cls . from_datetime ( at ) cmd . delay = at - now ( ) cmd . target = target return cmd
7368	async def read ( response , loads = loads , encoding = None ) : ctype = response . headers . get ( 'Content-Type' , "" ) . lower ( ) try : if "application/json" in ctype : logger . info ( "decoding data as json" ) return await response . json ( encoding = encoding , loads = loads ) if "text" in ctype : logger . info ( "decoding data as text" ) return await response . text ( encoding = encoding ) except ( UnicodeDecodeError , json . JSONDecodeError ) as exc : data = await response . read ( ) raise exceptions . PeonyDecodeError ( response = response , data = data , exception = exc ) return await response . read ( )
2707	def top_sentences ( kernel , path ) : key_sent = { } i = 0 if isinstance ( path , str ) : path = json_iter ( path ) for meta in path : graf = meta [ "graf" ] tagged_sent = [ WordNode . _make ( x ) for x in graf ] text = " " . join ( [ w . raw for w in tagged_sent ] ) m_sent = mh_digest ( [ str ( w . word_id ) for w in tagged_sent ] ) dist = sum ( [ m_sent . jaccard ( m ) * rl . rank for rl , m in kernel ] ) key_sent [ text ] = ( dist , i ) i += 1 for text , ( dist , i ) in sorted ( key_sent . items ( ) , key = lambda x : x [ 1 ] [ 0 ] , reverse = True ) : yield SummarySent ( dist = dist , idx = i , text = text )
9778	def login ( token , username , password ) : auth_client = PolyaxonClient ( ) . auth if username : if not password : password = click . prompt ( 'Please enter your password' , type = str , hide_input = True ) password = password . strip ( ) if not password : logger . info ( 'You entered an empty string. ' 'Please make sure you enter your password correctly.' ) sys . exit ( 1 ) credentials = CredentialsConfig ( username = username , password = password ) try : access_code = auth_client . login ( credentials = credentials ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not login.' ) Printer . print_error ( 'Error Message `{}`.' . format ( e ) ) sys . exit ( 1 ) if not access_code : Printer . print_error ( "Failed to login" ) return else : if not token : token_url = "{}/app/token" . format ( auth_client . config . http_host ) click . confirm ( 'Authentication token page will now open in your browser. Continue?' , abort = True , default = True ) click . launch ( token_url ) logger . info ( "Please copy and paste the authentication token." ) token = click . prompt ( 'This is an invisible field. Paste token and press ENTER' , type = str , hide_input = True ) if not token : logger . info ( "Empty token received. " "Make sure your shell is handling the token appropriately." ) logger . info ( "See docs for help: http://docs.polyaxon.com/polyaxon_cli/commands/auth" ) return access_code = token . strip ( " " ) try : AuthConfigManager . purge ( ) user = PolyaxonClient ( ) . auth . get_user ( token = access_code ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not load user info.' ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) access_token = AccessTokenConfig ( username = user . username , token = access_code ) AuthConfigManager . set_config ( access_token ) Printer . print_success ( "Login successful" ) server_version = get_server_version ( ) current_version = get_current_version ( ) log_handler = get_log_handler ( ) CliConfigManager . reset ( check_count = 0 , current_version = current_version , min_version = server_version . min_version , log_handler = log_handler )
9029	def _step ( self , row , position , passed ) : if row in passed or not self . _row_should_be_placed ( row , position ) : return self . _place_row ( row , position ) passed = [ row ] + passed for i , produced_mesh in enumerate ( row . produced_meshes ) : self . _expand_produced_mesh ( produced_mesh , i , position , passed ) for i , consumed_mesh in enumerate ( row . consumed_meshes ) : self . _expand_consumed_mesh ( consumed_mesh , i , position , passed )
7676	def intervals ( annotation , ** kwargs ) : times , labels = annotation . to_interval_values ( ) return mir_eval . display . labeled_intervals ( times , labels , ** kwargs )
7418	def make ( assembly , samples ) : longname = max ( [ len ( i ) for i in assembly . samples . keys ( ) ] ) names = [ i . name for i in samples ] partitions = makephy ( assembly , samples , longname ) makenex ( assembly , names , longname , partitions )
10146	def from_path ( self , path ) : path_components = path . split ( '/' ) param_names = [ comp [ 1 : - 1 ] for comp in path_components if comp . startswith ( '{' ) and comp . endswith ( '}' ) ] params = [ ] for name in param_names : param_schema = colander . SchemaNode ( colander . String ( ) , name = name ) param = self . parameter_converter ( 'path' , param_schema ) if self . ref : param = self . _ref ( param ) params . append ( param ) return params
10674	def load_data_auxi ( path = '' ) : compounds . clear ( ) if path == '' : path = default_data_path if not os . path . exists ( path ) : warnings . warn ( 'The specified data file path does not exist. (%s)' % path ) return files = glob . glob ( os . path . join ( path , 'Compound_*.json' ) ) for file in files : compound = Compound . read ( file ) compounds [ compound . formula ] = compound
8668	def lock_key ( key_name , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase ) try : click . echo ( 'Locking key...' ) stash . lock ( key_name = key_name ) click . echo ( 'Key locked successfully' ) except GhostError as ex : sys . exit ( ex )
12140	def load_table ( self , table ) : items , data_keys = [ ] , None for key , filename in table . items ( ) : data_dict = self . filetype . data ( filename [ 0 ] ) current_keys = tuple ( sorted ( data_dict . keys ( ) ) ) values = [ data_dict [ k ] for k in current_keys ] if data_keys is None : data_keys = current_keys elif data_keys != current_keys : raise Exception ( "Data keys are inconsistent" ) items . append ( ( key , values ) ) return Table ( items , kdims = table . kdims , vdims = data_keys )
6342	def docs_of_words ( self ) : r return [ [ words for sents in doc for words in sents ] for doc in self . corpus ]
928	def _getEndTime ( self , t ) : assert isinstance ( t , datetime . datetime ) if self . _aggTimeDelta : return t + self . _aggTimeDelta else : year = t . year + self . _aggYears + ( t . month - 1 + self . _aggMonths ) / 12 month = ( t . month - 1 + self . _aggMonths ) % 12 + 1 return t . replace ( year = year , month = month )
10037	def execute ( helper , config , args ) : out ( "Available solution stacks" ) for stack in helper . list_available_solution_stacks ( ) : out ( " " + str ( stack ) ) return 0
7816	def add_handler ( self , handler ) : if not isinstance ( handler , EventHandler ) : raise TypeError , "Not an EventHandler" with self . lock : if handler in self . handlers : return self . handlers . append ( handler ) self . _update_handlers ( )
13167	def insert ( self , before , name , attrs = None , data = None ) : if isinstance ( before , self . __class__ ) : if before . parent != self : raise ValueError ( 'Cannot insert before an element with a different parent.' ) before = before . index before = min ( max ( 0 , before ) , len ( self . _children ) ) elem = self . __class__ ( name , attrs , data , parent = self , index = before ) self . _children . insert ( before , elem ) for idx , c in enumerate ( self . _children ) : c . index = idx return elem
8769	def create_job ( context , body ) : LOG . info ( "create_job for tenant %s" % context . tenant_id ) if not context . is_admin : raise n_exc . NotAuthorized ( ) job = body . get ( 'job' ) if 'parent_id' in job : parent_id = job [ 'parent_id' ] if not parent_id : raise q_exc . JobNotFound ( job_id = parent_id ) parent_job = db_api . async_transaction_find ( context , id = parent_id , scope = db_api . ONE ) if not parent_job : raise q_exc . JobNotFound ( job_id = parent_id ) tid = parent_id if parent_job . get ( 'transaction_id' ) : tid = parent_job . get ( 'transaction_id' ) job [ 'transaction_id' ] = tid if not job : raise n_exc . BadRequest ( resource = "job" , msg = "Invalid request body." ) with context . session . begin ( subtransactions = True ) : new_job = db_api . async_transaction_create ( context , ** job ) return v . _make_job_dict ( new_job )
8224	def _mouse_pointer_moved ( self , x , y ) : self . _namespace [ 'MOUSEX' ] = x self . _namespace [ 'MOUSEY' ] = y
2091	def copy ( self , pk = None , new_name = None , ** kwargs ) : orig = self . read ( pk , fail_on_no_results = True , fail_on_multiple_results = True ) orig = orig [ 'results' ] [ 0 ] self . _pop_none ( kwargs ) newresource = copy ( orig ) newresource . pop ( 'id' ) basename = newresource [ 'name' ] . split ( '@' , 1 ) [ 0 ] . strip ( ) for field in self . fields : if field . multiple and field . name in newresource : newresource [ field . name ] = ( newresource . get ( field . name ) , ) if new_name is None : newresource [ 'name' ] = "%s @ %s" % ( basename , time . strftime ( '%X' ) ) newresource . update ( kwargs ) return self . write ( create_on_missing = True , fail_on_found = True , ** newresource ) else : if kwargs : raise exc . TowerCLIError ( 'Cannot override {} and also use --new-name.' . format ( kwargs . keys ( ) ) ) copy_endpoint = '{}/{}/copy/' . format ( self . endpoint . strip ( '/' ) , pk ) return client . post ( copy_endpoint , data = { 'name' : new_name } ) . json ( )
2722	def _perform_action ( self , params , return_dict = True ) : action = self . get_data ( "droplets/%s/actions/" % self . id , type = POST , params = params ) if return_dict : return action else : action = action [ u'action' ] return_action = Action ( token = self . token ) for attr in action . keys ( ) : setattr ( return_action , attr , action [ attr ] ) return return_action
6632	def islast ( generator ) : next_x = None first = True for x in generator : if not first : yield ( next_x , False ) next_x = x first = False if not first : yield ( next_x , True )
10380	def calculate_concordance_probability_by_annotation ( graph , annotation , key , cutoff = None , permutations = None , percentage = None , use_ambiguous = False ) : result = [ ( value , calculate_concordance_probability ( subgraph , key , cutoff = cutoff , permutations = permutations , percentage = percentage , use_ambiguous = use_ambiguous , ) ) for value , subgraph in get_subgraphs_by_annotation ( graph , annotation ) . items ( ) ] return dict ( result )
4419	async def play_previous ( self ) : if not self . previous : raise NoPreviousTrack self . queue . insert ( 0 , self . previous ) await self . play ( ignore_shuffle = True )
8901	def authenticate_credentials ( self , userargs , password , request = None ) : credentials = { 'password' : password } if "=" not in userargs : credentials [ get_user_model ( ) . USERNAME_FIELD ] = userargs else : for arg in userargs . split ( "&" ) : key , val = arg . split ( "=" ) credentials [ key ] = val user = authenticate ( ** credentials ) if user is None : raise exceptions . AuthenticationFailed ( 'Invalid credentials.' ) if not user . is_active : raise exceptions . AuthenticationFailed ( 'User inactive or deleted.' ) return ( user , None )
10963	def set_shape ( self , shape , inner ) : if self . shape != shape or self . inner != inner : self . shape = shape self . inner = inner self . initialize ( )
1518	def start_slave_nodes ( slaves , cl_args ) : pids = [ ] for slave in slaves : Log . info ( "Starting slave on %s" % slave ) cmd = "%s agent -config %s >> /tmp/nomad_client.log 2>&1 &" % ( get_nomad_path ( cl_args ) , get_nomad_slave_config_file ( cl_args ) ) if not is_self ( slave ) : cmd = ssh_remote_execute ( cmd , slave , cl_args ) Log . debug ( cmd ) pid = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) pids . append ( { "pid" : pid , "dest" : slave } ) errors = [ ] for entry in pids : pid = entry [ "pid" ] return_code = pid . wait ( ) output = pid . communicate ( ) Log . debug ( "return code: %s output: %s" % ( return_code , output ) ) if return_code != 0 : errors . append ( "Failed to start slave on %s with error:\n%s" % ( entry [ "dest" ] , output [ 1 ] ) ) if errors : for error in errors : Log . error ( error ) sys . exit ( - 1 ) Log . info ( "Done starting slaves" )
11251	def get_percentage ( a , b , i = False , r = False ) : if i is False and r is True : percentage = round ( 100.0 * ( float ( a ) / b ) , 2 ) elif ( i is True and r is True ) or ( i is True and r is False ) : percentage = int ( round ( 100 * ( float ( a ) / b ) ) ) if r is False : warnings . warn ( "If integer is set to True and Round is set to False, you will still get a rounded number if you pass floating point numbers as arguments." ) else : percentage = 100.0 * ( float ( a ) / b ) return percentage
13344	def mean ( a , axis = None , dtype = None , out = None , keepdims = False ) : if ( isinstance ( a , np . ndarray ) or isinstance ( a , RemoteArray ) or isinstance ( a , DistArray ) ) : return a . mean ( axis = axis , dtype = dtype , out = out , keepdims = keepdims ) else : return np . mean ( a , axis = axis , dtype = dtype , out = out , keepdims = keepdims )
4893	def _collect_certificate_data ( self , enterprise_enrollment ) : if self . certificates_api is None : self . certificates_api = CertificatesApiClient ( self . user ) course_id = enterprise_enrollment . course_id username = enterprise_enrollment . enterprise_customer_user . user . username try : certificate = self . certificates_api . get_course_certificate ( course_id , username ) completed_date = certificate . get ( 'created_date' ) if completed_date : completed_date = parse_datetime ( completed_date ) else : completed_date = timezone . now ( ) is_passing = certificate . get ( 'is_passing' ) grade = self . grade_passing if is_passing else self . grade_failing except HttpNotFoundError : completed_date = None grade = self . grade_incomplete is_passing = False return completed_date , grade , is_passing
13897	def DumpDirHashToStringIO ( directory , stringio , base = '' , exclude = None , include = None ) : import fnmatch import os files = [ ( os . path . join ( directory , i ) , i ) for i in os . listdir ( directory ) ] files = [ i for i in files if os . path . isfile ( i [ 0 ] ) ] for fullname , filename in files : if include is not None : if not fnmatch . fnmatch ( fullname , include ) : continue if exclude is not None : if fnmatch . fnmatch ( fullname , exclude ) : continue md5 = Md5Hex ( fullname ) if base : stringio . write ( '%s/%s=%s\n' % ( base , filename , md5 ) ) else : stringio . write ( '%s=%s\n' % ( filename , md5 ) )
5846	def load_file_as_yaml ( path ) : with open ( path , "r" ) as f : raw_yaml = f . read ( ) parsed_dict = yaml . load ( raw_yaml ) return parsed_dict
9739	def get_2d_markers ( self , component_info = None , data = None , component_position = None , index = None ) : return self . _get_2d_markers ( data , component_info , component_position , index = index )
6435	def sim_eudex ( src , tar , weights = 'exponential' , max_length = 8 ) : return Eudex ( ) . sim ( src , tar , weights , max_length )
4013	def _ensure_managed_repos_dir_exists ( ) : if not os . path . exists ( constants . REPOS_DIR ) : os . makedirs ( constants . REPOS_DIR )
23	def pickle_load ( path , compression = False ) : if compression : with zipfile . ZipFile ( path , "r" , compression = zipfile . ZIP_DEFLATED ) as myzip : with myzip . open ( "data" ) as f : return pickle . load ( f ) else : with open ( path , "rb" ) as f : return pickle . load ( f )
12590	def get_reliabledictionary_list ( client , application_name , service_name ) : cluster = Cluster . from_sfclient ( client ) service = cluster . get_application ( application_name ) . get_service ( service_name ) for dictionary in service . get_dictionaries ( ) : print ( dictionary . name )
9618	def UnPlug ( self , force = False ) : if force : _xinput . UnPlugForce ( c_uint ( self . id ) ) else : _xinput . UnPlug ( c_uint ( self . id ) ) while self . id not in self . available_ids ( ) : if self . id == 0 : break
4785	def starts_with ( self , prefix ) : if prefix is None : raise TypeError ( 'given prefix arg must not be none' ) if isinstance ( self . val , str_types ) : if not isinstance ( prefix , str_types ) : raise TypeError ( 'given prefix arg must be a string' ) if len ( prefix ) == 0 : raise ValueError ( 'given prefix arg must not be empty' ) if not self . val . startswith ( prefix ) : self . _err ( 'Expected <%s> to start with <%s>, but did not.' % ( self . val , prefix ) ) elif isinstance ( self . val , Iterable ) : if len ( self . val ) == 0 : raise ValueError ( 'val must not be empty' ) first = next ( iter ( self . val ) ) if first != prefix : self . _err ( 'Expected %s to start with <%s>, but did not.' % ( self . val , prefix ) ) else : raise TypeError ( 'val is not a string or iterable' ) return self
834	def run ( self ) : print "-" * 80 + "Computing the SDR" + "-" * 80 self . sp . compute ( self . inputArray , True , self . activeArray ) print self . activeArray . nonzero ( )
6829	def pull ( self , path , use_sudo = False , user = None , force = False ) : if path is None : raise ValueError ( "Path to the working copy is needed to pull from a remote repository." ) options = [ ] if force : options . append ( '--force' ) options = ' ' . join ( options ) cmd = 'git pull %s' % options with cd ( path ) : if use_sudo and user is None : run_as_root ( cmd ) elif use_sudo : sudo ( cmd , user = user ) else : run ( cmd )
13231	def get_def_macros ( tex_source ) : r macros = { } for match in DEF_PATTERN . finditer ( tex_source ) : macros [ match . group ( 'name' ) ] = match . group ( 'content' ) return macros
8558	def create_lan ( self , datacenter_id , lan ) : data = json . dumps ( self . _create_lan_dict ( lan ) ) response = self . _perform_request ( url = '/datacenters/%s/lans' % datacenter_id , method = 'POST' , data = data ) return response
5048	def delete ( self , request , customer_uuid ) : enterprise_customer = EnterpriseCustomer . objects . get ( uuid = customer_uuid ) email_to_unlink = request . GET [ "unlink_email" ] try : EnterpriseCustomerUser . objects . unlink_user ( enterprise_customer = enterprise_customer , user_email = email_to_unlink ) except ( EnterpriseCustomerUser . DoesNotExist , PendingEnterpriseCustomerUser . DoesNotExist ) : message = _ ( "Email {email} is not associated with Enterprise " "Customer {ec_name}" ) . format ( email = email_to_unlink , ec_name = enterprise_customer . name ) return HttpResponse ( message , content_type = "application/json" , status = 404 ) return HttpResponse ( json . dumps ( { } ) , content_type = "application/json" )
541	def __deleteModelCheckpoint ( self , modelID ) : checkpointID = self . _jobsDAO . modelsGetFields ( modelID , [ 'modelCheckpointId' ] ) [ 0 ] if checkpointID is None : return try : shutil . rmtree ( os . path . join ( self . _experimentDir , str ( self . _modelCheckpointGUID ) ) ) except : self . _logger . warn ( "Failed to delete model checkpoint %s. " "Assuming that another worker has already deleted it" , checkpointID ) return self . _jobsDAO . modelSetFields ( modelID , { 'modelCheckpointId' : None } , ignoreUnchanged = True ) return
13199	def format_content ( self , format = 'plain' , mathjax = False , smart = True , extra_args = None ) : output_text = convert_lsstdoc_tex ( self . _tex , format , mathjax = mathjax , smart = smart , extra_args = extra_args ) return output_text
13159	def update ( cls , cur , table : str , values : dict , where_keys : list ) -> tuple : keys = cls . _COMMA . join ( values . keys ( ) ) value_place_holder = cls . _PLACEHOLDER * len ( values ) where_clause , where_values = cls . _get_where_clause_with_values ( where_keys ) query = cls . _update_string . format ( table , keys , value_place_holder [ : - 1 ] , where_clause ) yield from cur . execute ( query , ( tuple ( values . values ( ) ) + where_values ) ) return ( yield from cur . fetchall ( ) )
232	def plot_sector_exposures_longshort ( long_exposures , short_exposures , sector_dict = SECTORS , ax = None ) : if ax is None : ax = plt . gca ( ) if sector_dict is None : sector_names = SECTORS . values ( ) else : sector_names = sector_dict . values ( ) color_list = plt . cm . gist_rainbow ( np . linspace ( 0 , 1 , 11 ) ) ax . stackplot ( long_exposures [ 0 ] . index , long_exposures , labels = sector_names , colors = color_list , alpha = 0.8 , baseline = 'zero' ) ax . stackplot ( long_exposures [ 0 ] . index , short_exposures , colors = color_list , alpha = 0.8 , baseline = 'zero' ) ax . axhline ( 0 , color = 'k' , linestyle = '-' ) ax . set ( title = 'Long and short exposures to sectors' , ylabel = 'Proportion of long/short exposure in sectors' ) ax . legend ( loc = 'upper left' , frameon = True , framealpha = 0.5 ) return ax
11073	def set_nested ( data , value , * keys ) : if len ( keys ) == 1 : data [ keys [ 0 ] ] = value else : if keys [ 0 ] not in data : data [ keys [ 0 ] ] = { } set_nested ( data [ keys [ 0 ] ] , value , * keys [ 1 : ] )
8619	def getServerInfo ( pbclient = None , dc_id = None ) : if pbclient is None : raise ValueError ( "argument 'pbclient' must not be None" ) if dc_id is None : raise ValueError ( "argument 'dc_id' must not be None" ) server_info = [ ] servers = pbclient . list_servers ( dc_id , 1 ) for server in servers [ 'items' ] : props = server [ 'properties' ] info = dict ( id = server [ 'id' ] , name = props [ 'name' ] , state = server [ 'metadata' ] [ 'state' ] , vmstate = props [ 'vmState' ] ) server_info . append ( info ) return server_info
11542	def read ( self , pin ) : if type ( pin ) is list : return [ self . read ( p ) for p in pin ] pin_id = self . _pin_mapping . get ( pin , None ) if pin_id : value = self . _read ( pin_id ) lpin = self . _pin_lin . get ( pin , None ) if lpin and type ( lpin [ 'read' ] ) is tuple : read_range = lpin [ 'read' ] value = self . _linear_interpolation ( value , * read_range ) return value else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
9782	def delete ( ctx ) : user , project_name , _build = get_build_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'build' ) ) if not click . confirm ( "Are sure you want to delete build job `{}`" . format ( _build ) ) : click . echo ( 'Existing without deleting build job.' ) sys . exit ( 1 ) try : response = PolyaxonClient ( ) . build_job . delete_build ( user , project_name , _build ) BuildJobManager . purge ( ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not delete job `{}`.' . format ( _build ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) if response . status_code == 204 : Printer . print_success ( "Build job `{}` was deleted successfully" . format ( _build ) )
6657	def calc_inbag ( n_samples , forest ) : if not forest . bootstrap : e_s = "Cannot calculate the inbag from a forest that has " e_s = " bootstrap=False" raise ValueError ( e_s ) n_trees = forest . n_estimators inbag = np . zeros ( ( n_samples , n_trees ) ) sample_idx = [ ] for t_idx in range ( n_trees ) : sample_idx . append ( _generate_sample_indices ( forest . estimators_ [ t_idx ] . random_state , n_samples ) ) inbag [ : , t_idx ] = np . bincount ( sample_idx [ - 1 ] , minlength = n_samples ) return inbag
11770	def name ( object ) : "Try to find some reasonable name for the object." return ( getattr ( object , 'name' , 0 ) or getattr ( object , '__name__' , 0 ) or getattr ( getattr ( object , '__class__' , 0 ) , '__name__' , 0 ) or str ( object ) )
2835	def write ( self , data , assert_ss = True , deassert_ss = True ) : if self . _mosi is None : raise RuntimeError ( 'Write attempted with no MOSI pin specified.' ) if assert_ss and self . _ss is not None : self . _gpio . set_low ( self . _ss ) for byte in data : for i in range ( 8 ) : if self . _write_shift ( byte , i ) & self . _mask : self . _gpio . set_high ( self . _mosi ) else : self . _gpio . set_low ( self . _mosi ) self . _gpio . output ( self . _sclk , not self . _clock_base ) self . _gpio . output ( self . _sclk , self . _clock_base ) if deassert_ss and self . _ss is not None : self . _gpio . set_high ( self . _ss )
9392	def calc_key_stats ( self , metric_store ) : stats_to_calculate = [ 'mean' , 'std' , 'min' , 'max' ] percentiles_to_calculate = range ( 0 , 100 , 1 ) for column , groups_store in metric_store . items ( ) : for group , time_store in groups_store . items ( ) : data = metric_store [ column ] [ group ] . values ( ) if self . groupby : column_name = group + '.' + column else : column_name = column if column . startswith ( 'qps' ) : self . calculated_stats [ column_name ] , self . calculated_percentiles [ column_name ] = naarad . utils . calculate_stats ( data , stats_to_calculate , percentiles_to_calculate ) else : self . calculated_stats [ column_name ] , self . calculated_percentiles [ column_name ] = naarad . utils . calculate_stats ( list ( heapq . merge ( * data ) ) , stats_to_calculate , percentiles_to_calculate ) self . update_summary_stats ( column_name )
145	def deepcopy ( self , exterior = None , label = None ) : return Polygon ( exterior = np . copy ( self . exterior ) if exterior is None else exterior , label = self . label if label is None else label )
9684	def sn ( self ) : string = [ ] self . cnxn . xfer ( [ 0x10 ] ) sleep ( 9e-3 ) for i in range ( 60 ) : resp = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] string . append ( chr ( resp ) ) sleep ( 0.1 ) return '' . join ( string )
4698	def fmt ( lbaf = 3 ) : if env ( ) : cij . err ( "cij.nvme.exists: Invalid NVMe ENV." ) return 1 nvme = cij . env_to_dict ( PREFIX , EXPORTED + REQUIRED ) cmd = [ "nvme" , "format" , nvme [ "DEV_PATH" ] , "-l" , str ( lbaf ) ] rcode , _ , _ = cij . ssh . command ( cmd , shell = True ) return rcode
5919	def center_fit ( self , ** kwargs ) : kwargs . setdefault ( 's' , self . tpr ) kwargs . setdefault ( 'n' , self . ndx ) kwargs [ 'f' ] = self . xtc kwargs . setdefault ( 'o' , self . outfile ( self . infix_filename ( None , self . xtc , '_centfit' , 'xtc' ) ) ) force = kwargs . pop ( 'force' , self . force ) logger . info ( "Centering and fitting trajectory {f!r}..." . format ( ** kwargs ) ) with utilities . in_dir ( self . dirname ) : if not self . check_file_exists ( kwargs [ 'o' ] , resolve = "indicate" , force = force ) : trj_fitandcenter ( ** kwargs ) logger . info ( "Centered and fit trajectory: {o!r}." . format ( ** kwargs ) ) return { 'tpr' : self . rp ( kwargs [ 's' ] ) , 'xtc' : self . rp ( kwargs [ 'o' ] ) }
5858	def __generate_search_template ( self , dataset_ids ) : data = { "dataset_ids" : dataset_ids } failure_message = "Failed to generate a search template from columns in dataset(s) {}" . format ( dataset_ids ) return self . _get_success_json ( self . _post_json ( 'v1/search_templates/builders/from-dataset-ids' , data , failure_message = failure_message ) ) [ 'data' ]
12018	def disassemble ( self ) : ser_pb = open ( self . input_file , 'rb' ) . read ( ) fd = FileDescriptorProto ( ) fd . ParseFromString ( ser_pb ) self . name = fd . name self . _print ( '// Reversed by pbd (https://github.com/rsc-dev/pbd)' ) self . _print ( 'syntax = "proto2";' ) self . _print ( '' ) if len ( fd . package ) > 0 : self . _print ( 'package {};' . format ( fd . package ) ) self . package = fd . package else : self . _print ( '// Package not defined' ) self . _walk ( fd )
9110	def _create_archive ( self ) : self . status = u'270 creating final encrypted backup of cleansed attachments' return self . _create_encrypted_zip ( source = 'clean' , fs_target_dir = self . container . fs_archive_cleansed )
1171	def format_option_strings ( self , option ) : if option . takes_value ( ) : metavar = option . metavar or option . dest . upper ( ) short_opts = [ self . _short_opt_fmt % ( sopt , metavar ) for sopt in option . _short_opts ] long_opts = [ self . _long_opt_fmt % ( lopt , metavar ) for lopt in option . _long_opts ] else : short_opts = option . _short_opts long_opts = option . _long_opts if self . short_first : opts = short_opts + long_opts else : opts = long_opts + short_opts return ", " . join ( opts )
4908	def _post ( self , url , data , scope ) : self . _create_session ( scope ) response = self . session . post ( url , data = data ) return response . status_code , response . text
2446	def create_package ( self , doc , name ) : if not self . package_set : self . package_set = True doc . package = package . Package ( name = name ) return True else : raise CardinalityError ( 'Package::Name' )
11958	def is_dec ( ip ) : try : dec = int ( str ( ip ) ) except ValueError : return False if dec > 4294967295 or dec < 0 : return False return True
13258	def _file_path ( self , uid ) : file_name = '%s.doentry' % ( uid ) return os . path . join ( self . dayone_journal_path , file_name )
51	def deepcopy ( self , x = None , y = None ) : x = self . x if x is None else x y = self . y if y is None else y return Keypoint ( x = x , y = y )
6052	def sparse_to_unmasked_sparse_from_mask_and_pixel_centres ( total_sparse_pixels , mask , unmasked_sparse_grid_pixel_centres ) : pix_to_full_pix = np . zeros ( total_sparse_pixels ) pixel_index = 0 for full_pixel_index in range ( unmasked_sparse_grid_pixel_centres . shape [ 0 ] ) : y = unmasked_sparse_grid_pixel_centres [ full_pixel_index , 0 ] x = unmasked_sparse_grid_pixel_centres [ full_pixel_index , 1 ] if not mask [ y , x ] : pix_to_full_pix [ pixel_index ] = full_pixel_index pixel_index += 1 return pix_to_full_pix
6434	def dist_eudex ( src , tar , weights = 'exponential' , max_length = 8 ) : return Eudex ( ) . dist ( src , tar , weights , max_length )
10888	def coords ( self , norm = False , form = 'broadcast' ) : if norm is False : norm = 1 if norm is True : norm = np . array ( self . shape ) norm = aN ( norm , self . dim , dtype = 'float' ) v = list ( np . arange ( self . l [ i ] , self . r [ i ] ) / norm [ i ] for i in range ( self . dim ) ) return self . _format_vector ( v , form = form )
9036	def walk_rows ( self , mapping = identity ) : row_in_grid = self . _walk . row_in_grid return map ( lambda row : mapping ( row_in_grid ( row ) ) , self . _rows )
13720	def main ( ) : ep = requests . get ( TRELLO_API_DOC ) . content root = html . fromstring ( ep ) links = root . xpath ( '//a[contains(@class, "reference internal")]/@href' ) pages = [ requests . get ( TRELLO_API_DOC + u ) for u in links if u . endswith ( 'index.html' ) ] endpoints = [ ] for page in pages : root = html . fromstring ( page . content ) sections = root . xpath ( '//div[@class="section"]/h2/..' ) for sec in sections : ep_html = etree . tostring ( sec ) . decode ( 'utf-8' ) ep_text = html2text ( ep_html ) . splitlines ( ) match = EP_DESC_REGEX . match ( ep_text [ 0 ] ) if not match : continue ep_method , ep_url = match . groups ( ) ep_text [ 0 ] = ' ' . join ( [ ep_method , ep_url ] ) ep_doc = b64encode ( gzip . compress ( '\n' . join ( ep_text ) . encode ( 'utf-8' ) ) ) endpoints . append ( ( ep_method , ep_url , ep_doc ) ) print ( yaml . dump ( create_tree ( endpoints ) ) )
10144	def from_schema ( self , schema_node , base_name = None ) : return self . _ref_recursive ( self . type_converter ( schema_node ) , self . ref , base_name )
13556	def all_comments ( self ) : ctype = ContentType . objects . get ( app_label__exact = "happenings" , model__exact = 'event' ) update_ctype = ContentType . objects . get ( app_label__exact = "happenings" , model__exact = 'update' ) update_ids = self . update_set . values_list ( 'id' , flat = True ) return Comment . objects . filter ( Q ( content_type = ctype . id , object_pk = self . id ) | Q ( content_type = update_ctype . id , object_pk__in = update_ids ) )
3059	def _get_backend ( filename ) : filename = os . path . abspath ( filename ) with _backends_lock : if filename not in _backends : _backends [ filename ] = _MultiprocessStorageBackend ( filename ) return _backends [ filename ]
9715	async def qtm_version ( self ) : return await asyncio . wait_for ( self . _protocol . send_command ( "qtmversion" ) , timeout = self . _timeout )
13832	def _SkipFieldValue ( tokenizer ) : if tokenizer . TryConsumeByteString ( ) : while tokenizer . TryConsumeByteString ( ) : pass return if ( not tokenizer . TryConsumeIdentifier ( ) and not tokenizer . TryConsumeInt64 ( ) and not tokenizer . TryConsumeUint64 ( ) and not tokenizer . TryConsumeFloat ( ) ) : raise ParseError ( 'Invalid field value: ' + tokenizer . token )
11986	async def _upload_file ( self , full_path ) : rel_path = os . path . relpath ( full_path , self . folder ) key = s3_key ( os . path . join ( self . key , rel_path ) ) ct = self . content_types . get ( key . split ( '.' ) [ - 1 ] ) with open ( full_path , 'rb' ) as fp : file = fp . read ( ) try : await self . botocore . upload_file ( self . bucket , file , key = key , ContentType = ct ) except Exception as exc : LOGGER . error ( 'Could not upload "%s": %s' , key , exc ) self . failures [ key ] = self . all . pop ( full_path ) return size = self . all . pop ( full_path ) self . success [ key ] = size self . total_size += size percentage = 100 * ( 1 - len ( self . all ) / self . total_files ) message = '{0:.0f}% completed - uploaded "{1}" - {2}' . format ( percentage , key , convert_bytes ( size ) ) LOGGER . info ( message )
4700	def get_sizeof_descriptor_table ( version = "Denali" ) : if version == "Denali" : return sizeof ( DescriptorTableDenali ) elif version == "Spec20" : return sizeof ( DescriptorTableSpec20 ) elif version == "Spec12" : return 0 else : raise RuntimeError ( "Error version!" )
6400	def stem ( self , word ) : wlen = len ( word ) - 2 if wlen > 2 and word [ - 1 ] == 's' : word = word [ : - 1 ] wlen -= 1 _endings = { 5 : { 'elser' , 'heten' } , 4 : { 'arne' , 'erna' , 'ande' , 'else' , 'aste' , 'orna' , 'aren' } , 3 : { 'are' , 'ast' , 'het' } , 2 : { 'ar' , 'er' , 'or' , 'en' , 'at' , 'te' , 'et' } , 1 : { 'a' , 'e' , 'n' , 't' } , } for end_len in range ( 5 , 0 , - 1 ) : if wlen > end_len and word [ - end_len : ] in _endings [ end_len ] : return word [ : - end_len ] return word
9983	def replace_funcname ( source : str , name : str ) : lines = source . splitlines ( ) atok = asttokens . ASTTokens ( source , parse = True ) for node in ast . walk ( atok . tree ) : if isinstance ( node , ast . FunctionDef ) : break i = node . first_token . index for i in range ( node . first_token . index , node . last_token . index ) : if ( atok . tokens [ i ] . type == token . NAME and atok . tokens [ i ] . string == "def" ) : break lineno , col_begin = atok . tokens [ i + 1 ] . start lineno_end , col_end = atok . tokens [ i + 1 ] . end assert lineno == lineno_end lines [ lineno - 1 ] = ( lines [ lineno - 1 ] [ : col_begin ] + name + lines [ lineno - 1 ] [ col_end : ] ) return "\n" . join ( lines ) + "\n"
12107	def cross_check_launchers ( self , launchers ) : if len ( launchers ) == 0 : raise Exception ( 'Empty launcher list' ) timestamps = [ launcher . timestamp for launcher in launchers ] if not all ( timestamps [ 0 ] == tstamp for tstamp in timestamps ) : raise Exception ( "Launcher timestamps not all equal. " "Consider setting timestamp explicitly." ) root_directories = [ ] for launcher in launchers : command = launcher . command args = launcher . args command . verify ( args ) root_directory = launcher . get_root_directory ( ) if os . path . isdir ( root_directory ) : raise Exception ( "Root directory already exists: %r" % root_directory ) if root_directory in root_directories : raise Exception ( "Each launcher requires a unique root directory" ) root_directories . append ( root_directory )
12648	def filter_objlist ( olist , fieldname , fieldval ) : return [ x for x in olist if getattr ( x , fieldname ) == fieldval ]
3696	def Watson ( T , Hvap_ref , T_Ref , Tc , exponent = 0.38 ) : Tr = T / Tc Trefr = T_Ref / Tc H2 = Hvap_ref * ( ( 1 - Tr ) / ( 1 - Trefr ) ) ** exponent return H2
5776	def _bcrypt_sign ( private_key , data , hash_algorithm , rsa_pss_padding = False ) : if hash_algorithm == 'raw' : digest = data else : hash_constant = { 'md5' : BcryptConst . BCRYPT_MD5_ALGORITHM , 'sha1' : BcryptConst . BCRYPT_SHA1_ALGORITHM , 'sha256' : BcryptConst . BCRYPT_SHA256_ALGORITHM , 'sha384' : BcryptConst . BCRYPT_SHA384_ALGORITHM , 'sha512' : BcryptConst . BCRYPT_SHA512_ALGORITHM } [ hash_algorithm ] digest = getattr ( hashlib , hash_algorithm ) ( data ) . digest ( ) padding_info = null ( ) flags = 0 if private_key . algorithm == 'rsa' : if rsa_pss_padding : hash_length = { 'md5' : 16 , 'sha1' : 20 , 'sha256' : 32 , 'sha384' : 48 , 'sha512' : 64 } [ hash_algorithm ] flags = BcryptConst . BCRYPT_PAD_PSS padding_info_struct_pointer = struct ( bcrypt , 'BCRYPT_PSS_PADDING_INFO' ) padding_info_struct = unwrap ( padding_info_struct_pointer ) hash_buffer = buffer_from_unicode ( hash_constant ) padding_info_struct . pszAlgId = cast ( bcrypt , 'wchar_t *' , hash_buffer ) padding_info_struct . cbSalt = hash_length else : flags = BcryptConst . BCRYPT_PAD_PKCS1 padding_info_struct_pointer = struct ( bcrypt , 'BCRYPT_PKCS1_PADDING_INFO' ) padding_info_struct = unwrap ( padding_info_struct_pointer ) if hash_algorithm == 'raw' : padding_info_struct . pszAlgId = null ( ) else : hash_buffer = buffer_from_unicode ( hash_constant ) padding_info_struct . pszAlgId = cast ( bcrypt , 'wchar_t *' , hash_buffer ) padding_info = cast ( bcrypt , 'void *' , padding_info_struct_pointer ) if private_key . algorithm == 'dsa' and private_key . bit_size > 1024 and hash_algorithm in set ( [ 'md5' , 'sha1' ] ) : raise ValueError ( pretty_message ( ) ) out_len = new ( bcrypt , 'DWORD *' ) res = bcrypt . BCryptSignHash ( private_key . key_handle , padding_info , digest , len ( digest ) , null ( ) , 0 , out_len , flags ) handle_error ( res ) buffer_len = deref ( out_len ) buffer = buffer_from_bytes ( buffer_len ) if private_key . algorithm == 'rsa' : padding_info = cast ( bcrypt , 'void *' , padding_info_struct_pointer ) res = bcrypt . BCryptSignHash ( private_key . key_handle , padding_info , digest , len ( digest ) , buffer , buffer_len , out_len , flags ) handle_error ( res ) signature = bytes_from_buffer ( buffer , deref ( out_len ) ) if private_key . algorithm != 'rsa' : signature = algos . DSASignature . from_p1363 ( signature ) . dump ( ) return signature
10564	def get_supported_filepaths ( filepaths , supported_extensions , max_depth = float ( 'inf' ) ) : supported_filepaths = [ ] for path in filepaths : if os . name == 'nt' and CYGPATH_RE . match ( path ) : path = convert_cygwin_path ( path ) if os . path . isdir ( path ) : for root , __ , files in walk_depth ( path , max_depth ) : for f in files : if f . lower ( ) . endswith ( supported_extensions ) : supported_filepaths . append ( os . path . join ( root , f ) ) elif os . path . isfile ( path ) and path . lower ( ) . endswith ( supported_extensions ) : supported_filepaths . append ( path ) return supported_filepaths
5757	def get_homogeneous ( package_descriptors , targets , repos_data ) : homogeneous = { } for package_descriptor in package_descriptors . values ( ) : pkg_name = package_descriptor . pkg_name debian_pkg_name = package_descriptor . debian_pkg_name versions = [ ] for repo_data in repos_data : versions . append ( set ( [ ] ) ) for target in targets : version = _strip_version_suffix ( repo_data . get ( target , { } ) . get ( debian_pkg_name , None ) ) versions [ - 1 ] . add ( version ) homogeneous [ pkg_name ] = max ( [ len ( v ) for v in versions ] ) == 1 return homogeneous
10384	def remove_inconsistent_edges ( graph : BELGraph ) -> None : for u , v in get_inconsistent_edges ( graph ) : edges = [ ( u , v , k ) for k in graph [ u ] [ v ] ] graph . remove_edges_from ( edges )
1997	def sync_svc ( state ) : syscall = state . cpu . R7 name = linux_syscalls . armv7 [ syscall ] logger . debug ( f"Syncing syscall: {name}" ) try : if 'mmap' in name : returned = gdb . getR ( 'R0' ) logger . debug ( f"Syncing mmap ({returned:x})" ) state . cpu . write_register ( 'R0' , returned ) if 'exit' in name : return except ValueError : for reg in state . cpu . canonical_registers : print ( f'{reg}: {state.cpu.read_register(reg):x}' ) raise
3990	def _nginx_http_spec ( port_spec , bridge_ip ) : server_string_spec = "\t server {\n" server_string_spec += "\t \t {}\n" . format ( _nginx_max_file_size_string ( ) ) server_string_spec += "\t \t {}\n" . format ( _nginx_listen_string ( port_spec ) ) server_string_spec += "\t \t {}\n" . format ( _nginx_server_name_string ( port_spec ) ) server_string_spec += _nginx_location_spec ( port_spec , bridge_ip ) server_string_spec += _custom_502_page ( ) server_string_spec += "\t }\n" return server_string_spec
7647	def scaper_to_tag ( annotation ) : annotation . namespace = 'tag_open' data = annotation . pop_data ( ) for obs in data : annotation . append ( time = obs . time , duration = obs . duration , confidence = obs . confidence , value = obs . value [ 'label' ] ) return annotation
96	def quokka_keypoints ( size = None , extract = None ) : from imgaug . augmentables . kps import Keypoint , KeypointsOnImage left , top = 0 , 0 if extract is not None : bb_extract = _quokka_normalize_extract ( extract ) left = bb_extract . x1 top = bb_extract . y1 with open ( QUOKKA_ANNOTATIONS_FP , "r" ) as f : json_dict = json . load ( f ) keypoints = [ ] for kp_dict in json_dict [ "keypoints" ] : keypoints . append ( Keypoint ( x = kp_dict [ "x" ] - left , y = kp_dict [ "y" ] - top ) ) if extract is not None : shape = ( bb_extract . height , bb_extract . width , 3 ) else : shape = ( 643 , 960 , 3 ) kpsoi = KeypointsOnImage ( keypoints , shape = shape ) if size is not None : shape_resized = _compute_resized_shape ( shape , size ) kpsoi = kpsoi . on ( shape_resized ) return kpsoi
6422	def _synoname_strip_punct ( self , word ) : stripped = '' for char in word : if char not in set ( ',-./:;"&\'()!{|}?$%*+<=>[\\]^_`~' ) : stripped += char return stripped . strip ( )
7317	def sendmail ( self , msg_from , msg_to , msg ) : SMTP_dummy . msg_from = msg_from SMTP_dummy . msg_to = msg_to SMTP_dummy . msg = msg
3632	def baseId ( resource_id , return_version = False ) : version = 0 resource_id = resource_id + 0xC4000000 while resource_id > 0x01000000 : version += 1 if version == 1 : resource_id -= 0x80000000 elif version == 2 : resource_id -= 0x03000000 else : resource_id -= 0x01000000 if return_version : return resource_id , version - 67 return resource_id
1235	def from_spec ( spec , kwargs ) : agent = util . get_object ( obj = spec , predefined_objects = tensorforce . agents . agents , kwargs = kwargs ) assert isinstance ( agent , Agent ) return agent
1958	def _open ( self , f ) : if None in self . files : fd = self . files . index ( None ) self . files [ fd ] = f else : fd = len ( self . files ) self . files . append ( f ) return fd
5402	def _get_prepare_env ( self , script , job_descriptor , inputs , outputs , mounts ) : docker_paths = sorted ( [ var . docker_path if var . recursive else os . path . dirname ( var . docker_path ) for var in inputs | outputs | mounts if var . value ] ) env = { _SCRIPT_VARNAME : repr ( script . value ) , _META_YAML_VARNAME : repr ( job_descriptor . to_yaml ( ) ) , 'DIR_COUNT' : str ( len ( docker_paths ) ) } for idx , path in enumerate ( docker_paths ) : env [ 'DIR_{}' . format ( idx ) ] = os . path . join ( providers_util . DATA_MOUNT_POINT , path ) return env
7198	def describe_images ( self , idaho_image_results ) : results = idaho_image_results [ 'results' ] results = [ r for r in results if 'IDAHOImage' in r [ 'type' ] ] self . logger . debug ( 'Describing %s IDAHO images.' % len ( results ) ) catids = set ( [ r [ 'properties' ] [ 'catalogID' ] for r in results ] ) description = { } for catid in catids : description [ catid ] = { } description [ catid ] [ 'parts' ] = { } images = [ r for r in results if r [ 'properties' ] [ 'catalogID' ] == catid ] for image in images : description [ catid ] [ 'sensorPlatformName' ] = image [ 'properties' ] [ 'sensorPlatformName' ] part = int ( image [ 'properties' ] [ 'vendorDatasetIdentifier' ] . split ( ':' ) [ 1 ] [ - 3 : ] ) color = image [ 'properties' ] [ 'colorInterpretation' ] bucket = image [ 'properties' ] [ 'tileBucketName' ] identifier = image [ 'identifier' ] boundstr = image [ 'properties' ] [ 'footprintWkt' ] try : description [ catid ] [ 'parts' ] [ part ] except : description [ catid ] [ 'parts' ] [ part ] = { } description [ catid ] [ 'parts' ] [ part ] [ color ] = { } description [ catid ] [ 'parts' ] [ part ] [ color ] [ 'id' ] = identifier description [ catid ] [ 'parts' ] [ part ] [ color ] [ 'bucket' ] = bucket description [ catid ] [ 'parts' ] [ part ] [ color ] [ 'boundstr' ] = boundstr return description
2868	def get_platform_gpio ( ** keywords ) : plat = Platform . platform_detect ( ) if plat == Platform . RASPBERRY_PI : import RPi . GPIO return RPiGPIOAdapter ( RPi . GPIO , ** keywords ) elif plat == Platform . BEAGLEBONE_BLACK : import Adafruit_BBIO . GPIO return AdafruitBBIOAdapter ( Adafruit_BBIO . GPIO , ** keywords ) elif plat == Platform . MINNOWBOARD : import mraa return AdafruitMinnowAdapter ( mraa , ** keywords ) elif plat == Platform . JETSON_NANO : import Jetson . GPIO return RPiGPIOAdapter ( Jetson . GPIO , ** keywords ) elif plat == Platform . UNKNOWN : raise RuntimeError ( 'Could not determine platform.' )
3131	def merge_results ( x , y ) : z = x . copy ( ) for key , value in y . items ( ) : if isinstance ( value , list ) and isinstance ( z . get ( key ) , list ) : z [ key ] += value else : z [ key ] = value return z
4462	def transpose ( label , n_semitones ) : match = re . match ( six . text_type ( '(?P<note>[A-G][b#]*)(?P<mod>.*)' ) , six . text_type ( label ) ) if not match : return label note = match . group ( 'note' ) new_note = librosa . midi_to_note ( librosa . note_to_midi ( note ) + n_semitones , octave = False ) return new_note + match . group ( 'mod' )
7394	def get_publications ( context , template = 'publications/publications.html' ) : types = Type . objects . filter ( hidden = False ) publications = Publication . objects . select_related ( ) publications = publications . filter ( external = False , type__in = types ) publications = publications . order_by ( '-year' , '-month' , '-id' ) if not publications : return '' populate ( publications ) return render_template ( template , context [ 'request' ] , { 'publications' : publications } )
11823	def exp_schedule ( k = 20 , lam = 0.005 , limit = 100 ) : "One possible schedule function for simulated annealing" return lambda t : if_ ( t < limit , k * math . exp ( - lam * t ) , 0 )
11171	def optionhelp ( self , indent = 0 , maxindent = 25 , width = 79 ) : def makelabels ( option ) : labels = '%*s--%s' % ( indent , ' ' , option . name ) if option . abbreviation : labels += ', -' + option . abbreviation return labels + ': ' docs = [ ] helpindent = _autoindent ( [ makelabels ( o ) for o in self . options . values ( ) ] , indent , maxindent ) for name in self . option_order : option = self . options [ name ] labels = makelabels ( option ) helpstring = "%s(%s). %s" % ( option . formatname , option . strvalue , option . docs ) wrapped = self . _wrap_labelled ( labels , helpstring , helpindent , width ) docs . extend ( wrapped ) return '\n' . join ( docs )
5833	def create_ml_configuration_from_datasets ( self , dataset_ids ) : available_columns = self . search_template_client . get_available_columns ( dataset_ids ) search_template = self . search_template_client . create ( dataset_ids , available_columns ) return self . create_ml_configuration ( search_template , available_columns , dataset_ids )
13448	def authed_get ( self , url , response_code = 200 , headers = { } , follow = False ) : if not self . authed : self . authorize ( ) response = self . client . get ( url , follow = follow , ** headers ) self . assertEqual ( response_code , response . status_code ) return response
10331	def group_nodes_by_annotation ( graph : BELGraph , annotation : str = 'Subgraph' ) -> Mapping [ str , Set [ BaseEntity ] ] : result = defaultdict ( set ) for u , v , d in graph . edges ( data = True ) : if not edge_has_annotation ( d , annotation ) : continue result [ d [ ANNOTATIONS ] [ annotation ] ] . add ( u ) result [ d [ ANNOTATIONS ] [ annotation ] ] . add ( v ) return dict ( result )
147	def draw_on_image ( self , image , color = ( 0 , 255 , 0 ) , color_face = None , color_lines = None , color_points = None , alpha = 1.0 , alpha_face = None , alpha_lines = None , alpha_points = None , size = 1 , size_lines = None , size_points = None , raise_if_out_of_image = False ) : for poly in self . polygons : image = poly . draw_on_image ( image , color = color , color_face = color_face , color_lines = color_lines , color_points = color_points , alpha = alpha , alpha_face = alpha_face , alpha_lines = alpha_lines , alpha_points = alpha_points , size = size , size_lines = size_lines , size_points = size_points , raise_if_out_of_image = raise_if_out_of_image ) return image
1379	def print_build_info ( zipped_pex = False ) : if zipped_pex : release_file = get_zipped_heron_release_file ( ) else : release_file = get_heron_release_file ( ) with open ( release_file ) as release_info : release_map = yaml . load ( release_info ) release_items = sorted ( release_map . items ( ) , key = lambda tup : tup [ 0 ] ) for key , value in release_items : print ( "%s : %s" % ( key , value ) )
1174	def lock ( self , function , argument ) : if self . testandset ( ) : function ( argument ) else : self . queue . append ( ( function , argument ) )
11725	def camel2word ( string ) : def wordize ( match ) : return ' ' + match . group ( 1 ) . lower ( ) return string [ 0 ] + re . sub ( r'([A-Z])' , wordize , string [ 1 : ] )
8407	def expand_range ( range , mul = 0 , add = 0 , zero_width = 1 ) : x = range try : x [ 0 ] except TypeError : x = ( x , x ) if zero_range ( x ) : new = x [ 0 ] - zero_width / 2 , x [ 0 ] + zero_width / 2 else : dx = ( x [ 1 ] - x [ 0 ] ) * mul + add new = x [ 0 ] - dx , x [ 1 ] + dx return new
237	def compute_volume_exposures ( shares_held , volumes , percentile ) : shares_held = shares_held . replace ( 0 , np . nan ) shares_longed = shares_held [ shares_held > 0 ] shares_shorted = - 1 * shares_held [ shares_held < 0 ] shares_grossed = shares_held . abs ( ) longed_frac = shares_longed . divide ( volumes ) shorted_frac = shares_shorted . divide ( volumes ) grossed_frac = shares_grossed . divide ( volumes ) longed_threshold = 100 * longed_frac . apply ( partial ( np . nanpercentile , q = 100 * percentile ) , axis = 'columns' , ) shorted_threshold = 100 * shorted_frac . apply ( partial ( np . nanpercentile , q = 100 * percentile ) , axis = 'columns' , ) grossed_threshold = 100 * grossed_frac . apply ( partial ( np . nanpercentile , q = 100 * percentile ) , axis = 'columns' , ) return longed_threshold , shorted_threshold , grossed_threshold
1527	def is_host_port_reachable ( self ) : for hostport in self . hostportlist : try : socket . create_connection ( hostport , StateManager . TIMEOUT_SECONDS ) return True except : LOG . info ( "StateManager %s Unable to connect to host: %s port %i" % ( self . name , hostport [ 0 ] , hostport [ 1 ] ) ) continue return False
5496	def from_file ( cls , file , * args , ** kwargs ) : try : cache = shelve . open ( file ) return cls ( file , cache , * args , ** kwargs ) except OSError as e : logger . debug ( "Loading {0} failed" . format ( file ) ) raise e
11821	def create ( self , name , value ) : if value is None : raise ValueError ( 'Setting value cannot be `None`.' ) model = Setting . get_model_for_value ( value ) obj = super ( SettingQuerySet , model . objects . all ( ) ) . create ( name = name , value = value ) return obj
1620	def FindNextMultiLineCommentStart ( lines , lineix ) : while lineix < len ( lines ) : if lines [ lineix ] . strip ( ) . startswith ( '/*' ) : if lines [ lineix ] . strip ( ) . find ( '*/' , 2 ) < 0 : return lineix lineix += 1 return len ( lines )
4849	def _serialize_items ( self , channel_metadata_items ) : return json . dumps ( self . _prepare_items_for_transmission ( channel_metadata_items ) , sort_keys = True ) . encode ( 'utf-8' )
10516	def verifyscrollbarhorizontal ( self , window_name , object_name ) : try : object_handle = self . _get_object_handle ( window_name , object_name ) if object_handle . AXOrientation == "AXHorizontalOrientation" : return 1 except : pass return 0
11845	def list_things_at ( self , location , tclass = Thing ) : "Return all things exactly at a given location." return [ thing for thing in self . things if thing . location == location and isinstance ( thing , tclass ) ]
4766	def is_same_as ( self , other ) : if self . val is not other : self . _err ( 'Expected <%s> to be identical to <%s>, but was not.' % ( self . val , other ) ) return self
698	def getParticleInfo ( self , modelId ) : entry = self . _allResults [ self . _modelIDToIdx [ modelId ] ] return ( entry [ 'modelParams' ] [ 'particleState' ] , modelId , entry [ 'errScore' ] , entry [ 'completed' ] , entry [ 'matured' ] )
13230	def get_macros ( tex_source ) : r macros = { } macros . update ( get_def_macros ( tex_source ) ) macros . update ( get_newcommand_macros ( tex_source ) ) return macros
5202	def create_connection ( port = _PORT_ , timeout = _TIMEOUT_ , restart = False ) : if _CON_SYM_ in globals ( ) : if not isinstance ( globals ( ) [ _CON_SYM_ ] , pdblp . BCon ) : del globals ( ) [ _CON_SYM_ ] if ( _CON_SYM_ in globals ( ) ) and ( not restart ) : con = globals ( ) [ _CON_SYM_ ] if getattr ( con , '_session' ) . start ( ) : con . start ( ) return con , False else : con = pdblp . BCon ( port = port , timeout = timeout ) globals ( ) [ _CON_SYM_ ] = con con . start ( ) return con , True
10695	def yiq_to_rgb ( yiq ) : y , i , q = yiq r = y + ( 0.956 * i ) + ( 0.621 * q ) g = y - ( 0.272 * i ) - ( 0.647 * q ) b = y - ( 1.108 * i ) + ( 1.705 * q ) r = 1 if r > 1 else max ( 0 , r ) g = 1 if g > 1 else max ( 0 , g ) b = 1 if b > 1 else max ( 0 , b ) return round ( r * 255 , 3 ) , round ( g * 255 , 3 ) , round ( b * 255 , 3 )
4537	def wheel_helper ( pos , length , cycle_step ) : return wheel_color ( ( pos * len ( _WHEEL ) / length ) + cycle_step )
13532	def ancestors_root ( self ) : if self . is_root ( ) : return [ ] ancestors = set ( [ ] ) self . _depth_ascend ( self , ancestors , True ) try : ancestors . remove ( self ) except KeyError : pass return list ( ancestors )
10592	def get_path_relative_to_module ( module_file_path , relative_target_path ) : module_path = os . path . dirname ( module_file_path ) path = os . path . join ( module_path , relative_target_path ) path = os . path . abspath ( path ) return path
1231	def tf_import_experience ( self , states , internals , actions , terminal , reward ) : return self . memory . store ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward )
6126	def norm_and_check ( source_tree , requested ) : if os . path . isabs ( requested ) : raise ValueError ( "paths must be relative" ) abs_source = os . path . abspath ( source_tree ) abs_requested = os . path . normpath ( os . path . join ( abs_source , requested ) ) norm_source = os . path . normcase ( abs_source ) norm_requested = os . path . normcase ( abs_requested ) if os . path . commonprefix ( [ norm_source , norm_requested ] ) != norm_source : raise ValueError ( "paths must be inside source tree" ) return abs_requested
11484	def _upload_folder_as_item ( local_folder , parent_folder_id , reuse_existing = False ) : item_id = _create_or_reuse_item ( local_folder , parent_folder_id , reuse_existing ) subdir_contents = sorted ( os . listdir ( local_folder ) ) filecount = len ( subdir_contents ) for ( ind , current_file ) in enumerate ( subdir_contents ) : file_path = os . path . join ( local_folder , current_file ) log_ind = '({0} of {1})' . format ( ind + 1 , filecount ) _create_bitstream ( file_path , current_file , item_id , log_ind ) for callback in session . item_upload_callbacks : callback ( session . communicator , session . token , item_id )
5385	def _get_operation_input_field_values ( self , metadata , file_input ) : input_args = metadata [ 'request' ] [ 'ephemeralPipeline' ] [ 'inputParameters' ] vals_dict = metadata [ 'request' ] [ 'pipelineArgs' ] [ 'inputs' ] names = [ arg [ 'name' ] for arg in input_args if ( 'localCopy' in arg ) == file_input ] return { name : vals_dict [ name ] for name in names if name in vals_dict }
5499	def add_tweets ( self , url , last_modified , tweets ) : try : self . cache [ url ] = { "last_modified" : last_modified , "tweets" : tweets } self . mark_updated ( ) return True except TypeError : return False
13346	def run ( * args , ** kwargs ) : kwargs . setdefault ( 'env' , os . environ ) kwargs . setdefault ( 'shell' , True ) try : subprocess . check_call ( ' ' . join ( args ) , ** kwargs ) return True except subprocess . CalledProcessError : logger . debug ( 'Error running: {}' . format ( args ) ) return False
10551	def update_result ( result ) : try : result_id = result . id result = _forbidden_attributes ( result ) res = _pybossa_req ( 'put' , 'result' , result_id , payload = result . data ) if res . get ( 'id' ) : return Result ( res ) else : return res except : raise
8312	def draw_list ( markup , x , y , w , padding = 5 , callback = None ) : try : from web import _ctx except : pass i = 1 for chunk in markup . split ( "\n" ) : if callback != None : callback ( chunk , i ) m = re . search ( "^([0-9]{1,3}\. )" , chunk . lstrip ( ) ) if m : indent = re . search ( "[0-9]" , chunk ) . start ( ) * padding * 2 bullet = m . group ( 1 ) dx = textwidth ( "000." ) chunk = chunk . lstrip ( m . group ( 1 ) + "\t" ) if chunk . lstrip ( ) . startswith ( "*" ) : indent = chunk . find ( "*" ) * padding * 2 bullet = u"•" dx = textwidth ( "*" ) chunk = chunk . lstrip ( "* \t" ) _ctx . text ( bullet , x + indent , y ) dx += padding + indent _ctx . text ( chunk , x + dx , y , width = w - dx ) y += _ctx . textheight ( chunk , width = w - dx ) y += _ctx . textheight ( " " ) * 0.25 i += 1
9542	def add_header_check ( self , code = HEADER_CHECK_FAILED , message = MESSAGES [ HEADER_CHECK_FAILED ] ) : t = code , message self . _header_checks . append ( t )
3803	def calculate ( self , T , P , zs , ws , method ) : r if method == SIMPLE : ks = [ i ( T , P ) for i in self . ThermalConductivityLiquids ] return mixing_simple ( zs , ks ) elif method == DIPPR_9H : ks = [ i ( T , P ) for i in self . ThermalConductivityLiquids ] return DIPPR9H ( ws , ks ) elif method == FILIPPOV : ks = [ i ( T , P ) for i in self . ThermalConductivityLiquids ] return Filippov ( ws , ks ) elif method == MAGOMEDOV : k_w = self . ThermalConductivityLiquids [ self . index_w ] ( T , P ) ws = list ( ws ) ws . pop ( self . index_w ) return thermal_conductivity_Magomedov ( T , P , ws , self . wCASs , k_w ) else : raise Exception ( 'Method not valid' )
738	def compute ( self , activeColumns , learn = True ) : bottomUpInput = numpy . zeros ( self . numberOfCols , dtype = dtype ) bottomUpInput [ list ( activeColumns ) ] = 1 super ( TemporalMemoryShim , self ) . compute ( bottomUpInput , enableLearn = learn , enableInference = True ) predictedState = self . getPredictedState ( ) self . predictiveCells = set ( numpy . flatnonzero ( predictedState ) )
6304	def find_effect_class ( self , path ) -> Type [ Effect ] : package_name , class_name = parse_package_string ( path ) if package_name : package = self . get_package ( package_name ) return package . find_effect_class ( class_name , raise_for_error = True ) for package in self . packages : effect_cls = package . find_effect_class ( class_name ) if effect_cls : return effect_cls raise EffectError ( "No effect class '{}' found in any packages" . format ( class_name ) )
9486	def ensure_instruction ( instruction : int ) -> bytes : if PY36 : return instruction . to_bytes ( 2 , byteorder = "little" ) else : return instruction . to_bytes ( 1 , byteorder = "little" )
5151	def merge_config ( template , config , list_identifiers = None ) : result = template . copy ( ) for key , value in config . items ( ) : if isinstance ( value , dict ) : node = result . get ( key , OrderedDict ( ) ) result [ key ] = merge_config ( node , value ) elif isinstance ( value , list ) and isinstance ( result . get ( key ) , list ) : result [ key ] = merge_list ( result [ key ] , value , list_identifiers ) else : result [ key ] = value return result
8410	def best_units ( self , sequence ) : ts_range = self . value ( max ( sequence ) ) - self . value ( min ( sequence ) ) package = self . determine_package ( sequence [ 0 ] ) if package == 'pandas' : cuts = [ ( 0.9 , 'us' ) , ( 0.9 , 'ms' ) , ( 0.9 , 's' ) , ( 9 , 'm' ) , ( 6 , 'h' ) , ( 4 , 'd' ) , ( 4 , 'w' ) , ( 4 , 'M' ) , ( 3 , 'y' ) ] denomination = NANOSECONDS base_units = 'ns' else : cuts = [ ( 0.9 , 's' ) , ( 9 , 'm' ) , ( 6 , 'h' ) , ( 4 , 'd' ) , ( 4 , 'w' ) , ( 4 , 'M' ) , ( 3 , 'y' ) ] denomination = SECONDS base_units = 'ms' for size , units in reversed ( cuts ) : if ts_range >= size * denomination [ units ] : return units return base_units
1538	def add_spout ( self , name , spout_cls , par , config = None , optional_outputs = None ) : spout_spec = spout_cls . spec ( name = name , par = par , config = config , optional_outputs = optional_outputs ) self . add_spec ( spout_spec ) return spout_spec
8117	def circle_line_intersection ( cx , cy , radius , x1 , y1 , x2 , y2 , infinite = False ) : dx = x2 - x1 dy = y2 - y1 A = dx * dx + dy * dy B = 2 * ( dx * ( x1 - cx ) + dy * ( y1 - cy ) ) C = pow ( x1 - cx , 2 ) + pow ( y1 - cy , 2 ) - radius * radius det = B * B - 4 * A * C if A <= 0.0000001 or det < 0 : return [ ] elif det == 0 : t = - B / ( 2 * A ) return [ ( x1 + t * dx , y1 + t * dy ) ] else : points = [ ] det2 = sqrt ( det ) t1 = ( - B + det2 ) / ( 2 * A ) t2 = ( - B - det2 ) / ( 2 * A ) if infinite or 0 <= t1 <= 1 : points . append ( ( x1 + t1 * dx , y1 + t1 * dy ) ) if infinite or 0 <= t2 <= 1 : points . append ( ( x1 + t2 * dx , y1 + t2 * dy ) ) return points
7643	def convert ( annotation , target_namespace ) : annotation . validate ( strict = True ) if annotation . namespace == target_namespace : return annotation if target_namespace in __CONVERSION__ : annotation = deepcopy ( annotation ) for source in __CONVERSION__ [ target_namespace ] : if annotation . search ( namespace = source ) : return __CONVERSION__ [ target_namespace ] [ source ] ( annotation ) raise NamespaceError ( 'Unable to convert annotation from namespace=' '"{0}" to "{1}"' . format ( annotation . namespace , target_namespace ) )
5743	def update_running_pids ( old_procs ) : new_procs = [ ] for proc in old_procs : if proc . poll ( ) is None and check_pid ( proc . pid ) : publisher . debug ( str ( proc . pid ) + ' is alive' ) new_procs . append ( proc ) else : try : publisher . debug ( str ( proc . pid ) + ' is gone' ) os . kill ( proc . pid , signal . SIGKILL ) except : pass return new_procs
1225	def setup_components_and_tf_funcs ( self , custom_getter = None ) : custom_getter = super ( MemoryModel , self ) . setup_components_and_tf_funcs ( custom_getter ) self . memory = Memory . from_spec ( spec = self . memory_spec , kwargs = dict ( states = self . states_spec , internals = self . internals_spec , actions = self . actions_spec , summary_labels = self . summary_labels ) ) self . optimizer = Optimizer . from_spec ( spec = self . optimizer_spec , kwargs = dict ( summary_labels = self . summary_labels ) ) self . fn_discounted_cumulative_reward = tf . make_template ( name_ = 'discounted-cumulative-reward' , func_ = self . tf_discounted_cumulative_reward , custom_getter_ = custom_getter ) self . fn_reference = tf . make_template ( name_ = 'reference' , func_ = self . tf_reference , custom_getter_ = custom_getter ) self . fn_loss_per_instance = tf . make_template ( name_ = 'loss-per-instance' , func_ = self . tf_loss_per_instance , custom_getter_ = custom_getter ) self . fn_regularization_losses = tf . make_template ( name_ = 'regularization-losses' , func_ = self . tf_regularization_losses , custom_getter_ = custom_getter ) self . fn_loss = tf . make_template ( name_ = 'loss' , func_ = self . tf_loss , custom_getter_ = custom_getter ) self . fn_optimization = tf . make_template ( name_ = 'optimization' , func_ = self . tf_optimization , custom_getter_ = custom_getter ) self . fn_import_experience = tf . make_template ( name_ = 'import-experience' , func_ = self . tf_import_experience , custom_getter_ = custom_getter ) return custom_getter
5386	def _format_task_name ( job_id , task_id , task_attempt ) : docker_name = '%s.%s' % ( job_id , 'task' if task_id is None else task_id ) if task_attempt is not None : docker_name += '.' + str ( task_attempt ) return 'dsub-{}' . format ( _convert_suffix_to_docker_chars ( docker_name ) )
4259	def read_markdown ( filename ) : global MD with open ( filename , 'r' , encoding = 'utf-8-sig' ) as f : text = f . read ( ) if MD is None : MD = Markdown ( extensions = [ 'markdown.extensions.meta' , 'markdown.extensions.tables' ] , output_format = 'html5' ) else : MD . reset ( ) MD . Meta = { } output = { 'description' : Markup ( MD . convert ( text ) ) } try : meta = MD . Meta . copy ( ) except AttributeError : pass else : output [ 'meta' ] = meta try : output [ 'title' ] = MD . Meta [ 'title' ] [ 0 ] except KeyError : pass return output
5420	def _get_job_resources ( args ) : logging = param_util . build_logging_param ( args . logging ) if args . logging else None timeout = param_util . timeout_in_seconds ( args . timeout ) log_interval = param_util . log_interval_in_seconds ( args . log_interval ) return job_model . Resources ( min_cores = args . min_cores , min_ram = args . min_ram , machine_type = args . machine_type , disk_size = args . disk_size , disk_type = args . disk_type , boot_disk_size = args . boot_disk_size , preemptible = args . preemptible , image = args . image , regions = args . regions , zones = args . zones , logging = logging , logging_path = None , service_account = args . service_account , scopes = args . scopes , keep_alive = args . keep_alive , cpu_platform = args . cpu_platform , network = args . network , subnetwork = args . subnetwork , use_private_address = args . use_private_address , accelerator_type = args . accelerator_type , accelerator_count = args . accelerator_count , nvidia_driver_version = args . nvidia_driver_version , timeout = timeout , log_interval = log_interval , ssh = args . ssh )
8176	def iterscan ( self , string , idx = 0 , context = None ) : match = self . scanner . scanner ( string , idx ) . match actions = self . actions lastend = idx end = len ( string ) while True : m = match ( ) if m is None : break matchbegin , matchend = m . span ( ) if lastend == matchend : break action = actions [ m . lastindex ] if action is not None : rval , next_pos = action ( m , context ) if next_pos is not None and next_pos != matchend : matchend = next_pos match = self . scanner . scanner ( string , matchend ) . match yield rval , matchend lastend = matchend
7824	def finish ( self , data ) : if not self . _server_first_message : logger . debug ( "Got success too early" ) return Failure ( "bad-success" ) if self . _finished : return Success ( { "username" : self . username , "authzid" : self . authzid } ) else : ret = self . _final_challenge ( data ) if isinstance ( ret , Failure ) : return ret if self . _finished : return Success ( { "username" : self . username , "authzid" : self . authzid } ) else : logger . debug ( "Something went wrong when processing additional" " data with success?" ) return Failure ( "bad-success" )
1748	def _get_offset ( self , index ) : if not self . _in_range ( index ) : raise IndexError ( 'Map index out of range' ) if isinstance ( index , slice ) : index = slice ( index . start - self . start , index . stop - self . start ) else : index -= self . start return index
4493	def list_ ( args ) : osf = _setup_osf ( args ) project = osf . project ( args . project ) for store in project . storages : prefix = store . name for file_ in store . files : path = file_ . path if path . startswith ( '/' ) : path = path [ 1 : ] print ( os . path . join ( prefix , path ) )
6175	def init_app ( self , app ) : _state . _register_app = self . original_register_app if not hasattr ( app , 'extensions' ) : app . extensions = dict ( ) if 'celery' in app . extensions : raise ValueError ( 'Already registered extension CELERY.' ) app . extensions [ 'celery' ] = _CeleryState ( self , app ) super ( Celery , self ) . __init__ ( app . import_name , broker = app . config [ 'CELERY_BROKER_URL' ] ) if 'CELERY_RESULT_BACKEND' in app . config : self . _preconf [ 'CELERY_RESULT_BACKEND' ] = app . config [ 'CELERY_RESULT_BACKEND' ] self . conf . update ( app . config ) task_base = self . Task class ContextTask ( task_base ) : def __call__ ( self , * _args , ** _kwargs ) : with app . app_context ( ) : return task_base . __call__ ( self , * _args , ** _kwargs ) setattr ( ContextTask , 'abstract' , True ) setattr ( self , 'Task' , ContextTask )
1908	def forward_events_to ( self , sink , include_source = False ) : assert isinstance ( sink , Eventful ) , f'{sink.__class__.__name__} is not Eventful' self . _forwards [ sink ] = include_source
5307	def rgb_to_ansi16 ( r , g , b , use_bright = False ) : ansi_b = round ( b / 255.0 ) << 2 ansi_g = round ( g / 255.0 ) << 1 ansi_r = round ( r / 255.0 ) ansi = ( 90 if use_bright else 30 ) + ( ansi_b | ansi_g | ansi_r ) return ansi
2470	def set_file_notice ( self , doc , text ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_notice_set : self . file_notice_set = True if validations . validate_file_notice ( text ) : self . file ( doc ) . notice = str_from_text ( text ) else : raise SPDXValueError ( 'File::Notice' ) else : raise CardinalityError ( 'File::Notice' ) else : raise OrderError ( 'File::Notice' )
3674	def draw_3d ( self , width = 300 , height = 500 , style = 'stick' , Hs = True ) : r try : import py3Dmol from IPython . display import display if Hs : mol = self . rdkitmol_Hs else : mol = self . rdkitmol AllChem . EmbedMultipleConfs ( mol ) mb = Chem . MolToMolBlock ( mol ) p = py3Dmol . view ( width = width , height = height ) p . addModel ( mb , 'sdf' ) p . setStyle ( { style : { } } ) p . zoomTo ( ) display ( p . show ( ) ) except : return 'py3Dmol, RDKit, and IPython are required for this feature.'
12374	def take_snapshot ( droplet , name ) : print "powering off" droplet . power_off ( ) droplet . wait ( ) print "taking snapshot" droplet . take_snapshot ( name ) droplet . wait ( ) snapshots = droplet . snapshots ( ) print "Current snapshots" print snapshots
4797	def is_before ( self , other ) : if type ( self . val ) is not datetime . datetime : raise TypeError ( 'val must be datetime, but was type <%s>' % type ( self . val ) . __name__ ) if type ( other ) is not datetime . datetime : raise TypeError ( 'given arg must be datetime, but was type <%s>' % type ( other ) . __name__ ) if self . val >= other : self . _err ( 'Expected <%s> to be before <%s>, but was not.' % ( self . val . strftime ( '%Y-%m-%d %H:%M:%S' ) , other . strftime ( '%Y-%m-%d %H:%M:%S' ) ) ) return self
404	def swish ( x , name = 'swish' ) : with tf . name_scope ( name ) : x = tf . nn . sigmoid ( x ) * x return x
4249	def netspeed_by_name ( self , hostname ) : addr = self . _gethostbyname ( hostname ) return self . netspeed_by_addr ( addr )
12041	def matrixToHTML ( data , names = None , units = None , bookName = None , sheetName = None , xCol = None ) : if not names : names = [ "" ] * len ( data [ 0 ] ) if data . dtype . names : names = list ( data . dtype . names ) if not units : units = [ "" ] * len ( data [ 0 ] ) for i in range ( len ( units ) ) : if names [ i ] in UNITS . keys ( ) : units [ i ] = UNITS [ names [ i ] ] if 'recarray' in str ( type ( data ) ) : data = data . view ( float ) . reshape ( data . shape + ( - 1 , ) ) if xCol and xCol in names : xCol = names . index ( xCol ) names . insert ( 0 , names [ xCol ] ) units . insert ( 0 , units [ xCol ] ) data = np . insert ( data , 0 , data [ : , xCol ] , 1 ) htmlFname = tempfile . gettempdir ( ) + "/swhlab/WKS-%s.%s.html" % ( bookName , sheetName ) html = html += "<h1>FauxRigin</h1>" if bookName or sheetName : html += '<code><b>%s / %s</b></code><br><br>' % ( bookName , sheetName ) html += "<table>" colNames = [ '' ] for i in range ( len ( units ) ) : label = "%s (%d)" % ( chr ( i + ord ( 'A' ) ) , i ) colNames . append ( label ) html += htmlListToTR ( colNames , 'labelCol' , 'labelCol' ) html += htmlListToTR ( [ 'Long Name' ] + list ( names ) , 'name' , td1Class = 'labelRow' ) html += htmlListToTR ( [ 'Units' ] + list ( units ) , 'units' , td1Class = 'labelRow' ) cutOff = False for y in range ( len ( data ) ) : html += htmlListToTR ( [ y + 1 ] + list ( data [ y ] ) , trClass = 'data%d' % ( y % 2 ) , td1Class = 'labelRow' ) if y >= 200 : cutOff = True break html += "</table>" html = html . replace ( ">nan<" , ">--<" ) html = html . replace ( ">None<" , "><" ) if cutOff : html += "<h3>... showing only %d of %d rows ...</h3>" % ( y , len ( data ) ) html += "</body></html>" with open ( htmlFname , 'w' ) as f : f . write ( html ) webbrowser . open ( htmlFname ) return
13493	def read ( args ) : if args . config_file is None or not isfile ( args . config_file ) : return logging . info ( "Reading configure file: %s" % args . config_file ) config = cparser . ConfigParser ( ) config . read ( args . config_file ) if not config . has_section ( 'lrcloud' ) : raise RuntimeError ( "Configure file has no [lrcloud] section!" ) for ( name , value ) in config . items ( 'lrcloud' ) : if value == "True" : value = True elif value == "False" : value = False if getattr ( args , name ) is None : setattr ( args , name , value )
9422	def _process_current ( self , handle , op , dest_path = None , dest_name = None ) : unrarlib . RARProcessFileW ( handle , op , dest_path , dest_name )
11867	def normalize ( self ) : "Return my probabilities; must be down to one variable." assert len ( self . vars ) == 1 return ProbDist ( self . vars [ 0 ] , dict ( ( k , v ) for ( ( k , ) , v ) in self . cpt . items ( ) ) )
7271	def load ( ) : for operator in operators : module , symbols = operator [ 0 ] , operator [ 1 : ] path = 'grappa.operators.{}' . format ( module ) operator = __import__ ( path , None , None , symbols ) for symbol in symbols : Engine . register ( getattr ( operator , symbol ) )
8783	def _get_base_network_info ( self , context , network_id , base_net_driver ) : driver_name = base_net_driver . get_name ( ) net_info = { "network_type" : driver_name } LOG . debug ( '_get_base_network_info: %s %s' % ( driver_name , network_id ) ) if driver_name == 'NVP' : LOG . debug ( 'looking up lswitch ids for network %s' % ( network_id ) ) lswitch_ids = base_net_driver . get_lswitch_ids_for_network ( context , network_id ) if not lswitch_ids or len ( lswitch_ids ) > 1 : msg = ( 'lswitch id lookup failed, %s ids found.' % ( len ( lswitch_ids ) ) ) LOG . error ( msg ) raise IronicException ( msg ) lswitch_id = lswitch_ids . pop ( ) LOG . info ( 'found lswitch for network %s: %s' % ( network_id , lswitch_id ) ) net_info [ 'lswitch_id' ] = lswitch_id LOG . debug ( '_get_base_network_info finished: %s %s %s' % ( driver_name , network_id , net_info ) ) return net_info
10815	def invite_by_emails ( self , emails ) : assert emails is None or isinstance ( emails , list ) results = [ ] for email in emails : try : user = User . query . filter_by ( email = email ) . one ( ) results . append ( self . invite ( user ) ) except NoResultFound : results . append ( None ) return results
13297	def default ( self , obj ) : if isinstance ( obj , datetime . datetime ) : return self . _encode_datetime ( obj ) return json . JSONEncoder . default ( self , obj )
3430	def add_reactions ( self , reaction_list ) : def existing_filter ( rxn ) : if rxn . id in self . reactions : LOGGER . warning ( "Ignoring reaction '%s' since it already exists." , rxn . id ) return False return True pruned = DictList ( filter ( existing_filter , reaction_list ) ) context = get_context ( self ) for reaction in pruned : reaction . _model = self for metabolite in list ( reaction . metabolites ) : if metabolite not in self . metabolites : self . add_metabolites ( metabolite ) else : stoichiometry = reaction . _metabolites . pop ( metabolite ) model_metabolite = self . metabolites . get_by_id ( metabolite . id ) reaction . _metabolites [ model_metabolite ] = stoichiometry model_metabolite . _reaction . add ( reaction ) if context : context ( partial ( model_metabolite . _reaction . remove , reaction ) ) for gene in list ( reaction . _genes ) : if not self . genes . has_id ( gene . id ) : self . genes += [ gene ] gene . _model = self if context : context ( partial ( self . genes . __isub__ , [ gene ] ) ) context ( partial ( setattr , gene , '_model' , None ) ) else : model_gene = self . genes . get_by_id ( gene . id ) if model_gene is not gene : reaction . _dissociate_gene ( gene ) reaction . _associate_gene ( model_gene ) self . reactions += pruned if context : context ( partial ( self . reactions . __isub__ , pruned ) ) self . _populate_solver ( pruned )
10961	def create_img ( ) : rad = 0.5 * np . random . randn ( POS . shape [ 0 ] ) + 4.5 part = objs . PlatonicSpheresCollection ( POS , rad , zscale = 0.89 ) slab = objs . Slab ( zpos = 4.92 , angles = ( - 4.7e-3 , - 7.3e-4 ) ) objects = comp . ComponentCollection ( [ part , slab ] , category = 'obj' ) p = exactpsf . FixedSSChebLinePSF ( kfki = 1.07 , zslab = - 29.3 , alpha = 1.17 , n2n1 = 0.98 , sigkf = - 0.33 , zscale = 0.89 , laser_wavelength = 0.45 ) i = ilms . BarnesStreakLegPoly2P1D ( npts = ( 16 , 10 , 8 , 4 ) , zorder = 8 ) b = ilms . LegendrePoly2P1D ( order = ( 7 , 2 , 2 ) , category = 'bkg' ) off = comp . GlobalScalar ( name = 'offset' , value = - 2.11 ) mdl = models . ConfocalImageModel ( ) st = states . ImageState ( util . NullImage ( shape = [ 48 , 64 , 64 ] ) , [ objects , p , i , b , off ] , mdl = mdl , model_as_data = True ) b . update ( b . params , BKGVALS ) i . update ( i . params , ILMVALS ) im = st . model + np . random . randn ( * st . model . shape ) * 0.03 return util . Image ( im )
12641	def get_config_value ( name , fallback = None ) : cli_config = CLIConfig ( SF_CLI_CONFIG_DIR , SF_CLI_ENV_VAR_PREFIX ) return cli_config . get ( 'servicefabric' , name , fallback )
10130	def timezone ( haystack_tz , version = LATEST_VER ) : tz_map = get_tz_map ( version = version ) try : tz_name = tz_map [ haystack_tz ] except KeyError : raise ValueError ( '%s is not a recognised timezone on this host' % haystack_tz ) return pytz . timezone ( tz_name )
11911	def get_version ( filename , pattern ) : with open ( filename ) as f : match = re . search ( r"^(\s*%s\s*=\s*')(.+?)(')(?sm)" % pattern , f . read ( ) ) if match : before , version , after = match . groups ( ) return version fail ( 'Could not find {} in {}' . format ( pattern , filename ) )
13559	def get_top_assets ( self ) : images = self . get_all_images ( ) [ 0 : 14 ] video = [ ] if supports_video : video = self . eventvideo_set . all ( ) [ 0 : 10 ] return list ( chain ( images , video ) ) [ 0 : 15 ]
10325	def _binomial_pmf ( n , p ) : n = int ( n ) ret = np . empty ( n + 1 ) nmax = int ( np . round ( p * n ) ) ret [ nmax ] = 1.0 old_settings = np . seterr ( under = 'ignore' ) for i in range ( nmax + 1 , n + 1 ) : ret [ i ] = ret [ i - 1 ] * ( n - i + 1.0 ) / i * p / ( 1.0 - p ) for i in range ( nmax - 1 , - 1 , - 1 ) : ret [ i ] = ret [ i + 1 ] * ( i + 1.0 ) / ( n - i ) * ( 1.0 - p ) / p np . seterr ( ** old_settings ) return ret / ret . sum ( )
10451	def grabfocus ( self , window_name , object_name = None ) : if not object_name : handle , name , app = self . _get_window_handle ( window_name ) else : handle = self . _get_object_handle ( window_name , object_name ) return self . _grabfocus ( handle )
4928	def transform_image ( self , content_metadata_item ) : image_url = '' if content_metadata_item [ 'content_type' ] in [ 'course' , 'program' ] : image_url = content_metadata_item . get ( 'card_image_url' ) elif content_metadata_item [ 'content_type' ] == 'courserun' : image_url = content_metadata_item . get ( 'image_url' ) return image_url
1867	def PMOVMSKB ( cpu , op0 , op1 ) : arg0 = op0 . read ( ) arg1 = op1 . read ( ) res = 0 for i in reversed ( range ( 7 , op1 . size , 8 ) ) : res = ( res << 1 ) | ( ( arg1 >> i ) & 1 ) op0 . write ( Operators . EXTRACT ( res , 0 , op0 . size ) )
12196	def get_tasks ( ) : task_classes = [ ] for task_path in TASKS : try : module , classname = task_path . rsplit ( '.' , 1 ) except ValueError : raise ImproperlyConfigured ( '%s isn\'t a task module' % task_path ) try : mod = import_module ( module ) except ImportError as e : raise ImproperlyConfigured ( 'Error importing task %s: "%s"' % ( module , e ) ) try : task_class = getattr ( mod , classname ) except AttributeError : raise ImproperlyConfigured ( 'Task module "%s" does not define a ' '"%s" class' % ( module , classname ) ) task_classes . append ( task_class ) return task_classes
5573	def read ( self , validity_check = True , no_neighbors = False , ** kwargs ) : if no_neighbors : raise NotImplementedError ( ) return self . _from_cache ( validity_check = validity_check )
11367	def locate ( pattern , root = os . curdir ) : for path , dummy , files in os . walk ( os . path . abspath ( root ) ) : for filename in fnmatch . filter ( files , pattern ) : yield os . path . join ( path , filename )
471	def read_analogies_file ( eval_file = 'questions-words.txt' , word2id = None ) : if word2id is None : word2id = { } questions = [ ] questions_skipped = 0 with open ( eval_file , "rb" ) as analogy_f : for line in analogy_f : if line . startswith ( b":" ) : continue words = line . strip ( ) . lower ( ) . split ( b" " ) ids = [ word2id . get ( w . strip ( ) ) for w in words ] if None in ids or len ( ids ) != 4 : questions_skipped += 1 else : questions . append ( np . array ( ids ) ) tl . logging . info ( "Eval analogy file: %s" % eval_file ) tl . logging . info ( "Questions: %d" , len ( questions ) ) tl . logging . info ( "Skipped: %d" , questions_skipped ) analogy_questions = np . array ( questions , dtype = np . int32 ) return analogy_questions
11602	def save_formset ( self , request , form , formset , change ) : instances = formset . save ( commit = False ) for instance in instances : if isinstance ( instance , Photo ) : instance . author = request . user instance . save ( )
11224	def dump_deque ( self , obj , class_name = "collections.deque" ) : return { "$" + class_name : [ self . _json_convert ( item ) for item in obj ] }
1273	def from_spec ( spec ) : exploration = util . get_object ( obj = spec , predefined_objects = tensorforce . core . explorations . explorations ) assert isinstance ( exploration , Exploration ) return exploration
11335	def prompt ( question , choices = None ) : if not re . match ( "\s$" , question ) : question = "{}: " . format ( question ) while True : if sys . version_info [ 0 ] > 2 : answer = input ( question ) else : answer = raw_input ( question ) if not choices or answer in choices : break return answer
11408	def record_delete_field ( rec , tag , ind1 = ' ' , ind2 = ' ' , field_position_global = None , field_position_local = None ) : error = _validate_record_field_positions_global ( rec ) if error : pass if tag not in rec : return False ind1 , ind2 = _wash_indicators ( ind1 , ind2 ) deleted = [ ] newfields = [ ] if field_position_global is None and field_position_local is None : for field in rec [ tag ] : if field [ 1 ] != ind1 or field [ 2 ] != ind2 : newfields . append ( field ) else : deleted . append ( field ) rec [ tag ] = newfields elif field_position_global is not None : for field in rec [ tag ] : if ( field [ 1 ] != ind1 and field [ 2 ] != ind2 or field [ 4 ] != field_position_global ) : newfields . append ( field ) else : deleted . append ( field ) rec [ tag ] = newfields elif field_position_local is not None : try : del rec [ tag ] [ field_position_local ] except IndexError : return [ ] if not rec [ tag ] : del rec [ tag ] return deleted
7615	def get_datetime ( self , timestamp : str , unix = True ) : time = datetime . strptime ( timestamp , '%Y%m%dT%H%M%S.%fZ' ) if unix : return int ( time . timestamp ( ) ) else : return time
417	def find_datasets ( self , dataset_name = None , ** kwargs ) : self . _fill_project_info ( kwargs ) if dataset_name is None : raise Exception ( "dataset_name is None, please give a dataset name" ) kwargs . update ( { 'dataset_name' : dataset_name } ) s = time . time ( ) pc = self . db . Dataset . find ( kwargs ) if pc is not None : dataset_id_list = pc . distinct ( 'dataset_id' ) dataset_list = [ ] for dataset_id in dataset_id_list : tmp = self . dataset_fs . get ( dataset_id ) . read ( ) dataset_list . append ( self . _deserialization ( tmp ) ) else : print ( "[Database] FAIL! Cannot find any dataset: {}" . format ( kwargs ) ) return False print ( "[Database] Find {} datasets SUCCESS, took: {}s" . format ( len ( dataset_list ) , round ( time . time ( ) - s , 2 ) ) ) return dataset_list
9448	def hangup_call ( self , call_params ) : path = '/' + self . api_version + '/HangupCall/' method = 'POST' return self . request ( path , method , call_params )
13561	def launch ( title , items , selected = None ) : resp = { "code" : - 1 , "done" : False } curses . wrapper ( Menu , title , items , selected , resp ) return resp
12081	def figure_protocols ( self ) : self . log . debug ( "creating overlayed protocols plot" ) self . figure ( ) for sweep in range ( self . abf . sweeps ) : self . abf . setsweep ( sweep ) plt . plot ( self . abf . protoX , self . abf . protoY , color = 'r' ) self . marginX = 0 self . decorate ( protocol = True )
10100	def get_snippet ( self , snippet_id , timeout = None ) : return self . _api_request ( self . SNIPPET_ENDPOINT % ( snippet_id ) , self . HTTP_GET , timeout = timeout )
2836	def read ( self , length , assert_ss = True , deassert_ss = True ) : if self . _miso is None : raise RuntimeError ( 'Read attempted with no MISO pin specified.' ) if assert_ss and self . _ss is not None : self . _gpio . set_low ( self . _ss ) result = bytearray ( length ) for i in range ( length ) : for j in range ( 8 ) : self . _gpio . output ( self . _sclk , not self . _clock_base ) if self . _read_leading : if self . _gpio . is_high ( self . _miso ) : result [ i ] |= self . _read_shift ( self . _mask , j ) else : result [ i ] &= ~ self . _read_shift ( self . _mask , j ) self . _gpio . output ( self . _sclk , self . _clock_base ) if not self . _read_leading : if self . _gpio . is_high ( self . _miso ) : result [ i ] |= self . _read_shift ( self . _mask , j ) else : result [ i ] &= ~ self . _read_shift ( self . _mask , j ) if deassert_ss and self . _ss is not None : self . _gpio . set_high ( self . _ss ) return result
7794	def register_fetcher ( self , object_class , fetcher_class ) : self . _lock . acquire ( ) try : cache = self . _caches . get ( object_class ) if not cache : cache = Cache ( self . max_items , self . default_freshness_period , self . default_expiration_period , self . default_purge_period ) self . _caches [ object_class ] = cache cache . set_fetcher ( fetcher_class ) finally : self . _lock . release ( )
6661	def generate_self_signed_certificate ( self , domain = '' , r = None ) : r = self . local_renderer r . env . domain = domain or r . env . domain assert r . env . domain , 'No SSL domain defined.' role = r or self . genv . ROLE or ALL ssl_dst = 'roles/%s/ssl' % ( role , ) if not os . path . isdir ( ssl_dst ) : os . makedirs ( ssl_dst ) r . env . base_dst = '%s/%s' % ( ssl_dst , r . env . domain ) r . local ( 'openssl req -new -newkey rsa:{ssl_length} ' '-days {ssl_days} -nodes -x509 ' '-subj "/C={ssl_country}/ST={ssl_state}/L={ssl_city}/O={ssl_organization}/CN={ssl_domain}" ' '-keyout {ssl_base_dst}.key -out {ssl_base_dst}.crt' )
3304	def _run_paste ( app , config , mode ) : from paste import httpserver version = "WsgiDAV/{} {} Python {}" . format ( __version__ , httpserver . WSGIHandler . server_version , util . PYTHON_VERSION ) _logger . info ( "Running {}..." . format ( version ) ) server = httpserver . serve ( app , host = config [ "host" ] , port = config [ "port" ] , server_version = version , protocol_version = "HTTP/1.1" , start_loop = False , ) if config [ "verbose" ] >= 5 : __handle_one_request = server . RequestHandlerClass . handle_one_request def handle_one_request ( self ) : __handle_one_request ( self ) if self . close_connection == 1 : _logger . debug ( "HTTP Connection : close" ) else : _logger . debug ( "HTTP Connection : continue" ) server . RequestHandlerClass . handle_one_request = handle_one_request server . RequestHandlerClass . handle_one_request = handle_one_request host , port = server . server_address if host == "0.0.0.0" : _logger . info ( "Serving on 0.0.0.0:{} view at {}://127.0.0.1:{}" . format ( port , "http" , port ) ) else : _logger . info ( "Serving on {}://{}:{}" . format ( "http" , host , port ) ) try : server . serve_forever ( ) except KeyboardInterrupt : _logger . warning ( "Caught Ctrl-C, shutting down..." ) return
13184	def row_to_dict ( cls , row ) : comment_code = row [ 3 ] if comment_code . lower ( ) == 'na' : comment_code = '' comp1 = row [ 4 ] if comp1 . lower ( ) == 'na' : comp1 = '' comp2 = row [ 5 ] if comp2 . lower ( ) == 'na' : comp2 = '' chart = row [ 6 ] if chart . lower ( ) == 'na' : chart = '' notes = row [ 7 ] if notes . lower ( ) == 'na' : notes = '' return { 'name' : row [ 0 ] , 'date' : row [ 1 ] , 'magnitude' : row [ 2 ] , 'comment_code' : comment_code , 'comp1' : comp1 , 'comp2' : comp2 , 'chart' : chart , 'notes' : notes , }
8582	def get_attached_volumes ( self , datacenter_id , server_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/servers/%s/volumes?depth=%s' % ( datacenter_id , server_id , str ( depth ) ) ) return response
10505	def stopEventLoop ( ) : stopper = PyObjCAppHelperRunLoopStopper_wrap . currentRunLoopStopper ( ) if stopper is None : if NSApp ( ) is not None : NSApp ( ) . terminate_ ( None ) return True return False NSTimer . scheduledTimerWithTimeInterval_target_selector_userInfo_repeats_ ( 0.0 , stopper , 'performStop:' , None , False ) return True
13128	def tree2commands ( self , adapter , session , lastcmds , xsync ) : assert xsync . tag == constants . NODE_SYNCML assert len ( xsync ) == 2 assert xsync [ 0 ] . tag == constants . CMD_SYNCHDR assert xsync [ 1 ] . tag == constants . NODE_SYNCBODY version = xsync [ 0 ] . findtext ( 'VerProto' ) if version != constants . SYNCML_VERSION_1_2 : raise common . FeatureNotSupported ( 'unsupported SyncML version "%s" (expected "%s")' % ( version , constants . SYNCML_VERSION_1_2 ) ) verdtd = xsync [ 0 ] . findtext ( 'VerDTD' ) if verdtd != constants . SYNCML_DTD_VERSION_1_2 : raise common . FeatureNotSupported ( 'unsupported SyncML DTD version "%s" (expected "%s")' % ( verdtd , constants . SYNCML_DTD_VERSION_1_2 ) ) ret = self . initialize ( adapter , session , xsync ) hdrcmd = ret [ 0 ] if session . isServer : log . debug ( 'received request SyncML message from "%s" (s%s.m%s)' , hdrcmd . target , hdrcmd . sessionID , hdrcmd . msgID ) else : log . debug ( 'received response SyncML message from "%s" (s%s.m%s)' , lastcmds [ 0 ] . target , lastcmds [ 0 ] . sessionID , lastcmds [ 0 ] . msgID ) try : return self . _tree2commands ( adapter , session , lastcmds , xsync , ret ) except Exception , e : if not session . isServer : raise code = '%s.%s' % ( e . __class__ . __module__ , e . __class__ . __name__ ) msg = '' . join ( traceback . format_exception_only ( type ( e ) , e ) ) . strip ( ) log . exception ( 'failed while interpreting command tree: %s' , msg ) return [ hdrcmd , state . Command ( name = constants . CMD_STATUS , cmdID = '1' , msgRef = session . pendingMsgID , cmdRef = 0 , sourceRef = xsync [ 0 ] . findtext ( 'Source/LocURI' ) , targetRef = xsync [ 0 ] . findtext ( 'Target/LocURI' ) , statusOf = constants . CMD_SYNCHDR , statusCode = constants . STATUS_COMMAND_FAILED , errorCode = code , errorMsg = msg , errorTrace = '' . join ( traceback . format_exception ( type ( e ) , e , sys . exc_info ( ) [ 2 ] ) ) , ) , state . Command ( name = constants . CMD_FINAL ) ]
10261	def _collapse_variants_by_function ( graph : BELGraph , func : str ) -> None : for parent_node , variant_node , data in graph . edges ( data = True ) : if data [ RELATION ] == HAS_VARIANT and parent_node . function == func : collapse_pair ( graph , from_node = variant_node , to_node = parent_node )
73	def Emboss ( alpha = 0 , strength = 1 , name = None , deterministic = False , random_state = None ) : alpha_param = iap . handle_continuous_param ( alpha , "alpha" , value_range = ( 0 , 1.0 ) , tuple_to_uniform = True , list_to_choice = True ) strength_param = iap . handle_continuous_param ( strength , "strength" , value_range = ( 0 , None ) , tuple_to_uniform = True , list_to_choice = True ) def create_matrices ( image , nb_channels , random_state_func ) : alpha_sample = alpha_param . draw_sample ( random_state = random_state_func ) ia . do_assert ( 0 <= alpha_sample <= 1.0 ) strength_sample = strength_param . draw_sample ( random_state = random_state_func ) matrix_nochange = np . array ( [ [ 0 , 0 , 0 ] , [ 0 , 1 , 0 ] , [ 0 , 0 , 0 ] ] , dtype = np . float32 ) matrix_effect = np . array ( [ [ - 1 - strength_sample , 0 - strength_sample , 0 ] , [ 0 - strength_sample , 1 , 0 + strength_sample ] , [ 0 , 0 + strength_sample , 1 + strength_sample ] ] , dtype = np . float32 ) matrix = ( 1 - alpha_sample ) * matrix_nochange + alpha_sample * matrix_effect return [ matrix ] * nb_channels if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return Convolve ( create_matrices , name = name , deterministic = deterministic , random_state = random_state )
8927	def dist ( ctx , devpi = False , egg = False , wheel = False , auto = True ) : config . load ( ) cmd = [ "python" , "setup.py" , "sdist" ] if auto : egg = sys . version_info . major == 2 try : import wheel as _ wheel = True except ImportError : wheel = False if egg : cmd . append ( "bdist_egg" ) if wheel : cmd . append ( "bdist_wheel" ) ctx . run ( "invoke clean --all build --docs test check" ) ctx . run ( ' ' . join ( cmd ) ) if devpi : ctx . run ( "devpi upload dist/*" )
2327	def orient_undirected_graph ( self , data , graph ) : self . arguments [ '{VERBOSE}' ] = str ( self . verbose ) . upper ( ) self . arguments [ '{SCORE}' ] = self . score self . arguments [ '{BETA}' ] = str ( self . beta ) self . arguments [ '{OPTIM}' ] = str ( self . optim ) . upper ( ) self . arguments [ '{ALPHA}' ] = str ( self . alpha ) whitelist = DataFrame ( list ( nx . edges ( graph ) ) , columns = [ "from" , "to" ] ) blacklist = DataFrame ( list ( nx . edges ( nx . DiGraph ( DataFrame ( - nx . adj_matrix ( graph , weight = None ) . to_dense ( ) + 1 , columns = list ( graph . nodes ( ) ) , index = list ( graph . nodes ( ) ) ) ) ) ) , columns = [ "from" , "to" ] ) results = self . _run_bnlearn ( data , whitelist = whitelist , blacklist = blacklist , verbose = self . verbose ) return nx . relabel_nodes ( nx . DiGraph ( results ) , { idx : i for idx , i in enumerate ( data . columns ) } )
11443	def _compare_lists ( list1 , list2 , custom_cmp ) : if len ( list1 ) != len ( list2 ) : return False for element1 , element2 in zip ( list1 , list2 ) : if not custom_cmp ( element1 , element2 ) : return False return True
11396	def get_record ( self ) : self . update_system_numbers ( ) self . add_systemnumber ( "CDS" ) self . fields_list = [ "024" , "041" , "035" , "037" , "088" , "100" , "110" , "111" , "242" , "245" , "246" , "260" , "269" , "300" , "502" , "650" , "653" , "693" , "700" , "710" , "773" , "856" , "520" , "500" , "980" ] self . keep_only_fields ( ) self . determine_collections ( ) self . add_cms_link ( ) self . update_languages ( ) self . update_reportnumbers ( ) self . update_date ( ) self . update_pagenumber ( ) self . update_authors ( ) self . update_subject_categories ( "SzGeCERN" , "INSPIRE" , "categories_inspire" ) self . update_keywords ( ) self . update_experiments ( ) self . update_collaboration ( ) self . update_journals ( ) self . update_links_and_ffts ( ) if 'THESIS' in self . collections : self . update_thesis_supervisors ( ) self . update_thesis_information ( ) if 'NOTE' in self . collections : self . add_notes ( ) for collection in self . collections : record_add_field ( self . record , tag = '980' , subfields = [ ( 'a' , collection ) ] ) self . remove_controlfields ( ) return self . record
6799	def database_renderer ( self , name = None , site = None , role = None ) : name = name or self . env . default_db_name site = site or self . genv . SITE role = role or self . genv . ROLE key = ( name , site , role ) self . vprint ( 'checking key:' , key ) if key not in self . _database_renderers : self . vprint ( 'No cached db renderer, generating...' ) if self . verbose : print ( 'db.name:' , name ) print ( 'db.databases:' , self . env . databases ) print ( 'db.databases[%s]:' % name , self . env . databases . get ( name ) ) d = type ( self . genv ) ( self . lenv ) d . update ( self . get_database_defaults ( ) ) d . update ( self . env . databases . get ( name , { } ) ) d [ 'db_name' ] = name if self . verbose : print ( 'db.d:' ) pprint ( d , indent = 4 ) print ( 'db.connection_handler:' , d . connection_handler ) if d . connection_handler == CONNECTION_HANDLER_DJANGO : self . vprint ( 'Using django handler...' ) dj = self . get_satchel ( 'dj' ) if self . verbose : print ( 'Loading Django DB settings for site {} and role {}.' . format ( site , role ) , file = sys . stderr ) dj . set_db ( name = name , site = site , role = role ) _d = dj . local_renderer . collect_genv ( include_local = True , include_global = False ) for k , v in _d . items ( ) : if k . startswith ( 'dj_db_' ) : _d [ k [ 3 : ] ] = v del _d [ k ] if self . verbose : print ( 'Loaded:' ) pprint ( _d ) d . update ( _d ) elif d . connection_handler and d . connection_handler . startswith ( CONNECTION_HANDLER_CUSTOM + ':' ) : _callable_str = d . connection_handler [ len ( CONNECTION_HANDLER_CUSTOM + ':' ) : ] self . vprint ( 'Using custom handler %s...' % _callable_str ) _d = str_to_callable ( _callable_str ) ( role = self . genv . ROLE ) if self . verbose : print ( 'Loaded:' ) pprint ( _d ) d . update ( _d ) r = LocalRenderer ( self , lenv = d ) self . set_root_login ( r ) self . _database_renderers [ key ] = r else : self . vprint ( 'Cached db renderer found.' ) return self . _database_renderers [ key ]
4489	def might_need_auth ( f ) : @ wraps ( f ) def wrapper ( cli_args ) : try : return_value = f ( cli_args ) except UnauthorizedException as e : config = config_from_env ( config_from_file ( ) ) username = _get_username ( cli_args , config ) if username is None : sys . exit ( "Please set a username (run `osf -h` for details)." ) else : sys . exit ( "You are not authorized to access this project." ) return return_value return wrapper
10204	def _parse_time ( self , tokens ) : return self . time_parser . parse ( self . parse_keyword ( Keyword . WHERE , tokens ) )
13539	def get_locations ( self ) : url = "/2/locations" data = self . _get_resource ( url ) locations = [ ] for entry in data [ 'locations' ] : locations . append ( self . location_from_json ( entry ) ) return locations
6060	def numpy_array_2d_from_fits ( file_path , hdu ) : hdu_list = fits . open ( file_path ) return np . flipud ( np . array ( hdu_list [ hdu ] . data ) )
13028	def exploit ( self ) : search = ServiceSearch ( ) host_search = HostSearch ( ) services = search . get_services ( tags = [ 'MS17-010' ] ) services = [ service for service in services ] if len ( services ) == 0 : print_error ( "No services found that are vulnerable for MS17-010" ) return if self . auto : print_success ( "Found {} services vulnerable for MS17-010" . format ( len ( services ) ) ) for service in services : print_success ( "Exploiting " + str ( service . address ) ) host = host_search . id_to_object ( str ( service . address ) ) system_os = '' if host . os : system_os = host . os else : system_os = self . detect_os ( str ( service . address ) ) host . os = system_os host . save ( ) text = self . exploit_single ( str ( service . address ) , system_os ) print_notification ( text ) else : service_list = [ ] for service in services : host = host_search . id_to_object ( str ( service . address ) ) system_os = '' if host . os : system_os = host . os else : system_os = self . detect_os ( str ( service . address ) ) host . os = system_os host . save ( ) service_list . append ( { 'ip' : service . address , 'os' : system_os , 'string' : "{ip} ({os}) {hostname}" . format ( ip = service . address , os = system_os , hostname = host . hostname ) } ) draw_interface ( service_list , self . callback , "Exploiting {ip} with OS: {os}" )
1160	def notify ( self , n = 1 ) : if not self . _is_owned ( ) : raise RuntimeError ( "cannot notify on un-acquired lock" ) __waiters = self . __waiters waiters = __waiters [ : n ] if not waiters : if __debug__ : self . _note ( "%s.notify(): no waiters" , self ) return self . _note ( "%s.notify(): notifying %d waiter%s" , self , n , n != 1 and "s" or "" ) for waiter in waiters : waiter . release ( ) try : __waiters . remove ( waiter ) except ValueError : pass
6772	def install_required ( self , type = None , service = None , list_only = 0 , ** kwargs ) : r = self . local_renderer list_only = int ( list_only ) type = ( type or '' ) . lower ( ) . strip ( ) assert not type or type in PACKAGE_TYPES , 'Unknown package type: %s' % ( type , ) lst = [ ] if type : types = [ type ] else : types = PACKAGE_TYPES for _type in types : if _type == SYSTEM : content = '\n' . join ( self . list_required ( type = _type , service = service ) ) if list_only : lst . extend ( _ for _ in content . split ( '\n' ) if _ . strip ( ) ) if self . verbose : print ( 'content:' , content ) break fd , fn = tempfile . mkstemp ( ) fout = open ( fn , 'w' ) fout . write ( content ) fout . close ( ) self . install_custom ( fn = fn ) else : raise NotImplementedError return lst
791	def jobSetCompleted ( self , jobID , completionReason , completionMsg , useConnectionID = True ) : with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET status=%%s, ' ' completion_reason=%%s, ' ' completion_msg=%%s, ' ' end_time=UTC_TIMESTAMP(), ' ' _eng_last_update_time=UTC_TIMESTAMP() ' ' WHERE job_id=%%s' % ( self . jobsTableName , ) sqlParams = [ self . STATUS_COMPLETED , completionReason , completionMsg , jobID ] if useConnectionID : query += ' AND _eng_cjm_conn_id=%s' sqlParams . append ( self . _connectionID ) result = conn . cursor . execute ( query , sqlParams ) if result != 1 : raise RuntimeError ( "Tried to change the status of jobID=%s to " "completed, but this job could not be found or " "belongs to some other CJM" % ( jobID ) )
5885	def get_canonical_link ( self ) : if self . article . final_url : kwargs = { 'tag' : 'link' , 'attr' : 'rel' , 'value' : 'canonical' } meta = self . parser . getElementsByTag ( self . article . doc , ** kwargs ) if meta is not None and len ( meta ) > 0 : href = self . parser . getAttribute ( meta [ 0 ] , 'href' ) if href : href = href . strip ( ) o = urlparse ( href ) if not o . hostname : tmp = urlparse ( self . article . final_url ) domain = '%s://%s' % ( tmp . scheme , tmp . hostname ) href = urljoin ( domain , href ) return href return self . article . final_url
4625	def change_password ( self , newpassword ) : if not self . unlocked ( ) : raise WalletLocked self . password = newpassword self . _save_encrypted_masterpassword ( )
278	def axes_style ( style = 'darkgrid' , rc = None ) : if rc is None : rc = { } rc_default = { } for name , val in rc_default . items ( ) : rc . setdefault ( name , val ) return sns . axes_style ( style = style , rc = rc )
11149	def rename ( self , key : Any , new_key : Any ) : if new_key == key : return required_locks = [ self . _key_locks [ key ] , self . _key_locks [ new_key ] ] ordered_required_locks = sorted ( required_locks , key = lambda x : id ( x ) ) for lock in ordered_required_locks : lock . acquire ( ) try : if key not in self . _data : raise KeyError ( "Attribute to rename \"%s\" does not exist" % key ) self . _data [ new_key ] = self [ key ] del self . _data [ key ] finally : for lock in required_locks : lock . release ( )
9548	def add_unique_check ( self , key , code = UNIQUE_CHECK_FAILED , message = MESSAGES [ UNIQUE_CHECK_FAILED ] ) : if isinstance ( key , basestring ) : assert key in self . _field_names , 'unexpected field name: %s' % key else : for f in key : assert f in self . _field_names , 'unexpected field name: %s' % key t = key , code , message self . _unique_checks . append ( t )
8068	def refresh ( self ) : self . reset ( ) self . parse ( self . source ) return self . output ( )
3657	def remove_cti_file ( self , file_path : str ) : if file_path in self . _cti_files : self . _cti_files . remove ( file_path ) self . _logger . info ( 'Removed {0} from the CTI file list.' . format ( file_path ) )
705	def recordModelProgress ( self , modelID , modelParams , modelParamsHash , results , completed , completionReason , matured , numRecords ) : if results is None : metricResult = None else : metricResult = results [ 1 ] . values ( ) [ 0 ] errScore = self . _resultsDB . update ( modelID = modelID , modelParams = modelParams , modelParamsHash = modelParamsHash , metricResult = metricResult , completed = completed , completionReason = completionReason , matured = matured , numRecords = numRecords ) self . logger . debug ( 'Received progress on model %d: completed: %s, ' 'cmpReason: %s, numRecords: %d, errScore: %s' , modelID , completed , completionReason , numRecords , errScore ) ( bestModelID , bestResult ) = self . _resultsDB . bestModelIdAndErrScore ( ) self . logger . debug ( 'Best err score seen so far: %s on model %s' % ( bestResult , bestModelID ) )
2335	def clr ( M , ** kwargs ) : R = np . zeros ( M . shape ) Id = [ [ 0 , 0 ] for i in range ( M . shape [ 0 ] ) ] for i in range ( M . shape [ 0 ] ) : mu_i = np . mean ( M [ i , : ] ) sigma_i = np . std ( M [ i , : ] ) Id [ i ] = [ mu_i , sigma_i ] for i in range ( M . shape [ 0 ] ) : for j in range ( i + 1 , M . shape [ 0 ] ) : z_i = np . max ( [ 0 , ( M [ i , j ] - Id [ i ] [ 0 ] ) / Id [ i ] [ 0 ] ] ) z_j = np . max ( [ 0 , ( M [ i , j ] - Id [ j ] [ 0 ] ) / Id [ j ] [ 0 ] ] ) R [ i , j ] = np . sqrt ( z_i ** 2 + z_j ** 2 ) R [ j , i ] = R [ i , j ] return R
9233	def fetch_commit ( self , event ) : gh = self . github user = self . options . user repo = self . options . project rc , data = gh . repos [ user ] [ repo ] . git . commits [ event [ "commit_id" ] ] . get ( ) if rc == 200 : return data self . raise_GitHubError ( rc , data , gh . getheaders ( ) )
9224	def convergent_round ( value , ndigits = 0 ) : if sys . version_info [ 0 ] < 3 : if value < 0.0 : return - convergent_round ( - value ) epsilon = 0.0000001 integral_part , _ = divmod ( value , 1 ) if abs ( value - ( integral_part + 0.5 ) ) < epsilon : if integral_part % 2.0 < epsilon : return integral_part else : nearest_even = integral_part + 0.5 return math . ceil ( nearest_even ) return round ( value , ndigits )
11165	def ctime ( self ) : try : return self . _stat . st_ctime except : self . _stat = self . stat ( ) return self . ctime
12059	def TK_message ( title , msg ) : root = tkinter . Tk ( ) root . withdraw ( ) root . attributes ( "-topmost" , True ) root . lift ( ) tkinter . messagebox . showwarning ( title , msg ) root . destroy ( )
9278	def to_decimal ( text ) : if not isinstance ( text , string_type ) : raise TypeError ( "expected str or unicode, %s given" % type ( text ) ) if findall ( r"[\x00-\x20\x7c-\xff]" , text ) : raise ValueError ( "invalid character in sequence" ) text = text . lstrip ( '!' ) decimal = 0 length = len ( text ) - 1 for i , char in enumerate ( text ) : decimal += ( ord ( char ) - 33 ) * ( 91 ** ( length - i ) ) return decimal if text != '' else 0
11317	def update_reportnumbers ( self ) : report_037_fields = record_get_field_instances ( self . record , '037' ) for field in report_037_fields : subs = field_get_subfields ( field ) for val in subs . get ( "a" , [ ] ) : if "arXiv" not in val : record_delete_field ( self . record , tag = "037" , field_position_global = field [ 4 ] ) new_subs = [ ( code , val [ 0 ] ) for code , val in subs . items ( ) ] record_add_field ( self . record , "088" , subfields = new_subs ) break
6667	def check_version ( ) : global CHECK_VERSION if not CHECK_VERSION : return CHECK_VERSION = 0 from six . moves . urllib . request import urlopen try : response = urlopen ( "https://pypi.org/pypi/burlap/json" ) data = json . loads ( response . read ( ) . decode ( ) ) remote_release = sorted ( tuple ( map ( int , _ . split ( '.' ) ) ) for _ in data [ 'releases' ] . keys ( ) ) [ - 1 ] remote_release_str = '.' . join ( map ( str , remote_release ) ) local_release = VERSION local_release_str = '.' . join ( map ( str , local_release ) ) if remote_release > local_release : print ( '\033[93m' ) print ( "You are using burlap version %s, however version %s is available." % ( local_release_str , remote_release_str ) ) print ( "You should consider upgrading via the 'pip install --upgrade burlap' command." ) print ( '\033[0m' ) except Exception as exc : print ( '\033[93m' ) print ( "Unable to check for updated burlap version: %s" % exc ) print ( '\033[0m' )
3852	def _get_lookup_spec ( identifier ) : if identifier . startswith ( '+' ) : return hangups . hangouts_pb2 . EntityLookupSpec ( phone = identifier , create_offnetwork_gaia = True ) elif '@' in identifier : return hangups . hangouts_pb2 . EntityLookupSpec ( email = identifier , create_offnetwork_gaia = True ) else : return hangups . hangouts_pb2 . EntityLookupSpec ( gaia_id = identifier )
3772	def phase_select_property ( phase = None , s = None , l = None , g = None , V_over_F = None ) : r if phase == 's' : return s elif phase == 'l' : return l elif phase == 'g' : return g elif phase == 'two-phase' : return None elif phase is None : return None else : raise Exception ( 'Property not recognized' )
9083	def get_by_uri ( self , uri ) : if not is_uri ( uri ) : raise ValueError ( '%s is not a valid URI.' % uri ) csuris = [ csuri for csuri in self . concept_scheme_uri_map . keys ( ) if uri . startswith ( csuri ) ] for csuri in csuris : c = self . get_provider ( csuri ) . get_by_uri ( uri ) if c : return c for p in self . providers . values ( ) : c = p . get_by_uri ( uri ) if c : return c return False
8757	def _validate_subnet_cidr ( context , network_id , new_subnet_cidr ) : if neutron_cfg . cfg . CONF . allow_overlapping_ips : return try : new_subnet_ipset = netaddr . IPSet ( [ new_subnet_cidr ] ) except TypeError : LOG . exception ( "Invalid or missing cidr: %s" % new_subnet_cidr ) raise n_exc . BadRequest ( resource = "subnet" , msg = "Invalid or missing cidr" ) filters = { 'network_id' : network_id , 'shared' : [ False ] } subnet_list = db_api . subnet_find ( context = context . elevated ( ) , ** filters ) for subnet in subnet_list : if ( netaddr . IPSet ( [ subnet . cidr ] ) & new_subnet_ipset ) : err_msg = ( _ ( "Requested subnet with cidr: %(cidr)s for " "network: %(network_id)s overlaps with another " "subnet" ) % { 'cidr' : new_subnet_cidr , 'network_id' : network_id } ) LOG . error ( _ ( "Validation for CIDR: %(new_cidr)s failed - " "overlaps with subnet %(subnet_id)s " "(CIDR: %(cidr)s)" ) , { 'new_cidr' : new_subnet_cidr , 'subnet_id' : subnet . id , 'cidr' : subnet . cidr } ) raise n_exc . InvalidInput ( error_message = err_msg )
4382	def is_allowed ( self , role , method , resource ) : return ( role , method , resource ) in self . _allowed
13573	def skip ( course , num = 1 ) : sel = None try : sel = Exercise . get_selected ( ) if sel . course . tid != course . tid : sel = None except NoExerciseSelected : pass if sel is None : sel = course . exercises . first ( ) else : try : sel = Exercise . get ( Exercise . id == sel . id + num ) except peewee . DoesNotExist : print ( "There are no more exercises in this course." ) return False sel . set_select ( ) list_all ( single = sel )
3978	def _get_referenced_libs ( specs ) : active_libs = set ( ) for app_spec in specs [ 'apps' ] . values ( ) : for lib in app_spec [ 'depends' ] [ 'libs' ] : active_libs . add ( lib ) return active_libs
3949	def execute ( self , using = None ) : if not using : using = self . get_connection ( ) inserted_entities = { } for klass in self . orders : number = self . quantities [ klass ] if klass not in inserted_entities : inserted_entities [ klass ] = [ ] for i in range ( 0 , number ) : entity = self . entities [ klass ] . execute ( using , inserted_entities ) inserted_entities [ klass ] . append ( entity ) return inserted_entities
7310	def without_tz ( request ) : t = Template ( '{% load tz %}{% get_current_timezone as TIME_ZONE %}{{ TIME_ZONE }}' ) c = RequestContext ( request ) response = t . render ( c ) return HttpResponse ( response )
13513	def reynolds_number ( length , speed , temperature = 25 ) : kinematic_viscosity = interpolate . interp1d ( [ 0 , 10 , 20 , 25 , 30 , 40 ] , np . array ( [ 18.54 , 13.60 , 10.50 , 9.37 , 8.42 , 6.95 ] ) / 10 ** 7 ) Re = length * speed / kinematic_viscosity ( temperature ) return Re
4091	def addSources ( self , * sources ) : self . _sources . extend ( sources ) debug . logger & debug . flagCompiler and debug . logger ( 'current MIB source(s): %s' % ', ' . join ( [ str ( x ) for x in self . _sources ] ) ) return self
8021	async def websocket_close ( self , message , stream_name ) : if stream_name in self . applications_accepting_frames : self . applications_accepting_frames . remove ( stream_name ) if self . closing : return if not self . applications_accepting_frames : await self . close ( message . get ( "code" ) )
10291	def enrich_unqualified ( graph : BELGraph ) : enrich_complexes ( graph ) enrich_composites ( graph ) enrich_reactions ( graph ) enrich_variants ( graph )
11333	def banner ( * lines , ** kwargs ) : sep = kwargs . get ( "sep" , "*" ) count = kwargs . get ( "width" , globals ( ) [ "WIDTH" ] ) out ( sep * count ) if lines : out ( sep ) for line in lines : out ( "{} {}" . format ( sep , line ) ) out ( sep ) out ( sep * count )
7460	def save_json2 ( data ) : datadict = OrderedDict ( [ ( "outfiles" , data . __dict__ [ "outfiles" ] ) , ( "stats_files" , dict ( data . __dict__ [ "stats_files" ] ) ) , ( "stats_dfs" , data . __dict__ [ "stats_dfs" ] ) ] )
5694	def create_table ( self , conn ) : cur = conn . cursor ( ) if self . tabledef is None : return if not self . tabledef . startswith ( 'CREATE' ) : cur . execute ( 'CREATE TABLE IF NOT EXISTS %s %s' % ( self . table , self . tabledef ) ) else : cur . execute ( self . tabledef ) conn . commit ( )
2329	def create_graph_from_data ( self , data ) : self . arguments [ '{SCORE}' ] = self . score self . arguments [ '{VERBOSE}' ] = str ( self . verbose ) . upper ( ) self . arguments [ '{BETA}' ] = str ( self . beta ) self . arguments [ '{OPTIM}' ] = str ( self . optim ) . upper ( ) self . arguments [ '{ALPHA}' ] = str ( self . alpha ) results = self . _run_bnlearn ( data , verbose = self . verbose ) graph = nx . DiGraph ( ) graph . add_edges_from ( results ) return graph
11410	def record_move_fields ( rec , tag , field_positions_local , field_position_local = None ) : fields = record_delete_fields ( rec , tag , field_positions_local = field_positions_local ) return record_add_fields ( rec , tag , fields , field_position_local = field_position_local )
819	def updateRow ( self , row , distribution ) : self . grow ( row + 1 , len ( distribution ) ) self . hist_ . axby ( row , 1 , 1 , distribution ) self . rowSums_ [ row ] += distribution . sum ( ) self . colSums_ += distribution self . hack_ = None
6053	def unmasked_sparse_to_sparse_from_mask_and_pixel_centres ( mask , unmasked_sparse_grid_pixel_centres , total_sparse_pixels ) : total_unmasked_sparse_pixels = unmasked_sparse_grid_pixel_centres . shape [ 0 ] unmasked_sparse_to_sparse = np . zeros ( total_unmasked_sparse_pixels ) pixel_index = 0 for unmasked_sparse_pixel_index in range ( total_unmasked_sparse_pixels ) : y = unmasked_sparse_grid_pixel_centres [ unmasked_sparse_pixel_index , 0 ] x = unmasked_sparse_grid_pixel_centres [ unmasked_sparse_pixel_index , 1 ] unmasked_sparse_to_sparse [ unmasked_sparse_pixel_index ] = pixel_index if not mask [ y , x ] : if pixel_index < total_sparse_pixels - 1 : pixel_index += 1 return unmasked_sparse_to_sparse
12263	def _load_meta ( self , size , md5 ) : if not hasattr ( self , 'local_hashes' ) : self . local_hashes = { } self . size = int ( size ) if ( re . match ( '^[a-fA-F0-9]{32}$' , md5 ) ) : self . md5 = md5
12097	def delete ( self , force = False , ** kwargs ) : if force : return super ( BaseActivatableModel , self ) . delete ( ** kwargs ) else : setattr ( self , self . ACTIVATABLE_FIELD_NAME , False ) return self . save ( update_fields = [ self . ACTIVATABLE_FIELD_NAME ] )
9019	def _rows ( self , spec ) : rows = self . new_row_collection ( ) for row in spec : rows . append ( self . _row ( row ) ) return rows
11241	def get_line_count ( fname ) : i = 0 with open ( fname ) as f : for i , l in enumerate ( f ) : pass return i + 1
8518	def fromdict ( cls , config , check_fields = True ) : m = super ( Config , cls ) . __new__ ( cls ) m . path = '.' m . verbose = False m . config = m . _merge_defaults ( config ) if check_fields : m . _check_fields ( ) return m
3388	def _bounds_dist ( self , p ) : prob = self . problem lb_dist = ( p - prob . variable_bounds [ 0 , ] ) . min ( ) ub_dist = ( prob . variable_bounds [ 1 , ] - p ) . min ( ) if prob . bounds . shape [ 0 ] > 0 : const = prob . inequalities . dot ( p ) const_lb_dist = ( const - prob . bounds [ 0 , ] ) . min ( ) const_ub_dist = ( prob . bounds [ 1 , ] - const ) . min ( ) lb_dist = min ( lb_dist , const_lb_dist ) ub_dist = min ( ub_dist , const_ub_dist ) return np . array ( [ lb_dist , ub_dist ] )
5089	def dropHistoricalTable ( apps , schema_editor ) : table_name = 'sap_success_factors_historicalsapsuccessfactorsenterprisecus80ad' if table_name in connection . introspection . table_names ( ) : migrations . DeleteModel ( name = table_name , )
5096	def refresh_persistent_maps ( self ) : for robot in self . _robots : resp2 = ( requests . get ( urljoin ( self . ENDPOINT , 'users/me/robots/{}/persistent_maps' . format ( robot . serial ) ) , headers = self . _headers ) ) resp2 . raise_for_status ( ) self . _persistent_maps . update ( { robot . serial : resp2 . json ( ) } )
7858	def make_error_response ( self , cond ) : if self . stanza_type in ( "result" , "error" ) : raise ValueError ( "Errors may not be generated for" " 'result' and 'error' iq" ) stanza = Iq ( stanza_type = "error" , from_jid = self . to_jid , to_jid = self . from_jid , stanza_id = self . stanza_id , error_cond = cond ) if self . _payload is None : self . decode_payload ( ) for payload in self . _payload : Stanza . add_payload ( stanza , payload ) return stanza
13838	def ConsumeInt32 ( self ) : try : result = ParseInteger ( self . token , is_signed = True , is_long = False ) except ValueError as e : raise self . _ParseError ( str ( e ) ) self . NextToken ( ) return result
10753	def open_archive ( fs_url , archive ) : it = pkg_resources . iter_entry_points ( 'fs.archive.open_archive' ) entry_point = next ( ( ep for ep in it if archive . endswith ( ep . name ) ) , None ) if entry_point is None : raise UnsupportedProtocol ( 'unknown archive extension: {}' . format ( archive ) ) try : archive_opener = entry_point . load ( ) except pkg_resources . DistributionNotFound as df : six . raise_from ( UnsupportedProtocol ( 'extension {} requires {}' . format ( entry_point . name , df . req ) ) , None ) try : binfile = None archive_fs = None fs = open_fs ( fs_url ) if issubclass ( archive_opener , base . ArchiveFS ) : try : binfile = fs . openbin ( archive , 'r+' ) except errors . ResourceNotFound : binfile = fs . openbin ( archive , 'w' ) except errors . ResourceReadOnly : binfile = fs . openbin ( archive , 'r' ) archive_opener = archive_opener . _read_fs_cls elif issubclass ( archive_opener , base . ArchiveReadFS ) : binfile = fs . openbin ( archive , 'r' ) if not hasattr ( binfile , 'name' ) : binfile . name = basename ( archive ) archive_fs = archive_opener ( binfile ) except Exception : getattr ( archive_fs , 'close' , lambda : None ) ( ) getattr ( binfile , 'close' , lambda : None ) ( ) raise else : return archive_fs
6370	def specificity ( self ) : r if self . _tn + self . _fp == 0 : return float ( 'NaN' ) return self . _tn / ( self . _tn + self . _fp )
5133	def generate_random_graph ( num_vertices = 250 , prob_loop = 0.5 , ** kwargs ) : g = minimal_random_graph ( num_vertices , ** kwargs ) for v in g . nodes ( ) : e = ( v , v ) if not g . is_edge ( e ) : if np . random . uniform ( ) < prob_loop : g . add_edge ( * e ) g = set_types_random ( g , ** kwargs ) return g
7152	def one ( prompt , * args , ** kwargs ) : indicator = '‣' if sys . version_info < ( 3 , 0 ) : indicator = '>' def go_back ( picker ) : return None , - 1 options , verbose_options = prepare_options ( args ) idx = kwargs . get ( 'idx' , 0 ) picker = Picker ( verbose_options , title = prompt , indicator = indicator , default_index = idx ) picker . register_custom_handler ( ord ( 'h' ) , go_back ) picker . register_custom_handler ( curses . KEY_LEFT , go_back ) with stdout_redirected ( sys . stderr ) : option , index = picker . start ( ) if index == - 1 : raise QuestionnaireGoBack if kwargs . get ( 'return_index' , False ) : return index return options [ index ]
6723	def get_or_create_ec2_key_pair ( name = None , verbose = 1 ) : verbose = int ( verbose ) name = name or env . vm_ec2_keypair_name pem_path = 'roles/%s/%s.pem' % ( env . ROLE , name ) conn = get_ec2_connection ( ) kp = conn . get_key_pair ( name ) if kp : print ( 'Key pair %s already exists.' % name ) else : kp = conn . create_key_pair ( name ) open ( pem_path , 'wb' ) . write ( kp . material ) os . system ( 'chmod 600 %s' % pem_path ) print ( 'Key pair %s created.' % name ) return pem_path
7791	def purge_items ( self ) : self . _lock . acquire ( ) try : il = self . _items_list num_items = len ( il ) need_remove = num_items - int ( 0.75 * self . max_items ) for _unused in range ( need_remove ) : item = il . pop ( 0 ) try : del self . _items [ item . address ] except KeyError : pass while il and il [ 0 ] . update_state ( ) == "purged" : item = il . pop ( 0 ) try : del self . _items [ item . address ] except KeyError : pass finally : self . _lock . release ( )
993	def _getFirstOnBit ( self , input ) : if input == SENTINEL_VALUE_FOR_MISSING_DATA : return [ None ] else : if input < self . minval : if self . clipInput and not self . periodic : if self . verbosity > 0 : print "Clipped input %s=%.2f to minval %.2f" % ( self . name , input , self . minval ) input = self . minval else : raise Exception ( 'input (%s) less than range (%s - %s)' % ( str ( input ) , str ( self . minval ) , str ( self . maxval ) ) ) if self . periodic : if input >= self . maxval : raise Exception ( 'input (%s) greater than periodic range (%s - %s)' % ( str ( input ) , str ( self . minval ) , str ( self . maxval ) ) ) else : if input > self . maxval : if self . clipInput : if self . verbosity > 0 : print "Clipped input %s=%.2f to maxval %.2f" % ( self . name , input , self . maxval ) input = self . maxval else : raise Exception ( 'input (%s) greater than range (%s - %s)' % ( str ( input ) , str ( self . minval ) , str ( self . maxval ) ) ) if self . periodic : centerbin = int ( ( input - self . minval ) * self . nInternal / self . range ) + self . padding else : centerbin = int ( ( ( input - self . minval ) + self . resolution / 2 ) / self . resolution ) + self . padding minbin = centerbin - self . halfwidth return [ minbin ]
621	def parseStringList ( s ) : assert isinstance ( s , basestring ) return [ int ( i ) for i in s . split ( ) ]
10453	def waittillguinotexist ( self , window_name , object_name = '' , guiTimeOut = 30 ) : timeout = 0 while timeout < guiTimeOut : if not self . guiexist ( window_name , object_name ) : return 1 time . sleep ( 1 ) timeout += 1 return 0
6916	def add_variability_to_fakelc_collection ( simbasedir , override_paramdists = None , overwrite_existingvar = False ) : infof = os . path . join ( simbasedir , 'fakelcs-info.pkl' ) with open ( infof , 'rb' ) as infd : lcinfo = pickle . load ( infd ) lclist = lcinfo [ 'lcfpath' ] varflag = lcinfo [ 'isvariable' ] vartypes = lcinfo [ 'vartype' ] vartind = 0 varinfo = { } for lc , varf , _lcind in zip ( lclist , varflag , range ( len ( lclist ) ) ) : if varf : thisvartype = vartypes [ vartind ] if ( override_paramdists and isinstance ( override_paramdists , dict ) and thisvartype in override_paramdists and isinstance ( override_paramdists [ thisvartype ] , dict ) ) : thisoverride_paramdists = override_paramdists [ thisvartype ] else : thisoverride_paramdists = None varlc = add_fakelc_variability ( lc , thisvartype , override_paramdists = thisoverride_paramdists , overwrite = overwrite_existingvar ) varinfo [ varlc [ 'objectid' ] ] = { 'params' : varlc [ 'actual_varparams' ] , 'vartype' : varlc [ 'actual_vartype' ] } vartind = vartind + 1 else : varlc = add_fakelc_variability ( lc , None , overwrite = overwrite_existingvar ) varinfo [ varlc [ 'objectid' ] ] = { 'params' : varlc [ 'actual_varparams' ] , 'vartype' : varlc [ 'actual_vartype' ] } lcinfo [ 'varinfo' ] = varinfo tempoutf = '%s.%s' % ( infof , md5 ( npr . bytes ( 4 ) ) . hexdigest ( ) [ - 8 : ] ) with open ( tempoutf , 'wb' ) as outfd : pickle . dump ( lcinfo , outfd , pickle . HIGHEST_PROTOCOL ) if os . path . exists ( tempoutf ) : shutil . copy ( tempoutf , infof ) os . remove ( tempoutf ) else : LOGEXCEPTION ( 'could not write output light curve file to dir: %s' % os . path . dirname ( tempoutf ) ) raise return lcinfo
494	def acquireConnection ( self ) : self . _logger . debug ( "Acquiring connection" ) dbConn = SteadyDB . connect ( ** _getCommonSteadyDBArgsDict ( ) ) connWrap = ConnectionWrapper ( dbConn = dbConn , cursor = dbConn . cursor ( ) , releaser = self . _releaseConnection , logger = self . _logger ) return connWrap
4783	def is_equal_to_ignoring_case ( self , other ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a string' ) if not isinstance ( other , str_types ) : raise TypeError ( 'given arg must be a string' ) if self . val . lower ( ) != other . lower ( ) : self . _err ( 'Expected <%s> to be case-insensitive equal to <%s>, but was not.' % ( self . val , other ) ) return self
6557	def assert_penaltymodel_factory_available ( ) : from pkg_resources import iter_entry_points from penaltymodel . core import FACTORY_ENTRYPOINT from itertools import chain supported = ( 'maxgap' , 'mip' ) factories = chain ( * ( iter_entry_points ( FACTORY_ENTRYPOINT , name ) for name in supported ) ) try : next ( factories ) except StopIteration : raise AssertionError ( "To use 'dwavebinarycsp', at least one penaltymodel factory must be installed. " "Try {}." . format ( " or " . join ( "'pip install dwavebinarycsp[{}]'" . format ( name ) for name in supported ) ) )
6758	def set_site_specifics ( self , site ) : r = self . local_renderer site_data = self . genv . sites [ site ] . copy ( ) r . env . site = site if self . verbose : print ( 'set_site_specifics.data:' ) pprint ( site_data , indent = 4 ) local_ns = { } for k , v in list ( site_data . items ( ) ) : if k . startswith ( self . name + '_' ) : _k = k [ len ( self . name + '_' ) : ] local_ns [ _k ] = v del site_data [ k ] r . env . update ( local_ns ) r . env . update ( site_data )
13765	def insert ( self , index , value ) : self . _list . insert ( index , value ) self . _sync ( )
4304	def _get_valid_formats ( ) : if NO_SOX : return [ ] so = subprocess . check_output ( [ 'sox' , '-h' ] ) if type ( so ) is not str : so = str ( so , encoding = 'UTF-8' ) so = so . split ( '\n' ) idx = [ i for i in range ( len ( so ) ) if 'AUDIO FILE FORMATS:' in so [ i ] ] [ 0 ] formats = so [ idx ] . split ( ' ' ) [ 3 : ] return formats
13680	def get_translated_data ( self ) : j = { } for k in self . data : d = { } for l in self . data [ k ] : d [ self . translation_keys [ l ] ] = self . data [ k ] [ l ] j [ k ] = d return j
5914	def _process_command ( self , command , name = None ) : self . _command_counter += 1 if name is None : name = "CMD{0:03d}" . format ( self . _command_counter ) try : fd , tmp_ndx = tempfile . mkstemp ( suffix = '.ndx' , prefix = 'tmp_' + name + '__' ) cmd = [ command , '' , 'q' ] rc , out , err = self . make_ndx ( o = tmp_ndx , input = cmd ) self . check_output ( out , "No atoms found for selection {command!r}." . format ( ** vars ( ) ) , err = err ) groups = parse_ndxlist ( out ) last = groups [ - 1 ] fd , ndx = tempfile . mkstemp ( suffix = '.ndx' , prefix = name + '__' ) name_cmd = [ "keep {0:d}" . format ( last [ 'nr' ] ) , "name 0 {0!s}" . format ( name ) , 'q' ] rc , out , err = self . make_ndx ( n = tmp_ndx , o = ndx , input = name_cmd ) finally : utilities . unlink_gmx ( tmp_ndx ) return name , ndx
11057	def _remove_by_pk ( self , key , flush = True ) : try : del self . store [ key ] except Exception as error : pass if flush : self . flush ( )
18	def learn ( network , env , seed = None , nsteps = 5 , total_timesteps = int ( 80e6 ) , vf_coef = 0.5 , ent_coef = 0.01 , max_grad_norm = 0.5 , lr = 7e-4 , lrschedule = 'linear' , epsilon = 1e-5 , alpha = 0.99 , gamma = 0.99 , log_interval = 100 , load_path = None , ** network_kwargs ) : set_global_seeds ( seed ) nenvs = env . num_envs policy = build_policy ( env , network , ** network_kwargs ) model = Model ( policy = policy , env = env , nsteps = nsteps , ent_coef = ent_coef , vf_coef = vf_coef , max_grad_norm = max_grad_norm , lr = lr , alpha = alpha , epsilon = epsilon , total_timesteps = total_timesteps , lrschedule = lrschedule ) if load_path is not None : model . load ( load_path ) runner = Runner ( env , model , nsteps = nsteps , gamma = gamma ) epinfobuf = deque ( maxlen = 100 ) nbatch = nenvs * nsteps tstart = time . time ( ) for update in range ( 1 , total_timesteps // nbatch + 1 ) : obs , states , rewards , masks , actions , values , epinfos = runner . run ( ) epinfobuf . extend ( epinfos ) policy_loss , value_loss , policy_entropy = model . train ( obs , states , rewards , masks , actions , values ) nseconds = time . time ( ) - tstart fps = int ( ( update * nbatch ) / nseconds ) if update % log_interval == 0 or update == 1 : ev = explained_variance ( values , rewards ) logger . record_tabular ( "nupdates" , update ) logger . record_tabular ( "total_timesteps" , update * nbatch ) logger . record_tabular ( "fps" , fps ) logger . record_tabular ( "policy_entropy" , float ( policy_entropy ) ) logger . record_tabular ( "value_loss" , float ( value_loss ) ) logger . record_tabular ( "explained_variance" , float ( ev ) ) logger . record_tabular ( "eprewmean" , safemean ( [ epinfo [ 'r' ] for epinfo in epinfobuf ] ) ) logger . record_tabular ( "eplenmean" , safemean ( [ epinfo [ 'l' ] for epinfo in epinfobuf ] ) ) logger . dump_tabular ( ) return model
6543	def exec_command ( self , cmdstr ) : if self . is_terminated : raise TerminatedError ( "this TerminalClient instance has been terminated" ) log . debug ( "sending command: %s" , cmdstr ) c = Command ( self . app , cmdstr ) start = time . time ( ) c . execute ( ) elapsed = time . time ( ) - start log . debug ( "elapsed execution: {0}" . format ( elapsed ) ) self . status = Status ( c . status_line ) return c
6396	def sim_minkowski ( src , tar , qval = 2 , pval = 1 , alphabet = None ) : return Minkowski ( ) . sim ( src , tar , qval , pval , alphabet )
9287	def consumer ( self , callback , blocking = True , immortal = False , raw = False ) : if not self . _connected : raise ConnectionError ( "not connected to a server" ) line = b'' while True : try : for line in self . _socket_readlines ( blocking ) : if line [ 0 : 1 ] != b'#' : if raw : callback ( line ) else : callback ( self . _parse ( line ) ) else : self . logger . debug ( "Server: %s" , line . decode ( 'utf8' ) ) except ParseError as exp : self . logger . log ( 11 , "%s\n Packet: %s" , exp . message , exp . packet ) except UnknownFormat as exp : self . logger . log ( 9 , "%s\n Packet: %s" , exp . message , exp . packet ) except LoginError as exp : self . logger . error ( "%s: %s" , exp . __class__ . __name__ , exp . message ) except ( KeyboardInterrupt , SystemExit ) : raise except ( ConnectionDrop , ConnectionError ) : self . close ( ) if not immortal : raise else : self . connect ( blocking = blocking ) continue except GenericError : pass except StopIteration : break except : self . logger . error ( "APRS Packet: %s" , line ) raise if not blocking : break
10800	def _newcall ( self , rvecs ) : sigma = 1 * self . filter_size out = self . _eval_firstorder ( rvecs , self . d , sigma ) ondata = self . _eval_firstorder ( self . x , self . d , sigma ) for i in range ( self . iterations ) : out += self . _eval_firstorder ( rvecs , self . d - ondata , sigma ) ondata += self . _eval_firstorder ( self . x , self . d - ondata , sigma ) sigma *= self . damp return out
1990	def rm ( self , key ) : path = os . path . join ( self . uri , key ) os . remove ( path )
9015	def _row ( self , values ) : row_id = self . _to_id ( values [ ID ] ) row = self . _spec . new_row ( row_id , values , self ) if SAME_AS in values : self . _delay_inheritance ( row , self . _to_id ( values [ SAME_AS ] ) ) self . _delay_instructions ( row ) self . _id_cache [ row_id ] = row return row
7063	def sqs_get_item ( queue_url , max_items = 1 , wait_time_seconds = 5 , client = None , raiseonfail = False ) : if not client : client = boto3 . client ( 'sqs' ) try : resp = client . receive_message ( QueueUrl = queue_url , AttributeNames = [ 'All' ] , MaxNumberOfMessages = max_items , WaitTimeSeconds = wait_time_seconds ) if not resp : LOGERROR ( 'could not receive messages from queue: %s' % queue_url ) else : messages = [ ] for msg in resp . get ( 'Messages' , [ ] ) : try : messages . append ( { 'id' : msg [ 'MessageId' ] , 'receipt_handle' : msg [ 'ReceiptHandle' ] , 'md5' : msg [ 'MD5OfBody' ] , 'attributes' : msg [ 'Attributes' ] , 'item' : json . loads ( msg [ 'Body' ] ) , } ) except Exception as e : LOGEXCEPTION ( 'could not deserialize message ID: %s, body: %s' % ( msg [ 'MessageId' ] , msg [ 'Body' ] ) ) continue return messages except Exception as e : LOGEXCEPTION ( 'could not get items from queue: %s' % queue_url ) if raiseonfail : raise return None
5638	def _temporal_distance_cdf ( self ) : distance_split_points = set ( ) for block in self . _profile_blocks : if block . distance_start != float ( 'inf' ) : distance_split_points . add ( block . distance_end ) distance_split_points . add ( block . distance_start ) distance_split_points_ordered = numpy . array ( sorted ( list ( distance_split_points ) ) ) temporal_distance_split_widths = distance_split_points_ordered [ 1 : ] - distance_split_points_ordered [ : - 1 ] trip_counts = numpy . zeros ( len ( temporal_distance_split_widths ) ) delta_peaks = defaultdict ( lambda : 0 ) for block in self . _profile_blocks : if block . distance_start == block . distance_end : delta_peaks [ block . distance_end ] += block . width ( ) else : start_index = numpy . searchsorted ( distance_split_points_ordered , block . distance_end ) end_index = numpy . searchsorted ( distance_split_points_ordered , block . distance_start ) trip_counts [ start_index : end_index ] += 1 unnormalized_cdf = numpy . array ( [ 0 ] + list ( numpy . cumsum ( temporal_distance_split_widths * trip_counts ) ) ) if not ( numpy . isclose ( [ unnormalized_cdf [ - 1 ] ] , [ self . _end_time - self . _start_time - sum ( delta_peaks . values ( ) ) ] , atol = 1E-4 ) . all ( ) ) : print ( unnormalized_cdf [ - 1 ] , self . _end_time - self . _start_time - sum ( delta_peaks . values ( ) ) ) raise RuntimeError ( "Something went wrong with cdf computation!" ) if len ( delta_peaks ) > 0 : for peak in delta_peaks . keys ( ) : if peak == float ( 'inf' ) : continue index = numpy . nonzero ( distance_split_points_ordered == peak ) [ 0 ] [ 0 ] unnormalized_cdf = numpy . insert ( unnormalized_cdf , index , unnormalized_cdf [ index ] ) distance_split_points_ordered = numpy . insert ( distance_split_points_ordered , index , distance_split_points_ordered [ index ] ) unnormalized_cdf [ ( index + 1 ) : ] = unnormalized_cdf [ ( index + 1 ) : ] + delta_peaks [ peak ] norm_cdf = unnormalized_cdf / ( unnormalized_cdf [ - 1 ] + delta_peaks [ float ( 'inf' ) ] ) return distance_split_points_ordered , norm_cdf
5311	def translate_colorname_to_ansi_code ( colorname , offset , colormode , colorpalette ) : try : red , green , blue = colorpalette [ colorname ] except KeyError : raise ColorfulError ( 'the color "{0}" is unknown. Use a color in your color palette (by default: X11 rgb.txt)' . format ( colorname ) ) else : return translate_rgb_to_ansi_code ( red , green , blue , offset , colormode )
12024	def adopt ( self , old_parent , new_parent ) : try : old_id = old_parent [ 'attributes' ] [ 'ID' ] except TypeError : try : old_id = self . lines [ old_parent ] [ 'attributes' ] [ 'ID' ] except TypeError : old_id = old_parent old_feature = self . features [ old_id ] old_indexes = [ ld [ 'line_index' ] for ld in old_feature ] try : new_id = new_parent [ 'attributes' ] [ 'ID' ] except TypeError : try : new_id = self . lines [ new_parent ] [ 'attributes' ] [ 'ID' ] except TypeError : new_id = new_parent new_feature = self . features [ new_id ] new_indexes = [ ld [ 'line_index' ] for ld in new_feature ] children = old_feature [ 0 ] [ 'children' ] new_parent_children_set = set ( [ ld [ 'line_index' ] for ld in new_feature [ 0 ] [ 'children' ] ] ) for child in children : if child [ 'line_index' ] not in new_parent_children_set : new_parent_children_set . add ( child [ 'line_index' ] ) for new_ld in new_feature : new_ld [ 'children' ] . append ( child ) child [ 'parents' ] . append ( new_feature ) child [ 'attributes' ] [ 'Parent' ] . append ( new_id ) child [ 'parents' ] = [ f for f in child [ 'parents' ] if f [ 0 ] [ 'attributes' ] [ 'ID' ] != old_id ] child [ 'attributes' ] [ 'Parent' ] = [ d for d in child [ 'attributes' ] [ 'Parent' ] if d != old_id ] for old_ld in old_feature : old_ld [ 'children' ] = [ ] return children
10897	def get_scale_from_raw ( raw , scaled ) : t0 , t1 = scaled . min ( ) , scaled . max ( ) r0 , r1 = float ( raw . min ( ) ) , float ( raw . max ( ) ) rmin = ( t1 * r0 - t0 * r1 ) / ( t1 - t0 ) rmax = ( r1 - r0 ) / ( t1 - t0 ) + rmin return ( rmin , rmax )
2923	def ancestors ( self ) : results = [ ] def recursive_find_ancestors ( task , stack ) : for input in task . inputs : if input not in stack : stack . append ( input ) recursive_find_ancestors ( input , stack ) recursive_find_ancestors ( self , results ) return results
5729	def main ( verbose = True ) : find_executable ( MAKE_CMD ) if not find_executable ( MAKE_CMD ) : print ( 'Could not find executable "%s". Ensure it is installed and on your $PATH.' % MAKE_CMD ) exit ( 1 ) subprocess . check_output ( [ MAKE_CMD , "-C" , SAMPLE_C_CODE_DIR , "--quiet" ] ) gdbmi = GdbController ( verbose = verbose ) responses = gdbmi . write ( "-file-exec-and-symbols %s" % SAMPLE_C_BINARY ) responses = gdbmi . write ( "-file-list-exec-source-files" ) responses = gdbmi . write ( "-break-insert main" ) responses = gdbmi . write ( "-exec-run" ) responses = gdbmi . write ( "-exec-next" ) responses = gdbmi . write ( "-exec-next" ) responses = gdbmi . write ( "-exec-continue" ) gdbmi . exit ( )
8882	def predict_proba ( self , X ) : check_is_fitted ( self , [ 'inverse_influence_matrix' ] ) X = check_array ( X ) return self . __find_leverages ( X , self . inverse_influence_matrix )
2089	def get ( self , pk = None , ** kwargs ) : if kwargs . pop ( 'include_debug_header' , True ) : debug . log ( 'Getting the record.' , header = 'details' ) response = self . read ( pk = pk , fail_on_no_results = True , fail_on_multiple_results = True , ** kwargs ) return response [ 'results' ] [ 0 ]
8650	def post_review ( session , review ) : response = make_post_request ( session , 'reviews' , json_data = review ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise ReviewNotPostedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
9592	def set_window_position ( self , x , y , window_handle = 'current' ) : self . _execute ( Command . SET_WINDOW_POSITION , { 'x' : int ( x ) , 'y' : int ( y ) , 'window_handle' : window_handle } )
9360	def _to_lower_alpha_only ( s ) : s = re . sub ( r'\n' , ' ' , s . lower ( ) ) return re . sub ( r'[^a-z\s]' , '' , s )
7416	def loci2migrate ( name , locifile , popdict , mindict = 1 ) : outfile = open ( name + ".migrate" , 'w' ) infile = open ( locifile , 'r' ) if isinstance ( mindict , int ) : mindict = { pop : mindict for pop in popdict } else : mindict = mindict keep = [ ] MINS = zip ( taxa . keys ( ) , minhits ) loci = infile . read ( ) . strip ( ) . split ( "|" ) [ : - 1 ] for loc in loci : samps = [ i . split ( ) [ 0 ] . replace ( ">" , "" ) for i in loc . split ( "\n" ) if ">" in i ] GG = [ ] for group , mins in MINS : GG . append ( sum ( [ i in samps for i in taxa [ group ] ] ) >= int ( mins ) ) if all ( GG ) : keep . append ( loc ) print >> outfile , len ( taxa ) , len ( keep ) , "( npops nloci for data set" , data . name + ".loci" , ")" done = 0 for group in taxa : if not done : loclens = [ len ( loc . split ( "\n" ) [ 1 ] . split ( ) [ - 1 ] . replace ( "x" , "n" ) . replace ( "n" , "" ) ) for loc in keep ] print >> outfile , " " . join ( map ( str , loclens ) ) done += 1 indslist = [ ] for loc in keep : samps = [ i . split ( ) [ 0 ] . replace ( ">" , "" ) for i in loc . split ( "\n" ) if ">" in i ] inds = sum ( [ i in samps for i in taxa [ group ] ] ) indslist . append ( inds ) print >> outfile , " " . join ( map ( str , indslist ) ) , group for loc in range ( len ( keep ) ) : seqs = [ i . split ( ) [ - 1 ] for i in keep [ loc ] . split ( "\n" ) if i . split ( ) [ 0 ] . replace ( ">" , "" ) in taxa [ group ] ] for i in range ( len ( seqs ) ) : print >> outfile , group [ 0 : 8 ] + "_" + str ( i ) + ( " " * ( 10 - len ( group [ 0 : 8 ] + "_" + str ( i ) ) ) ) + seqs [ i ] . replace ( "x" , "n" ) . replace ( "n" , "" ) outfile . close ( )
2672	def upload ( src , requirements = None , local_package = None , config_file = 'config.yaml' , profile_name = None , ) : path_to_config_file = os . path . join ( src , config_file ) cfg = read_cfg ( path_to_config_file , profile_name ) path_to_zip_file = build ( src , config_file = config_file , requirements = requirements , local_package = local_package , ) upload_s3 ( cfg , path_to_zip_file )
5639	def _temporal_distance_pdf ( self ) : temporal_distance_split_points_ordered , norm_cdf = self . _temporal_distance_cdf ( ) delta_peak_loc_to_probability_mass = { } non_delta_peak_split_points = [ temporal_distance_split_points_ordered [ 0 ] ] non_delta_peak_densities = [ ] for i in range ( 0 , len ( temporal_distance_split_points_ordered ) - 1 ) : left = temporal_distance_split_points_ordered [ i ] right = temporal_distance_split_points_ordered [ i + 1 ] width = right - left prob_mass = norm_cdf [ i + 1 ] - norm_cdf [ i ] if width == 0.0 : delta_peak_loc_to_probability_mass [ left ] = prob_mass else : non_delta_peak_split_points . append ( right ) non_delta_peak_densities . append ( prob_mass / float ( width ) ) assert ( len ( non_delta_peak_densities ) == len ( non_delta_peak_split_points ) - 1 ) return numpy . array ( non_delta_peak_split_points ) , numpy . array ( non_delta_peak_densities ) , delta_peak_loc_to_probability_mass
5141	def new_comment ( self , string , start , end , line ) : prefix = line [ : start [ 1 ] ] if prefix . strip ( ) : self . current_block . add ( string , start , end , line ) else : block = Comment ( start [ 0 ] , end [ 0 ] , string ) self . blocks . append ( block ) self . current_block = block
37	def share_file ( comm , path ) : localrank , _ = get_local_rank_size ( comm ) if comm . Get_rank ( ) == 0 : with open ( path , 'rb' ) as fh : data = fh . read ( ) comm . bcast ( data ) else : data = comm . bcast ( None ) if localrank == 0 : os . makedirs ( os . path . dirname ( path ) , exist_ok = True ) with open ( path , 'wb' ) as fh : fh . write ( data ) comm . Barrier ( )
11380	def do_oembed ( parser , token ) : args = token . split_contents ( ) template_dir = None var_name = None if len ( args ) > 2 : if len ( args ) == 3 and args [ 1 ] == 'in' : template_dir = args [ 2 ] elif len ( args ) == 3 and args [ 1 ] == 'as' : var_name = args [ 2 ] elif len ( args ) == 4 and args [ 2 ] == 'in' : template_dir = args [ 3 ] elif len ( args ) == 4 and args [ 2 ] == 'as' : var_name = args [ 3 ] elif len ( args ) == 6 and args [ 4 ] == 'as' : template_dir = args [ 3 ] var_name = args [ 5 ] else : raise template . TemplateSyntaxError ( "OEmbed either takes a single " "(optional) argument: WIDTHxHEIGHT, where WIDTH and HEIGHT " "are positive integers, and or an optional 'in " " \"template_dir\"' argument set." ) if template_dir : if not ( template_dir [ 0 ] == template_dir [ - 1 ] and template_dir [ 0 ] in ( '"' , "'" ) ) : raise template . TemplateSyntaxError ( "template_dir must be quoted" ) template_dir = template_dir [ 1 : - 1 ] if len ( args ) >= 2 and 'x' in args [ 1 ] : width , height = args [ 1 ] . lower ( ) . split ( 'x' ) if not width and height : raise template . TemplateSyntaxError ( "OEmbed's optional WIDTHxHEIGH" "T argument requires WIDTH and HEIGHT to be positive integers." ) else : width , height = None , None nodelist = parser . parse ( ( 'endoembed' , ) ) parser . delete_first_token ( ) return OEmbedNode ( nodelist , width , height , template_dir , var_name )
7850	def add_feature ( self , var ) : if self . has_feature ( var ) : return n = self . xmlnode . newChild ( None , "feature" , None ) n . setProp ( "var" , to_utf8 ( var ) )
12865	def setup ( self , app ) : super ( ) . setup ( app ) self . database . initialize ( connect ( self . cfg . connection , ** self . cfg . connection_params ) ) if self . database . database == ':memory:' : self . cfg . connection_manual = True if not self . cfg . migrations_enabled : return self . router = Router ( self . database , migrate_dir = self . cfg . migrations_path ) def pw_migrate ( name : str = None , fake : bool = False ) : self . router . run ( name , fake = fake ) self . app . manage . command ( pw_migrate ) def pw_rollback ( name : str = None ) : if not name : name = self . router . done [ - 1 ] self . router . rollback ( name ) self . app . manage . command ( pw_rollback ) def pw_create ( name : str = 'auto' , auto : bool = False ) : if auto : auto = list ( self . models . values ( ) ) self . router . create ( name , auto ) self . app . manage . command ( pw_create ) def pw_list ( ) : self . router . logger . info ( 'Migrations are done:' ) self . router . logger . info ( '\n' . join ( self . router . done ) ) self . router . logger . info ( '' ) self . router . logger . info ( 'Migrations are undone:' ) self . router . logger . info ( '\n' . join ( self . router . diff ) ) self . app . manage . command ( pw_list ) @ self . app . manage . command def pw_merge ( ) : self . router . merge ( ) self . app . manage . command ( pw_merge )
1500	def fail ( self , tup ) : if not isinstance ( tup , HeronTuple ) : Log . error ( "Only HeronTuple type is supported in fail()" ) return if self . acking_enabled : fail_tuple = tuple_pb2 . AckTuple ( ) fail_tuple . ackedtuple = int ( tup . id ) tuple_size_in_bytes = 0 for rt in tup . roots : to_add = fail_tuple . roots . add ( ) to_add . CopyFrom ( rt ) tuple_size_in_bytes += rt . ByteSize ( ) super ( BoltInstance , self ) . admit_control_tuple ( fail_tuple , tuple_size_in_bytes , False ) fail_latency_ns = ( time . time ( ) - tup . creation_time ) * system_constants . SEC_TO_NS self . pplan_helper . context . invoke_hook_bolt_fail ( tup , fail_latency_ns ) self . bolt_metrics . failed_tuple ( tup . stream , tup . component , fail_latency_ns )
2294	def integral_approx_estimator ( x , y ) : a , b = ( 0. , 0. ) x = np . array ( x ) y = np . array ( y ) idx , idy = ( np . argsort ( x ) , np . argsort ( y ) ) for x1 , x2 , y1 , y2 in zip ( x [ [ idx ] ] [ : - 1 ] , x [ [ idx ] ] [ 1 : ] , y [ [ idx ] ] [ : - 1 ] , y [ [ idx ] ] [ 1 : ] ) : if x1 != x2 and y1 != y2 : a = a + np . log ( np . abs ( ( y2 - y1 ) / ( x2 - x1 ) ) ) for x1 , x2 , y1 , y2 in zip ( x [ [ idy ] ] [ : - 1 ] , x [ [ idy ] ] [ 1 : ] , y [ [ idy ] ] [ : - 1 ] , y [ [ idy ] ] [ 1 : ] ) : if x1 != x2 and y1 != y2 : b = b + np . log ( np . abs ( ( x2 - x1 ) / ( y2 - y1 ) ) ) return ( a - b ) / len ( x )
5728	def _get_responses_unix ( self , timeout_sec ) : timeout_time_sec = time . time ( ) + timeout_sec responses = [ ] while True : select_timeout = timeout_time_sec - time . time ( ) if select_timeout <= 0 : select_timeout = 0 events , _ , _ = select . select ( self . read_list , [ ] , [ ] , select_timeout ) responses_list = None try : for fileno in events : if fileno == self . stdout_fileno : self . gdb_process . stdout . flush ( ) raw_output = self . gdb_process . stdout . read ( ) stream = "stdout" elif fileno == self . stderr_fileno : self . gdb_process . stderr . flush ( ) raw_output = self . gdb_process . stderr . read ( ) stream = "stderr" else : raise ValueError ( "Developer error. Got unexpected file number %d" % fileno ) responses_list = self . _get_responses_list ( raw_output , stream ) responses += responses_list except IOError : pass if timeout_sec == 0 : break elif responses_list and self . _allow_overwrite_timeout_times : timeout_time_sec = min ( time . time ( ) + self . time_to_check_for_additional_output_sec , timeout_time_sec , ) elif time . time ( ) > timeout_time_sec : break return responses
5750	def downloadURL ( url , filename ) : path_temp_bviewfile = os . path . join ( c . raw_data , c . bview_dir , 'tmp' , filename ) path_bviewfile = os . path . join ( c . raw_data , c . bview_dir , filename ) try : f = urlopen ( url ) except : return False if f . getcode ( ) != 200 : publisher . warning ( '{} unavailable, code: {}' . format ( url , f . getcode ( ) ) ) return False try : with open ( path_temp_bviewfile , 'w' ) as outfile : outfile . write ( f . read ( ) ) os . rename ( path_temp_bviewfile , path_bviewfile ) except : os . remove ( path_temp_bviewfile ) return False return True
4771	def contains ( self , * items ) : if len ( items ) == 0 : raise ValueError ( 'one or more args must be given' ) elif len ( items ) == 1 : if items [ 0 ] not in self . val : if self . _check_dict_like ( self . val , return_as_bool = True ) : self . _err ( 'Expected <%s> to contain key <%s>, but did not.' % ( self . val , items [ 0 ] ) ) else : self . _err ( 'Expected <%s> to contain item <%s>, but did not.' % ( self . val , items [ 0 ] ) ) else : missing = [ ] for i in items : if i not in self . val : missing . append ( i ) if missing : if self . _check_dict_like ( self . val , return_as_bool = True ) : self . _err ( 'Expected <%s> to contain keys %s, but did not contain key%s %s.' % ( self . val , self . _fmt_items ( items ) , '' if len ( missing ) == 0 else 's' , self . _fmt_items ( missing ) ) ) else : self . _err ( 'Expected <%s> to contain items %s, but did not contain %s.' % ( self . val , self . _fmt_items ( items ) , self . _fmt_items ( missing ) ) ) return self
5097	def _calculate_distance ( latlon1 , latlon2 ) : lat1 , lon1 = latlon1 lat2 , lon2 = latlon2 dlon = lon2 - lon1 dlat = lat2 - lat1 R = 6371 a = np . sin ( dlat / 2 ) ** 2 + np . cos ( lat1 ) * np . cos ( lat2 ) * ( np . sin ( dlon / 2 ) ) ** 2 c = 2 * np . pi * R * np . arctan2 ( np . sqrt ( a ) , np . sqrt ( 1 - a ) ) / 180 return c
8880	def predict_proba ( self , X ) : check_is_fitted ( self , [ 'tree' ] ) X = check_array ( X ) return self . tree . query ( X ) [ 0 ] . flatten ( )
6832	def ssh_config ( self , name = '' ) : r = self . local_renderer with self . settings ( hide ( 'running' ) ) : output = r . local ( 'vagrant ssh-config %s' % name , capture = True ) config = { } for line in output . splitlines ( ) [ 1 : ] : key , value = line . strip ( ) . split ( ' ' , 2 ) config [ key ] = value return config
5418	def format_logging_uri ( uri , job_metadata , task_metadata ) : fmt = str ( uri ) if '{' not in fmt : if uri . endswith ( '.log' ) : fmt = os . path . splitext ( uri ) [ 0 ] else : fmt = os . path . join ( uri , '{job-id}' ) if task_metadata . get ( 'task-id' ) is not None : fmt += '.{task-id}' if task_metadata . get ( 'task-attempt' ) is not None : fmt += '.{task-attempt}' fmt += '.log' return _format_task_uri ( fmt , job_metadata , task_metadata )
10531	def find_project ( ** kwargs ) : try : res = _pybossa_req ( 'get' , 'project' , params = kwargs ) if type ( res ) . __name__ == 'list' : return [ Project ( project ) for project in res ] else : return res except : raise
10619	def get_compound_amount ( self , compound ) : index = self . material . get_compound_index ( compound ) return stoich . amount ( compound , self . _compound_masses [ index ] )
9004	def to_svg ( self , zoom ) : def on_dump ( ) : knitting_pattern = self . patterns . at ( 0 ) layout = GridLayout ( knitting_pattern ) instruction_to_svg = default_instruction_svg_cache ( ) builder = SVGBuilder ( ) kp_to_svg = KnittingPatternToSVG ( knitting_pattern , layout , instruction_to_svg , builder , zoom ) return kp_to_svg . build_SVG_dict ( ) return XMLDumper ( on_dump )
7577	def _get_clumpp_table ( self , kpop , max_var_multiple , quiet ) : reps , excluded = _concat_reps ( self , kpop , max_var_multiple , quiet ) if reps : ninds = reps [ 0 ] . inds nreps = len ( reps ) else : ninds = nreps = 0 if not reps : return "no result files found" clumphandle = os . path . join ( self . workdir , "tmp.clumppparams.txt" ) self . clumppparams . kpop = kpop self . clumppparams . c = ninds self . clumppparams . r = nreps with open ( clumphandle , 'w' ) as tmp_c : tmp_c . write ( self . clumppparams . _asfile ( ) ) outfile = os . path . join ( self . workdir , "{}-K-{}.outfile" . format ( self . name , kpop ) ) indfile = os . path . join ( self . workdir , "{}-K-{}.indfile" . format ( self . name , kpop ) ) miscfile = os . path . join ( self . workdir , "{}-K-{}.miscfile" . format ( self . name , kpop ) ) cmd = [ "CLUMPP" , clumphandle , "-i" , indfile , "-o" , outfile , "-j" , miscfile , "-r" , str ( nreps ) , "-c" , str ( ninds ) , "-k" , str ( kpop ) ] proc = subprocess . Popen ( cmd , stderr = subprocess . STDOUT , stdout = subprocess . PIPE ) _ = proc . communicate ( ) for rfile in [ indfile , miscfile ] : if os . path . exists ( rfile ) : os . remove ( rfile ) ofile = os . path . join ( self . workdir , "{}-K-{}.outfile" . format ( self . name , kpop ) ) if os . path . exists ( ofile ) : csvtable = pd . read_csv ( ofile , delim_whitespace = True , header = None ) table = csvtable . loc [ : , 5 : ] table . columns = range ( table . shape [ 1 ] ) table . index = self . labels if not quiet : sys . stderr . write ( "[K{}] {}/{} results permuted across replicates (max_var={}).\n" . format ( kpop , nreps , nreps + excluded , max_var_multiple ) ) return table else : sys . stderr . write ( "No files ready for {}-K-{} in {}\n" . format ( self . name , kpop , self . workdir ) ) return
7141	def transfer_multiple ( self , destinations , priority = prio . NORMAL , payment_id = None , unlock_time = 0 , relay = True ) : return self . accounts [ 0 ] . transfer_multiple ( destinations , priority = priority , payment_id = payment_id , unlock_time = unlock_time , relay = relay )
1877	def MOVSD ( cpu , dest , src ) : assert dest . type != 'memory' or src . type != 'memory' value = Operators . EXTRACT ( src . read ( ) , 0 , 64 ) if dest . size > src . size : value = Operators . ZEXTEND ( value , dest . size ) dest . write ( value )
7919	def __from_unicode ( cls , data , check = True ) : parts1 = data . split ( u"/" , 1 ) parts2 = parts1 [ 0 ] . split ( u"@" , 1 ) if len ( parts2 ) == 2 : local = parts2 [ 0 ] domain = parts2 [ 1 ] if check : local = cls . __prepare_local ( local ) domain = cls . __prepare_domain ( domain ) else : local = None domain = parts2 [ 0 ] if check : domain = cls . __prepare_domain ( domain ) if len ( parts1 ) == 2 : resource = parts1 [ 1 ] if check : resource = cls . __prepare_resource ( parts1 [ 1 ] ) else : resource = None if not domain : raise JIDError ( "Domain is required in JID." ) return ( local , domain , resource )
9483	def to_bytes_36 ( self , previous : bytes ) : bc = b"" it_bc = util . generate_bytecode_from_obb ( self . iterator , previous ) bc += it_bc bc += util . ensure_instruction ( tokens . GET_ITER )
12266	def docstring ( docstr ) : def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , ** kwargs ) : return func ( * args , ** kwargs ) wrapper . __doc__ = docstr return wrapper return decorator
9147	def web ( connection , host , port ) : from bio2bel . web . application import create_application app = create_application ( connection = connection ) app . run ( host = host , port = port )
7540	def basecaller ( arrayed , mindepth_majrule , mindepth_statistical , estH , estE ) : cons = np . zeros ( arrayed . shape [ 1 ] , dtype = np . uint8 ) cons . fill ( 78 ) arr = arrayed . view ( np . uint8 ) for col in xrange ( arr . shape [ 1 ] ) : carr = arr [ : , col ] mask = carr == 45 mask += carr == 78 marr = carr [ ~ mask ] if not marr . shape [ 0 ] : cons [ col ] = 78 elif np . all ( marr == marr [ 0 ] ) : cons [ col ] = marr [ 0 ] else : counts = np . bincount ( marr ) pbase = np . argmax ( counts ) nump = counts [ pbase ] counts [ pbase ] = 0 qbase = np . argmax ( counts ) numq = counts [ qbase ] counts [ qbase ] = 0 rbase = np . argmax ( counts ) numr = counts [ rbase ] bidepth = nump + numq if bidepth < mindepth_majrule : cons [ col ] = 78 else : if bidepth > 500 : base1 = int ( 500 * ( nump / float ( bidepth ) ) ) base2 = int ( 500 * ( numq / float ( bidepth ) ) ) else : base1 = nump base2 = numq if bidepth >= mindepth_statistical : ishet , prob = get_binom ( base1 , base2 , estE , estH ) if prob < 0.95 : cons [ col ] = 78 else : if ishet : cons [ col ] = TRANS [ ( pbase , qbase ) ] else : cons [ col ] = pbase else : if nump == numq : cons [ col ] = TRANS [ ( pbase , qbase ) ] else : cons [ col ] = pbase return cons . view ( "S1" )
12356	def wait ( self ) : interval_seconds = 5 while True : actions = self . actions ( ) slept = False for a in actions : if a [ 'status' ] == 'in-progress' : time . sleep ( interval_seconds ) slept = True break if not slept : break
2544	def set_file_comment ( self , doc , text ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_comment_set : self . file_comment_set = True self . file ( doc ) . comment = text return True else : raise CardinalityError ( 'File::Comment' ) else : raise OrderError ( 'File::Comment' )
12368	def records ( self , name ) : if self . get ( name ) : return DomainRecords ( self . api , name )
5647	def write_temporal_networks_by_route_type ( gtfs , extract_output_dir ) : util . makedirs ( extract_output_dir ) for route_type in route_types . TRANSIT_ROUTE_TYPES : pandas_data_frame = temporal_network ( gtfs , start_time_ut = None , end_time_ut = None , route_type = route_type ) tag = route_types . ROUTE_TYPE_TO_LOWERCASE_TAG [ route_type ] out_file_name = os . path . join ( extract_output_dir , tag + ".tnet" ) pandas_data_frame . to_csv ( out_file_name , encoding = 'utf-8' , index = False )
13210	def _load_bib_db ( self ) : r command = LatexCommand ( 'bibliography' , { 'name' : 'bib_names' , 'required' : True , 'bracket' : '{' } ) try : parsed = next ( command . parse ( self . _tex ) ) bib_names = [ n . strip ( ) for n in parsed [ 'bib_names' ] . split ( ',' ) ] except StopIteration : self . _logger . warning ( 'lsstdoc has no bibliography command' ) bib_names = [ ] custom_bib_names = [ n for n in bib_names if n not in KNOWN_LSSTTEXMF_BIB_NAMES ] custom_bibs = [ ] for custom_bib_name in custom_bib_names : custom_bib_path = os . path . join ( os . path . join ( self . _root_dir ) , custom_bib_name + '.bib' ) if not os . path . exists ( custom_bib_path ) : self . _logger . warning ( 'Could not find bibliography %r' , custom_bib_path ) continue with open ( custom_bib_path , 'r' ) as file_handle : custom_bibs . append ( file_handle . read ( ) ) if len ( custom_bibs ) > 0 : custom_bibtex = '\n\n' . join ( custom_bibs ) else : custom_bibtex = None db = get_bibliography ( bibtex = custom_bibtex ) self . _bib_db = db
5918	def outfile ( self , p ) : if self . outdir is not None : return os . path . join ( self . outdir , os . path . basename ( p ) ) else : return p
13519	def prop_power ( self , propulsion_eff = 0.7 , sea_margin = 0.2 ) : PP = ( 1 + sea_margin ) * self . resistance ( ) * self . speed / propulsion_eff return PP
10060	def jsonschemas ( self ) : _jsonschemas = { k : v [ 'jsonschema' ] for k , v in self . app . config [ 'DEPOSIT_RECORDS_UI_ENDPOINTS' ] . items ( ) if 'jsonschema' in v } return defaultdict ( lambda : self . app . config [ 'DEPOSIT_DEFAULT_JSONSCHEMA' ] , _jsonschemas )
6726	def delete ( name = None , group = None , release = None , except_release = None , dryrun = 1 , verbose = 1 ) : verbose = int ( verbose ) if env . vm_type == EC2 : conn = get_ec2_connection ( ) instances = list_instances ( name = name , group = group , release = release , except_release = except_release , ) for instance_name , instance_data in instances . items ( ) : public_dns_name = instance_data [ 'public_dns_name' ] print ( '\nDeleting %s (%s)...' % ( instance_name , instance_data [ 'id' ] ) ) if not get_dryrun ( ) : conn . terminate_instances ( instance_ids = [ instance_data [ 'id' ] ] ) known_hosts = os . path . expanduser ( '~/.ssh/known_hosts' ) cmd = 'ssh-keygen -f "%s" -R %s' % ( known_hosts , public_dns_name ) local_or_dryrun ( cmd ) else : raise NotImplementedError
2902	def complete_next ( self , pick_up = True , halt_on_manual = True ) : blacklist = [ ] if pick_up and self . last_task is not None : try : iter = Task . Iterator ( self . last_task , Task . READY ) task = next ( iter ) except StopIteration : task = None self . last_task = None if task is not None : if not ( halt_on_manual and task . task_spec . manual ) : if task . complete ( ) : self . last_task = task return True blacklist . append ( task ) for task in Task . Iterator ( self . task_tree , Task . READY ) : for blacklisted_task in blacklist : if task . _is_descendant_of ( blacklisted_task ) : continue if not ( halt_on_manual and task . task_spec . manual ) : if task . complete ( ) : self . last_task = task return True blacklist . append ( task ) for task in Task . Iterator ( self . task_tree , Task . WAITING ) : task . task_spec . _update ( task ) if not task . _has_state ( Task . WAITING ) : self . last_task = task return True return False
5163	def __intermediate_address ( self , address ) : for key in self . _address_keys : if key in address : del address [ key ] return address
8508	def _get_dataset ( self , X , y = None ) : from pylearn2 . datasets import DenseDesignMatrix X = np . asarray ( X ) assert X . ndim > 1 if y is not None : y = self . _get_labels ( y ) if X . ndim == 2 : return DenseDesignMatrix ( X = X , y = y ) return DenseDesignMatrix ( topo_view = X , y = y )
13172	def last ( self , name = None ) : for c in self . children ( name , reverse = True ) : return c
12479	def rcfile ( appname , section = None , args = { } , strip_dashes = True ) : if strip_dashes : for k in args . keys ( ) : args [ k . lstrip ( '-' ) ] = args . pop ( k ) environ = get_environment ( appname ) if section is None : section = appname config = get_config ( appname , section , args . get ( 'config' , '' ) , args . get ( 'path' , '' ) ) config = merge ( merge ( args , config ) , environ ) if not config : raise IOError ( 'Could not find any rcfile for application ' '{}.' . format ( appname ) ) return config
4589	def serpentine_x ( x , y , matrix ) : if y % 2 : return matrix . columns - 1 - x , y return x , y
13202	def format_abstract ( self , format = 'html5' , deparagraph = False , mathjax = False , smart = True , extra_args = None ) : if self . abstract is None : return None abstract_latex = self . _prep_snippet_for_pandoc ( self . abstract ) output_text = convert_lsstdoc_tex ( abstract_latex , format , deparagraph = deparagraph , mathjax = mathjax , smart = smart , extra_args = extra_args ) return output_text
12578	def apply_smoothing ( self , smooth_fwhm ) : if smooth_fwhm <= 0 : return old_smooth_fwhm = self . _smooth_fwhm self . _smooth_fwhm = smooth_fwhm try : data = self . get_data ( smoothed = True , masked = True , safe_copy = True ) except ValueError as ve : self . _smooth_fwhm = old_smooth_fwhm raise else : self . _smooth_fwhm = smooth_fwhm return data
1674	def ParseArguments ( args ) : try : ( opts , filenames ) = getopt . getopt ( args , '' , [ 'help' , 'output=' , 'verbose=' , 'counting=' , 'filter=' , 'root=' , 'repository=' , 'linelength=' , 'extensions=' , 'exclude=' , 'headers=' , 'quiet' , 'recursive' ] ) except getopt . GetoptError : PrintUsage ( 'Invalid arguments.' ) verbosity = _VerboseLevel ( ) output_format = _OutputFormat ( ) filters = '' counting_style = '' recursive = False for ( opt , val ) in opts : if opt == '--help' : PrintUsage ( None ) elif opt == '--output' : if val not in ( 'emacs' , 'vs7' , 'eclipse' , 'junit' ) : PrintUsage ( 'The only allowed output formats are emacs, vs7, eclipse ' 'and junit.' ) output_format = val elif opt == '--verbose' : verbosity = int ( val ) elif opt == '--filter' : filters = val if not filters : PrintCategories ( ) elif opt == '--counting' : if val not in ( 'total' , 'toplevel' , 'detailed' ) : PrintUsage ( 'Valid counting options are total, toplevel, and detailed' ) counting_style = val elif opt == '--root' : global _root _root = val elif opt == '--repository' : global _repository _repository = val elif opt == '--linelength' : global _line_length try : _line_length = int ( val ) except ValueError : PrintUsage ( 'Line length must be digits.' ) elif opt == '--exclude' : global _excludes if not _excludes : _excludes = set ( ) _excludes . update ( glob . glob ( val ) ) elif opt == '--extensions' : global _valid_extensions try : _valid_extensions = set ( val . split ( ',' ) ) except ValueError : PrintUsage ( 'Extensions must be comma seperated list.' ) elif opt == '--headers' : global _header_extensions try : _header_extensions = set ( val . split ( ',' ) ) except ValueError : PrintUsage ( 'Extensions must be comma seperated list.' ) elif opt == '--recursive' : recursive = True elif opt == '--quiet' : global _quiet _quiet = True if not filenames : PrintUsage ( 'No files were specified.' ) if recursive : filenames = _ExpandDirectories ( filenames ) if _excludes : filenames = _FilterExcludedFiles ( filenames ) _SetOutputFormat ( output_format ) _SetVerboseLevel ( verbosity ) _SetFilters ( filters ) _SetCountingStyle ( counting_style ) return filenames
7730	def clear_muc_child ( self ) : if self . muc_child : self . muc_child . free_borrowed ( ) self . muc_child = None if not self . xmlnode . children : return n = self . xmlnode . children while n : if n . name not in ( "x" , "query" ) : n = n . next continue ns = n . ns ( ) if not ns : n = n . next continue ns_uri = ns . getContent ( ) if ns_uri in ( MUC_NS , MUC_USER_NS , MUC_ADMIN_NS , MUC_OWNER_NS ) : n . unlinkNode ( ) n . freeNode ( ) n = n . next
11894	def readtxt ( filepath ) : with open ( filepath , 'rt' ) as f : lines = f . readlines ( ) return '' . join ( lines )
6473	def human ( self , size , base = 1000 , units = ' kMGTZ' ) : sign = '+' if size >= 0 else '-' size = abs ( size ) if size < 1000 : return '%s%d' % ( sign , size ) for i , suffix in enumerate ( units ) : unit = 1000 ** ( i + 1 ) if size < unit : return ( '%s%.01f%s' % ( sign , size / float ( unit ) * base , suffix , ) ) . strip ( ) raise OverflowError
12391	def indexesOptional ( f ) : stack = inspect . stack ( ) _NO_INDEX_CHECK_NEEDED . add ( '%s.%s.%s' % ( f . __module__ , stack [ 1 ] [ 3 ] , f . __name__ ) ) del stack return f
13066	def make_members ( self , collection , lang = None ) : objects = sorted ( [ self . expose_ancestors_or_children ( member , collection , lang = lang ) for member in collection . members if member . get_label ( ) ] , key = itemgetter ( "label" ) ) return objects
7398	def parse ( string ) : bib = [ ] if not isinstance ( string , six . text_type ) : string = string . decode ( 'utf-8' ) for key , value in special_chars : string = string . replace ( key , value ) string = re . sub ( r'\\[cuHvs]{?([a-zA-Z])}?' , r'\1' , string ) entries = re . findall ( r'(?u)@(\w+)[ \t]?{[ \t]*([^,\s]*)[ \t]*,?\s*((?:[^=,\s]+\s*\=\s*(?:"[^"]*"|{(?:[^{}]*|{[^{}]*})*}|[^,}]*),?\s*?)+)\s*}' , string ) for entry in entries : pairs = re . findall ( r'(?u)([^=,\s]+)\s*\=\s*("[^"]*"|{(?:[^{}]*|{[^{}]*})*}|[^,]*)' , entry [ 2 ] ) bib . append ( { 'type' : entry [ 0 ] . lower ( ) , 'key' : entry [ 1 ] } ) for key , value in pairs : key = key . lower ( ) if value and value [ 0 ] == '"' and value [ - 1 ] == '"' : value = value [ 1 : - 1 ] if value and value [ 0 ] == '{' and value [ - 1 ] == '}' : value = value [ 1 : - 1 ] if key not in [ 'booktitle' , 'title' ] : value = value . replace ( '}' , '' ) . replace ( '{' , '' ) else : if value . startswith ( '{' ) and value . endswith ( '}' ) : value = value [ 1 : ] value = value [ : - 1 ] value = value . strip ( ) value = re . sub ( r'\s+' , ' ' , value ) bib [ - 1 ] [ key ] = value return bib
11952	def execute ( varsfile , templatefile , outputfile = None , configfile = None , dryrun = False , build = False , push = False , verbose = False ) : if dryrun and ( build or push ) : jocker_lgr . error ( 'dryrun requested, cannot build.' ) sys . exit ( 100 ) _set_global_verbosity_level ( verbose ) j = Jocker ( varsfile , templatefile , outputfile , configfile , dryrun , build , push ) formatted_text = j . generate ( ) if dryrun : g = j . dryrun ( formatted_text ) if build or push : j . build_image ( ) if push : j . push_image ( ) if dryrun : return g
8766	def _fix_missing_tenant_id ( self , context , body , key ) : if not body : raise n_exc . BadRequest ( resource = key , msg = "Body malformed" ) resource = body . get ( key ) if not resource : raise n_exc . BadRequest ( resource = key , msg = "Body malformed" ) if context . tenant_id is None : context . tenant_id = resource . get ( "tenant_id" ) if context . tenant_id is None : msg = _ ( "Running without keystone AuthN requires " "that tenant_id is specified" ) raise n_exc . BadRequest ( resource = key , msg = msg )
12614	def update_unique ( self , table_name , fields , data , cond = None , unique_fields = None , * , raise_if_not_found = False ) : eid = find_unique ( self . table ( table_name ) , data , unique_fields ) if eid is None : if raise_if_not_found : msg = 'Could not find {} with {}' . format ( table_name , data ) if cond is not None : msg += ' where {}.' . format ( cond ) raise IndexError ( msg ) else : self . table ( table_name ) . update ( _to_string ( fields ) , cond = cond , eids = [ eid ] ) return eid
9749	def create_body_index ( xml_string ) : xml = ET . fromstring ( xml_string ) body_to_index = { } for index , body in enumerate ( xml . findall ( "*/Body/Name" ) ) : body_to_index [ body . text . strip ( ) ] = index return body_to_index
7849	def has_feature ( self , var ) : if not var : raise ValueError ( "var is None" ) if '"' not in var : expr = u'd:feature[@var="%s"]' % ( var , ) elif "'" not in var : expr = u"d:feature[@var='%s']" % ( var , ) else : raise ValueError ( "Invalid feature name" ) l = self . xpath_ctxt . xpathEval ( to_utf8 ( expr ) ) if l : return True else : return False
6499	def search ( self , query_string = None , field_dictionary = None , filter_dictionary = None , exclude_dictionary = None , facet_terms = None , exclude_ids = None , use_field_match = False , ** kwargs ) : log . debug ( "searching index with %s" , query_string ) elastic_queries = [ ] elastic_filters = [ ] if query_string : if six . PY2 : query_string = query_string . encode ( 'utf-8' ) . translate ( None , RESERVED_CHARACTERS ) else : query_string = query_string . translate ( query_string . maketrans ( '' , '' , RESERVED_CHARACTERS ) ) elastic_queries . append ( { "query_string" : { "fields" : [ "content.*" ] , "query" : query_string } } ) if field_dictionary : if use_field_match : elastic_queries . extend ( _process_field_queries ( field_dictionary ) ) else : elastic_filters . extend ( _process_field_filters ( field_dictionary ) ) if filter_dictionary : elastic_filters . extend ( _process_filters ( filter_dictionary ) ) if exclude_ids : if not exclude_dictionary : exclude_dictionary = { } if "_id" not in exclude_dictionary : exclude_dictionary [ "_id" ] = [ ] exclude_dictionary [ "_id" ] . extend ( exclude_ids ) if exclude_dictionary : elastic_filters . append ( _process_exclude_dictionary ( exclude_dictionary ) ) query_segment = { "match_all" : { } } if elastic_queries : query_segment = { "bool" : { "must" : elastic_queries } } query = query_segment if elastic_filters : filter_segment = { "bool" : { "must" : elastic_filters } } query = { "filtered" : { "query" : query_segment , "filter" : filter_segment , } } body = { "query" : query } if facet_terms : facet_query = _process_facet_terms ( facet_terms ) if facet_query : body [ "facets" ] = facet_query try : es_response = self . _es . search ( index = self . index_name , body = body , ** kwargs ) except exceptions . ElasticsearchException as ex : message = six . text_type ( ex ) if 'QueryParsingException' in message : log . exception ( "Malformed search query: %s" , message ) raise QueryParseError ( 'Malformed search query.' ) else : log . exception ( "error while searching index - %s" , str ( message ) ) raise return _translate_hits ( es_response )
13807	def __get_current_datetime ( self ) : self . wql_time = "SELECT LocalDateTime FROM Win32_OperatingSystem" self . current_time = self . query ( self . wql_time ) self . current_time_string = str ( self . current_time [ 0 ] . get ( 'LocalDateTime' ) . split ( '.' ) [ 0 ] ) self . current_time_format = datetime . datetime . strptime ( self . current_time_string , '%Y%m%d%H%M%S' ) return self . current_time_format
8252	def image_to_rgb ( self , path , n = 10 ) : from PIL import Image img = Image . open ( path ) p = img . getdata ( ) f = lambda p : choice ( p ) for i in _range ( n ) : rgba = f ( p ) rgba = _list ( rgba ) if len ( rgba ) == 3 : rgba . append ( 255 ) r , g , b , a = [ v / 255.0 for v in rgba ] clr = color ( r , g , b , a , mode = "rgb" ) self . append ( clr )
10982	def locate_spheres ( image , feature_rad , dofilter = False , order = ( 3 , 3 , 3 ) , trim_edge = True , ** kwargs ) : m = models . SmoothFieldModel ( ) I = ilms . LegendrePoly2P1D ( order = order , constval = image . get_image ( ) . mean ( ) ) s = states . ImageState ( image , [ I ] , pad = 0 , mdl = m ) if dofilter : opt . do_levmarq ( s , s . params ) pos = addsub . feature_guess ( s , feature_rad , trim_edge = trim_edge , ** kwargs ) [ 0 ] return pos
6275	def resolve_loader ( self , meta : ResourceDescription ) : meta . loader_cls = self . get_loader ( meta , raise_on_error = True )
11581	def run ( self ) : self . command_dispatch . update ( { self . REPORT_VERSION : [ self . report_version , 2 ] } ) self . command_dispatch . update ( { self . REPORT_FIRMWARE : [ self . report_firmware , 1 ] } ) self . command_dispatch . update ( { self . ANALOG_MESSAGE : [ self . analog_message , 2 ] } ) self . command_dispatch . update ( { self . DIGITAL_MESSAGE : [ self . digital_message , 2 ] } ) self . command_dispatch . update ( { self . ENCODER_DATA : [ self . encoder_data , 3 ] } ) self . command_dispatch . update ( { self . SONAR_DATA : [ self . sonar_data , 3 ] } ) self . command_dispatch . update ( { self . STRING_DATA : [ self . _string_data , 2 ] } ) self . command_dispatch . update ( { self . I2C_REPLY : [ self . i2c_reply , 2 ] } ) self . command_dispatch . update ( { self . CAPABILITY_RESPONSE : [ self . capability_response , 2 ] } ) self . command_dispatch . update ( { self . PIN_STATE_RESPONSE : [ self . pin_state_response , 2 ] } ) self . command_dispatch . update ( { self . ANALOG_MAPPING_RESPONSE : [ self . analog_mapping_response , 2 ] } ) self . command_dispatch . update ( { self . STEPPER_DATA : [ self . stepper_version_response , 2 ] } ) while not self . is_stopped ( ) : if len ( self . pymata . command_deque ) : data = self . pymata . command_deque . popleft ( ) command_data = [ ] if data == self . START_SYSEX : while len ( self . pymata . command_deque ) == 0 : pass sysex_command = self . pymata . command_deque . popleft ( ) dispatch_entry = self . command_dispatch . get ( sysex_command ) method = dispatch_entry [ 0 ] end_of_sysex = False while not end_of_sysex : while len ( self . pymata . command_deque ) == 0 : pass data = self . pymata . command_deque . popleft ( ) if data != self . END_SYSEX : command_data . append ( data ) else : end_of_sysex = True method ( command_data ) continue elif 0x80 <= data <= 0xff : if 0x90 <= data <= 0x9f : port = data & 0xf command_data . append ( port ) data = 0x90 elif 0xe0 <= data <= 0xef : pin = data & 0xf command_data . append ( pin ) data = 0xe0 else : pass dispatch_entry = self . command_dispatch . get ( data ) method = dispatch_entry [ 0 ] num_args = dispatch_entry [ 1 ] for i in range ( num_args ) : while len ( self . pymata . command_deque ) == 0 : pass data = self . pymata . command_deque . popleft ( ) command_data . append ( data ) method ( command_data ) continue else : time . sleep ( .1 )
11474	def login ( email = None , password = None , api_key = None , application = 'Default' , url = None , verify_ssl_certificate = True ) : try : input_ = raw_input except NameError : input_ = input if url is None : url = input_ ( 'Server URL: ' ) url = url . rstrip ( '/' ) if session . communicator is None : session . communicator = Communicator ( url ) else : session . communicator . url = url session . communicator . verify_ssl_certificate = verify_ssl_certificate if email is None : email = input_ ( 'Email: ' ) session . email = email if api_key is None : if password is None : password = getpass . getpass ( ) session . api_key = session . communicator . get_default_api_key ( session . email , password ) session . application = 'Default' else : session . api_key = api_key session . application = application return renew_token ( )
11936	def display ( self ) : if not self . is_group ( ) : return self . _display return ( ( force_text ( k ) , v ) for k , v in self . _display )
1051	def format_exception_only ( etype , value ) : if ( isinstance ( etype , BaseException ) or etype is None or type ( etype ) is str ) : return [ _format_final_exc_line ( etype , value ) ] stype = etype . __name__ if not issubclass ( etype , SyntaxError ) : return [ _format_final_exc_line ( stype , value ) ] lines = [ ] try : msg , ( filename , lineno , offset , badline ) = value . args except Exception : pass else : filename = filename or "<string>" lines . append ( ' File "%s", line %d\n' % ( filename , lineno ) ) if badline is not None : lines . append ( ' %s\n' % badline . strip ( ) ) if offset is not None : caretspace = badline . rstrip ( '\n' ) offset = min ( len ( caretspace ) , offset ) - 1 caretspace = caretspace [ : offset ] . lstrip ( ) caretspace = ( ( c . isspace ( ) and c or ' ' ) for c in caretspace ) lines . append ( ' %s^\n' % '' . join ( caretspace ) ) value = msg lines . append ( _format_final_exc_line ( stype , value ) ) return lines
13218	def connection_url ( self , name = None ) : return 'postgresql://{user}@{host}:{port}/{dbname}' . format ( ** { k : v for k , v in self . _connect_options ( name ) } )
3693	def Tb ( CASRN , AvailableMethods = False , Method = None , IgnoreMethods = [ PSAT_DEFINITION ] ) : r def list_methods ( ) : methods = [ ] if CASRN in CRC_inorganic_data . index and not np . isnan ( CRC_inorganic_data . at [ CASRN , 'Tb' ] ) : methods . append ( CRC_INORG ) if CASRN in CRC_organic_data . index and not np . isnan ( CRC_organic_data . at [ CASRN , 'Tb' ] ) : methods . append ( CRC_ORG ) if CASRN in Yaws_data . index : methods . append ( YAWS ) if PSAT_DEFINITION not in IgnoreMethods : try : VaporPressure ( CASRN = CASRN ) . solve_prop ( 101325. ) methods . append ( PSAT_DEFINITION ) except : pass if IgnoreMethods : for Method in IgnoreMethods : if Method in methods : methods . remove ( Method ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == CRC_INORG : return float ( CRC_inorganic_data . at [ CASRN , 'Tb' ] ) elif Method == CRC_ORG : return float ( CRC_organic_data . at [ CASRN , 'Tb' ] ) elif Method == YAWS : return float ( Yaws_data . at [ CASRN , 'Tb' ] ) elif Method == PSAT_DEFINITION : return VaporPressure ( CASRN = CASRN ) . solve_prop ( 101325. ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' )
8442	def _code_search ( query , github_user = None ) : github_client = temple . utils . GithubClient ( ) headers = { 'Accept' : 'application/vnd.github.v3.text-match+json' } resp = github_client . get ( '/search/code' , params = { 'q' : query , 'per_page' : 100 } , headers = headers ) if resp . status_code == requests . codes . unprocessable_entity and github_user : raise temple . exceptions . InvalidGithubUserError ( 'Invalid Github user or org - "{}"' . format ( github_user ) ) resp . raise_for_status ( ) resp_data = resp . json ( ) repositories = collections . defaultdict ( dict ) while True : repositories . update ( { 'git@github.com:{}.git' . format ( repo [ 'repository' ] [ 'full_name' ] ) : repo [ 'repository' ] for repo in resp_data [ 'items' ] } ) next_url = _parse_link_header ( resp . headers ) . get ( 'next' ) if next_url : resp = requests . get ( next_url , headers = headers ) resp . raise_for_status ( ) resp_data = resp . json ( ) else : break return repositories
12801	def get_rooms ( self , sort = True ) : rooms = self . _connection . get ( "rooms" ) if sort : rooms . sort ( key = operator . itemgetter ( "name" ) ) return rooms
10445	def getchild ( self , window_name , child_name = '' , role = '' , parent = '' ) : matches = [ ] if role : role = re . sub ( ' ' , '_' , role ) self . _windows = { } if parent and ( child_name or role ) : _window_handle , _window_name = self . _get_window_handle ( window_name ) [ 0 : 2 ] if not _window_handle : raise LdtpServerException ( 'Unable to find window "%s"' % window_name ) appmap = self . _get_appmap ( _window_handle , _window_name ) obj = self . _get_object_map ( window_name , parent ) def _get_all_children_under_obj ( obj , child_list ) : if role and obj [ 'class' ] == role : child_list . append ( obj [ 'label' ] ) elif child_name and self . _match_name_to_appmap ( child_name , obj ) : child_list . append ( obj [ 'label' ] ) if obj : children = obj [ 'children' ] if not children : return child_list for child in children . split ( ) : return _get_all_children_under_obj ( appmap [ child ] , child_list ) matches = _get_all_children_under_obj ( obj , [ ] ) if not matches : if child_name : _name = 'name "%s" ' % child_name if role : _role = 'role "%s" ' % role if parent : _parent = 'parent "%s"' % parent exception = 'Could not find a child %s%s%s' % ( _name , _role , _parent ) raise LdtpServerException ( exception ) return matches _window_handle , _window_name = self . _get_window_handle ( window_name ) [ 0 : 2 ] if not _window_handle : raise LdtpServerException ( 'Unable to find window "%s"' % window_name ) appmap = self . _get_appmap ( _window_handle , _window_name ) for name in appmap . keys ( ) : obj = appmap [ name ] if role and not child_name and obj [ 'class' ] == role : matches . append ( name ) if parent and child_name and not role and self . _match_name_to_appmap ( parent , obj ) : matches . append ( name ) if child_name and not role and self . _match_name_to_appmap ( child_name , obj ) : return name matches . append ( name ) if role and child_name and obj [ 'class' ] == role and self . _match_name_to_appmap ( child_name , obj ) : matches . append ( name ) if not matches : _name = '' _role = '' _parent = '' if child_name : _name = 'name "%s" ' % child_name if role : _role = 'role "%s" ' % role if parent : _parent = 'parent "%s"' % parent exception = 'Could not find a child %s%s%s' % ( _name , _role , _parent ) raise LdtpServerException ( exception ) return matches
6928	def close_cursor ( self , handle ) : if handle in self . cursors : self . cursors [ handle ] . close ( ) else : raise KeyError ( 'cursor with handle %s was not found' % handle )
90	def copy_random_state ( random_state , force_copy = False ) : if random_state == np . random and not force_copy : return random_state else : rs_copy = dummy_random_state ( ) orig_state = random_state . get_state ( ) rs_copy . set_state ( orig_state ) return rs_copy
5356	def set_param ( self , section , param , value ) : if section not in self . conf or param not in self . conf [ section ] : logger . error ( 'Config section %s and param %s not exists' , section , param ) else : self . conf [ section ] [ param ] = value
4692	def env ( ) : ipmi = cij . env_to_dict ( PREFIX , REQUIRED ) if ipmi is None : ipmi [ "USER" ] = "admin" ipmi [ "PASS" ] = "admin" ipmi [ "HOST" ] = "localhost" ipmi [ "PORT" ] = "623" cij . info ( "ipmi.env: USER: %s, PASS: %s, HOST: %s, PORT: %s" % ( ipmi [ "USER" ] , ipmi [ "PASS" ] , ipmi [ "HOST" ] , ipmi [ "PORT" ] ) ) cij . env_export ( PREFIX , EXPORTED , ipmi ) return 0
9205	def before_constant ( self , constant , key ) : newlines_split = split_on_newlines ( constant ) for c in newlines_split : if is_newline ( c ) : self . current . advance_line ( ) if self . current . line > self . target . line : return self . STOP else : advance_by = len ( c ) if self . is_on_targetted_node ( advance_by ) : self . found_path = deepcopy ( self . current_path ) return self . STOP self . current . advance_columns ( advance_by )
5119	def initialize ( self , nActive = 1 , queues = None , edges = None , edge_type = None ) : if queues is None and edges is None and edge_type is None : if nActive >= 1 and isinstance ( nActive , numbers . Integral ) : qs = [ q . edge [ 2 ] for q in self . edge2queue if q . edge [ 3 ] != 0 ] n = min ( nActive , len ( qs ) ) queues = np . random . choice ( qs , size = n , replace = False ) elif not isinstance ( nActive , numbers . Integral ) : msg = "If queues is None, then nActive must be an integer." raise TypeError ( msg ) else : msg = ( "If queues is None, then nActive must be a " "positive int." ) raise ValueError ( msg ) else : queues = _get_queues ( self . g , queues , edges , edge_type ) queues = [ e for e in queues if self . edge2queue [ e ] . edge [ 3 ] != 0 ] if len ( queues ) == 0 : raise QueueingToolError ( "There were no queues to initialize." ) if len ( queues ) > self . max_agents : queues = queues [ : self . max_agents ] for ei in queues : self . edge2queue [ ei ] . set_active ( ) self . num_agents [ ei ] = self . edge2queue [ ei ] . _num_total keys = [ q . _key ( ) for q in self . edge2queue if q . _time < np . infty ] self . _fancy_heap = PriorityQueue ( keys , self . nE ) self . _initialized = True
8649	def delete_milestone_request ( session , milestone_request_id ) : params_data = { 'action' : 'delete' , } endpoint = 'milestone_requests/{}' . format ( milestone_request_id ) response = make_put_request ( session , endpoint , params_data = params_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise MilestoneRequestNotDeletedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
12096	def save ( self , * args , ** kwargs ) : current_activable_value = getattr ( self , self . ACTIVATABLE_FIELD_NAME ) is_active_changed = self . id is None or self . __original_activatable_value != current_activable_value self . __original_activatable_value = current_activable_value ret_val = super ( BaseActivatableModel , self ) . save ( * args , ** kwargs ) if is_active_changed : model_activations_changed . send ( self . __class__ , instance_ids = [ self . id ] , is_active = current_activable_value ) if self . activatable_field_updated : model_activations_updated . send ( self . __class__ , instance_ids = [ self . id ] , is_active = current_activable_value ) return ret_val
13294	def convert_text ( content , from_fmt , to_fmt , deparagraph = False , mathjax = False , smart = True , extra_args = None ) : logger = logging . getLogger ( __name__ ) if extra_args is not None : extra_args = list ( extra_args ) else : extra_args = [ ] if mathjax : extra_args . append ( '--mathjax' ) if smart : extra_args . append ( '--smart' ) if deparagraph : extra_args . append ( '--filter=lsstprojectmeta-deparagraph' ) extra_args . append ( '--wrap=none' ) extra_args = set ( extra_args ) logger . debug ( 'Running pandoc from %s to %s with extra_args %s' , from_fmt , to_fmt , extra_args ) output = pypandoc . convert_text ( content , to_fmt , format = from_fmt , extra_args = extra_args ) return output
12772	def follow_markers ( self , start = 0 , end = 1e100 , states = None ) : if states is not None : self . skeleton . set_body_states ( states ) for frame_no , frame in enumerate ( self . markers ) : if frame_no < start : continue if frame_no >= end : break for states in self . _step_to_marker_frame ( frame_no ) : yield states
4554	def pointOnCircle ( cx , cy , radius , angle ) : angle = math . radians ( angle ) - ( math . pi / 2 ) x = cx + radius * math . cos ( angle ) if x < cx : x = math . ceil ( x ) else : x = math . floor ( x ) y = cy + radius * math . sin ( angle ) if y < cy : y = math . ceil ( y ) else : y = math . floor ( y ) return ( int ( x ) , int ( y ) )
9268	def detect_link_tag_time ( self , tag ) : newer_tag_time = self . get_time_of_tag ( tag ) if tag else datetime . datetime . now ( ) if tag [ "name" ] == self . options . unreleased_label and self . options . future_release : newer_tag_name = self . options . future_release newer_tag_link = self . options . future_release elif tag [ "name" ] is not self . options . unreleased_label : newer_tag_name = tag [ "name" ] newer_tag_link = newer_tag_name else : newer_tag_name = self . options . unreleased_label newer_tag_link = "HEAD" return [ newer_tag_link , newer_tag_name , newer_tag_time ]
12006	def _read_version ( self , data ) : version = ord ( data [ 0 ] ) if version not in self . VERSIONS : raise Exception ( 'Version not defined: %d' % version ) return version
7644	def can_convert ( annotation , target_namespace ) : if annotation . namespace == target_namespace : return True if target_namespace in __CONVERSION__ : for source in __CONVERSION__ [ target_namespace ] : if annotation . search ( namespace = source ) : return True return False
2393	def gen_preds ( clf , arr ) : if ( hasattr ( clf , "predict_proba" ) ) : ret = clf . predict ( arr ) else : ret = clf . predict ( arr ) return ret
12464	def print_message ( message = None ) : kwargs = { 'stdout' : sys . stdout , 'stderr' : sys . stderr , 'shell' : True } return subprocess . call ( 'echo "{0}"' . format ( message or '' ) , ** kwargs )
6708	def get_file_hash ( fin , block_size = 2 ** 20 ) : if isinstance ( fin , six . string_types ) : fin = open ( fin ) h = hashlib . sha512 ( ) while True : data = fin . read ( block_size ) if not data : break try : h . update ( data ) except TypeError : h . update ( data . encode ( 'utf-8' ) ) return h . hexdigest ( )
12452	def check_pre_requirements ( pre_requirements ) : pre_requirements = set ( pre_requirements or [ ] ) pre_requirements . add ( 'virtualenv' ) for requirement in pre_requirements : if not which ( requirement ) : print_error ( 'Requirement {0!r} is not found in system' . format ( requirement ) ) return False return True
13714	def upload ( self ) : success = False batch = self . next ( ) if len ( batch ) == 0 : return False try : self . request ( batch ) success = True except Exception as e : self . log . error ( 'error uploading: %s' , e ) success = False if self . on_error : self . on_error ( e , batch ) finally : for item in batch : self . queue . task_done ( ) return success
1915	def put ( self , state_id ) : self . _states . append ( state_id ) self . _lock . notify_all ( ) return state_id
1689	def InTemplateArgumentList ( self , clean_lines , linenum , pos ) : while linenum < clean_lines . NumLines ( ) : line = clean_lines . elided [ linenum ] match = Match ( r'^[^{};=\[\]\.<>]*(.)' , line [ pos : ] ) if not match : linenum += 1 pos = 0 continue token = match . group ( 1 ) pos += len ( match . group ( 0 ) ) if token in ( '{' , '}' , ';' ) : return False if token in ( '>' , '=' , '[' , ']' , '.' ) : return True if token != '<' : pos += 1 if pos >= len ( line ) : linenum += 1 pos = 0 continue ( _ , end_line , end_pos ) = CloseExpression ( clean_lines , linenum , pos - 1 ) if end_pos < 0 : return False linenum = end_line pos = end_pos return False
5706	def clean ( self ) : cleaned_data = super ( AuthForm , self ) . clean ( ) user = self . get_user ( ) if self . staff_only and ( not user or not user . is_staff ) : raise forms . ValidationError ( 'Sorry, only staff are allowed.' ) if self . superusers_only and ( not user or not user . is_superuser ) : raise forms . ValidationError ( 'Sorry, only superusers are allowed.' ) return cleaned_data
8164	def inheritFromContext ( self , ignore = ( ) ) : for canvas_attr , grob_attr in STATES . items ( ) : if canvas_attr in ignore : continue setattr ( self , grob_attr , getattr ( self . _bot . _canvas , canvas_attr ) )
1976	def sys_receive ( self , cpu , fd , buf , count , rx_bytes ) : data = '' if count != 0 : if not self . _is_open ( fd ) : logger . info ( "RECEIVE: Not valid file descriptor on receive. Returning EBADF" ) return Decree . CGC_EBADF if buf not in cpu . memory : logger . info ( "RECEIVE: buf points to invalid address. Returning EFAULT" ) return Decree . CGC_EFAULT if fd > 2 and self . files [ fd ] . is_empty ( ) : cpu . PC -= cpu . instruction . size self . wait ( [ fd ] , [ ] , None ) raise RestartSyscall ( ) data = self . files [ fd ] . receive ( count ) self . syscall_trace . append ( ( "_receive" , fd , data ) ) cpu . write_bytes ( buf , data ) self . signal_receive ( fd ) if rx_bytes : if rx_bytes not in cpu . memory : logger . info ( "RECEIVE: Not valid file descriptor on receive. Returning EFAULT" ) return Decree . CGC_EFAULT cpu . write_int ( rx_bytes , len ( data ) , 32 ) logger . info ( "RECEIVE(%d, 0x%08x, %d, 0x%08x) -> <%s> (size:%d)" % ( fd , buf , count , rx_bytes , repr ( data ) [ : min ( count , 10 ) ] , len ( data ) ) ) return 0
1882	def new_symbolic_buffer ( self , nbytes , ** options ) : label = options . get ( 'label' ) avoid_collisions = False if label is None : label = 'buffer' avoid_collisions = True taint = options . get ( 'taint' , frozenset ( ) ) expr = self . _constraints . new_array ( name = label , index_max = nbytes , value_bits = 8 , taint = taint , avoid_collisions = avoid_collisions ) self . _input_symbols . append ( expr ) if options . get ( 'cstring' , False ) : for i in range ( nbytes - 1 ) : self . _constraints . add ( expr [ i ] != 0 ) return expr
13609	def contact ( request ) : form = ContactForm ( request . POST or None ) if form . is_valid ( ) : subject = form . cleaned_data [ 'subject' ] message = form . cleaned_data [ 'message' ] sender = form . cleaned_data [ 'sender' ] cc_myself = form . cleaned_data [ 'cc_myself' ] recipients = settings . CONTACTFORM_RECIPIENTS if cc_myself : recipients . append ( sender ) send_mail ( getattr ( settings , "CONTACTFORM_SUBJECT_PREFIX" , '' ) + subject , message , sender , recipients ) return render ( request , 'contactform/thanks.html' ) return render ( request , 'contactform/contact.html' , { 'form' : form } )
2578	def submit ( self , func , * args , executors = 'all' , fn_hash = None , cache = False , ** kwargs ) : if self . cleanup_called : raise ValueError ( "Cannot submit to a DFK that has been cleaned up" ) task_id = self . task_count self . task_count += 1 if isinstance ( executors , str ) and executors . lower ( ) == 'all' : choices = list ( e for e in self . executors if e != 'data_manager' ) elif isinstance ( executors , list ) : choices = executors executor = random . choice ( choices ) args , kwargs = self . _add_input_deps ( executor , args , kwargs ) task_def = { 'depends' : None , 'executor' : executor , 'func' : func , 'func_name' : func . __name__ , 'args' : args , 'kwargs' : kwargs , 'fn_hash' : fn_hash , 'memoize' : cache , 'callback' : None , 'exec_fu' : None , 'checkpoint' : None , 'fail_count' : 0 , 'fail_history' : [ ] , 'env' : None , 'status' : States . unsched , 'id' : task_id , 'time_submitted' : None , 'time_returned' : None , 'app_fu' : None } if task_id in self . tasks : raise DuplicateTaskError ( "internal consistency error: Task {0} already exists in task list" . format ( task_id ) ) else : self . tasks [ task_id ] = task_def dep_cnt , depends = self . _gather_all_deps ( args , kwargs ) self . tasks [ task_id ] [ 'depends' ] = depends task_stdout = kwargs . get ( 'stdout' ) task_stderr = kwargs . get ( 'stderr' ) logger . info ( "Task {} submitted for App {}, waiting on tasks {}" . format ( task_id , task_def [ 'func_name' ] , [ fu . tid for fu in depends ] ) ) self . tasks [ task_id ] [ 'task_launch_lock' ] = threading . Lock ( ) app_fu = AppFuture ( tid = task_id , stdout = task_stdout , stderr = task_stderr ) self . tasks [ task_id ] [ 'app_fu' ] = app_fu app_fu . add_done_callback ( partial ( self . handle_app_update , task_id ) ) self . tasks [ task_id ] [ 'status' ] = States . pending logger . debug ( "Task {} set to pending state with AppFuture: {}" . format ( task_id , task_def [ 'app_fu' ] ) ) for d in depends : def callback_adapter ( dep_fut ) : self . launch_if_ready ( task_id ) try : d . add_done_callback ( callback_adapter ) except Exception as e : logger . error ( "add_done_callback got an exception {} which will be ignored" . format ( e ) ) self . launch_if_ready ( task_id ) return task_def [ 'app_fu' ]
1280	def hard_wrap ( self ) : self . linebreak = re . compile ( r'^ *\n(?!\s*$)' ) self . text = re . compile ( r'^[\s\S]+?(?=[\\<!\[_*`~]|https?://| *\n|$)' )
10141	def parse_arguments ( args , clone_list ) : returned_string = "" host_number = args . host if args . show_list : print ( generate_host_string ( clone_list , "Available hosts: " ) ) exit ( ) if args . decrypt : for i in args . files : print ( decrypt_files ( i ) ) exit ( ) if args . files : for i in args . files : if args . limit_size : if args . host == host_number and host_number is not None : if not check_max_filesize ( i , clone_list [ host_number ] [ 3 ] ) : host_number = None for n , host in enumerate ( clone_list ) : if not check_max_filesize ( i , host [ 3 ] ) : clone_list [ n ] = None if not clone_list : print ( 'None of the clones is able to support so big file.' ) if args . no_cloudflare : if args . host == host_number and host_number is not None and not clone_list [ host_number ] [ 4 ] : print ( "This host uses Cloudflare, please choose different host." ) exit ( 1 ) else : for n , host in enumerate ( clone_list ) : if not host [ 4 ] : clone_list [ n ] = None clone_list = list ( filter ( None , clone_list ) ) if host_number is None or args . host != host_number : host_number = random . randrange ( 0 , len ( clone_list ) ) while True : try : if args . encrypt : returned_string = encrypt_files ( clone_list [ host_number ] , args . only_link , i ) else : returned_string = upload_files ( open ( i , 'rb' ) , clone_list [ host_number ] , args . only_link , i ) if args . only_link : print ( returned_string [ 0 ] ) else : print ( returned_string ) except IndexError : host_number = random . randrange ( 0 , len ( clone_list ) ) continue except IsADirectoryError : print ( 'limf does not support directory upload, if you want to upload ' 'every file in directory use limf {}/*.' . format ( i . replace ( '/' , '' ) ) ) if args . log : with open ( os . path . expanduser ( args . logfile ) , "a+" ) as logfile : if args . only_link : logfile . write ( returned_string [ 1 ] ) else : logfile . write ( returned_string ) logfile . write ( "\n" ) break else : print ( "limf: try 'limf -h' for more information" )
10831	def get ( cls , group , admin ) : try : ga = cls . query . filter_by ( group = group , admin_id = admin . get_id ( ) , admin_type = resolve_admin_type ( admin ) ) . one ( ) return ga except Exception : return None
5054	def get_identity_provider ( provider_id ) : try : from third_party_auth . provider import Registry except ImportError as exception : LOGGER . warning ( "Could not import Registry from third_party_auth.provider" ) LOGGER . warning ( exception ) Registry = None try : return Registry and Registry . get ( provider_id ) except ValueError : return None
4163	def parse_sphinx_searchindex ( searchindex ) : if hasattr ( searchindex , 'decode' ) : searchindex = searchindex . decode ( 'UTF-8' ) query = 'objects:' pos = searchindex . find ( query ) if pos < 0 : raise ValueError ( '"objects:" not found in search index' ) sel = _select_block ( searchindex [ pos : ] , '{' , '}' ) objects = _parse_dict_recursive ( sel ) query = 'filenames:' pos = searchindex . find ( query ) if pos < 0 : raise ValueError ( '"filenames:" not found in search index' ) filenames = searchindex [ pos + len ( query ) + 1 : ] filenames = filenames [ : filenames . find ( ']' ) ] filenames = [ f . strip ( '"' ) for f in filenames . split ( ',' ) ] return filenames , objects
7032	def check_existing_apikey ( lcc_server ) : USERHOME = os . path . expanduser ( '~' ) APIKEYFILE = os . path . join ( USERHOME , '.astrobase' , 'lccs' , 'apikey-%s' % lcc_server . replace ( 'https://' , 'https-' ) . replace ( 'http://' , 'http-' ) ) if os . path . exists ( APIKEYFILE ) : fileperm = oct ( os . stat ( APIKEYFILE ) [ stat . ST_MODE ] ) if fileperm == '0100600' or fileperm == '0o100600' : with open ( APIKEYFILE ) as infd : apikey , expires = infd . read ( ) . strip ( '\n' ) . split ( ) now = datetime . now ( utc ) if sys . version_info [ : 2 ] < ( 3 , 7 ) : expdt = datetime . strptime ( expires . replace ( 'Z' , '' ) , '%Y-%m-%dT%H:%M:%S.%f' ) . replace ( tzinfo = utc ) else : expdt = datetime . fromisoformat ( expires . replace ( 'Z' , '+00:00' ) ) if now > expdt : LOGERROR ( 'API key has expired. expiry was on: %s' % expires ) return False , apikey , expires else : return True , apikey , expires else : LOGWARNING ( 'The API key file %s has bad permissions ' 'and is insecure, not reading it.\n' '(you need to chmod 600 this file)' % APIKEYFILE ) return False , None , None else : LOGWARNING ( 'No LCC-Server API key ' 'found in: {apikeyfile}' . format ( apikeyfile = APIKEYFILE ) ) return False , None , None
2598	def can_sequence ( obj ) : if istype ( obj , sequence_types ) : t = type ( obj ) return t ( [ can ( i ) for i in obj ] ) else : return obj
12734	def set_body_states ( self , states ) : for state in states : self . get_body ( state . name ) . state = state
6694	def static ( self ) : fn = self . render_to_file ( 'ip/ip_interfaces_static.template' ) r = self . local_renderer r . put ( local_path = fn , remote_path = r . env . interfaces_fn , use_sudo = True )
1968	def wait ( self , readfds , writefds , timeout ) : logger . debug ( "WAIT:" ) logger . debug ( f"\tProcess {self._current} is going to wait for [ {readfds!r} {writefds!r} {timeout!r} ]" ) logger . debug ( f"\tProcess: {self.procs!r}" ) logger . debug ( f"\tRunning: {self.running!r}" ) logger . debug ( f"\tRWait: {self.rwait!r}" ) logger . debug ( f"\tTWait: {self.twait!r}" ) logger . debug ( f"\tTimers: {self.timers!r}" ) for fd in readfds : self . rwait [ fd ] . add ( self . _current ) for fd in writefds : self . twait [ fd ] . add ( self . _current ) if timeout is not None : self . timers [ self . _current ] = self . clocks + timeout procid = self . _current next_index = ( self . running . index ( procid ) + 1 ) % len ( self . running ) self . _current = self . running [ next_index ] logger . debug ( f"\tTransfer control from process {procid} to {self._current}" ) logger . debug ( f"\tREMOVING {procid!r} from {self.running!r}. Current: {self._current!r}" ) self . running . remove ( procid ) if self . _current not in self . running : logger . debug ( "\tCurrent not running. Checking for timers..." ) self . _current = None self . check_timers ( )
13535	def prune ( self ) : targets = self . descendents_root ( ) try : targets . remove ( self . graph . root ) except ValueError : pass results = [ n . data for n in targets ] results . append ( self . data ) for node in targets : node . delete ( ) for parent in self . parents . all ( ) : parent . children . remove ( self ) self . delete ( ) return results
6531	def get_user_config ( project_path , use_cache = True ) : if sys . platform == 'win32' : user_config = os . path . expanduser ( r'~\\tidypy' ) else : user_config = os . path . join ( os . getenv ( 'XDG_CONFIG_HOME' ) or os . path . expanduser ( '~/.config' ) , 'tidypy' ) if os . path . exists ( user_config ) : with open ( user_config , 'r' ) as config_file : config = pytoml . load ( config_file ) . get ( 'tidypy' , { } ) config = merge_dict ( get_default_config ( ) , config ) config = process_extensions ( config , project_path , use_cache = use_cache ) return config return None
8748	def update_scalingip ( context , id , content ) : LOG . info ( 'update_scalingip %s for tenant %s and body %s' % ( id , context . tenant_id , content ) ) requested_ports = content . get ( 'ports' , [ ] ) flip = _update_flip ( context , id , ip_types . SCALING , requested_ports ) return v . _make_scaling_ip_dict ( flip )
6812	def deploy ( self ) : for service in self . genv . services : service = service . strip ( ) . upper ( ) funcs = common . service_deployers . get ( service ) if funcs : print ( 'Deploying service %s...' % ( service , ) ) for func in funcs : if not self . dryrun : func ( )
669	def createDataOutLink ( network , sensorRegionName , regionName ) : network . link ( sensorRegionName , regionName , "UniformLink" , "" , srcOutput = "dataOut" , destInput = "bottomUpIn" )
10175	def _format_range_dt ( self , d ) : if not isinstance ( d , six . string_types ) : d = d . isoformat ( ) return '{0}||/{1}' . format ( d , self . dt_rounding_map [ self . aggregation_interval ] )
11933	def load_widgets ( context , ** kwargs ) : _soft = kwargs . pop ( '_soft' , False ) try : widgets = context . render_context [ WIDGET_CONTEXT_KEY ] except KeyError : widgets = context . render_context [ WIDGET_CONTEXT_KEY ] = { } for alias , template_name in kwargs . items ( ) : if _soft and alias in widgets : continue with context . render_context . push ( { BLOCK_CONTEXT_KEY : BlockContext ( ) } ) : blocks = resolve_blocks ( template_name , context ) widgets [ alias ] = blocks return ''
6296	def instance ( self , program : moderngl . Program ) -> moderngl . VertexArray : vao = self . vaos . get ( program . glo ) if vao : return vao program_attributes = [ name for name , attr in program . _members . items ( ) if isinstance ( attr , moderngl . Attribute ) ] for attrib_name in program_attributes : if attrib_name . startswith ( 'gl_' ) : continue if not sum ( buffer . has_attribute ( attrib_name ) for buffer in self . buffers ) : raise VAOError ( "VAO {} doesn't have attribute {} for program {}" . format ( self . name , attrib_name , program . name ) ) vao_content = [ ] for buffer in self . buffers : content = buffer . content ( program_attributes ) if content : vao_content . append ( content ) if program_attributes : for attrib_name in program_attributes : if attrib_name . startswith ( 'gl_' ) : continue raise VAOError ( "Did not find a buffer mapping for {}" . format ( [ n for n in program_attributes ] ) ) if self . _index_buffer : vao = context . ctx ( ) . vertex_array ( program , vao_content , self . _index_buffer , self . _index_element_size ) else : vao = context . ctx ( ) . vertex_array ( program , vao_content ) self . vaos [ program . glo ] = vao return vao
13801	def revoke_token ( self , token , callback ) : yield Task ( self . data_store . remove , 'tokens' , token = token ) callback ( )
12459	def main ( * args ) : r with disable_error_handler ( ) : args = parse_args ( args or sys . argv [ 1 : ] ) config = read_config ( args . config , args ) if config is None : return True bootstrap = config [ __script__ ] if not check_pre_requirements ( bootstrap [ 'pre_requirements' ] ) : return True env_args = prepare_args ( config [ 'virtualenv' ] , bootstrap ) if not create_env ( bootstrap [ 'env' ] , env_args , bootstrap [ 'recreate' ] , bootstrap [ 'ignore_activated' ] , bootstrap [ 'quiet' ] ) : return True pip_args = prepare_args ( config [ 'pip' ] , bootstrap ) if not install ( bootstrap [ 'env' ] , bootstrap [ 'requirements' ] , pip_args , bootstrap [ 'ignore_activated' ] , bootstrap [ 'install_dev_requirements' ] , bootstrap [ 'quiet' ] ) : return True run_hook ( bootstrap [ 'hook' ] , bootstrap , bootstrap [ 'quiet' ] ) if not bootstrap [ 'quiet' ] : print_message ( 'All OK!' ) return False
243	def daily_txns_with_bar_data ( transactions , market_data ) : transactions . index . name = 'date' txn_daily = pd . DataFrame ( transactions . assign ( amount = abs ( transactions . amount ) ) . groupby ( [ 'symbol' , pd . TimeGrouper ( 'D' ) ] ) . sum ( ) [ 'amount' ] ) txn_daily [ 'price' ] = market_data [ 'price' ] . unstack ( ) txn_daily [ 'volume' ] = market_data [ 'volume' ] . unstack ( ) txn_daily = txn_daily . reset_index ( ) . set_index ( 'date' ) return txn_daily
2872	def all_info_files ( self ) : 'Returns a generator of "Path"s' try : for info_file in list_files_in_dir ( self . info_dir ) : if not os . path . basename ( info_file ) . endswith ( '.trashinfo' ) : self . on_non_trashinfo_found ( ) else : yield info_file except OSError : pass
7482	def assembly_cleanup ( data ) : data . stats_dfs . s2 = data . _build_stat ( "s2" ) data . stats_files . s2 = os . path . join ( data . dirs . edits , 's2_rawedit_stats.txt' ) with io . open ( data . stats_files . s2 , 'w' , encoding = 'utf-8' ) as outfile : data . stats_dfs . s2 . fillna ( value = 0 ) . astype ( np . int ) . to_string ( outfile )
6865	def get_time_flux_errs_from_Ames_lightcurve ( infile , lctype , cadence_min = 2 ) : warnings . warn ( "Use the astrotess.read_tess_fitslc and " "astrotess.consolidate_tess_fitslc functions instead of this function. " "This function will be removed in astrobase v0.4.2." , FutureWarning ) if lctype not in ( 'PDCSAP' , 'SAP' ) : raise ValueError ( 'unknown light curve type requested: %s' % lctype ) hdulist = pyfits . open ( infile ) main_hdr = hdulist [ 0 ] . header lc_hdr = hdulist [ 1 ] . header lc = hdulist [ 1 ] . data if ( ( 'Ames' not in main_hdr [ 'ORIGIN' ] ) or ( 'LIGHTCURVE' not in lc_hdr [ 'EXTNAME' ] ) ) : raise ValueError ( 'could not understand input LC format. ' 'Is it a TESS TOI LC file?' ) time = lc [ 'TIME' ] flux = lc [ '{:s}_FLUX' . format ( lctype ) ] err_flux = lc [ '{:s}_FLUX_ERR' . format ( lctype ) ] sel = ( lc [ 'QUALITY' ] == 0 ) sel &= np . isfinite ( time ) sel &= np . isfinite ( flux ) sel &= np . isfinite ( err_flux ) sel &= ~ np . isnan ( time ) sel &= ~ np . isnan ( flux ) sel &= ~ np . isnan ( err_flux ) sel &= ( time != 0 ) sel &= ( flux != 0 ) sel &= ( err_flux != 0 ) time = time [ sel ] flux = flux [ sel ] err_flux = err_flux [ sel ] lc_cadence_diff = np . abs ( np . nanmedian ( np . diff ( time ) ) * 24 * 60 - cadence_min ) if lc_cadence_diff > 1.0e-2 : raise ValueError ( 'the light curve is not at the required cadence specified: %.2f' % cadence_min ) fluxmedian = np . nanmedian ( flux ) flux /= fluxmedian err_flux /= fluxmedian return time , flux , err_flux
12816	def _send_to_consumer ( self , block ) : self . _consumer . write ( block ) self . _sent += len ( block ) if self . _callback : self . _callback ( self . _sent , self . length )
5175	def facts ( self , ** kwargs ) : return self . __api . facts ( query = EqualsOperator ( "certname" , self . name ) , ** kwargs )
13175	def prev ( self , name = None ) : if self . parent is None or self . index is None : return None for idx in xrange ( self . index - 1 , - 1 , - 1 ) : if name is None or self . parent [ idx ] . tagname == name : return self . parent [ idx ]
9396	def extract_metric_name ( self , metric_name ) : for metric_type in self . supported_sar_types : if metric_type in metric_name : return metric_type logger . error ( 'Section [%s] does not contain a valid metric type, using type: "SAR-generic". Naarad works better ' 'if it knows the metric type. Valid SAR metric names are: %s' , metric_name , self . supported_sar_types ) return 'SAR-generic'
1746	def access_ok ( self , access ) : for c in access : if c not in self . perms : return False return True
320	def get_max_drawdown_underwater ( underwater ) : valley = np . argmin ( underwater ) peak = underwater [ : valley ] [ underwater [ : valley ] == 0 ] . index [ - 1 ] try : recovery = underwater [ valley : ] [ underwater [ valley : ] == 0 ] . index [ 0 ] except IndexError : recovery = np . nan return peak , valley , recovery
7969	def _remove_io_handler ( self , handler ) : if handler not in self . io_handlers : return self . io_handlers . remove ( handler ) for thread in self . io_threads : if thread . io_handler is handler : thread . stop ( )
12823	def fspaths ( draw , allow_pathlike = None ) : has_pathlike = hasattr ( os , 'PathLike' ) if allow_pathlike is None : allow_pathlike = has_pathlike if allow_pathlike and not has_pathlike : raise InvalidArgument ( 'allow_pathlike: os.PathLike not supported, use None instead ' 'to enable it only when available' ) result_type = draw ( sampled_from ( [ bytes , text_type ] ) ) def tp ( s = '' ) : return _str_to_path ( s , result_type ) special_component = sampled_from ( [ tp ( os . curdir ) , tp ( os . pardir ) ] ) normal_component = _filename ( result_type ) path_component = one_of ( normal_component , special_component ) extension = normal_component . map ( lambda f : tp ( os . extsep ) + f ) root = _path_root ( result_type ) def optional ( st ) : return one_of ( st , just ( result_type ( ) ) ) sep = sampled_from ( [ os . sep , os . altsep or os . sep ] ) . map ( tp ) path_part = builds ( lambda s , l : s . join ( l ) , sep , lists ( path_component ) ) main_strategy = builds ( lambda * x : tp ( ) . join ( x ) , optional ( root ) , path_part , optional ( extension ) ) if allow_pathlike and hasattr ( os , 'fspath' ) : pathlike_strategy = main_strategy . map ( lambda p : _PathLike ( p ) ) main_strategy = one_of ( main_strategy , pathlike_strategy ) return draw ( main_strategy )
4286	def write ( self , album ) : page = self . template . render ( ** self . generate_context ( album ) ) output_file = os . path . join ( album . dst_path , album . output_file ) with open ( output_file , 'w' , encoding = 'utf-8' ) as f : f . write ( page )
2985	def try_match ( request_origin , maybe_regex ) : if isinstance ( maybe_regex , RegexObject ) : return re . match ( maybe_regex , request_origin ) elif probably_regex ( maybe_regex ) : return re . match ( maybe_regex , request_origin , flags = re . IGNORECASE ) else : try : return request_origin . lower ( ) == maybe_regex . lower ( ) except AttributeError : return request_origin == maybe_regex
9320	def _validate_collection ( self ) : if not self . _id : msg = "No 'id' in Collection for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if not self . _title : msg = "No 'title' in Collection for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . _can_read is None : msg = "No 'can_read' in Collection for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . _can_write is None : msg = "No 'can_write' in Collection for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . _id not in self . url : msg = "The collection '{}' does not match the url for queries '{}'" raise ValidationError ( msg . format ( self . _id , self . url ) )
9927	def get_user ( self , user_id ) : try : return get_user_model ( ) . objects . get ( id = user_id ) except get_user_model ( ) . DoesNotExist : return None
9377	def calculate_stats ( data_list , stats_to_calculate = [ 'mean' , 'std' ] , percentiles_to_calculate = [ ] ) : stats_to_numpy_method_map = { 'mean' : numpy . mean , 'avg' : numpy . mean , 'std' : numpy . std , 'standard_deviation' : numpy . std , 'median' : numpy . median , 'min' : numpy . amin , 'max' : numpy . amax } calculated_stats = { } calculated_percentiles = { } if len ( data_list ) == 0 : return calculated_stats , calculated_percentiles for stat in stats_to_calculate : if stat in stats_to_numpy_method_map . keys ( ) : calculated_stats [ stat ] = stats_to_numpy_method_map [ stat ] ( data_list ) else : logger . error ( "Unsupported stat : " + str ( stat ) ) for percentile in percentiles_to_calculate : if isinstance ( percentile , float ) or isinstance ( percentile , int ) : calculated_percentiles [ percentile ] = numpy . percentile ( data_list , percentile ) else : logger . error ( "Unsupported percentile requested (should be int or float): " + str ( percentile ) ) return calculated_stats , calculated_percentiles
2791	def get_object ( cls , api_token , cert_id ) : certificate = cls ( token = api_token , id = cert_id ) certificate . load ( ) return certificate
12664	def union_mask ( filelist ) : firstimg = check_img ( filelist [ 0 ] ) mask = np . zeros_like ( firstimg . get_data ( ) ) try : for volf in filelist : roiimg = check_img ( volf ) check_img_compatibility ( firstimg , roiimg ) mask += get_img_data ( roiimg ) except Exception as exc : raise ValueError ( 'Error joining mask {} and {}.' . format ( repr_imgs ( firstimg ) , repr_imgs ( volf ) ) ) from exc else : return as_ndarray ( mask > 0 , dtype = bool )
6869	def normalize_magseries ( times , mags , mingap = 4.0 , normto = 'globalmedian' , magsarefluxes = False , debugmode = False ) : ngroups , timegroups = find_lc_timegroups ( times , mingap = mingap ) finite_ind = np . isfinite ( mags ) if any ( finite_ind ) : global_mag_median = np . median ( mags [ finite_ind ] ) for tgind , tg in enumerate ( timegroups ) : finite_ind = np . isfinite ( mags [ tg ] ) group_median = np . median ( ( mags [ tg ] ) [ finite_ind ] ) if magsarefluxes : mags [ tg ] = mags [ tg ] / group_median else : mags [ tg ] = mags [ tg ] - group_median if debugmode : LOGDEBUG ( 'group %s: elems %s, ' 'finite elems %s, median mag %s' % ( tgind , len ( mags [ tg ] ) , len ( finite_ind ) , group_median ) ) if isinstance ( normto , str ) and normto == 'globalmedian' : if magsarefluxes : mags = mags * global_mag_median else : mags = mags + global_mag_median elif isinstance ( normto , float ) : if magsarefluxes : mags = mags * normto else : mags = mags + normto return times , mags else : LOGERROR ( 'measurements are all nan!' ) return None , None
3480	def _f_gene ( sid , prefix = "G_" ) : sid = sid . replace ( SBML_DOT , "." ) return _clip ( sid , prefix )
8191	def nodes_by_eigenvalue ( self , treshold = 0.0 ) : nodes = [ ( n . eigenvalue , n ) for n in self . nodes if n . eigenvalue > treshold ] nodes . sort ( ) nodes . reverse ( ) return [ n for w , n in nodes ]
13706	def iter_space_block ( self , text = None , width = 60 , fmtfunc = str ) : if width < 1 : width = 1 curline = '' text = ( self . text if text is None else text ) or '' for word in text . split ( ) : possibleline = ' ' . join ( ( curline , word ) ) if curline else word codelen = sum ( len ( s ) for s in get_codes ( possibleline ) ) reallen = len ( possibleline ) - codelen if reallen > width : yield fmtfunc ( curline ) curline = word else : curline = possibleline if curline : yield fmtfunc ( curline )
3549	def rssi ( self , timeout_sec = TIMEOUT_SEC ) : self . _rssi_read . clear ( ) self . _peripheral . readRSSI ( ) if not self . _rssi_read . wait ( timeout_sec ) : raise RuntimeError ( 'Exceeded timeout waiting for RSSI value!' ) return self . _rssi
1995	def _named_stream ( self , name , binary = False ) : with self . _store . save_stream ( self . _named_key ( name ) , binary = binary ) as s : yield s
12689	def send ( * args , ** kwargs ) : queue_flag = kwargs . pop ( "queue" , False ) now_flag = kwargs . pop ( "now" , False ) assert not ( queue_flag and now_flag ) , "'queue' and 'now' cannot both be True." if queue_flag : return queue ( * args , ** kwargs ) elif now_flag : return send_now ( * args , ** kwargs ) else : if QUEUE_ALL : return queue ( * args , ** kwargs ) else : return send_now ( * args , ** kwargs )
935	def readFromCheckpoint ( cls , checkpointDir ) : checkpointPath = cls . _getModelCheckpointFilePath ( checkpointDir ) with open ( checkpointPath , 'r' ) as f : proto = cls . getSchema ( ) . read ( f , traversal_limit_in_words = _TRAVERSAL_LIMIT_IN_WORDS ) model = cls . read ( proto ) return model
11328	def select ( options = None ) : if not options : return None width = len ( str ( len ( options ) ) ) for x , option in enumerate ( options ) : sys . stdout . write ( '{:{width}}) {}\n' . format ( x + 1 , option , width = width ) ) sys . stdout . write ( '{:>{width}} ' . format ( '#?' , width = width + 1 ) ) sys . stdout . flush ( ) if sys . stdin . isatty ( ) : try : response = raw_input ( ) . strip ( ) except ( EOFError , KeyboardInterrupt ) : response = '' else : sys . stdin = open ( "/dev/tty" ) try : response = '' while True : response += sys . stdin . read ( 1 ) if response . endswith ( '\n' ) : break except ( EOFError , KeyboardInterrupt ) : sys . stdout . flush ( ) pass try : response = int ( response ) - 1 except ValueError : return None if response < 0 or response >= len ( options ) : return None return options [ response ]
608	def _indentLines ( str , indentLevels = 1 , indentFirstLine = True ) : indent = _ONE_INDENT * indentLevels lines = str . splitlines ( True ) result = '' if len ( lines ) > 0 and not indentFirstLine : first = 1 result += lines [ 0 ] else : first = 0 for line in lines [ first : ] : result += indent + line return result
12928	def _parse_info ( self , info_field ) : info = dict ( ) for item in info_field . split ( ';' ) : info_item_data = item . split ( '=' ) if len ( info_item_data ) == 1 : info [ info_item_data [ 0 ] ] = True elif len ( info_item_data ) == 2 : info [ info_item_data [ 0 ] ] = info_item_data [ 1 ] return info
5194	def Process ( self , info , values ) : visitor_class_types = { opendnp3 . ICollectionIndexedBinary : VisitorIndexedBinary , opendnp3 . ICollectionIndexedDoubleBitBinary : VisitorIndexedDoubleBitBinary , opendnp3 . ICollectionIndexedCounter : VisitorIndexedCounter , opendnp3 . ICollectionIndexedFrozenCounter : VisitorIndexedFrozenCounter , opendnp3 . ICollectionIndexedAnalog : VisitorIndexedAnalog , opendnp3 . ICollectionIndexedBinaryOutputStatus : VisitorIndexedBinaryOutputStatus , opendnp3 . ICollectionIndexedAnalogOutputStatus : VisitorIndexedAnalogOutputStatus , opendnp3 . ICollectionIndexedTimeAndInterval : VisitorIndexedTimeAndInterval } visitor_class = visitor_class_types [ type ( values ) ] visitor = visitor_class ( ) values . Foreach ( visitor ) for index , value in visitor . index_and_value : log_string = 'SOEHandler.Process {0}\theaderIndex={1}\tdata_type={2}\tindex={3}\tvalue={4}' _log . debug ( log_string . format ( info . gv , info . headerIndex , type ( values ) . __name__ , index , value ) )
10318	def _microcanonical_average_spanning_cluster ( has_spanning_cluster , alpha ) : r ret = dict ( ) runs = has_spanning_cluster . size k = has_spanning_cluster . sum ( dtype = np . float ) ret [ 'spanning_cluster' ] = ( ( k + 1 ) / ( runs + 2 ) ) ret [ 'spanning_cluster_ci' ] = scipy . stats . beta . ppf ( [ alpha / 2 , 1 - alpha / 2 ] , k + 1 , runs - k + 1 ) return ret
1047	def format_list ( extracted_list ) : list = [ ] for filename , lineno , name , line in extracted_list : item = ' File "%s", line %d, in %s\n' % ( filename , lineno , name ) if line : item = item + ' %s\n' % line . strip ( ) list . append ( item ) return list
8848	def mousePressEvent ( self , e ) : super ( PyInteractiveConsole , self ) . mousePressEvent ( e ) cursor = self . cursorForPosition ( e . pos ( ) ) p = cursor . positionInBlock ( ) usd = cursor . block ( ) . userData ( ) if usd and usd . start_pos_in_block <= p <= usd . end_pos_in_block : if e . button ( ) == QtCore . Qt . LeftButton : self . open_file_requested . emit ( usd . filename , usd . line )
4567	def _write ( self , filename , frames , fps , loop = 0 , palette = 256 ) : from PIL import Image images = [ ] for f in frames : data = open ( f , 'rb' ) . read ( ) images . append ( Image . open ( io . BytesIO ( data ) ) ) duration = round ( 1 / fps , 2 ) im = images . pop ( 0 ) im . save ( filename , save_all = True , append_images = images , duration = duration , loop = loop , palette = palette )
4361	def _watcher ( self ) : while True : gevent . sleep ( 1.0 ) if not self . connected : for ns_name , ns in list ( six . iteritems ( self . active_ns ) ) : ns . recv_disconnect ( ) gevent . killall ( self . jobs ) break
982	def create ( * args , ** kwargs ) : impl = kwargs . pop ( 'implementation' , None ) if impl is None : impl = Configuration . get ( 'nupic.opf.sdrClassifier.implementation' ) if impl == 'py' : return SDRClassifier ( * args , ** kwargs ) elif impl == 'cpp' : return FastSDRClassifier ( * args , ** kwargs ) elif impl == 'diff' : return SDRClassifierDiff ( * args , ** kwargs ) else : raise ValueError ( 'Invalid classifier implementation (%r). Value must be ' '"py", "cpp" or "diff".' % impl )
8397	def gettrans ( t ) : obj = t if isinstance ( obj , str ) : name = '{}_trans' . format ( obj ) obj = globals ( ) [ name ] ( ) if callable ( obj ) : obj = obj ( ) if isinstance ( obj , type ) : obj = obj ( ) if not isinstance ( obj , trans ) : raise ValueError ( "Could not get transform object." ) return obj
205	def deepcopy ( self ) : segmap = SegmentationMapOnImage ( self . arr , shape = self . shape , nb_classes = self . nb_classes ) segmap . input_was = self . input_was return segmap
4653	def add_required_fees ( self , ops , asset_id = "1.3.0" ) : ws = self . blockchain . rpc fees = ws . get_required_fees ( [ i . json ( ) for i in ops ] , asset_id ) for i , d in enumerate ( ops ) : if isinstance ( fees [ i ] , list ) : ops [ i ] . op . data [ "fee" ] = Asset ( amount = fees [ i ] [ 0 ] [ "amount" ] , asset_id = fees [ i ] [ 0 ] [ "asset_id" ] ) for j , _ in enumerate ( ops [ i ] . op . data [ "proposed_ops" ] . data ) : ops [ i ] . op . data [ "proposed_ops" ] . data [ j ] . data [ "op" ] . op . data [ "fee" ] = Asset ( amount = fees [ i ] [ 1 ] [ j ] [ "amount" ] , asset_id = fees [ i ] [ 1 ] [ j ] [ "asset_id" ] , ) else : ops [ i ] . op . data [ "fee" ] = Asset ( amount = fees [ i ] [ "amount" ] , asset_id = fees [ i ] [ "asset_id" ] ) return ops
1332	def gradient ( self , image = None , label = None , strict = True ) : assert self . has_gradient ( ) if image is None : image = self . __original_image if label is None : label = self . __original_class assert not strict or self . in_bounds ( image ) self . _total_gradient_calls += 1 gradient = self . __model . gradient ( image , label ) assert gradient . shape == image . shape return gradient
285	def plot_drawdown_underwater ( returns , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) y_axis_formatter = FuncFormatter ( utils . percentage ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) df_cum_rets = ep . cum_returns ( returns , starting_value = 1.0 ) running_max = np . maximum . accumulate ( df_cum_rets ) underwater = - 100 * ( ( running_max - df_cum_rets ) / running_max ) ( underwater ) . plot ( ax = ax , kind = 'area' , color = 'coral' , alpha = 0.7 , ** kwargs ) ax . set_ylabel ( 'Drawdown' ) ax . set_title ( 'Underwater plot' ) ax . set_xlabel ( '' ) return ax
4345	def stats ( self , input_filepath ) : effect_args = [ 'channels' , '1' , 'stats' ] _ , _ , stats_output = self . build ( input_filepath , None , extra_args = effect_args , return_output = True ) stats_dict = { } lines = stats_output . split ( '\n' ) for line in lines : split_line = line . split ( ) if len ( split_line ) == 0 : continue value = split_line [ - 1 ] key = ' ' . join ( split_line [ : - 1 ] ) stats_dict [ key ] = value return stats_dict
55	def copy ( self , keypoints = None , shape = None ) : result = copy . copy ( self ) if keypoints is not None : result . keypoints = keypoints if shape is not None : result . shape = shape return result
939	def runExperiment ( args , model = None ) : opt = _parseCommandLineOptions ( args ) model = _runExperimentImpl ( opt , model ) return model
10910	def name_globals ( s , remove_params = None ) : all_params = s . params for p in s . param_particle ( np . arange ( s . obj_get_positions ( ) . shape [ 0 ] ) ) : all_params . remove ( p ) if remove_params is not None : for p in set ( remove_params ) : all_params . remove ( p ) return all_params
470	def read_words ( filename = "nietzsche.txt" , replace = None ) : if replace is None : replace = [ '\n' , '<eos>' ] with tf . gfile . GFile ( filename , "r" ) as f : try : context_list = f . read ( ) . replace ( * replace ) . split ( ) except Exception : f . seek ( 0 ) replace = [ x . encode ( 'utf-8' ) for x in replace ] context_list = f . read ( ) . replace ( * replace ) . split ( ) return context_list
13290	def _iter_filepaths_with_extension ( extname , root_dir = '.' ) : if not extname . startswith ( '.' ) : extname = '.' + extname root_dir = os . path . abspath ( root_dir ) for dirname , sub_dirnames , filenames in os . walk ( root_dir ) : for filename in filenames : if os . path . splitext ( filename ) [ - 1 ] == extname : full_filename = os . path . join ( dirname , filename ) rel_filepath = os . path . relpath ( full_filename , start = root_dir ) yield rel_filepath
7668	def trim ( self , start_time , end_time , strict = False ) : trimmed_array = AnnotationArray ( ) for ann in self : trimmed_array . append ( ann . trim ( start_time , end_time , strict = strict ) ) return trimmed_array
1015	def _getCellForNewSegment ( self , colIdx ) : if self . maxSegmentsPerCell < 0 : if self . cellsPerColumn > 1 : i = self . _random . getUInt32 ( self . cellsPerColumn - 1 ) + 1 else : i = 0 return i candidateCellIdxs = [ ] if self . cellsPerColumn == 1 : minIdx = 0 maxIdx = 0 else : minIdx = 1 maxIdx = self . cellsPerColumn - 1 for i in xrange ( minIdx , maxIdx + 1 ) : numSegs = len ( self . cells [ colIdx ] [ i ] ) if numSegs < self . maxSegmentsPerCell : candidateCellIdxs . append ( i ) if len ( candidateCellIdxs ) > 0 : candidateCellIdx = ( candidateCellIdxs [ self . _random . getUInt32 ( len ( candidateCellIdxs ) ) ] ) if self . verbosity >= 5 : print "Cell [%d,%d] chosen for new segment, # of segs is %d" % ( colIdx , candidateCellIdx , len ( self . cells [ colIdx ] [ candidateCellIdx ] ) ) return candidateCellIdx candidateSegment = None candidateSegmentDC = 1.0 for i in xrange ( minIdx , maxIdx + 1 ) : for s in self . cells [ colIdx ] [ i ] : dc = s . dutyCycle ( ) if dc < candidateSegmentDC : candidateCellIdx = i candidateSegmentDC = dc candidateSegment = s if self . verbosity >= 5 : print ( "Deleting segment #%d for cell[%d,%d] to make room for new " "segment" % ( candidateSegment . segID , colIdx , candidateCellIdx ) ) candidateSegment . debugPrint ( ) self . _cleanUpdatesList ( colIdx , candidateCellIdx , candidateSegment ) self . cells [ colIdx ] [ candidateCellIdx ] . remove ( candidateSegment ) return candidateCellIdx
10116	def append ( self , key , value = MARKER , replace = True ) : return self . add_item ( key , value , replace = replace )
7501	def get_shape ( spans , loci ) : width = 0 for idx in xrange ( loci . shape [ 0 ] ) : width += spans [ loci [ idx ] , 1 ] - spans [ loci [ idx ] , 0 ] return width
4075	def set_cfg_value ( config , section , option , value ) : if isinstance ( value , list ) : value = '\n' . join ( value ) config [ section ] [ option ] = value
5972	def generate_submit_scripts ( templates , prefix = None , deffnm = 'md' , jobname = 'MD' , budget = None , mdrun_opts = None , walltime = 1.0 , jobarray_string = None , startdir = None , npme = None , ** kwargs ) : if not jobname [ 0 ] . isalpha ( ) : jobname = 'MD_' + jobname wmsg = "To make the jobname legal it must start with a letter: changed to {0!r}" . format ( jobname ) logger . warn ( wmsg ) warnings . warn ( wmsg , category = AutoCorrectionWarning ) if prefix is None : prefix = "" if mdrun_opts is not None : mdrun_opts = '"' + str ( mdrun_opts ) + '"' dirname = kwargs . pop ( 'dirname' , os . path . curdir ) wt = Timedelta ( hours = walltime ) walltime = wt . strftime ( "%h:%M:%S" ) wall_hours = wt . ashours def write_script ( template ) : submitscript = os . path . join ( dirname , prefix + os . path . basename ( template ) ) logger . info ( "Setting up queuing system script {submitscript!r}..." . format ( ** vars ( ) ) ) qsystem = detect_queuing_system ( template ) if qsystem is not None and ( qsystem . name == 'Slurm' ) : cbook . edit_txt ( template , [ ( '^ *DEFFNM=' , '(?<==)(.*)' , deffnm ) , ( '^#.*(-J)' , '((?<=-J\s))\s*\w+' , jobname ) , ( '^#.*(-A|account_no)' , '((?<=-A\s)|(?<=account_no\s))\s*\w+' , budget ) , ( '^#.*(-t)' , '(?<=-t\s)(\d+:\d+:\d+)' , walltime ) , ( '^ *WALL_HOURS=' , '(?<==)(.*)' , wall_hours ) , ( '^ *STARTDIR=' , '(?<==)(.*)' , startdir ) , ( '^ *NPME=' , '(?<==)(.*)' , npme ) , ( '^ *MDRUN_OPTS=' , '(?<==)("")' , mdrun_opts ) , ( '^# JOB_ARRAY_PLACEHOLDER' , '^.*$' , jobarray_string ) , ] , newname = submitscript ) ext = os . path . splitext ( submitscript ) [ 1 ] else : cbook . edit_txt ( template , [ ( '^ *DEFFNM=' , '(?<==)(.*)' , deffnm ) , ( '^#.*(-N|job_name)' , '((?<=-N\s)|(?<=job_name\s))\s*\w+' , jobname ) , ( '^#.*(-A|account_no)' , '((?<=-A\s)|(?<=account_no\s))\s*\w+' , budget ) , ( '^#.*(-l walltime|wall_clock_limit)' , '(?<==)(\d+:\d+:\d+)' , walltime ) , ( '^ *WALL_HOURS=' , '(?<==)(.*)' , wall_hours ) , ( '^ *STARTDIR=' , '(?<==)(.*)' , startdir ) , ( '^ *NPME=' , '(?<==)(.*)' , npme ) , ( '^ *MDRUN_OPTS=' , '(?<==)("")' , mdrun_opts ) , ( '^# JOB_ARRAY_PLACEHOLDER' , '^.*$' , jobarray_string ) , ] , newname = submitscript ) ext = os . path . splitext ( submitscript ) [ 1 ] if ext in ( '.sh' , '.csh' , '.bash' ) : os . chmod ( submitscript , 0o755 ) return submitscript return [ write_script ( template ) for template in config . get_templates ( templates ) ]
4241	def ip2long ( ip ) : try : return int ( binascii . hexlify ( socket . inet_aton ( ip ) ) , 16 ) except socket . error : return int ( binascii . hexlify ( socket . inet_pton ( socket . AF_INET6 , ip ) ) , 16 )
8070	def replace_entities ( ustring , placeholder = " " ) : def _repl_func ( match ) : try : if match . group ( 1 ) : return unichr ( int ( match . group ( 2 ) ) ) else : try : return cp1252 [ unichr ( int ( match . group ( 3 ) ) ) ] . strip ( ) except : return unichr ( name2codepoint [ match . group ( 3 ) ] ) except : return placeholder if not isinstance ( ustring , unicode ) : ustring = UnicodeDammit ( ustring ) . unicode ustring = ustring . replace ( "&nbsp;" , " " ) _entity_re = re . compile ( r'&(?:(#)(\d+)|([^;^> ]+));' ) return _entity_re . sub ( _repl_func , ustring )
11315	def update_notes ( self ) : fields = record_get_field_instances ( self . record , '500' ) for field in fields : subs = field_get_subfields ( field ) for sub in subs . get ( 'a' , [ ] ) : sub = sub . strip ( ) if sub . startswith ( "*" ) and sub . endswith ( "*" ) : record_delete_field ( self . record , tag = "500" , field_position_global = field [ 4 ] )
9731	def get_force_single ( self , component_info = None , data = None , component_position = None ) : components = [ ] append_components = components . append for _ in range ( component_info . plate_count ) : component_position , plate = QRTPacket . _get_exact ( RTForcePlateSingle , data , component_position ) component_position , force = QRTPacket . _get_exact ( RTForce , data , component_position ) append_components ( ( plate , force ) ) return components
9349	def date ( past = False , min_delta = 0 , max_delta = 20 ) : timedelta = dt . timedelta ( days = _delta ( past , min_delta , max_delta ) ) return dt . date . today ( ) + timedelta
6820	def configure_modevasive ( self ) : r = self . local_renderer if r . env . modevasive_enabled : self . install_packages ( ) fn = r . render_to_file ( 'apache/apache_modevasive.template.conf' ) r . put ( local_path = fn , remote_path = '/etc/apache2/mods-available/mod-evasive.conf' , use_sudo = True ) r . put ( local_path = fn , remote_path = '/etc/apache2/mods-available/evasive.conf' , use_sudo = True ) self . enable_mod ( 'evasive' ) else : if self . last_manifest . modevasive_enabled : self . disable_mod ( 'evasive' )
7358	def _check_peptide_inputs ( self , peptides ) : require_iterable_of ( peptides , string_types ) check_X = not self . allow_X_in_peptides check_lower = not self . allow_lowercase_in_peptides check_min_length = self . min_peptide_length is not None min_length = self . min_peptide_length check_max_length = self . max_peptide_length is not None max_length = self . max_peptide_length for p in peptides : if not p . isalpha ( ) : raise ValueError ( "Invalid characters in peptide '%s'" % p ) elif check_X and "X" in p : raise ValueError ( "Invalid character 'X' in peptide '%s'" % p ) elif check_lower and not p . isupper ( ) : raise ValueError ( "Invalid lowercase letters in peptide '%s'" % p ) elif check_min_length and len ( p ) < min_length : raise ValueError ( "Peptide '%s' too short (%d chars), must be at least %d" % ( p , len ( p ) , min_length ) ) elif check_max_length and len ( p ) > max_length : raise ValueError ( "Peptide '%s' too long (%d chars), must be at least %d" % ( p , len ( p ) , max_length ) )
9643	def pydevd ( context ) : global pdevd_not_available if pdevd_not_available : return '' try : import pydevd except ImportError : pdevd_not_available = True return '' render = lambda s : template . Template ( s ) . render ( context ) availables = get_variables ( context ) for var in availables : locals ( ) [ var ] = context [ var ] try : pydevd . settrace ( ) except socket . error : pdevd_not_available = True return ''
4959	def get_course_runs_from_program ( program ) : course_runs = set ( ) for course in program . get ( "courses" , [ ] ) : for run in course . get ( "course_runs" , [ ] ) : if "key" in run and run [ "key" ] : course_runs . add ( run [ "key" ] ) return course_runs
2521	def p_file_type ( self , f_term , predicate ) : try : for _ , _ , ftype in self . graph . triples ( ( f_term , predicate , None ) ) : try : if ftype . endswith ( 'binary' ) : ftype = 'BINARY' elif ftype . endswith ( 'source' ) : ftype = 'SOURCE' elif ftype . endswith ( 'other' ) : ftype = 'OTHER' elif ftype . endswith ( 'archive' ) : ftype = 'ARCHIVE' self . builder . set_file_type ( self . doc , ftype ) except SPDXValueError : self . value_error ( 'FILE_TYPE' , ftype ) except CardinalityError : self . more_than_one_error ( 'file type' )
2005	def _serialize_uint ( value , size = 32 , padding = 0 ) : if size <= 0 or size > 32 : raise ValueError from . account import EVMAccount if not isinstance ( value , ( int , BitVec , EVMAccount ) ) : raise ValueError if issymbolic ( value ) : bytes = ArrayVariable ( index_bits = 256 , index_max = 32 , value_bits = 8 , name = 'temp{}' . format ( uuid . uuid1 ( ) ) ) if value . size <= size * 8 : value = Operators . ZEXTEND ( value , size * 8 ) else : value = Operators . EXTRACT ( value , 0 , size * 8 ) bytes = ArrayProxy ( bytes . write_BE ( padding , value , size ) ) else : value = int ( value ) bytes = bytearray ( ) for _ in range ( padding ) : bytes . append ( 0 ) for position in reversed ( range ( size ) ) : bytes . append ( Operators . EXTRACT ( value , position * 8 , 8 ) ) assert len ( bytes ) == size + padding return bytes
4860	def force_fresh_session ( view ) : @ wraps ( view ) def wrapper ( request , * args , ** kwargs ) : if not request . GET . get ( FRESH_LOGIN_PARAMETER ) : enterprise_customer = get_enterprise_customer_or_404 ( kwargs . get ( 'enterprise_uuid' ) ) provider_id = enterprise_customer . identity_provider or '' sso_provider = get_identity_provider ( provider_id ) if sso_provider : scheme , netloc , path , params , query , fragment = urlparse ( request . get_full_path ( ) ) redirect_url = urlunparse ( ( scheme , netloc , quote ( path ) , params , query , fragment ) ) return redirect ( '{logout_url}?{params}' . format ( logout_url = '/logout' , params = urlencode ( { 'redirect_url' : redirect_url } ) ) ) return view ( request , * args , ** kwargs ) return wrapper
9570	async def _get_response ( self , message ) : view = self . discovery_view ( message ) if not view : return if inspect . iscoroutinefunction ( view ) : response = await view ( message ) else : response = view ( message ) return self . prepare_response ( response , message )
12954	def _rem_id_from_keys ( self , pk , conn = None ) : if conn is None : conn = self . _get_connection ( ) conn . srem ( self . _get_ids_key ( ) , pk )
16	def value ( self , t ) : for ( l_t , l ) , ( r_t , r ) in zip ( self . _endpoints [ : - 1 ] , self . _endpoints [ 1 : ] ) : if l_t <= t and t < r_t : alpha = float ( t - l_t ) / ( r_t - l_t ) return self . _interpolation ( l , r , alpha ) assert self . _outside_value is not None return self . _outside_value
12159	def abfGroupFiles ( groups , folder ) : assert os . path . exists ( folder ) files = os . listdir ( folder ) group2 = { } for parent in groups . keys ( ) : if not parent in group2 . keys ( ) : group2 [ parent ] = [ ] for ID in groups [ parent ] : for fname in [ x . lower ( ) for x in files if ID in x . lower ( ) ] : group2 [ parent ] . extend ( [ fname ] ) return group2
97	def quokka_bounding_boxes ( size = None , extract = None ) : from imgaug . augmentables . bbs import BoundingBox , BoundingBoxesOnImage left , top = 0 , 0 if extract is not None : bb_extract = _quokka_normalize_extract ( extract ) left = bb_extract . x1 top = bb_extract . y1 with open ( QUOKKA_ANNOTATIONS_FP , "r" ) as f : json_dict = json . load ( f ) bbs = [ ] for bb_dict in json_dict [ "bounding_boxes" ] : bbs . append ( BoundingBox ( x1 = bb_dict [ "x1" ] - left , y1 = bb_dict [ "y1" ] - top , x2 = bb_dict [ "x2" ] - left , y2 = bb_dict [ "y2" ] - top ) ) if extract is not None : shape = ( bb_extract . height , bb_extract . width , 3 ) else : shape = ( 643 , 960 , 3 ) bbsoi = BoundingBoxesOnImage ( bbs , shape = shape ) if size is not None : shape_resized = _compute_resized_shape ( shape , size ) bbsoi = bbsoi . on ( shape_resized ) return bbsoi
13908	def create_subparsers ( self , parser ) : subparsers = parser . add_subparsers ( ) for name in self . config [ 'subparsers' ] : subparser = subparsers . add_parser ( name ) self . create_commands ( self . config [ 'subparsers' ] [ name ] , subparser )
2048	def new_address ( self , sender = None , nonce = None ) : if sender is not None and nonce is None : nonce = self . get_nonce ( sender ) new_address = self . calculate_new_address ( sender , nonce ) if sender is None and new_address in self : return self . new_address ( sender , nonce ) return new_address
1042	def compare ( left , right , compare_locs = False ) : if type ( left ) != type ( right ) : return False if isinstance ( left , ast . AST ) : for field in left . _fields : if not compare ( getattr ( left , field ) , getattr ( right , field ) ) : return False if compare_locs : for loc in left . _locs : if getattr ( left , loc ) != getattr ( right , loc ) : return False return True elif isinstance ( left , list ) : if len ( left ) != len ( right ) : return False for left_elt , right_elt in zip ( left , right ) : if not compare ( left_elt , right_elt ) : return False return True else : return left == right
2334	def predict_proba ( self , a , b , idx = 0 , ** kwargs ) : return self . predict_dataset ( DataFrame ( [ [ a , b ] ] , columns = [ 'A' , 'B' ] ) )
8827	def update_ports_for_sg ( self , context , portid , jobid ) : port = db_api . port_find ( context , id = portid , scope = db_api . ONE ) if not port : LOG . warning ( "Port not found" ) return net_driver = port_api . _get_net_driver ( port . network , port = port ) base_net_driver = port_api . _get_net_driver ( port . network ) sg_list = [ sg for sg in port . security_groups ] success = False error = None retries = 3 retry_delay = 2 for retry in xrange ( retries ) : try : net_driver . update_port ( context , port_id = port [ "backend_key" ] , mac_address = port [ "mac_address" ] , device_id = port [ "device_id" ] , base_net_driver = base_net_driver , security_groups = sg_list ) success = True error = None break except Exception as error : LOG . warning ( "Could not connect to redis, but retrying soon" ) time . sleep ( retry_delay ) status_str = "" if not success : status_str = "Port %s update failed after %d tries. Error: %s" % ( portid , retries , error ) update_body = dict ( completed = True , status = status_str ) update_body = dict ( job = update_body ) job_api . update_job ( context . elevated ( ) , jobid , update_body )
7150	def decode ( cls , phrase ) : phrase = phrase . split ( " " ) out = "" for i in range ( len ( phrase ) // 3 ) : word1 , word2 , word3 = phrase [ 3 * i : 3 * i + 3 ] w1 = cls . word_list . index ( word1 ) w2 = cls . word_list . index ( word2 ) % cls . n w3 = cls . word_list . index ( word3 ) % cls . n x = w1 + cls . n * ( ( w2 - w1 ) % cls . n ) + cls . n * cls . n * ( ( w3 - w2 ) % cls . n ) out += endian_swap ( "%08x" % x ) return out
7591	def run ( self , force = False , ipyclient = None , name_fields = 30 , name_separator = "_" , dry_run = False ) : try : if not os . path . exists ( self . workdir ) : os . makedirs ( self . workdir ) self . _set_vdbconfig_path ( ) if ipyclient : self . _ipcluster [ "pids" ] = { } for eid in ipyclient . ids : engine = ipyclient [ eid ] if not engine . outstanding : pid = engine . apply ( os . getpid ) . get ( ) self . _ipcluster [ "pids" ] [ eid ] = pid self . _submit_jobs ( force = force , ipyclient = ipyclient , name_fields = name_fields , name_separator = name_separator , dry_run = dry_run , ) except IPyradWarningExit as inst : print ( inst ) except KeyboardInterrupt : print ( "keyboard interrupt..." ) except Exception as inst : print ( "Exception in run() - {}" . format ( inst ) ) finally : self . _restore_vdbconfig_path ( ) sradir = os . path . join ( self . workdir , "sra" ) if os . path . exists ( sradir ) and ( not os . listdir ( sradir ) ) : shutil . rmtree ( sradir ) else : try : print ( FAILED_DOWNLOAD . format ( os . listdir ( sradir ) ) ) except OSError as inst : raise IPyradWarningExit ( "Download failed. Exiting." ) for srr in os . listdir ( sradir ) : isrr = srr . split ( "." ) [ 0 ] ipath = os . path . join ( self . workdir , "*_{}*.gz" . format ( isrr ) ) ifile = glob . glob ( ipath ) [ 0 ] if os . path . exists ( ifile ) : os . remove ( ifile ) shutil . rmtree ( sradir ) if ipyclient : try : ipyclient . abort ( ) time . sleep ( 0.5 ) for engine_id , pid in self . _ipcluster [ "pids" ] . items ( ) : if ipyclient . queue_status ( ) [ engine_id ] [ "tasks" ] : os . kill ( pid , 2 ) time . sleep ( 0.1 ) except ipp . NoEnginesRegistered : pass if not ipyclient . outstanding : ipyclient . purge_everything ( ) else : ipyclient . shutdown ( hub = True , block = False ) ipyclient . close ( ) print ( "\nwarning: ipcluster shutdown and must be restarted" )
11258	def pack ( prev , n , rest = False , ** kw ) : if 'padding' in kw : use_padding = True padding = kw [ 'padding' ] else : use_padding = False padding = None items = [ ] for i , data in enumerate ( prev , 1 ) : items . append ( data ) if ( i % n ) == 0 : yield items items = [ ] if len ( items ) != 0 and rest : if use_padding : items . extend ( [ padding , ] * ( n - ( i % n ) ) ) yield items
2449	def set_pkg_supplier ( self , doc , entity ) : self . assert_package_exists ( ) if not self . package_supplier_set : self . package_supplier_set = True if validations . validate_pkg_supplier ( entity ) : doc . package . supplier = entity return True else : raise SPDXValueError ( 'Package::Supplier' ) else : raise CardinalityError ( 'Package::Supplier' )
1537	def add_spec ( self , * specs ) : for spec in specs : if not isinstance ( spec , HeronComponentSpec ) : raise TypeError ( "Argument to add_spec needs to be HeronComponentSpec, given: %s" % str ( spec ) ) if spec . name is None : raise ValueError ( "TopologyBuilder cannot take a spec without name" ) if spec . name == "config" : raise ValueError ( "config is a reserved name" ) if spec . name in self . _specs : raise ValueError ( "Attempting to add duplicate spec name: %r %r" % ( spec . name , spec ) ) self . _specs [ spec . name ] = spec
4535	def fillRGB ( self , r , g , b , start = 0 , end = - 1 ) : self . fill ( ( r , g , b ) , start , end )
1888	def must_be_true ( self , constraints , expression ) -> bool : solutions = self . get_all_values ( constraints , expression , maxcnt = 2 , silent = True ) return solutions == [ True ]
8701	def close ( self ) : try : if self . baud != self . start_baud : self . __set_baudrate ( self . start_baud ) self . _port . flush ( ) self . __clear_buffers ( ) except serial . serialutil . SerialException : pass log . debug ( 'closing port' ) self . _port . close ( )
5992	def plot_figure ( array , as_subplot , units , kpc_per_arcsec , figsize , aspect , cmap , norm , norm_min , norm_max , linthresh , linscale , xticks_manual , yticks_manual ) : fig = plotter_util . setup_figure ( figsize = figsize , as_subplot = as_subplot ) norm_min , norm_max = get_normalization_min_max ( array = array , norm_min = norm_min , norm_max = norm_max ) norm_scale = get_normalization_scale ( norm = norm , norm_min = norm_min , norm_max = norm_max , linthresh = linthresh , linscale = linscale ) extent = get_extent ( array = array , units = units , kpc_per_arcsec = kpc_per_arcsec , xticks_manual = xticks_manual , yticks_manual = yticks_manual ) plt . imshow ( array , aspect = aspect , cmap = cmap , norm = norm_scale , extent = extent ) return fig
1094	def findall ( pattern , string , flags = 0 ) : return _compile ( pattern , flags ) . findall ( string ) def finditer ( pattern , string , flags = 0 ) : return _compile ( pattern , flags ) . finditer ( string )
9403	def _isobject ( self , name , exist ) : if exist in [ 2 , 5 ] : return False cmd = 'isobject(%s)' % name resp = self . _engine . eval ( cmd , silent = True ) . strip ( ) return resp == 'ans = 1'
13332	def localize ( name ) : env = cpenv . get_active_env ( ) if not env : click . echo ( 'You need to activate an environment first.' ) return try : r = cpenv . resolve ( name ) except cpenv . ResolveError as e : click . echo ( '\n' + str ( e ) ) module = r . resolved [ 0 ] if isinstance ( module , cpenv . VirtualEnvironment ) : click . echo ( '\nCan only localize a module not an environment' ) return active_modules = cpenv . get_active_modules ( ) if module in active_modules : click . echo ( '\nCan not localize an active module.' ) return if module in env . get_modules ( ) : click . echo ( '\n{} is already local to {}' . format ( module . name , env . name ) ) return if click . confirm ( '\nAdd {} to env {}?' . format ( module . name , env . name ) ) : click . echo ( 'Adding module...' , nl = False ) try : module = env . add_module ( module . name , module . path ) except : click . echo ( bold_red ( 'FAILED' ) ) raise else : click . echo ( bold_green ( 'OK!' ) ) click . echo ( '\nActivate the localize module:' ) click . echo ( ' cpenv activate {} {}' . format ( env . name , module . name ) )
11560	def i2c_stop_reading ( self , address ) : data = [ address , self . I2C_STOP_READING ] self . _command_handler . send_sysex ( self . _command_handler . I2C_REQUEST , data )
9646	def get_attributes ( var ) : is_valid = partial ( is_valid_in_template , var ) return list ( filter ( is_valid , dir ( var ) ) )
13667	def execute ( self , command , timeout = None ) : try : self . channel = self . ssh . get_transport ( ) . open_session ( ) except paramiko . SSHException as e : self . unknown ( "Create channel error: %s" % e ) try : self . channel . settimeout ( self . args . timeout if not timeout else timeout ) except socket . timeout as e : self . unknown ( "Settimeout for channel error: %s" % e ) try : self . logger . debug ( "command: {}" . format ( command ) ) self . channel . exec_command ( command ) except paramiko . SSHException as e : self . unknown ( "Execute command error: %s" % e ) try : self . stdin = self . channel . makefile ( 'wb' , - 1 ) self . stderr = map ( string . strip , self . channel . makefile_stderr ( 'rb' , - 1 ) . readlines ( ) ) self . stdout = map ( string . strip , self . channel . makefile ( 'rb' , - 1 ) . readlines ( ) ) except Exception as e : self . unknown ( "Get result error: %s" % e ) try : self . status = self . channel . recv_exit_status ( ) except paramiko . SSHException as e : self . unknown ( "Get return code error: %s" % e ) else : if self . status != 0 : self . unknown ( "Return code: %d , stderr: %s" % ( self . status , self . errors ) ) else : return self . stdout finally : self . logger . debug ( "Execute command finish." )
6287	def get ( self , name ) -> Track : name = name . lower ( ) track = self . track_map . get ( name ) if not track : track = Track ( name ) self . tacks . append ( track ) self . track_map [ name ] = track return track
2356	def is_element_present ( self , strategy , locator ) : return self . driver_adapter . is_element_present ( strategy , locator , root = self . root )
195	def MotionBlur ( k = 5 , angle = ( 0 , 360 ) , direction = ( - 1.0 , 1.0 ) , order = 1 , name = None , deterministic = False , random_state = None ) : k_param = iap . handle_discrete_param ( k , "k" , value_range = ( 3 , None ) , tuple_to_uniform = True , list_to_choice = True , allow_floats = False ) angle_param = iap . handle_continuous_param ( angle , "angle" , value_range = None , tuple_to_uniform = True , list_to_choice = True ) direction_param = iap . handle_continuous_param ( direction , "direction" , value_range = ( - 1.0 - 1e-6 , 1.0 + 1e-6 ) , tuple_to_uniform = True , list_to_choice = True ) def create_matrices ( image , nb_channels , random_state_func ) : from . import geometric as iaa_geometric k_sample = int ( k_param . draw_sample ( random_state = random_state_func ) ) angle_sample = angle_param . draw_sample ( random_state = random_state_func ) direction_sample = direction_param . draw_sample ( random_state = random_state_func ) k_sample = k_sample if k_sample % 2 != 0 else k_sample + 1 direction_sample = np . clip ( direction_sample , - 1.0 , 1.0 ) direction_sample = ( direction_sample + 1.0 ) / 2.0 matrix = np . zeros ( ( k_sample , k_sample ) , dtype = np . float32 ) matrix [ : , k_sample // 2 ] = np . linspace ( float ( direction_sample ) , 1.0 - float ( direction_sample ) , num = k_sample ) rot = iaa_geometric . Affine ( rotate = angle_sample , order = order ) matrix = ( rot . augment_image ( ( matrix * 255 ) . astype ( np . uint8 ) ) / 255.0 ) . astype ( np . float32 ) return [ matrix / np . sum ( matrix ) ] * nb_channels if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return iaa_convolutional . Convolve ( create_matrices , name = name , deterministic = deterministic , random_state = random_state )
6626	def availableBranches ( self ) : return [ GithubComponentVersion ( '' , b [ 0 ] , b [ 1 ] , self . name , cache_key = None ) for b in _getBranchHeads ( self . repo ) . items ( ) ]
4456	def limit ( self , offset , num ) : limit = Limit ( offset , num ) if self . _groups : self . _groups [ - 1 ] . limit = limit else : self . _limit = limit return self
4128	def readwav ( filename ) : from scipy . io . wavfile import read as readwav samplerate , signal = readwav ( filename ) return signal , samplerate
2139	def associate ( self , group , parent , ** kwargs ) : parent_id = self . lookup_with_inventory ( parent , kwargs . get ( 'inventory' , None ) ) [ 'id' ] group_id = self . lookup_with_inventory ( group , kwargs . get ( 'inventory' , None ) ) [ 'id' ] return self . _assoc ( 'children' , parent_id , group_id )
7458	def _countmatrix ( lxs ) : share = np . zeros ( ( lxs . shape [ 0 ] , lxs . shape [ 0 ] ) ) names = range ( lxs . shape [ 0 ] ) for row in lxs : for samp1 , samp2 in itertools . combinations ( names , 2 ) : shared = lxs [ samp1 , lxs [ samp2 ] > 0 ] . sum ( ) share [ samp1 , samp2 ] = shared for row in xrange ( len ( names ) ) : share [ row , row ] = lxs [ row ] . sum ( ) return share
3958	def resolve ( cls , all_known_repos , name ) : match = None for repo in all_known_repos : if repo . remote_path == name : return repo if name == repo . short_name : if match is None : match = repo else : raise RuntimeError ( 'Short repo name {} is ambiguous. It matches both {} and {}' . format ( name , match . remote_path , repo . remote_path ) ) if match is None : raise RuntimeError ( 'Short repo name {} does not match any known repos' . format ( name ) ) return match
5292	def post ( self , request , * args , ** kwargs ) : form_class = self . get_form_class ( ) form = self . get_form ( form_class ) if form . is_valid ( ) : self . object = form . save ( commit = False ) form_validated = True else : form_validated = False inlines = self . construct_inlines ( ) if all_valid ( inlines ) and form_validated : return self . forms_valid ( form , inlines ) return self . forms_invalid ( form , inlines )
1509	def stop_cluster ( cl_args ) : Log . info ( "Terminating cluster..." ) roles = read_and_parse_roles ( cl_args ) masters = roles [ Role . MASTERS ] slaves = roles [ Role . SLAVES ] dist_nodes = masters . union ( slaves ) if masters : try : single_master = list ( masters ) [ 0 ] jobs = get_jobs ( cl_args , single_master ) for job in jobs : job_id = job [ "ID" ] Log . info ( "Terminating job %s" % job_id ) delete_job ( cl_args , job_id , single_master ) except : Log . debug ( "Error stopping jobs" ) Log . debug ( sys . exc_info ( ) [ 0 ] ) for node in dist_nodes : Log . info ( "Terminating processes on %s" % node ) if not is_self ( node ) : cmd = "ps aux | grep heron-nomad | awk '{print \$2}' " "| xargs kill" cmd = ssh_remote_execute ( cmd , node , cl_args ) else : cmd = "ps aux | grep heron-nomad | awk '{print $2}' " "| xargs kill" Log . debug ( cmd ) pid = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) return_code = pid . wait ( ) output = pid . communicate ( ) Log . debug ( "return code: %s output: %s" % ( return_code , output ) ) Log . info ( "Cleaning up directories on %s" % node ) cmd = "rm -rf /tmp/slave ; rm -rf /tmp/master" if not is_self ( node ) : cmd = ssh_remote_execute ( cmd , node , cl_args ) Log . debug ( cmd ) pid = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) return_code = pid . wait ( ) output = pid . communicate ( ) Log . debug ( "return code: %s output: %s" % ( return_code , output ) )
10559	def convert_cygwin_path ( path ) : try : win_path = subprocess . check_output ( [ "cygpath" , "-aw" , path ] , universal_newlines = True ) . strip ( ) except ( FileNotFoundError , subprocess . CalledProcessError ) : logger . exception ( "Call to cygpath failed." ) raise return win_path
11035	def _request ( self , endpoint , * args , ** kwargs ) : kwargs [ 'url' ] = endpoint return ( super ( MarathonLbClient , self ) . request ( * args , ** kwargs ) . addCallback ( raise_for_status ) )
1700	def consume ( self , consume_function ) : from heronpy . streamlet . impl . consumebolt import ConsumeStreamlet consume_streamlet = ConsumeStreamlet ( consume_function , self ) self . _add_child ( consume_streamlet ) return
12401	def satisfied_by_checked ( self , req ) : req_man = RequirementsManager ( [ req ] ) return any ( req_man . check ( * checked ) for checked in self . checked )
8771	def _add_default_tz_bindings ( self , context , switch , network_id ) : default_tz = CONF . NVP . default_tz if not default_tz : LOG . warn ( "additional_default_tz_types specified, " "but no default_tz. Skipping " "_add_default_tz_bindings()." ) return if not network_id : LOG . warn ( "neutron network_id not specified, skipping " "_add_default_tz_bindings()" ) return for net_type in CONF . NVP . additional_default_tz_types : if net_type in TZ_BINDINGS : binding = TZ_BINDINGS [ net_type ] binding . add ( context , switch , default_tz , network_id ) else : LOG . warn ( "Unknown default tz type %s" % ( net_type ) )
12999	def hr_diagram_figure ( cluster ) : temps , lums = round_teff_luminosity ( cluster ) x , y = temps , lums colors , color_mapper = hr_diagram_color_helper ( temps ) x_range = [ max ( x ) + max ( x ) * 0.05 , min ( x ) - min ( x ) * 0.05 ] source = ColumnDataSource ( data = dict ( x = x , y = y , color = colors ) ) pf = figure ( y_axis_type = 'log' , x_range = x_range , name = 'hr' , tools = 'box_select,lasso_select,reset,hover' , title = 'H-R Diagram for {0}' . format ( cluster . name ) ) pf . select ( BoxSelectTool ) . select_every_mousemove = False pf . select ( LassoSelectTool ) . select_every_mousemove = False hover = pf . select ( HoverTool ) [ 0 ] hover . tooltips = [ ( "Temperature (Kelvin)" , "@x{0}" ) , ( "Luminosity (solar units)" , "@y{0.00}" ) ] _diagram ( source = source , plot_figure = pf , name = 'hr' , color = { 'field' : 'color' , 'transform' : color_mapper } , xaxis_label = 'Temperature (Kelvin)' , yaxis_label = 'Luminosity (solar units)' ) return pf
3780	def load_all_methods ( self ) : r methods = [ ] Tmins , Tmaxs = [ ] , [ ] if self . CASRN in [ '7732-18-5' , '67-56-1' , '64-17-5' ] : methods . append ( TEST_METHOD_1 ) self . TEST_METHOD_1_Tmin = 200. self . TEST_METHOD_1_Tmax = 350 self . TEST_METHOD_1_coeffs = [ 1 , .002 ] Tmins . append ( self . TEST_METHOD_1_Tmin ) Tmaxs . append ( self . TEST_METHOD_1_Tmax ) if self . CASRN in [ '67-56-1' ] : methods . append ( TEST_METHOD_2 ) self . TEST_METHOD_2_Tmin = 300. self . TEST_METHOD_2_Tmax = 400 self . TEST_METHOD_2_coeffs = [ 1 , .003 ] Tmins . append ( self . TEST_METHOD_2_Tmin ) Tmaxs . append ( self . TEST_METHOD_2_Tmax ) self . all_methods = set ( methods ) if Tmins and Tmaxs : self . Tmin = min ( Tmins ) self . Tmax = max ( Tmaxs )
9924	def create ( self , * args , ** kwargs ) : is_primary = kwargs . pop ( "is_primary" , False ) with transaction . atomic ( ) : email = super ( EmailAddressManager , self ) . create ( * args , ** kwargs ) if is_primary : email . set_primary ( ) return email
3795	def setup_a_alpha_and_derivatives ( self , i , T = None ) : r if not hasattr ( self , 'kappas' ) : self . kappas = [ kappa0 + kappa1 * ( 1 + ( T / Tc ) ** 0.5 ) * ( 0.7 - ( T / Tc ) ) for kappa0 , kappa1 , Tc in zip ( self . kappa0s , self . kappa1s , self . Tcs ) ] self . a , self . kappa , self . kappa0 , self . kappa1 , self . Tc = self . ais [ i ] , self . kappas [ i ] , self . kappa0s [ i ] , self . kappa1s [ i ] , self . Tcs [ i ]
12091	def proto_02_01_MT70 ( abf = exampleABF ) : standard_overlayWithAverage ( abf ) swhlab . memtest . memtest ( abf ) swhlab . memtest . checkSweep ( abf ) swhlab . plot . save ( abf , tag = 'check' , resize = False )
6866	def _pkl_periodogram ( lspinfo , plotdpi = 100 , override_pfmethod = None ) : pgramylabel = PLOTYLABELS [ lspinfo [ 'method' ] ] periods = lspinfo [ 'periods' ] lspvals = lspinfo [ 'lspvals' ] bestperiod = lspinfo [ 'bestperiod' ] nbestperiods = lspinfo [ 'nbestperiods' ] nbestlspvals = lspinfo [ 'nbestlspvals' ] pgramfig = plt . figure ( figsize = ( 7.5 , 4.8 ) , dpi = plotdpi ) plt . plot ( periods , lspvals ) plt . xscale ( 'log' , basex = 10 ) plt . xlabel ( 'Period [days]' ) plt . ylabel ( pgramylabel ) plottitle = '%s - %.6f d' % ( METHODLABELS [ lspinfo [ 'method' ] ] , bestperiod ) plt . title ( plottitle ) for xbestperiod , xbestpeak in zip ( nbestperiods , nbestlspvals ) : plt . annotate ( '%.6f' % xbestperiod , xy = ( xbestperiod , xbestpeak ) , xycoords = 'data' , xytext = ( 0.0 , 25.0 ) , textcoords = 'offset points' , arrowprops = dict ( arrowstyle = "->" ) , fontsize = '14.0' ) plt . grid ( color = '#a9a9a9' , alpha = 0.9 , zorder = 0 , linewidth = 1.0 , linestyle = ':' ) pgrampng = StrIO ( ) pgramfig . savefig ( pgrampng , pad_inches = 0.0 , format = 'png' ) plt . close ( ) pgrampng . seek ( 0 ) pgramb64 = base64 . b64encode ( pgrampng . read ( ) ) pgrampng . close ( ) if not override_pfmethod : checkplotdict = { lspinfo [ 'method' ] : { 'periods' : periods , 'lspvals' : lspvals , 'bestperiod' : bestperiod , 'nbestperiods' : nbestperiods , 'nbestlspvals' : nbestlspvals , 'periodogram' : pgramb64 , } } else : checkplotdict = { override_pfmethod : { 'periods' : periods , 'lspvals' : lspvals , 'bestperiod' : bestperiod , 'nbestperiods' : nbestperiods , 'nbestlspvals' : nbestlspvals , 'periodogram' : pgramb64 , } } return checkplotdict
8387	def check_main ( argv ) : if len ( argv ) != 1 : print ( "Please provide the name of a file to check." ) return 1 filename = argv [ 0 ] if os . path . exists ( filename ) : print ( u"Checking existing copy of %s" % filename ) tef = TamperEvidentFile ( filename ) if tef . validate ( ) : print ( u"Your copy of %s is good" % filename ) else : print ( u"Your copy of %s seems to have been edited" % filename ) else : print ( u"You don't have a copy of %s" % filename ) return 0
10314	def canonical_circulation ( elements : T , key : Optional [ Callable [ [ T ] , bool ] ] = None ) -> T : return min ( get_circulations ( elements ) , key = key )
4932	def transform_courserun_schedule ( self , content_metadata_item ) : start = content_metadata_item . get ( 'start' ) or UNIX_MIN_DATE_STRING end = content_metadata_item . get ( 'end' ) or UNIX_MAX_DATE_STRING return [ { 'startDate' : parse_datetime_to_epoch_millis ( start ) , 'endDate' : parse_datetime_to_epoch_millis ( end ) , 'active' : current_time_is_in_interval ( start , end ) } ]
1619	def CleanseRawStrings ( raw_lines ) : delimiter = None lines_without_raw_strings = [ ] for line in raw_lines : if delimiter : end = line . find ( delimiter ) if end >= 0 : leading_space = Match ( r'^(\s*)\S' , line ) line = leading_space . group ( 1 ) + '""' + line [ end + len ( delimiter ) : ] delimiter = None else : line = '""' while delimiter is None : matched = Match ( r'^(.*?)\b(?:R|u8R|uR|UR|LR)"([^\s\\()]*)\((.*)$' , line ) if ( matched and not Match ( r'^([^\'"]|\'(\\.|[^\'])*\'|"(\\.|[^"])*")*//' , matched . group ( 1 ) ) ) : delimiter = ')' + matched . group ( 2 ) + '"' end = matched . group ( 3 ) . find ( delimiter ) if end >= 0 : line = ( matched . group ( 1 ) + '""' + matched . group ( 3 ) [ end + len ( delimiter ) : ] ) delimiter = None else : line = matched . group ( 1 ) + '""' else : break lines_without_raw_strings . append ( line ) return lines_without_raw_strings
4611	def block_time ( self , block_num ) : return self . block_class ( block_num , blockchain_instance = self . blockchain ) . time ( )
5575	def available_input_formats ( ) : input_formats = [ ] for v in pkg_resources . iter_entry_points ( DRIVERS_ENTRY_POINT ) : logger . debug ( "driver found: %s" , v ) driver_ = v . load ( ) if hasattr ( driver_ , "METADATA" ) and ( driver_ . METADATA [ "mode" ] in [ "r" , "rw" ] ) : input_formats . append ( driver_ . METADATA [ "driver_name" ] ) return input_formats
3966	def case_insensitive_rename ( src , dst ) : temp_dir = tempfile . mkdtemp ( ) shutil . rmtree ( temp_dir ) shutil . move ( src , temp_dir ) shutil . move ( temp_dir , dst )
347	def load_imdb_dataset ( path = 'data' , nb_words = None , skip_top = 0 , maxlen = None , test_split = 0.2 , seed = 113 , start_char = 1 , oov_char = 2 , index_from = 3 ) : path = os . path . join ( path , 'imdb' ) filename = "imdb.pkl" url = 'https://s3.amazonaws.com/text-datasets/' maybe_download_and_extract ( filename , path , url ) if filename . endswith ( ".gz" ) : f = gzip . open ( os . path . join ( path , filename ) , 'rb' ) else : f = open ( os . path . join ( path , filename ) , 'rb' ) X , labels = cPickle . load ( f ) f . close ( ) np . random . seed ( seed ) np . random . shuffle ( X ) np . random . seed ( seed ) np . random . shuffle ( labels ) if start_char is not None : X = [ [ start_char ] + [ w + index_from for w in x ] for x in X ] elif index_from : X = [ [ w + index_from for w in x ] for x in X ] if maxlen : new_X = [ ] new_labels = [ ] for x , y in zip ( X , labels ) : if len ( x ) < maxlen : new_X . append ( x ) new_labels . append ( y ) X = new_X labels = new_labels if not X : raise Exception ( 'After filtering for sequences shorter than maxlen=' + str ( maxlen ) + ', no sequence was kept. ' 'Increase maxlen.' ) if not nb_words : nb_words = max ( [ max ( x ) for x in X ] ) if oov_char is not None : X = [ [ oov_char if ( w >= nb_words or w < skip_top ) else w for w in x ] for x in X ] else : nX = [ ] for x in X : nx = [ ] for w in x : if ( w >= nb_words or w < skip_top ) : nx . append ( w ) nX . append ( nx ) X = nX X_train = np . array ( X [ : int ( len ( X ) * ( 1 - test_split ) ) ] ) y_train = np . array ( labels [ : int ( len ( X ) * ( 1 - test_split ) ) ] ) X_test = np . array ( X [ int ( len ( X ) * ( 1 - test_split ) ) : ] ) y_test = np . array ( labels [ int ( len ( X ) * ( 1 - test_split ) ) : ] ) return X_train , y_train , X_test , y_test
11146	def get_file_info ( self , relativePath ) : relativePath = self . to_repo_relative_path ( path = relativePath , split = False ) fileName = os . path . basename ( relativePath ) isRepoFile , fileOnDisk , infoOnDisk , classOnDisk = self . is_repository_file ( relativePath ) if not isRepoFile : return None , "file is not a registered repository file." if not infoOnDisk : return None , "file is a registered repository file but info file missing" fileInfoPath = os . path . join ( self . __path , os . path . dirname ( relativePath ) , self . __fileInfo % fileName ) try : with open ( fileInfoPath , 'rb' ) as fd : info = pickle . load ( fd ) except Exception as err : return None , "Unable to read file info from disk (%s)" % str ( err ) return info , ''
3237	def modify ( item , output = 'camelized' ) : if output == 'camelized' : return _modify ( item , camelize ) elif output == 'underscored' : return _modify ( item , underscore )
4331	def loudness ( self , gain_db = - 10.0 , reference_level = 65.0 ) : if not is_number ( gain_db ) : raise ValueError ( 'gain_db must be a number.' ) if not is_number ( reference_level ) : raise ValueError ( 'reference_level must be a number' ) if reference_level > 75 or reference_level < 50 : raise ValueError ( 'reference_level must be between 50 and 75' ) effect_args = [ 'loudness' , '{:f}' . format ( gain_db ) , '{:f}' . format ( reference_level ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'loudness' ) return self
463	def open_tensorboard ( log_dir = '/tmp/tensorflow' , port = 6006 ) : text = "[TL] Open tensorboard, go to localhost:" + str ( port ) + " to access" text2 = " not yet supported by this function (tl.ops.open_tb)" if not tl . files . exists_or_mkdir ( log_dir , verbose = False ) : tl . logging . info ( "[TL] Log reportory was created at %s" % log_dir ) if _platform == "linux" or _platform == "linux2" : raise NotImplementedError ( ) elif _platform == "darwin" : tl . logging . info ( 'OS X: %s' % text ) subprocess . Popen ( sys . prefix + " | python -m tensorflow.tensorboard --logdir=" + log_dir + " --port=" + str ( port ) , shell = True ) elif _platform == "win32" : raise NotImplementedError ( "this function is not supported on the Windows platform" ) else : tl . logging . info ( _platform + text2 )
3394	def find_gene_knockout_reactions ( cobra_model , gene_list , compiled_gene_reaction_rules = None ) : potential_reactions = set ( ) for gene in gene_list : if isinstance ( gene , string_types ) : gene = cobra_model . genes . get_by_id ( gene ) potential_reactions . update ( gene . _reaction ) gene_set = { str ( i ) for i in gene_list } if compiled_gene_reaction_rules is None : compiled_gene_reaction_rules = { r : parse_gpr ( r . gene_reaction_rule ) [ 0 ] for r in potential_reactions } return [ r for r in potential_reactions if not eval_gpr ( compiled_gene_reaction_rules [ r ] , gene_set ) ]
2254	def argunique ( items , key = None ) : if key is None : return unique ( range ( len ( items ) ) , key = lambda i : items [ i ] ) else : return unique ( range ( len ( items ) ) , key = lambda i : key ( items [ i ] ) )
4827	def get_course_enrollment ( self , username , course_id ) : endpoint = getattr ( self . client . enrollment , '{username},{course_id}' . format ( username = username , course_id = course_id ) ) try : result = endpoint . get ( ) except HttpNotFoundError : LOGGER . error ( 'Course enrollment details not found for invalid username or course; username=[%s], course=[%s]' , username , course_id ) return None if not result : LOGGER . info ( 'Failed to find course enrollment details for user [%s] and course [%s]' , username , course_id ) return None return result
13864	def tsms ( when , tz = None ) : if not when : return None when = totz ( when , tz ) return calendar . timegm ( when . timetuple ( ) ) * 1000 + int ( round ( when . microsecond / 1000.0 ) )
9435	def _read_a_packet ( file_h , hdrp , layers = 0 ) : raw_packet_header = file_h . read ( 16 ) if not raw_packet_header or len ( raw_packet_header ) != 16 : return None if hdrp [ 0 ] . byteorder == 'big' : packet_header = struct . unpack ( '>IIII' , raw_packet_header ) else : packet_header = struct . unpack ( '<IIII' , raw_packet_header ) ( timestamp , timestamp_us , capture_len , packet_len ) = packet_header raw_packet_data = file_h . read ( capture_len ) if not raw_packet_data or len ( raw_packet_data ) != capture_len : return None if layers > 0 : layers -= 1 raw_packet = linklayer . clookup ( hdrp [ 0 ] . ll_type ) ( raw_packet_data , layers = layers ) else : raw_packet = raw_packet_data packet = pcap_packet ( hdrp , timestamp , timestamp_us , capture_len , packet_len , raw_packet ) return packet
12872	def one_of ( these ) : ch = peek ( ) try : if ( ch is EndOfFile ) or ( ch not in these ) : fail ( list ( these ) ) except TypeError : if ch != these : fail ( [ these ] ) next ( ) return ch
89	def new_random_state ( seed = None , fully_random = False ) : if seed is None : if not fully_random : seed = CURRENT_RANDOM_STATE . randint ( SEED_MIN_VALUE , SEED_MAX_VALUE , 1 ) [ 0 ] return np . random . RandomState ( seed )
6322	def resolve_loader ( self , meta : ProgramDescription ) : if not meta . loader : meta . loader = 'single' if meta . path else 'separate' for loader_cls in self . _loaders : if loader_cls . name == meta . loader : meta . loader_cls = loader_cls break else : raise ImproperlyConfigured ( ( "Program {} has no loader class registered." "Check PROGRAM_LOADERS or PROGRAM_DIRS" ) . format ( meta . path ) )
11502	def folder_get ( self , token , folder_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'id' ] = folder_id response = self . request ( 'midas.folder.get' , parameters ) return response
742	def readFromFile ( cls , f , packed = True ) : schema = cls . getSchema ( ) if packed : proto = schema . read_packed ( f ) else : proto = schema . read ( f ) return cls . read ( proto )
606	def nupicBindingsPrereleaseInstalled ( ) : try : nupicDistribution = pkg_resources . get_distribution ( "nupic.bindings" ) if pkg_resources . parse_version ( nupicDistribution . version ) . is_prerelease : return True except pkg_resources . DistributionNotFound : pass return False
12935	def _parse_allele_data ( self ) : pref_freq , frequencies = self . _parse_frequencies ( ) info_clnvar_single_tags = [ 'ALLELEID' , 'CLNSIG' , 'CLNHGVS' ] cln_data = { x . lower ( ) : self . info [ x ] if x in self . info else None for x in info_clnvar_single_tags } cln_data . update ( { 'clndisdb' : [ x . split ( ',' ) for x in self . info [ 'CLNDISDB' ] . split ( '|' ) ] if 'CLNDISDB' in self . info else [ ] } ) cln_data . update ( { 'clndn' : self . info [ 'CLNDN' ] . split ( '|' ) if 'CLNDN' in self . info else [ ] } ) cln_data . update ( { 'clnvi' : self . info [ 'CLNVI' ] . split ( ',' ) if 'CLNVI' in self . info else [ ] } ) try : sequence = self . alt_alleles [ 0 ] except IndexError : sequence = self . ref_allele allele = ClinVarAllele ( frequency = pref_freq , sequence = sequence , ** cln_data ) if not cln_data [ 'clnsig' ] : return [ ] return [ allele ]
10594	def Nu_x ( self , L , theta , Ts , ** statef ) : Tf = statef [ 'T' ] thetar = radians ( theta ) if self . _isgas : self . Tr = Ts - 0.38 * ( Ts - Tf ) beta = self . _fluid . beta ( T = Tf ) else : self . Tr = Ts - 0.5 * ( Ts - Tf ) beta = self . _fluid . beta ( T = self . Tr ) if Ts > Tf : if 0.0 < theta < 45.0 : g = const . g * cos ( thetar ) else : g = const . g else : if - 45.0 < theta < 0.0 : g = const . g * cos ( thetar ) else : g = const . g nu = self . _fluid . nu ( T = self . Tr ) alpha = self . _fluid . alpha ( T = self . Tr ) Gr = dq . Gr ( L , Ts , Tf , beta , nu , g ) Pr = dq . Pr ( nu , alpha ) Ra = Gr * Pr eq = [ self . equation_dict [ r ] for r in self . regions if r . contains_point ( theta , Ra ) ] [ 0 ] return eq ( self , Ra , Pr )
10173	def get_bookmark ( self ) : if not Index ( self . aggregation_alias , using = self . client ) . exists ( ) : if not Index ( self . event_index , using = self . client ) . exists ( ) : return datetime . date . today ( ) return self . _get_oldest_event_timestamp ( ) query_bookmark = Search ( using = self . client , index = self . aggregation_alias , doc_type = self . bookmark_doc_type ) [ 0 : 1 ] . sort ( { 'date' : { 'order' : 'desc' } } ) bookmarks = query_bookmark . execute ( ) if len ( bookmarks ) == 0 : return self . _get_oldest_event_timestamp ( ) bookmark = datetime . datetime . strptime ( bookmarks [ 0 ] . date , self . doc_id_suffix ) return bookmark
1926	def save ( f ) : global _groups c = { } for group_name , group in _groups . items ( ) : section = { var . name : var . value for var in group . updated_vars ( ) } if not section : continue c [ group_name ] = section yaml . safe_dump ( c , f , line_break = True )
13782	def _ConvertEnumDescriptor ( self , enum_proto , package = None , file_desc = None , containing_type = None , scope = None ) : if package : enum_name = '.' . join ( ( package , enum_proto . name ) ) else : enum_name = enum_proto . name if file_desc is None : file_name = None else : file_name = file_desc . name values = [ self . _MakeEnumValueDescriptor ( value , index ) for index , value in enumerate ( enum_proto . value ) ] desc = descriptor . EnumDescriptor ( name = enum_proto . name , full_name = enum_name , filename = file_name , file = file_desc , values = values , containing_type = containing_type , options = enum_proto . options ) scope [ '.%s' % enum_name ] = desc self . _enum_descriptors [ enum_name ] = desc return desc
779	def _getOneMatchingRowNoRetries ( self , tableInfo , conn , fieldsToMatch , selectFieldNames ) : rows = self . _getMatchingRowsNoRetries ( tableInfo , conn , fieldsToMatch , selectFieldNames , maxRows = 1 ) if rows : assert len ( rows ) == 1 , repr ( len ( rows ) ) result = rows [ 0 ] else : result = None return result
969	def _extractCallingMethodArgs ( ) : import inspect import copy callingFrame = inspect . stack ( ) [ 1 ] [ 0 ] argNames , _ , _ , frameLocalVarDict = inspect . getargvalues ( callingFrame ) argNames . remove ( "self" ) args = copy . copy ( frameLocalVarDict ) for varName in frameLocalVarDict : if varName not in argNames : args . pop ( varName ) return args
6410	def lehmer_mean ( nums , exp = 2 ) : r return sum ( x ** exp for x in nums ) / sum ( x ** ( exp - 1 ) for x in nums )
6757	def reboot_or_dryrun ( self , * args , ** kwargs ) : warnings . warn ( 'Use self.run() instead.' , DeprecationWarning , stacklevel = 2 ) self . reboot ( * args , ** kwargs )
12790	def create_from_settings ( settings ) : return Connection ( settings [ "url" ] , settings [ "base_url" ] , settings [ "user" ] , settings [ "password" ] , authorizations = settings [ "authorizations" ] , debug = settings [ "debug" ] )
3588	def cbuuid_to_uuid ( cbuuid ) : data = cbuuid . data ( ) . bytes ( ) template = '{:0>8}-0000-1000-8000-00805f9b34fb' if len ( data ) <= 4 else '{:0>32}' value = template . format ( hexlify ( data . tobytes ( ) [ : 16 ] ) . decode ( 'ascii' ) ) return uuid . UUID ( hex = value )
9112	def replies ( self ) : fs_reply_path = join ( self . fs_replies_path , 'message_001.txt' ) if exists ( fs_reply_path ) : return [ load ( open ( fs_reply_path , 'r' ) ) ] else : return [ ]
1150	def _show_warning ( message , category , filename , lineno , file = None , line = None ) : if file is None : file = sys . stderr if file is None : return try : file . write ( formatwarning ( message , category , filename , lineno , line ) ) except ( IOError , UnicodeError ) : pass
3698	def Hsub ( T = 298.15 , P = 101325 , MW = None , AvailableMethods = False , Method = None , CASRN = '' ) : def list_methods ( ) : methods = [ ] if CASRN in GharagheiziHsub_data . index : methods . append ( 'Ghazerati Appendix, at 298K' ) methods . append ( 'None' ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == 'Ghazerati Appendix, at 298K' : _Hsub = float ( GharagheiziHsub_data . at [ CASRN , 'Hsub' ] ) elif Method == 'None' or not _Hsub or not MW : return None else : raise Exception ( 'Failure in in function' ) _Hsub = property_molar_to_mass ( _Hsub , MW ) return _Hsub
8971	def next ( self , data ) : self . __length += 1 result = self . __kdf . calculate ( self . __key , data , 64 ) self . __key = result [ : 32 ] return result [ 32 : ]
7775	def rfc2425encode ( name , value , parameters = None , charset = "utf-8" ) : if not parameters : parameters = { } if type ( value ) is unicode : value = value . replace ( u"\r\n" , u"\\n" ) value = value . replace ( u"\n" , u"\\n" ) value = value . replace ( u"\r" , u"\\n" ) value = value . encode ( charset , "replace" ) elif type ( value ) is not str : raise TypeError ( "Bad type for rfc2425 value" ) elif not valid_string_re . match ( value ) : parameters [ "encoding" ] = "b" value = binascii . b2a_base64 ( value ) ret = str ( name ) . lower ( ) for k , v in parameters . items ( ) : ret += ";%s=%s" % ( str ( k ) , str ( v ) ) ret += ":" while ( len ( value ) > 70 ) : ret += value [ : 70 ] + "\r\n " value = value [ 70 : ] ret += value + "\r\n" return ret
3167	def pause ( self , campaign_id ) : self . campaign_id = campaign_id return self . _mc_client . _post ( url = self . _build_path ( campaign_id , 'actions/pause' ) )
2981	def cmd_daemon ( opts ) : if opts . data_dir is None : raise BlockadeError ( "You must supply a data directory for the daemon" ) rest . start ( data_dir = opts . data_dir , port = opts . port , debug = opts . debug , host_exec = get_host_exec ( ) )
1436	def update_count ( self , name , incr_by = 1 , key = None ) : if name not in self . metrics : Log . error ( "In update_count(): %s is not registered in the metric" , name ) if key is None and isinstance ( self . metrics [ name ] , CountMetric ) : self . metrics [ name ] . incr ( incr_by ) elif key is not None and isinstance ( self . metrics [ name ] , MultiCountMetric ) : self . metrics [ name ] . incr ( key , incr_by ) else : Log . error ( "In update_count(): %s is registered but not supported with this method" , name )
2022	def SIGNEXTEND ( self , size , value ) : testbit = Operators . ITEBV ( 256 , size <= 31 , size * 8 + 7 , 257 ) result1 = ( value | ( TT256 - ( 1 << testbit ) ) ) result2 = ( value & ( ( 1 << testbit ) - 1 ) ) result = Operators . ITEBV ( 256 , ( value & ( 1 << testbit ) ) != 0 , result1 , result2 ) return Operators . ITEBV ( 256 , size <= 31 , result , value )
7854	def add_identity ( self , item_name , item_category = None , item_type = None ) : return DiscoIdentity ( self , item_name , item_category , item_type )
3043	def _expires_in ( self ) : if self . token_expiry : now = _UTCNOW ( ) if self . token_expiry > now : time_delta = self . token_expiry - now return time_delta . days * 86400 + time_delta . seconds else : return 0
9698	def deliveries ( self ) : key = make_key ( event = self . object . event , owner_name = self . object . owner . username , identifier = self . object . identifier ) return redis . lrange ( key , 0 , 20 )
11055	def rm_back_refs ( obj ) : for ref in _collect_refs ( obj ) : ref [ 'value' ] . _remove_backref ( ref [ 'field_instance' ] . _backref_field_name , obj , ref [ 'field_name' ] , strict = False )
2683	def get_function_config ( cfg ) : function_name = cfg . get ( 'function_name' ) profile_name = cfg . get ( 'profile' ) aws_access_key_id = cfg . get ( 'aws_access_key_id' ) aws_secret_access_key = cfg . get ( 'aws_secret_access_key' ) client = get_client ( 'lambda' , profile_name , aws_access_key_id , aws_secret_access_key , cfg . get ( 'region' ) , ) try : return client . get_function ( FunctionName = function_name ) except client . exceptions . ResourceNotFoundException as e : if 'Function not found' in str ( e ) : return False
3182	def create ( self , data ) : if 'id' not in data : raise KeyError ( 'The store must have an id' ) if 'list_id' not in data : raise KeyError ( 'The store must have a list_id' ) if 'name' not in data : raise KeyError ( 'The store must have a name' ) if 'currency_code' not in data : raise KeyError ( 'The store must have a currency_code' ) if not re . match ( r"^[A-Z]{3}$" , data [ 'currency_code' ] ) : raise ValueError ( 'The currency_code must be a valid 3-letter ISO 4217 currency code' ) response = self . _mc_client . _post ( url = self . _build_path ( ) , data = data ) if response is not None : self . store_id = response [ 'id' ] else : self . store_id = None return response
13761	def _handle_response ( self , response ) : if not str ( response . status_code ) . startswith ( '2' ) : raise get_api_error ( response ) return response
13842	def arkt_to_unixt ( ark_timestamp ) : res = datetime . datetime ( 2017 , 3 , 21 , 15 , 55 , 44 ) + datetime . timedelta ( seconds = ark_timestamp ) return res . timestamp ( )
407	def state_size ( self ) : return ( LSTMStateTuple ( self . _num_units , self . _num_units ) if self . _state_is_tuple else 2 * self . _num_units )
5671	def plot_temporal_distance_cdf ( self ) : xvalues , cdf = self . profile_block_analyzer . _temporal_distance_cdf ( ) fig = plt . figure ( ) ax = fig . add_subplot ( 111 ) xvalues = numpy . array ( xvalues ) / 60.0 ax . plot ( xvalues , cdf , "-k" ) ax . fill_between ( xvalues , cdf , color = "red" , alpha = 0.2 ) ax . set_ylabel ( "CDF(t)" ) ax . set_xlabel ( "Temporal distance t (min)" ) return fig
6895	def pwd_phasebin ( phases , mags , binsize = 0.002 , minbin = 9 ) : bins = np . arange ( 0.0 , 1.0 , binsize ) binnedphaseinds = npdigitize ( phases , bins ) binnedphases , binnedmags = [ ] , [ ] for x in npunique ( binnedphaseinds ) : thisbin_inds = binnedphaseinds == x thisbin_phases = phases [ thisbin_inds ] thisbin_mags = mags [ thisbin_inds ] if thisbin_inds . size > minbin : binnedphases . append ( npmedian ( thisbin_phases ) ) binnedmags . append ( npmedian ( thisbin_mags ) ) return np . array ( binnedphases ) , np . array ( binnedmags )
4794	def contains_value ( self , * values ) : self . _check_dict_like ( self . val , check_getitem = False ) if len ( values ) == 0 : raise ValueError ( 'one or more value args must be given' ) missing = [ ] for v in values : if v not in self . val . values ( ) : missing . append ( v ) if missing : self . _err ( 'Expected <%s> to contain values %s, but did not contain %s.' % ( self . val , self . _fmt_items ( values ) , self . _fmt_items ( missing ) ) ) return self
9885	def load_all_variables ( self ) : self . data = { } file_var_names = self . z_variable_info . keys ( ) dim_sizes = [ ] rec_nums = [ ] data_types = [ ] names = [ ] for i , name in enumerate ( file_var_names ) : dim_sizes . extend ( self . z_variable_info [ name ] [ 'dim_sizes' ] ) rec_nums . append ( self . z_variable_info [ name ] [ 'rec_num' ] ) data_types . append ( self . z_variable_info [ name ] [ 'data_type' ] ) names . append ( name . ljust ( 256 ) ) dim_sizes = np . array ( dim_sizes ) rec_nums = np . array ( rec_nums ) data_types = np . array ( data_types ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'real4' ] , fortran_cdf . get_multi_z_real4 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'float' ] , fortran_cdf . get_multi_z_real4 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'real8' ] , fortran_cdf . get_multi_z_real8 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'double' ] , fortran_cdf . get_multi_z_real8 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'int4' ] , fortran_cdf . get_multi_z_int4 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'uint4' ] , fortran_cdf . get_multi_z_int4 , data_offset = 2 ** 32 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'int2' ] , fortran_cdf . get_multi_z_int2 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'uint2' ] , fortran_cdf . get_multi_z_int2 , data_offset = 2 ** 16 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'int1' ] , fortran_cdf . get_multi_z_int1 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'uint1' ] , fortran_cdf . get_multi_z_int1 , data_offset = 2 ** 8 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'byte' ] , fortran_cdf . get_multi_z_int1 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'epoch' ] , fortran_cdf . get_multi_z_real8 , epoch = True ) self . _call_multi_fortran_z ( names , data_types , rec_nums , 2 * dim_sizes , self . cdf_data_types [ 'epoch16' ] , fortran_cdf . get_multi_z_epoch16 , epoch16 = True ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'TT2000' ] , fortran_cdf . get_multi_z_tt2000 , epoch = True ) self . data_loaded = True
8222	def do_toggle_variables ( self , action ) : self . show_vars = action . get_active ( ) if self . show_vars : self . show_variables_window ( ) else : self . hide_variables_window ( )
8719	def backup ( self , path ) : log . info ( 'Backing up in ' + path ) files = self . file_list ( ) self . prepare ( ) for f in files : self . read_file ( f [ 0 ] , os . path . join ( path , f [ 0 ] ) )
4442	def add_suggestions ( self , * suggestions , ** kwargs ) : pipe = self . redis . pipeline ( ) for sug in suggestions : args = [ AutoCompleter . SUGADD_COMMAND , self . key , sug . string , sug . score ] if kwargs . get ( 'increment' ) : args . append ( AutoCompleter . INCR ) if sug . payload : args . append ( 'PAYLOAD' ) args . append ( sug . payload ) pipe . execute_command ( * args ) return pipe . execute ( ) [ - 1 ]
2562	def recv_task_request_from_workers ( self ) : info = MPI . Status ( ) comm . recv ( source = MPI . ANY_SOURCE , tag = TASK_REQUEST_TAG , status = info ) worker_rank = info . Get_source ( ) logger . info ( "Received task request from worker:{}" . format ( worker_rank ) ) return worker_rank
10844	def sent ( self ) : sent_updates = [ ] url = PATHS [ 'GET_SENT' ] % self . profile_id response = self . api . get ( url = url ) for update in response [ 'updates' ] : sent_updates . append ( Update ( api = self . api , raw_response = update ) ) self . __sent = sent_updates return self . __sent
3786	def TP_dependent_property_derivative_T ( self , T , P , order = 1 ) : r sorted_valid_methods_P = self . select_valid_methods_P ( T , P ) for method in sorted_valid_methods_P : try : return self . calculate_derivative_T ( T , P , method , order ) except : pass return None
1046	def context ( self , * notes ) : self . _appended_notes += notes yield del self . _appended_notes [ - len ( notes ) : ]
10138	def nearest ( self , ver ) : if not isinstance ( ver , Version ) : ver = Version ( ver ) if ver in OFFICIAL_VERSIONS : return ver versions = list ( OFFICIAL_VERSIONS ) versions . sort ( reverse = True ) best = None for candidate in versions : if candidate == ver : return candidate if ( best is None ) and ( candidate < ver ) : warnings . warn ( 'This version of hszinc does not yet ' 'support version %s, please seek a newer version ' 'or file a bug. Closest (older) version supported is %s.' % ( ver , candidate ) ) return candidate if candidate > ver : best = candidate assert best is not None warnings . warn ( 'This version of hszinc does not yet ' 'support version %s, please seek a newer version ' 'or file a bug. Closest (newer) version supported is %s.' % ( ver , best ) ) return best
1675	def _ExpandDirectories ( filenames ) : expanded = set ( ) for filename in filenames : if not os . path . isdir ( filename ) : expanded . add ( filename ) continue for root , _ , files in os . walk ( filename ) : for loopfile in files : fullname = os . path . join ( root , loopfile ) if fullname . startswith ( '.' + os . path . sep ) : fullname = fullname [ len ( '.' + os . path . sep ) : ] expanded . add ( fullname ) filtered = [ ] for filename in expanded : if os . path . splitext ( filename ) [ 1 ] [ 1 : ] in GetAllExtensions ( ) : filtered . append ( filename ) return filtered
11182	def release ( self ) : self . _lock . release ( ) with self . _stat_lock : self . _locked = False self . _last_released = datetime . now ( )
12716	def position_rates ( self ) : return [ self . ode_obj . getPositionRate ( i ) for i in range ( self . LDOF ) ]
2532	def parse_ext_doc_ref ( self , ext_doc_ref_term ) : for _s , _p , o in self . graph . triples ( ( ext_doc_ref_term , self . spdx_namespace [ 'externalDocumentId' ] , None ) ) : try : self . builder . set_ext_doc_id ( self . doc , six . text_type ( o ) ) except SPDXValueError : self . value_error ( 'EXT_DOC_REF_VALUE' , 'External Document ID' ) break for _s , _p , o in self . graph . triples ( ( ext_doc_ref_term , self . spdx_namespace [ 'spdxDocument' ] , None ) ) : try : self . builder . set_spdx_doc_uri ( self . doc , six . text_type ( o ) ) except SPDXValueError : self . value_error ( 'EXT_DOC_REF_VALUE' , 'SPDX Document URI' ) break for _s , _p , checksum in self . graph . triples ( ( ext_doc_ref_term , self . spdx_namespace [ 'checksum' ] , None ) ) : for _ , _ , value in self . graph . triples ( ( checksum , self . spdx_namespace [ 'checksumValue' ] , None ) ) : try : self . builder . set_chksum ( self . doc , six . text_type ( value ) ) except SPDXValueError : self . value_error ( 'EXT_DOC_REF_VALUE' , 'Checksum' ) break
11914	def initialize ( self , templates_path , global_data ) : self . env = Environment ( loader = FileSystemLoader ( templates_path ) ) self . env . trim_blocks = True self . global_data = global_data
1845	def JP ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . PF , target . read ( ) , cpu . PC )
11804	def assign ( self , var , val , assignment ) : "Assign var, and keep track of conflicts." oldval = assignment . get ( var , None ) if val != oldval : if oldval is not None : self . record_conflict ( assignment , var , oldval , - 1 ) self . record_conflict ( assignment , var , val , + 1 ) CSP . assign ( self , var , val , assignment )
12945	def hasSameValues ( self , other , cascadeObject = True ) : if self . FIELDS != other . FIELDS : return False oga = object . __getattribute__ for field in self . FIELDS : thisVal = oga ( self , field ) otherVal = oga ( other , field ) if thisVal != otherVal : return False if cascadeObject is True and issubclass ( field . __class__ , IRForeignLinkFieldBase ) : if thisVal and thisVal . isFetched ( ) : if otherVal and otherVal . isFetched ( ) : theseForeign = thisVal . getObjs ( ) othersForeign = otherVal . getObjs ( ) for i in range ( len ( theseForeign ) ) : if not theseForeign [ i ] . hasSameValues ( othersForeign [ i ] ) : return False else : theseForeign = thisVal . getObjs ( ) for i in range ( len ( theseForeign ) ) : if theseForeign [ i ] . hasUnsavedChanges ( cascadeObjects = True ) : return False else : if otherVal and otherVal . isFetched ( ) : othersForeign = otherVal . getObjs ( ) for i in range ( len ( othersForeign ) ) : if othersForeign [ i ] . hasUnsavedChanges ( cascadeObjects = True ) : return False return True
3004	def _get_oauth2_client_id_and_secret ( settings_instance ) : secret_json = getattr ( settings_instance , 'GOOGLE_OAUTH2_CLIENT_SECRETS_JSON' , None ) if secret_json is not None : return _load_client_secrets ( secret_json ) else : client_id = getattr ( settings_instance , "GOOGLE_OAUTH2_CLIENT_ID" , None ) client_secret = getattr ( settings_instance , "GOOGLE_OAUTH2_CLIENT_SECRET" , None ) if client_id is not None and client_secret is not None : return client_id , client_secret else : raise exceptions . ImproperlyConfigured ( "Must specify either GOOGLE_OAUTH2_CLIENT_SECRETS_JSON, or " "both GOOGLE_OAUTH2_CLIENT_ID and " "GOOGLE_OAUTH2_CLIENT_SECRET in settings.py" )
12110	def input_options ( self , options , prompt = 'Select option' , default = None ) : check_options = [ x . lower ( ) for x in options ] while True : response = input ( '%s [%s]: ' % ( prompt , ', ' . join ( options ) ) ) . lower ( ) if response in check_options : return response . strip ( ) elif response == '' and default is not None : return default . lower ( ) . strip ( )
2371	def settings ( self ) : for table in self . tables : if isinstance ( table , SettingTable ) : for statement in table . statements : yield statement
4981	def set_final_prices ( self , modes , request ) : result = [ ] for mode in modes : if mode [ 'premium' ] : mode [ 'final_price' ] = EcommerceApiClient ( request . user ) . get_course_final_price ( mode = mode , enterprise_catalog_uuid = request . GET . get ( 'catalog' ) if request . method == 'GET' else None , ) result . append ( mode ) return result
4884	def handle_transmission_error ( self , learner_data , request_exception ) : try : sys_msg = request_exception . response . content except AttributeError : pass else : if 'user account is inactive' in sys_msg : ecu = EnterpriseCustomerUser . objects . get ( enterprise_enrollments__id = learner_data . enterprise_course_enrollment_id ) ecu . active = False ecu . save ( ) LOGGER . warning ( 'User %s with ID %s and email %s is a former employee of %s ' 'and has been marked inactive in SAPSF. Now marking inactive internally.' , ecu . username , ecu . user_id , ecu . user_email , ecu . enterprise_customer ) return super ( SapSuccessFactorsLearnerTransmitter , self ) . handle_transmission_error ( learner_data , request_exception )
8999	def _dump_knitting_pattern ( self , file ) : knitting_pattern_set = self . __on_dump ( ) knitting_pattern = knitting_pattern_set . patterns . at ( 0 ) layout = GridLayout ( knitting_pattern ) builder = AYABPNGBuilder ( * layout . bounding_box ) builder . set_colors_in_grid ( layout . walk_instructions ( ) ) builder . write_to_file ( file )
9381	def set_sla ( obj , metric , sub_metric , rules ) : if not hasattr ( obj , 'sla_map' ) : return False rules_list = rules . split ( ) for rule in rules_list : if '<' in rule : stat , threshold = rule . split ( '<' ) sla = SLA ( metric , sub_metric , stat , threshold , 'lt' ) elif '>' in rule : stat , threshold = rule . split ( '>' ) sla = SLA ( metric , sub_metric , stat , threshold , 'gt' ) else : if hasattr ( obj , 'logger' ) : obj . logger . error ( 'Unsupported SLA type defined : ' + rule ) sla = None obj . sla_map [ metric ] [ sub_metric ] [ stat ] = sla if hasattr ( obj , 'sla_list' ) : obj . sla_list . append ( sla ) return True
3634	def search ( self , ctype , level = None , category = None , assetId = None , defId = None , min_price = None , max_price = None , min_buy = None , max_buy = None , league = None , club = None , position = None , zone = None , nationality = None , rare = False , playStyle = None , start = 0 , page_size = itemsPerPage [ 'transferMarket' ] , fast = False ) : method = 'GET' url = 'transfermarket' if start == 0 : events = [ self . pin . event ( 'page_view' , 'Hub - Transfers' ) , self . pin . event ( 'page_view' , 'Transfer Market Search' ) ] self . pin . send ( events , fast = fast ) params = { 'start' : start , 'num' : page_size , 'type' : ctype , } if level : params [ 'lev' ] = level if category : params [ 'cat' ] = category if assetId : params [ 'maskedDefId' ] = assetId if defId : params [ 'definitionId' ] = defId if min_price : params [ 'micr' ] = min_price if max_price : params [ 'macr' ] = max_price if min_buy : params [ 'minb' ] = min_buy if max_buy : params [ 'maxb' ] = max_buy if league : params [ 'leag' ] = league if club : params [ 'team' ] = club if position : params [ 'pos' ] = position if zone : params [ 'zone' ] = zone if nationality : params [ 'nat' ] = nationality if rare : params [ 'rare' ] = 'SP' if playStyle : params [ 'playStyle' ] = playStyle rc = self . __request__ ( method , url , params = params , fast = fast ) if start == 0 : events = [ self . pin . event ( 'page_view' , 'Transfer Market Results - List View' ) , self . pin . event ( 'page_view' , 'Item - Detail View' ) ] self . pin . send ( events , fast = fast ) return [ itemParse ( i ) for i in rc . get ( 'auctionInfo' , ( ) ) ]
4678	def getActiveKeyForAccount ( self , name ) : account = self . rpc . get_account ( name ) for authority in account [ "active" ] [ "key_auths" ] : try : return self . getPrivateKeyForPublicKey ( authority [ 0 ] ) except Exception : pass return False
13070	def r_references ( self , objectId , lang = None ) : collection , reffs = self . get_reffs ( objectId = objectId , export_collection = True ) return { "template" : "main::references.html" , "objectId" : objectId , "citation" : collection . citation , "collections" : { "current" : { "label" : collection . get_label ( lang ) , "id" : collection . id , "model" : str ( collection . model ) , "type" : str ( collection . type ) , } , "parents" : self . make_parents ( collection , lang = lang ) } , "reffs" : reffs }
12307	def auto_get_repo ( autooptions , debug = False ) : pluginmgr = plugins_get_mgr ( ) repomgr = pluginmgr . get ( what = 'repomanager' , name = 'git' ) repo = None try : if debug : print ( "Looking repo" ) repo = repomgr . lookup ( username = autooptions [ 'username' ] , reponame = autooptions [ 'reponame' ] ) except : try : print ( "Checking and cloning if the dataset exists on backend" ) url = autooptions [ 'remoteurl' ] if debug : print ( "Doesnt exist. trying to clone: {}" . format ( url ) ) common_clone ( url ) repo = repomgr . lookup ( username = autooptions [ 'username' ] , reponame = autooptions [ 'reponame' ] ) if debug : print ( "Cloning successful" ) except : yes = input ( "Repo doesnt exist. Should I create one? [yN]" ) if yes == 'y' : setup = "git" if autooptions [ 'remoteurl' ] . startswith ( 's3://' ) : setup = 'git+s3' repo = common_init ( username = autooptions [ 'username' ] , reponame = autooptions [ 'reponame' ] , setup = setup , force = True , options = autooptions ) if debug : print ( "Successfully inited repo" ) else : raise Exception ( "Cannot load repo" ) repo . options = autooptions return repo
11226	def dump_nparray ( self , obj , class_name = numpy_ndarray_class_name ) : return { "$" + class_name : self . _json_convert ( obj . tolist ( ) ) }
9822	def list ( page ) : user = AuthConfigManager . get_value ( 'username' ) if not user : Printer . print_error ( 'Please login first. `polyaxon login --help`' ) page = page or 1 try : response = PolyaxonClient ( ) . project . list_projects ( user , page = page ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get list of projects.' ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) meta = get_meta_response ( response ) if meta : Printer . print_header ( 'Projects for current user' ) Printer . print_header ( 'Navigation:' ) dict_tabulate ( meta ) else : Printer . print_header ( 'No projects found for current user' ) objects = list_dicts_to_tabulate ( [ o . to_light_dict ( humanize_values = True , exclude_attrs = [ 'uuid' , 'experiment_groups' , 'experiments' , 'description' , 'num_experiments' , 'num_independent_experiments' , 'num_experiment_groups' , 'num_jobs' , 'num_builds' , 'unique_name' ] ) for o in response [ 'results' ] ] ) if objects : Printer . print_header ( "Projects:" ) dict_tabulate ( objects , is_list_dict = True )
8700	def __exchange ( self , output , timeout = None ) : self . __writeln ( output ) self . _port . flush ( ) return self . __expect ( timeout = timeout or self . _timeout )
13727	def set_delegate ( address = None , pubkey = None , secret = None ) : c . DELEGATE [ 'ADDRESS' ] = address c . DELEGATE [ 'PUBKEY' ] = pubkey c . DELEGATE [ 'PASSPHRASE' ] = secret
6955	def _get_value ( quantitystr , fitparams , fixedparams ) : fitparamskeys , fixedparamskeys = fitparams . keys ( ) , fixedparams . keys ( ) if quantitystr in fitparamskeys : quantity = fitparams [ quantitystr ] elif quantitystr in fixedparamskeys : quantity = fixedparams [ quantitystr ] return quantity
6460	def _ends_in_doubled_cons ( self , term ) : return ( len ( term ) > 1 and term [ - 1 ] not in self . _vowels and term [ - 2 ] == term [ - 1 ] )
6905	def great_circle_dist ( ra1 , dec1 , ra2 , dec2 ) : in_ra1 = ra1 % 360.0 in_ra1 = in_ra1 + 360.0 * ( in_ra1 < 0.0 ) in_ra2 = ra2 % 360.0 in_ra2 = in_ra2 + 360.0 * ( in_ra1 < 0.0 ) ra1_rad , dec1_rad = np . deg2rad ( in_ra1 ) , np . deg2rad ( dec1 ) ra2_rad , dec2_rad = np . deg2rad ( in_ra2 ) , np . deg2rad ( dec2 ) del_dec2 = ( dec2_rad - dec1_rad ) / 2.0 del_ra2 = ( ra2_rad - ra1_rad ) / 2.0 sin_dist = np . sqrt ( np . sin ( del_dec2 ) * np . sin ( del_dec2 ) + np . cos ( dec1_rad ) * np . cos ( dec2_rad ) * np . sin ( del_ra2 ) * np . sin ( del_ra2 ) ) dist_rad = 2.0 * np . arcsin ( sin_dist ) return np . rad2deg ( dist_rad ) * 3600.0
2467	def set_file_license_in_file ( self , doc , lic ) : if self . has_package ( doc ) and self . has_file ( doc ) : if validations . validate_file_lics_in_file ( lic ) : self . file ( doc ) . add_lics ( lic ) return True else : raise SPDXValueError ( 'File::LicenseInFile' ) else : raise OrderError ( 'File::LicenseInFile' )
13549	def pip_install ( * args ) : download_cache = ( '--download-cache=%s ' % options . paved . pip . download_cache ) if options . paved . pip . download_cache else '' shv ( 'pip install %s%s' % ( download_cache , ' ' . join ( args ) ) )
5122	def set_transitions ( self , mat ) : if isinstance ( mat , dict ) : for key , value in mat . items ( ) : probs = list ( value . values ( ) ) if key not in self . g . node : msg = "One of the keys don't correspond to a vertex." raise ValueError ( msg ) elif len ( self . out_edges [ key ] ) > 0 and not np . isclose ( sum ( probs ) , 1 ) : msg = "Sum of transition probabilities at a vertex was not 1." raise ValueError ( msg ) elif ( np . array ( probs ) < 0 ) . any ( ) : msg = "Some transition probabilities were negative." raise ValueError ( msg ) for k , e in enumerate ( sorted ( self . g . out_edges ( key ) ) ) : self . _route_probs [ key ] [ k ] = value . get ( e [ 1 ] , 0 ) elif isinstance ( mat , np . ndarray ) : non_terminal = np . array ( [ self . g . out_degree ( v ) > 0 for v in self . g . nodes ( ) ] ) if mat . shape != ( self . nV , self . nV ) : msg = ( "Matrix is the wrong shape, should " "be {0} x {1}." ) . format ( self . nV , self . nV ) raise ValueError ( msg ) elif not np . allclose ( np . sum ( mat [ non_terminal , : ] , axis = 1 ) , 1 ) : msg = "Sum of transition probabilities at a vertex was not 1." raise ValueError ( msg ) elif ( mat < 0 ) . any ( ) : raise ValueError ( "Some transition probabilities were negative." ) for k in range ( self . nV ) : for j , e in enumerate ( sorted ( self . g . out_edges ( k ) ) ) : self . _route_probs [ k ] [ j ] = mat [ k , e [ 1 ] ] else : raise TypeError ( "mat must be a numpy array or a dict." )
12297	def plugins_show ( what = None , name = None , version = None , details = False ) : global pluginmgr return pluginmgr . show ( what , name , version , details )
8102	def open_socket ( self ) : self . socket = socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) self . socket . setsockopt ( socket . SOL_SOCKET , socket . SO_REUSEADDR , 1 ) self . socket . setblocking ( 0 ) self . socket . bind ( ( self . host , self . port ) )
12422	def dumps ( obj , startindex = 1 , separator = DEFAULT , index_separator = DEFAULT ) : try : firstkey = next ( iter ( obj . keys ( ) ) ) except StopIteration : return str ( ) if isinstance ( firstkey , six . text_type ) : io = StringIO ( ) else : io = BytesIO ( ) dump ( obj = obj , fp = io , startindex = startindex , separator = separator , index_separator = index_separator , ) return io . getvalue ( )
5328	def __get_uuids_from_profile_name ( self , profile_name ) : uuids = [ ] with self . db . connect ( ) as session : query = session . query ( Profile ) . filter ( Profile . name == profile_name ) profiles = query . all ( ) if profiles : for p in profiles : uuids . append ( p . uuid ) return uuids
7686	def downbeat ( annotation , sr = 22050 , length = None , ** kwargs ) : beat_click = mkclick ( 440 * 2 , sr = sr ) downbeat_click = mkclick ( 440 * 3 , sr = sr ) intervals , values = annotation . to_interval_values ( ) beats , downbeats = [ ] , [ ] for time , value in zip ( intervals [ : , 0 ] , values ) : if value [ 'position' ] == 1 : downbeats . append ( time ) else : beats . append ( time ) if length is None : length = int ( sr * np . max ( intervals ) ) + len ( beat_click ) + 1 y = filter_kwargs ( mir_eval . sonify . clicks , np . asarray ( beats ) , fs = sr , length = length , click = beat_click ) y += filter_kwargs ( mir_eval . sonify . clicks , np . asarray ( downbeats ) , fs = sr , length = length , click = downbeat_click ) return y
3954	def remove_exited_dusty_containers ( ) : client = get_docker_client ( ) exited_containers = get_exited_dusty_containers ( ) removed_containers = [ ] for container in exited_containers : log_to_client ( "Removing container {}" . format ( container [ 'Names' ] [ 0 ] ) ) try : client . remove_container ( container [ 'Id' ] , v = True ) removed_containers . append ( container ) except Exception as e : log_to_client ( e . message or str ( e ) ) return removed_containers
9721	async def start ( self , rtfromfile = False ) : cmd = "start" + ( " rtfromfile" if rtfromfile else "" ) return await asyncio . wait_for ( self . _protocol . send_command ( cmd ) , timeout = self . _timeout )
931	def next ( self , record , curInputBookmark ) : outRecord = None retInputBookmark = None if record is not None : self . _inIdx += 1 if self . _filter != None and not self . _filter [ 0 ] ( self . _filter [ 1 ] , record ) : return ( None , None ) if self . _nullAggregation : return ( record , curInputBookmark ) t = record [ self . _timeFieldIdx ] if self . _firstSequenceStartTime == None : self . _firstSequenceStartTime = t if self . _startTime is None : self . _startTime = t if self . _endTime is None : self . _endTime = self . _getEndTime ( t ) assert self . _endTime > t if self . _resetFieldIdx is not None : resetSignal = record [ self . _resetFieldIdx ] else : resetSignal = None if self . _sequenceIdFieldIdx is not None : currSequenceId = record [ self . _sequenceIdFieldIdx ] else : currSequenceId = None newSequence = ( resetSignal == 1 and self . _inIdx > 0 ) or self . _sequenceId != currSequenceId or self . _inIdx == 0 if newSequence : self . _sequenceId = currSequenceId sliceEnded = ( t >= self . _endTime or t < self . _startTime ) if ( newSequence or sliceEnded ) and len ( self . _slice ) > 0 : for j , f in enumerate ( self . _fields ) : index = f [ 0 ] if index == self . _timeFieldIdx : self . _slice [ j ] [ 0 ] = self . _startTime break outRecord = self . _createAggregateRecord ( ) retInputBookmark = self . _aggrInputBookmark self . _slice = defaultdict ( list ) for j , f in enumerate ( self . _fields ) : index = f [ 0 ] self . _slice [ j ] . append ( record [ index ] ) self . _aggrInputBookmark = curInputBookmark if newSequence : self . _startTime = t self . _endTime = self . _getEndTime ( t ) if sliceEnded : if t < self . _startTime : self . _endTime = self . _firstSequenceStartTime while t >= self . _endTime : self . _startTime = self . _endTime self . _endTime = self . _getEndTime ( self . _endTime ) if outRecord is not None : return ( outRecord , retInputBookmark ) elif self . _slice : for j , f in enumerate ( self . _fields ) : index = f [ 0 ] if index == self . _timeFieldIdx : self . _slice [ j ] [ 0 ] = self . _startTime break outRecord = self . _createAggregateRecord ( ) retInputBookmark = self . _aggrInputBookmark self . _slice = defaultdict ( list ) return ( outRecord , retInputBookmark )
4616	def refresh ( self ) : asset = self . blockchain . rpc . get_asset ( self . identifier ) if not asset : raise AssetDoesNotExistsException ( self . identifier ) super ( Asset , self ) . __init__ ( asset , blockchain_instance = self . blockchain ) if self . full : if "bitasset_data_id" in asset : self [ "bitasset_data" ] = self . blockchain . rpc . get_object ( asset [ "bitasset_data_id" ] ) self [ "dynamic_asset_data" ] = self . blockchain . rpc . get_object ( asset [ "dynamic_asset_data_id" ] )
5390	def _get_task_from_task_dir ( self , job_id , user_id , task_id , task_attempt ) : task_dir = self . _task_directory ( job_id , task_id , task_attempt ) job_descriptor = self . _read_task_metadata ( task_dir ) if not job_descriptor : return None if not job_descriptor . job_metadata . get ( 'user-id' ) : job_descriptor . job_metadata [ 'user-id' ] = user_id pid = - 1 try : with open ( os . path . join ( task_dir , 'task.pid' ) , 'r' ) as f : pid = int ( f . readline ( ) . strip ( ) ) except ( IOError , OSError ) : pass script = None script_name = job_descriptor . job_metadata . get ( 'script-name' ) if script_name : script = self . _read_script ( task_dir , script_name ) end_time = self . _get_end_time_from_task_dir ( task_dir ) last_update = self . _get_last_update_time_from_task_dir ( task_dir ) events = self . _get_events_from_task_dir ( task_dir ) status = self . _get_status_from_task_dir ( task_dir ) log_detail = self . _get_log_detail_from_task_dir ( task_dir ) if not status : status = 'RUNNING' log_detail = [ 'Pending' ] return LocalTask ( task_status = status , events = events , log_detail = log_detail , job_descriptor = job_descriptor , end_time = end_time , last_update = last_update , pid = pid , script = script )
6105	def masses_of_galaxies_within_circles_in_units ( self , radius : dim . Length , unit_mass = 'angular' , critical_surface_density = None ) : return list ( map ( lambda galaxy : galaxy . mass_within_circle_in_units ( radius = radius , unit_mass = unit_mass , kpc_per_arcsec = self . kpc_per_arcsec , critical_surface_density = critical_surface_density ) , self . galaxies ) )
3708	def COSTALD_mixture ( xs , T , Tcs , Vcs , omegas ) : r cmps = range ( len ( xs ) ) if not none_and_length_check ( [ xs , Tcs , Vcs , omegas ] ) : raise Exception ( 'Function inputs are incorrect format' ) sum1 = sum ( [ xi * Vci for xi , Vci in zip ( xs , Vcs ) ] ) sum2 = sum ( [ xi * Vci ** ( 2 / 3. ) for xi , Vci in zip ( xs , Vcs ) ] ) sum3 = sum ( [ xi * Vci ** ( 1 / 3. ) for xi , Vci in zip ( xs , Vcs ) ] ) Vm = 0.25 * ( sum1 + 3. * sum2 * sum3 ) VijTcij = [ [ ( Tcs [ i ] * Tcs [ j ] * Vcs [ i ] * Vcs [ j ] ) ** 0.5 for j in cmps ] for i in cmps ] omega = mixing_simple ( xs , omegas ) Tcm = sum ( [ xs [ i ] * xs [ j ] * VijTcij [ i ] [ j ] / Vm for j in cmps for i in cmps ] ) return COSTALD ( T , Tcm , Vm , omega )
5620	def get_best_zoom_level ( input_file , tile_pyramid_type ) : tile_pyramid = BufferedTilePyramid ( tile_pyramid_type ) with rasterio . open ( input_file , "r" ) as src : xmin , ymin , xmax , ymax = reproject_geometry ( segmentize_geometry ( box ( src . bounds . left , src . bounds . bottom , src . bounds . right , src . bounds . top ) , get_segmentize_value ( input_file , tile_pyramid ) ) , src_crs = src . crs , dst_crs = tile_pyramid . crs ) . bounds x_dif = xmax - xmin y_dif = ymax - ymin size = float ( src . width + src . height ) avg_resolution = ( ( x_dif / float ( src . width ) ) * ( float ( src . width ) / size ) + ( y_dif / float ( src . height ) ) * ( float ( src . height ) / size ) ) for zoom in range ( 0 , 40 ) : if tile_pyramid . pixel_x_size ( zoom ) <= avg_resolution : return zoom - 1
3143	def get ( self , file_id , ** queryparams ) : self . file_id = file_id return self . _mc_client . _get ( url = self . _build_path ( file_id ) , ** queryparams )
4490	def init ( args ) : config = config_from_file ( ) config_ = configparser . ConfigParser ( ) config_ . add_section ( 'osf' ) if 'username' not in config . keys ( ) : config_ . set ( 'osf' , 'username' , '' ) else : config_ . set ( 'osf' , 'username' , config [ 'username' ] ) if 'project' not in config . keys ( ) : config_ . set ( 'osf' , 'project' , '' ) else : config_ . set ( 'osf' , 'project' , config [ 'project' ] ) print ( 'Provide a username for the config file [current username: {}]:' . format ( config_ . get ( 'osf' , 'username' ) ) ) username = input ( ) if username : config_ . set ( 'osf' , 'username' , username ) print ( 'Provide a project for the config file [current project: {}]:' . format ( config_ . get ( 'osf' , 'project' ) ) ) project = input ( ) if project : config_ . set ( 'osf' , 'project' , project ) cfgfile = open ( ".osfcli.config" , "w" ) config_ . write ( cfgfile ) cfgfile . close ( )
5682	def get_closest_stop ( self , lat , lon ) : cur = self . conn . cursor ( ) min_dist = float ( "inf" ) min_stop_I = None rows = cur . execute ( "SELECT stop_I, lat, lon FROM stops" ) for stop_I , lat_s , lon_s in rows : dist_now = wgs84_distance ( lat , lon , lat_s , lon_s ) if dist_now < min_dist : min_dist = dist_now min_stop_I = stop_I return min_stop_I
12273	def iso_reference_valid_char ( c , raise_error = True ) : if c in ISO_REFERENCE_VALID : return True if raise_error : raise ValueError ( "'%s' is not in '%s'" % ( c , ISO_REFERENCE_VALID ) ) return False
5510	def release ( self ) : if self . value is not None : self . value += 1 if self . value > self . maximum_value : raise ValueError ( "Too many releases" )
1049	def print_exception ( etype , value , tb , limit = None , file = None ) : if file is None : file = open ( '/dev/stderr' , 'w' ) if tb : _print ( file , 'Traceback (most recent call last):' ) print_tb ( tb , limit , file ) lines = format_exception_only ( etype , value ) for line in lines : _print ( file , line , '' )
4058	def _bib_processor ( self , retrieved ) : items = [ ] for bib in retrieved . entries : items . append ( bib [ "content" ] [ 0 ] [ "value" ] ) self . url_params = None return items
11939	def broadcast_message ( level , message_text , extra_tags = '' , date = None , url = None , fail_silently = False ) : from django . contrib . auth import get_user_model users = get_user_model ( ) . objects . all ( ) add_message_for ( users , level , message_text , extra_tags = extra_tags , date = date , url = url , fail_silently = fail_silently )
6026	def geometry_from_grid ( self , grid , pixel_centres , pixel_neighbors , pixel_neighbors_size , buffer = 1e-8 ) : y_min = np . min ( grid [ : , 0 ] ) - buffer y_max = np . max ( grid [ : , 0 ] ) + buffer x_min = np . min ( grid [ : , 1 ] ) - buffer x_max = np . max ( grid [ : , 1 ] ) + buffer shape_arcsec = ( y_max - y_min , x_max - x_min ) origin = ( ( y_max + y_min ) / 2.0 , ( x_max + x_min ) / 2.0 ) return self . Geometry ( shape_arcsec = shape_arcsec , pixel_centres = pixel_centres , origin = origin , pixel_neighbors = pixel_neighbors , pixel_neighbors_size = pixel_neighbors_size )
2136	def disassociate_notification_template ( self , workflow , notification_template , status ) : return self . _disassoc ( 'notification_templates_%s' % status , workflow , notification_template )
5768	def _advapi32_interpret_dsa_key_blob ( bit_size , public_blob , private_blob ) : len1 = 20 len2 = bit_size // 8 q_offset = len2 g_offset = q_offset + len1 x_offset = g_offset + len2 y_offset = x_offset p = int_from_bytes ( private_blob [ 0 : q_offset ] [ : : - 1 ] ) q = int_from_bytes ( private_blob [ q_offset : g_offset ] [ : : - 1 ] ) g = int_from_bytes ( private_blob [ g_offset : x_offset ] [ : : - 1 ] ) x = int_from_bytes ( private_blob [ x_offset : x_offset + len1 ] [ : : - 1 ] ) y = int_from_bytes ( public_blob [ y_offset : y_offset + len2 ] [ : : - 1 ] ) public_key_info = keys . PublicKeyInfo ( { 'algorithm' : keys . PublicKeyAlgorithm ( { 'algorithm' : 'dsa' , 'parameters' : keys . DSAParams ( { 'p' : p , 'q' : q , 'g' : g , } ) } ) , 'public_key' : core . Integer ( y ) , } ) private_key_info = keys . PrivateKeyInfo ( { 'version' : 0 , 'private_key_algorithm' : keys . PrivateKeyAlgorithm ( { 'algorithm' : 'dsa' , 'parameters' : keys . DSAParams ( { 'p' : p , 'q' : q , 'g' : g , } ) } ) , 'private_key' : core . Integer ( x ) , } ) return ( public_key_info , private_key_info )
2275	def _win32_is_hardlinked ( fpath1 , fpath2 ) : def get_read_handle ( fpath ) : if os . path . isdir ( fpath ) : dwFlagsAndAttributes = jwfs . api . FILE_FLAG_BACKUP_SEMANTICS else : dwFlagsAndAttributes = 0 hFile = jwfs . api . CreateFile ( fpath , jwfs . api . GENERIC_READ , jwfs . api . FILE_SHARE_READ , None , jwfs . api . OPEN_EXISTING , dwFlagsAndAttributes , None ) return hFile def get_unique_id ( hFile ) : info = jwfs . api . BY_HANDLE_FILE_INFORMATION ( ) res = jwfs . api . GetFileInformationByHandle ( hFile , info ) jwfs . handle_nonzero_success ( res ) unique_id = ( info . volume_serial_number , info . file_index_high , info . file_index_low ) return unique_id hFile1 = get_read_handle ( fpath1 ) hFile2 = get_read_handle ( fpath2 ) try : are_equal = ( get_unique_id ( hFile1 ) == get_unique_id ( hFile2 ) ) except Exception : raise finally : jwfs . api . CloseHandle ( hFile1 ) jwfs . api . CloseHandle ( hFile2 ) return are_equal
5965	def make_main_index ( struct , selection = '"Protein"' , ndx = 'main.ndx' , oldndx = None ) : logger . info ( "Building the main index file {ndx!r}..." . format ( ** vars ( ) ) ) _ , out , _ = gromacs . make_ndx ( f = struct , n = oldndx , o = ndx , stdout = False , input = ( "" , "q" ) ) groups = cbook . parse_ndxlist ( out ) selection = selection . strip ( "\"" ) selected_groups = [ g for g in groups if g [ 'name' ] . lower ( ) == selection . lower ( ) ] if len ( selected_groups ) > 1 : logging . warn ( "make_ndx created duplicated groups, performing work around" ) if len ( selected_groups ) <= 0 : msg = "no groups found for selection {0}, available groups are {1}" . format ( selection , groups ) logging . error ( msg ) raise ValueError ( msg ) last = len ( groups ) - 1 assert last == groups [ - 1 ] [ 'nr' ] group = selected_groups [ 0 ] _ , out , _ = gromacs . make_ndx ( f = struct , n = ndx , o = ndx , stdout = False , input = ( "{0}" . format ( group [ 'nr' ] ) , "name {0} __main__" . format ( last + 1 ) , "! \"__main__\"" , "name {0} __environment__" . format ( last + 2 ) , "" , "q" ) ) return cbook . parse_ndxlist ( out )
9214	def t_t_isopen ( self , t ) : r'"|\'' if t . value [ 0 ] == '"' : t . lexer . push_state ( 'istringquotes' ) elif t . value [ 0 ] == '\'' : t . lexer . push_state ( 'istringapostrophe' ) return t
5721	def _convert_schemas ( mapping , schemas ) : schemas = deepcopy ( schemas ) for schema in schemas : for fk in schema . get ( 'foreignKeys' , [ ] ) : resource = fk [ 'reference' ] [ 'resource' ] if resource != 'self' : if resource not in mapping : message = 'Not resource "%s" for foreign key "%s"' message = message % ( resource , fk ) raise ValueError ( message ) fk [ 'reference' ] [ 'resource' ] = mapping [ resource ] return schemas
6257	def find ( self , path : Path ) : if getattr ( self , 'settings_attr' , None ) : self . paths = getattr ( settings , self . settings_attr ) path_found = None for entry in self . paths : abspath = entry / path if abspath . exists ( ) : path_found = abspath return path_found
2071	def col_transform ( self , col , digits ) : if col is None or float ( col ) < 0.0 : return None else : col = self . number_to_base ( int ( col ) , self . base , digits ) if len ( col ) == digits : return col else : return [ 0 for _ in range ( digits - len ( col ) ) ] + col
9609	def find_exception_by_code ( code ) : errorName = None for error in WebDriverError : if error . value . code == code : errorName = error break return errorName
11363	def download_file ( from_url , to_filename = None , chunk_size = 1024 * 8 , retry_count = 3 ) : if not to_filename : to_filename = get_temporary_file ( ) session = requests . Session ( ) adapter = requests . adapters . HTTPAdapter ( max_retries = retry_count ) session . mount ( from_url , adapter ) response = session . get ( from_url , stream = True ) with open ( to_filename , 'wb' ) as fd : for chunk in response . iter_content ( chunk_size ) : fd . write ( chunk ) return to_filename
12515	def new_img_like ( ref_niimg , data , affine = None , copy_header = False ) : if not ( hasattr ( ref_niimg , 'get_data' ) and hasattr ( ref_niimg , 'get_affine' ) ) : if isinstance ( ref_niimg , _basestring ) : ref_niimg = nib . load ( ref_niimg ) elif operator . isSequenceType ( ref_niimg ) : ref_niimg = nib . load ( ref_niimg [ 0 ] ) else : raise TypeError ( ( 'The reference image should be a niimg, %r ' 'was passed' ) % ref_niimg ) if affine is None : affine = ref_niimg . get_affine ( ) if data . dtype == bool : default_dtype = np . int8 if ( LooseVersion ( nib . __version__ ) >= LooseVersion ( '1.2.0' ) and isinstance ( ref_niimg , nib . freesurfer . mghformat . MGHImage ) ) : default_dtype = np . uint8 data = as_ndarray ( data , dtype = default_dtype ) header = None if copy_header : header = copy . copy ( ref_niimg . get_header ( ) ) header [ 'scl_slope' ] = 0. header [ 'scl_inter' ] = 0. header [ 'glmax' ] = 0. header [ 'cal_max' ] = np . max ( data ) if data . size > 0 else 0. header [ 'cal_max' ] = np . min ( data ) if data . size > 0 else 0. return ref_niimg . __class__ ( data , affine , header = header )
11273	def get_dict ( self ) : return dict ( current_page = self . current_page , total_page_count = self . total_page_count , items = self . items , total_item_count = self . total_item_count , page_size = self . page_size )
6022	def from_fits_with_scale ( cls , file_path , hdu , pixel_scale ) : return cls ( array = array_util . numpy_array_2d_from_fits ( file_path , hdu ) , pixel_scale = pixel_scale )
12115	def fileModifiedTimestamp ( fname ) : modifiedTime = os . path . getmtime ( fname ) stamp = time . strftime ( '%Y-%m-%d' , time . localtime ( modifiedTime ) ) return stamp
7427	def refmap_init ( data , sample , force ) : sample . files . unmapped_reads = os . path . join ( data . dirs . edits , "{}-refmap_derep.fastq" . format ( sample . name ) ) sample . files . mapped_reads = os . path . join ( data . dirs . refmapping , "{}-mapped-sorted.bam" . format ( sample . name ) )
9047	def multivariate_normal ( random , mean , cov ) : from numpy . linalg import cholesky L = cholesky ( cov ) return L @ random . randn ( L . shape [ 0 ] ) + mean
2556	def get ( self , tag = None , ** kwargs ) : if tag is None : tag = dom_tag attrs = [ ( dom_tag . clean_attribute ( attr ) , value ) for attr , value in kwargs . items ( ) ] results = [ ] for child in self . children : if ( isinstance ( tag , basestring ) and type ( child ) . __name__ == tag ) or ( not isinstance ( tag , basestring ) and isinstance ( child , tag ) ) : if all ( child . attributes . get ( attribute ) == value for attribute , value in attrs ) : results . append ( child ) if isinstance ( child , dom_tag ) : results . extend ( child . get ( tag , ** kwargs ) ) return results
3733	def charge ( self ) : try : return self . _charge except AttributeError : self . _charge = charge_from_formula ( self . formula ) return self . _charge
9169	def parse_archive_uri ( uri ) : parsed = urlparse ( uri ) path = parsed . path . rstrip ( '/' ) . split ( '/' ) ident_hash = path [ - 1 ] ident_hash = unquote ( ident_hash ) return ident_hash
6577	def from_json ( cls , api_client , data ) : self = cls ( api_client ) PandoraModel . populate_fields ( api_client , self , data ) return self
12584	def spatialimg_to_hdfgroup ( h5group , spatial_img ) : try : h5group [ 'data' ] = spatial_img . get_data ( ) h5group [ 'affine' ] = spatial_img . get_affine ( ) if hasattr ( h5group , 'get_extra' ) : h5group [ 'extra' ] = spatial_img . get_extra ( ) hdr = spatial_img . get_header ( ) for k in list ( hdr . keys ( ) ) : h5group [ 'data' ] . attrs [ k ] = hdr [ k ] except ValueError as ve : raise Exception ( 'Error creating group ' + h5group . name ) from ve
6343	def raw ( self ) : r doc_list = [ ] for doc in self . corpus : sent_list = [ ] for sent in doc : sent_list . append ( ' ' . join ( sent ) ) doc_list . append ( self . sent_split . join ( sent_list ) ) del sent_list return self . doc_split . join ( doc_list )
4792	def is_unicode ( self ) : if type ( self . val ) is not unicode : self . _err ( 'Expected <%s> to be unicode, but was <%s>.' % ( self . val , type ( self . val ) . __name__ ) ) return self
9127	def create_all ( engine , checkfirst = True ) : Base . metadata . create_all ( bind = engine , checkfirst = checkfirst )
362	def maybe_download_and_extract ( filename , working_directory , url_source , extract = False , expected_bytes = None ) : def _download ( filename , working_directory , url_source ) : progress_bar = progressbar . ProgressBar ( ) def _dlProgress ( count , blockSize , totalSize , pbar = progress_bar ) : if ( totalSize != 0 ) : if not pbar . max_value : totalBlocks = math . ceil ( float ( totalSize ) / float ( blockSize ) ) pbar . max_value = int ( totalBlocks ) pbar . update ( count , force = True ) filepath = os . path . join ( working_directory , filename ) logging . info ( 'Downloading %s...\n' % filename ) urlretrieve ( url_source + filename , filepath , reporthook = _dlProgress ) exists_or_mkdir ( working_directory , verbose = False ) filepath = os . path . join ( working_directory , filename ) if not os . path . exists ( filepath ) : _download ( filename , working_directory , url_source ) statinfo = os . stat ( filepath ) logging . info ( 'Succesfully downloaded %s %s bytes.' % ( filename , statinfo . st_size ) ) if ( not ( expected_bytes is None ) and ( expected_bytes != statinfo . st_size ) ) : raise Exception ( 'Failed to verify ' + filename + '. Can you get to it with a browser?' ) if ( extract ) : if tarfile . is_tarfile ( filepath ) : logging . info ( 'Trying to extract tar file' ) tarfile . open ( filepath , 'r' ) . extractall ( working_directory ) logging . info ( '... Success!' ) elif zipfile . is_zipfile ( filepath ) : logging . info ( 'Trying to extract zip file' ) with zipfile . ZipFile ( filepath ) as zf : zf . extractall ( working_directory ) logging . info ( '... Success!' ) else : logging . info ( "Unknown compression_format only .tar.gz/.tar.bz2/.tar and .zip supported" ) return filepath
7798	def sasl_mechanism ( name , secure , preference = 50 ) : def decorator ( klass ) : klass . _pyxmpp_sasl_secure = secure klass . _pyxmpp_sasl_preference = preference if issubclass ( klass , ClientAuthenticator ) : _register_client_authenticator ( klass , name ) elif issubclass ( klass , ServerAuthenticator ) : _register_server_authenticator ( klass , name ) else : raise TypeError ( "Not a ClientAuthenticator" " or ServerAuthenticator class" ) return klass return decorator
724	def getDataRowCount ( self ) : inputRowCountAfterAggregation = 0 while True : record = self . getNextRecord ( ) if record is None : return inputRowCountAfterAggregation inputRowCountAfterAggregation += 1 if inputRowCountAfterAggregation > 10000 : raise RuntimeError ( 'No end of datastream found.' )
1896	def _is_sat ( self ) -> bool : logger . debug ( "Solver.check() " ) start = time . time ( ) self . _send ( '(check-sat)' ) status = self . _recv ( ) logger . debug ( "Check took %s seconds (%s)" , time . time ( ) - start , status ) if status not in ( 'sat' , 'unsat' , 'unknown' ) : raise SolverError ( status ) if consider_unknown_as_unsat : if status == 'unknown' : logger . info ( 'Found an unknown core, probably a solver timeout' ) status = 'unsat' if status == 'unknown' : raise SolverUnknown ( status ) return status == 'sat'
8947	def run_elective ( self , cmd , * args , ** kwargs ) : if self . _commit : return self . run ( cmd , * args , ** kwargs ) else : notify . warning ( "WOULD RUN: {}" . format ( cmd ) ) kwargs = kwargs . copy ( ) kwargs [ 'echo' ] = False return self . run ( 'true' , * args , ** kwargs )
6012	def load_exposure_time_map ( exposure_time_map_path , exposure_time_map_hdu , pixel_scale , shape , exposure_time , exposure_time_map_from_inverse_noise_map , inverse_noise_map ) : exposure_time_map_options = sum ( [ exposure_time_map_from_inverse_noise_map ] ) if exposure_time is not None and exposure_time_map_path is not None : raise exc . DataException ( 'You have supplied both a exposure_time_map_path to an exposure time map and an exposure time. Only' 'one quantity should be supplied.' ) if exposure_time_map_options == 0 : if exposure_time is not None and exposure_time_map_path is None : return ExposureTimeMap . single_value ( value = exposure_time , pixel_scale = pixel_scale , shape = shape ) elif exposure_time is None and exposure_time_map_path is not None : return ExposureTimeMap . from_fits_with_pixel_scale ( file_path = exposure_time_map_path , hdu = exposure_time_map_hdu , pixel_scale = pixel_scale ) else : if exposure_time_map_from_inverse_noise_map : return ExposureTimeMap . from_exposure_time_and_inverse_noise_map ( pixel_scale = pixel_scale , exposure_time = exposure_time , inverse_noise_map = inverse_noise_map )
4348	def trim ( self , start_time , end_time = None ) : if not is_number ( start_time ) or start_time < 0 : raise ValueError ( "start_time must be a positive number." ) effect_args = [ 'trim' , '{:f}' . format ( start_time ) ] if end_time is not None : if not is_number ( end_time ) or end_time < 0 : raise ValueError ( "end_time must be a positive number." ) if start_time >= end_time : raise ValueError ( "start_time must be smaller than end_time." ) effect_args . append ( '{:f}' . format ( end_time - start_time ) ) self . effects . extend ( effect_args ) self . effects_log . append ( 'trim' ) return self
2140	def disassociate ( self , group , parent , ** kwargs ) : parent_id = self . lookup_with_inventory ( parent , kwargs . get ( 'inventory' , None ) ) [ 'id' ] group_id = self . lookup_with_inventory ( group , kwargs . get ( 'inventory' , None ) ) [ 'id' ] return self . _disassoc ( 'children' , parent_id , group_id )
6634	def displayOutdated ( modules , dependency_specs , use_colours ) : if use_colours : DIM = colorama . Style . DIM NORMAL = colorama . Style . NORMAL BRIGHT = colorama . Style . BRIGHT YELLOW = colorama . Fore . YELLOW RED = colorama . Fore . RED GREEN = colorama . Fore . GREEN RESET = colorama . Style . RESET_ALL else : DIM = BRIGHT = YELLOW = RED = GREEN = RESET = u'' status = 0 from yotta . lib import access from yotta . lib import access_common from yotta . lib import sourceparse for name , m in modules . items ( ) : if m . isTestDependency ( ) : continue try : latest_v = access . latestSuitableVersion ( name , '*' , registry = 'modules' , quiet = True ) except access_common . Unavailable as e : latest_v = None if not m : m_version = u' ' + RESET + BRIGHT + RED + u"missing" + RESET else : m_version = DIM + u'@%s' % ( m . version ) if not latest_v : print ( u'%s%s%s%s not available from the registry%s' % ( RED , name , m_version , NORMAL , RESET ) ) status = 2 continue elif not m or m . version < latest_v : update_prevented_by = '' if m : specs_preventing_update = [ x for x in dependency_specs if x . name == name and not sourceparse . parseSourceURL ( x . nonShrinkwrappedVersionReq ( ) ) . semanticSpecMatches ( latest_v ) ] shrinkwrap_prevents_update = [ x for x in dependency_specs if x . name == name and x . isShrinkwrapped ( ) and not sourceparse . parseSourceURL ( x . versionReq ( ) ) . semanticSpecMatches ( latest_v ) ] if len ( specs_preventing_update ) : update_prevented_by = ' (update prevented by specifications: %s)' % ( ', ' . join ( [ '%s from %s' % ( x . version_req , x . specifying_module ) for x in specs_preventing_update ] ) ) if len ( shrinkwrap_prevents_update ) : update_prevented_by += ' yotta-shrinkwrap.json prevents update' if m . version . major ( ) < latest_v . major ( ) : colour = GREEN elif m . version . minor ( ) < latest_v . minor ( ) : colour = YELLOW else : colour = RED else : colour = RED print ( u'%s%s%s latest: %s%s%s%s' % ( name , m_version , RESET , colour , latest_v . version , update_prevented_by , RESET ) ) if not status : status = 1 return status
7934	def _compute_handshake ( self ) : return hashlib . sha1 ( to_utf8 ( self . stream_id ) + to_utf8 ( self . secret ) ) . hexdigest ( )
7493	def n_choose_k ( n , k ) : return int ( reduce ( MUL , ( Fraction ( n - i , i + 1 ) for i in range ( k ) ) , 1 ) )
8735	def date_range ( start = None , stop = None , step = None ) : if step is None : step = datetime . timedelta ( days = 1 ) if start is None : start = datetime . datetime . now ( ) while start < stop : yield start start += step
6445	def _cond_x ( self , word , suffix_len ) : return word [ - suffix_len - 1 ] in { 'i' , 'l' } or ( word [ - suffix_len - 3 : - suffix_len ] == 'u' and word [ - suffix_len - 1 ] == 'e' )
12086	def html_index ( self , launch = False , showChildren = False ) : self . makePics ( ) html = '<a href="index_splash.html" target="content">./%s/</a><br>' % os . path . basename ( self . abfFolder ) for ID in smartSort ( self . fnamesByCell . keys ( ) ) : link = '' if ID + ".html" in self . fnames2 : link = 'href="%s.html" target="content"' % ID html += ( '<a %s>%s</a><br>' % ( link , ID ) ) if showChildren : for fname in self . fnamesByCell [ ID ] : thisID = os . path . splitext ( fname ) [ 0 ] files2 = [ x for x in self . fnames2 if x . startswith ( thisID ) and not x . endswith ( ".html" ) ] html += '<i>%s</i>' % thisID if len ( files2 ) : html += ' (%s)' % len ( files2 ) html += '<br>' html += "<br>" style . save ( html , self . abfFolder2 + "/index_menu.html" ) self . html_index_splash ( ) style . frames ( self . abfFolder2 + "/index.html" , launch = launch )
3621	def get_adapter ( self , model ) : if not self . is_registered ( model ) : raise RegistrationError ( '{} is not registered with Algolia engine' . format ( model ) ) return self . __registered_models [ model ]
1412	def _get_topologies_with_watch ( self , callback , isWatching ) : path = self . get_topologies_path ( ) if isWatching : LOG . info ( "Adding children watch for path: " + path ) @ self . client . ChildrenWatch ( path ) def watch_topologies ( topologies ) : callback ( topologies ) return isWatching
4254	def time_zone_by_country_and_region ( country_code , region_code = None ) : timezone = country_dict . get ( country_code ) if not timezone : return None if isinstance ( timezone , str ) : return timezone return timezone . get ( region_code )
9319	def _validate_status ( self ) : if not self . id : msg = "No 'id' in Status for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if not self . status : msg = "No 'status' in Status for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . total_count is None : msg = "No 'total_count' in Status for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . success_count is None : msg = "No 'success_count' in Status for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . failure_count is None : msg = "No 'failure_count' in Status for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . pending_count is None : msg = "No 'pending_count' in Status for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if len ( self . successes ) != self . success_count : msg = "Found successes={}, but success_count={} in status '{}'" raise ValidationError ( msg . format ( self . successes , self . success_count , self . id ) ) if len ( self . pendings ) != self . pending_count : msg = "Found pendings={}, but pending_count={} in status '{}'" raise ValidationError ( msg . format ( self . pendings , self . pending_count , self . id ) ) if len ( self . failures ) != self . failure_count : msg = "Found failures={}, but failure_count={} in status '{}'" raise ValidationError ( msg . format ( self . failures , self . failure_count , self . id ) ) if ( self . success_count + self . pending_count + self . failure_count != self . total_count ) : msg = ( "(success_count={} + pending_count={} + " "failure_count={}) != total_count={} in status '{}'" ) raise ValidationError ( msg . format ( self . success_count , self . pending_count , self . failure_count , self . total_count , self . id ) )
11645	def fit ( self , X , y = None ) : n = X . shape [ 0 ] if X . shape != ( n , n ) : raise TypeError ( "Input must be a square matrix." ) memory = get_memory ( self . memory ) vals , vecs = memory . cache ( scipy . linalg . eigh , ignore = [ 'overwrite_a' ] ) ( X , overwrite_a = not self . copy ) vals = vals [ : , None ] self . flip_ = np . dot ( vecs , np . sign ( vals ) * vecs . T ) return self
829	def encodedBitDescription ( self , bitOffset , formatted = False ) : ( prevFieldName , prevFieldOffset ) = ( None , None ) description = self . getDescription ( ) for i in xrange ( len ( description ) ) : ( name , offset ) = description [ i ] if formatted : offset = offset + i if bitOffset == offset - 1 : prevFieldName = "separator" prevFieldOffset = bitOffset break if bitOffset < offset : break ( prevFieldName , prevFieldOffset ) = ( name , offset ) width = self . getDisplayWidth ( ) if formatted else self . getWidth ( ) if prevFieldOffset is None or bitOffset > self . getWidth ( ) : raise IndexError ( "Bit is outside of allowable range: [0 - %d]" % width ) return ( prevFieldName , bitOffset - prevFieldOffset )
6906	def total_proper_motion ( pmra , pmdecl , decl ) : pm = np . sqrt ( pmdecl * pmdecl + pmra * pmra * np . cos ( np . radians ( decl ) ) * np . cos ( np . radians ( decl ) ) ) return pm
6864	def normalized_flux_to_mag ( lcdict , columns = ( 'sap.sap_flux' , 'sap.sap_flux_err' , 'sap.sap_bkg' , 'sap.sap_bkg_err' , 'pdc.pdcsap_flux' , 'pdc.pdcsap_flux_err' ) ) : tess_mag = lcdict [ 'objectinfo' ] [ 'tessmag' ] for key in columns : k1 , k2 = key . split ( '.' ) if 'err' not in k2 : lcdict [ k1 ] [ k2 . replace ( 'flux' , 'mag' ) ] = ( tess_mag - 2.5 * np . log10 ( lcdict [ k1 ] [ k2 ] ) ) else : lcdict [ k1 ] [ k2 . replace ( 'flux' , 'mag' ) ] = ( - 2.5 * np . log10 ( 1.0 - lcdict [ k1 ] [ k2 ] ) ) return lcdict
5211	def bds ( tickers , flds , ** kwargs ) : logger = logs . get_logger ( bds , level = kwargs . pop ( 'log' , logs . LOG_LEVEL ) ) con , _ = create_connection ( ) ovrds = assist . proc_ovrds ( ** kwargs ) logger . info ( f'loading block data from Bloomberg:\n' f'{assist.info_qry(tickers=tickers, flds=flds)}' ) data = con . bulkref ( tickers = tickers , flds = flds , ovrds = ovrds ) if not kwargs . get ( 'cache' , False ) : return [ data ] qry_data = [ ] for ( ticker , fld ) , grp in data . groupby ( [ 'ticker' , 'field' ] ) : data_file = storage . ref_file ( ticker = ticker , fld = fld , ext = 'pkl' , has_date = kwargs . get ( 'has_date' , True ) , ** kwargs ) if data_file : if not files . exists ( data_file ) : qry_data . append ( grp ) files . create_folder ( data_file , is_file = True ) grp . reset_index ( drop = True ) . to_pickle ( data_file ) return qry_data
5298	def get_queryset ( self ) : qs = super ( BaseCalendarMonthView , self ) . get_queryset ( ) year = self . get_year ( ) month = self . get_month ( ) date_field = self . get_date_field ( ) end_date_field = self . get_end_date_field ( ) date = _date_from_string ( year , self . get_year_format ( ) , month , self . get_month_format ( ) ) since = date until = self . get_next_month ( date ) if since . weekday ( ) != self . get_first_of_week ( ) : diff = math . fabs ( since . weekday ( ) - self . get_first_of_week ( ) ) since = since - datetime . timedelta ( days = diff ) if until . weekday ( ) != ( ( self . get_first_of_week ( ) + 6 ) % 7 ) : diff = math . fabs ( ( ( self . get_first_of_week ( ) + 6 ) % 7 ) - until . weekday ( ) ) until = until + datetime . timedelta ( days = diff ) if end_date_field : predicate1 = Q ( ** { '%s__gte' % date_field : since , end_date_field : None } ) predicate2 = Q ( ** { '%s__gte' % date_field : since , '%s__lt' % end_date_field : until } ) predicate3 = Q ( ** { '%s__lt' % date_field : since , '%s__gte' % end_date_field : since , '%s__lt' % end_date_field : until } ) predicate4 = Q ( ** { '%s__gte' % date_field : since , '%s__lt' % date_field : until , '%s__gte' % end_date_field : until } ) predicate5 = Q ( ** { '%s__lt' % date_field : since , '%s__gte' % end_date_field : until } ) return qs . filter ( predicate1 | predicate2 | predicate3 | predicate4 | predicate5 ) return qs . filter ( ** { '%s__gte' % date_field : since } )
1005	def _learnBacktrackFrom ( self , startOffset , readOnly = True ) : numPrevPatterns = len ( self . _prevLrnPatterns ) currentTimeStepsOffset = numPrevPatterns - 1 if not readOnly : self . segmentUpdates = { } if self . verbosity >= 3 : if readOnly : print ( "Trying to lock-on using startCell state from %d steps ago:" % ( numPrevPatterns - 1 - startOffset ) , self . _prevLrnPatterns [ startOffset ] ) else : print ( "Locking on using startCell state from %d steps ago:" % ( numPrevPatterns - 1 - startOffset ) , self . _prevLrnPatterns [ startOffset ] ) inSequence = True for offset in range ( startOffset , numPrevPatterns ) : self . lrnPredictedState [ 't-1' ] [ : , : ] = self . lrnPredictedState [ 't' ] [ : , : ] self . lrnActiveState [ 't-1' ] [ : , : ] = self . lrnActiveState [ 't' ] [ : , : ] inputColumns = self . _prevLrnPatterns [ offset ] if not readOnly : self . _processSegmentUpdates ( inputColumns ) if offset == startOffset : self . lrnActiveState [ 't' ] . fill ( 0 ) for c in inputColumns : self . lrnActiveState [ 't' ] [ c , 0 ] = 1 inSequence = True else : inSequence = self . _learnPhase1 ( inputColumns , readOnly = readOnly ) if not inSequence or offset == currentTimeStepsOffset : break if self . verbosity >= 3 : print " backtrack: computing predictions from " , inputColumns self . _learnPhase2 ( readOnly = readOnly ) return inSequence
10612	def _calculate_T ( self , H ) : x = list ( ) x . append ( self . _T ) x . append ( self . _T + 10.0 ) y = list ( ) y . append ( self . _calculate_H ( x [ 0 ] ) - H ) y . append ( self . _calculate_H ( x [ 1 ] ) - H ) for i in range ( 2 , 50 ) : x . append ( x [ i - 1 ] - y [ i - 1 ] * ( ( x [ i - 1 ] - x [ i - 2 ] ) / ( y [ i - 1 ] - y [ i - 2 ] ) ) ) y . append ( self . _calculate_H ( x [ i ] ) - H ) if abs ( y [ i - 1 ] ) < 1.0e-5 : break return x [ len ( x ) - 1 ]
9877	def _ratio_metric ( v1 , v2 , ** _kwargs ) : return ( ( ( v1 - v2 ) / ( v1 + v2 ) ) ** 2 ) if v1 + v2 != 0 else 0
13505	def exists ( self , server ) : try : server . get ( 'challenge' , replacements = { 'slug' : self . slug } ) except Exception : return False return True
6208	def save_photon_hdf5 ( self , identity = None , overwrite = True , path = None ) : filepath = self . filepath if path is not None : filepath = Path ( path , filepath . name ) self . merge_da ( ) data = self . _make_photon_hdf5 ( identity = identity ) phc . hdf5 . save_photon_hdf5 ( data , h5_fname = str ( filepath ) , overwrite = overwrite )
10781	def feature_guess ( st , rad , invert = 'guess' , minmass = None , use_tp = False , trim_edge = False , ** kwargs ) : if invert == 'guess' : invert = guess_invert ( st ) if invert : im = 1 - st . residuals else : im = st . residuals return _feature_guess ( im , rad , minmass = minmass , use_tp = use_tp , trim_edge = trim_edge )
5126	def start_collecting_data ( self , queues = None , edge = None , edge_type = None ) : queues = _get_queues ( self . g , queues , edge , edge_type ) for k in queues : self . edge2queue [ k ] . collect_data = True
10583	def create_account ( self , name , number = None , description = None ) : new_account = GeneralLedgerAccount ( name , description , number , self . account_type ) new_account . set_parent_path ( self . path ) self . accounts . append ( new_account ) return new_account
5944	def isstream ( obj ) : signature_methods = ( "close" , ) alternative_methods = ( ( "read" , "readline" , "readlines" ) , ( "write" , "writeline" , "writelines" ) ) for m in signature_methods : if not hasmethod ( obj , m ) : return False alternative_results = [ numpy . all ( [ hasmethod ( obj , m ) for m in alternatives ] ) for alternatives in alternative_methods ] return numpy . any ( alternative_results )
4901	def get_course_enrollments ( self , enterprise_customer , days ) : return CourseEnrollment . objects . filter ( created__gt = datetime . datetime . now ( ) - datetime . timedelta ( days = days ) ) . filter ( user_id__in = enterprise_customer . enterprise_customer_users . values_list ( 'user_id' , flat = True ) )
3557	def find_device ( cls , timeout_sec = TIMEOUT_SEC ) : return get_provider ( ) . find_device ( service_uuids = cls . ADVERTISED , timeout_sec = timeout_sec )
6646	def _mirrorStructure ( dictionary , value ) : result = type ( dictionary ) ( ) for k in dictionary . keys ( ) : if isinstance ( dictionary [ k ] , dict ) : result [ k ] = _mirrorStructure ( dictionary [ k ] , value ) else : result [ k ] = value return result
10273	def remove_unweighted_sources ( graph : BELGraph , key : Optional [ str ] = None ) -> None : nodes = list ( get_unweighted_sources ( graph , key = key ) ) graph . remove_nodes_from ( nodes )
11601	def save_model ( self , request , obj , form , change ) : obj . author = request . user obj . save ( )
6336	def dist_abs ( self , src , tar , * args , ** kwargs ) : return self . dist ( src , tar , * args , ** kwargs )
8023	def retry ( exceptions = ( Exception , ) , interval = 0 , max_retries = 10 , success = None , timeout = - 1 ) : if not exceptions and success is None : raise TypeError ( '`exceptions` and `success` parameter can not both be None' ) exceptions = exceptions or ( _DummyException , ) _retries_error_msg = ( 'Exceeded maximum number of retries {} at ' 'an interval of {}s for function {}' ) _timeout_error_msg = 'Maximum timeout of {}s reached for function {}' @ decorator def wrapper ( func , * args , ** kwargs ) : signal . signal ( signal . SIGALRM , _timeout ( _timeout_error_msg . format ( timeout , func . __name__ ) ) ) run_func = functools . partial ( func , * args , ** kwargs ) logger = logging . getLogger ( func . __module__ ) if max_retries < 0 : iterator = itertools . count ( ) else : iterator = range ( max_retries ) if timeout > 0 : signal . alarm ( timeout ) for num , _ in enumerate ( iterator , 1 ) : try : result = run_func ( ) if success is None or success ( result ) : signal . alarm ( 0 ) return result except exceptions : logger . exception ( 'Exception experienced when trying function {}' . format ( func . __name__ ) ) if num == max_retries : raise logger . warning ( 'Retrying {} in {}s...' . format ( func . __name__ , interval ) ) time . sleep ( interval ) else : raise MaximumRetriesExceeded ( _retries_error_msg . format ( max_retries , interval , func . __name__ ) ) return wrapper
1633	def CheckForNewlineAtEOF ( filename , lines , error ) : if len ( lines ) < 3 or lines [ - 2 ] : error ( filename , len ( lines ) - 2 , 'whitespace/ending_newline' , 5 , 'Could not find a newline character at the end of the file.' )
10070	def preserve ( method = None , result = True , fields = None ) : if method is None : return partial ( preserve , result = result , fields = fields ) fields = fields or ( '_deposit' , ) @ wraps ( method ) def wrapper ( self , * args , ** kwargs ) : data = { field : self [ field ] for field in fields if field in self } result_ = method ( self , * args , ** kwargs ) replace = result_ if result else self for field in data : replace [ field ] = data [ field ] return result_ return wrapper
4454	def group_by ( self , fields , * reducers ) : group = Group ( fields , reducers ) self . _groups . append ( group ) return self
7887	def filter_mechanism_list ( mechanisms , properties , allow_insecure = False , server_side = False ) : result = [ ] for mechanism in mechanisms : try : if server_side : klass = SERVER_MECHANISMS_D [ mechanism ] else : klass = CLIENT_MECHANISMS_D [ mechanism ] except KeyError : logger . debug ( " skipping {0} - not supported" . format ( mechanism ) ) continue secure = properties . get ( "security-layer" ) if not allow_insecure and not klass . _pyxmpp_sasl_secure and not secure : logger . debug ( " skipping {0}, as it is not secure" . format ( mechanism ) ) continue if not klass . are_properties_sufficient ( properties ) : logger . debug ( " skipping {0}, as the properties are not sufficient" . format ( mechanism ) ) continue result . append ( mechanism ) return result
1358	def get_argument_component ( self ) : try : component = self . get_argument ( constants . PARAM_COMPONENT ) return component except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
8903	def add_syncable_models ( ) : import django . apps from morango . models import SyncableModel from morango . manager import SyncableModelManager from morango . query import SyncableModelQuerySet model_list = [ ] for model_class in django . apps . apps . get_models ( ) : if issubclass ( model_class , SyncableModel ) : name = model_class . __name__ if _multiple_self_ref_fk_check ( model_class ) : raise InvalidMorangoModelConfiguration ( "Syncing models with more than 1 self referential ForeignKey is not supported." ) try : from mptt import models from morango . utils . morango_mptt import MorangoMPTTModel , MorangoMPTTTreeManager , MorangoTreeQuerySet if issubclass ( model_class , models . MPTTModel ) : if not issubclass ( model_class , MorangoMPTTModel ) : raise InvalidMorangoModelConfiguration ( "{} that inherits from MPTTModel, should instead inherit from MorangoMPTTModel." . format ( name ) ) if not isinstance ( model_class . objects , MorangoMPTTTreeManager ) : raise InvalidMPTTManager ( "Manager for {} must inherit from MorangoMPTTTreeManager." . format ( name ) ) if not isinstance ( model_class . objects . none ( ) , MorangoTreeQuerySet ) : raise InvalidMPTTQuerySet ( "Queryset for {} model must inherit from MorangoTreeQuerySet." . format ( name ) ) except ImportError : pass if not isinstance ( model_class . objects , SyncableModelManager ) : raise InvalidSyncableManager ( "Manager for {} must inherit from SyncableModelManager." . format ( name ) ) if not isinstance ( model_class . objects . none ( ) , SyncableModelQuerySet ) : raise InvalidSyncableQueryset ( "Queryset for {} model must inherit from SyncableModelQuerySet." . format ( name ) ) if model_class . _meta . many_to_many : raise UnsupportedFieldType ( "{} model with a ManyToManyField is not supported in morango." ) if not hasattr ( model_class , 'morango_model_name' ) : raise InvalidMorangoModelConfiguration ( "{} model must define a morango_model_name attribute" . format ( name ) ) if not hasattr ( model_class , 'morango_profile' ) : raise InvalidMorangoModelConfiguration ( "{} model must define a morango_profile attribute" . format ( name ) ) profile = model_class . morango_profile _profile_models [ profile ] = _profile_models . get ( profile , [ ] ) if model_class . morango_model_name is not None : _insert_model_into_profile_dict ( model_class , profile ) for profile , model_list in iteritems ( _profile_models ) : syncable_models_dict = OrderedDict ( ) for model_class in model_list : syncable_models_dict [ model_class . morango_model_name ] = model_class _profile_models [ profile ] = syncable_models_dict
9312	def generate_key ( cls , secret_key , region , service , date , intermediates = False ) : init_key = ( 'AWS4' + secret_key ) . encode ( 'utf-8' ) date_key = cls . sign_sha256 ( init_key , date ) region_key = cls . sign_sha256 ( date_key , region ) service_key = cls . sign_sha256 ( region_key , service ) key = cls . sign_sha256 ( service_key , 'aws4_request' ) if intermediates : return ( key , date_key , region_key , service_key ) else : return key
180	def to_bounding_box ( self ) : from . bbs import BoundingBox if len ( self . coords ) == 0 : return None return BoundingBox ( x1 = np . min ( self . xx ) , y1 = np . min ( self . yy ) , x2 = np . max ( self . xx ) , y2 = np . max ( self . yy ) , label = self . label )
685	def getTotalw ( self ) : w = sum ( [ field . w for field in self . fields ] ) return w
32	def wrap_deepmind ( env , episode_life = True , clip_rewards = True , frame_stack = False , scale = False ) : if episode_life : env = EpisodicLifeEnv ( env ) if 'FIRE' in env . unwrapped . get_action_meanings ( ) : env = FireResetEnv ( env ) env = WarpFrame ( env ) if scale : env = ScaledFloatFrame ( env ) if clip_rewards : env = ClipRewardEnv ( env ) if frame_stack : env = FrameStack ( env , 4 ) return env
13155	def nt_cursor ( func ) : @ wraps ( func ) def wrapper ( cls , * args , ** kwargs ) : with ( yield from cls . get_cursor ( _CursorType . NAMEDTUPLE ) ) as c : return ( yield from func ( cls , c , * args , ** kwargs ) ) return wrapper
4718	def tsuite_enter ( trun , tsuite ) : if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:tsuite:enter { name: %r }" % tsuite [ "name" ] ) rcode = 0 for hook in tsuite [ "hooks" ] [ "enter" ] : rcode = script_run ( trun , hook ) if rcode : break if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:tsuite:enter { rcode: %r } " % rcode , rcode ) return rcode
7031	def specwindow_lsp ( times , mags , errs , magsarefluxes = False , startp = None , endp = None , stepsize = 1.0e-4 , autofreq = True , nbestpeaks = 5 , periodepsilon = 0.1 , sigclip = 10.0 , nworkers = None , glspfunc = _glsp_worker_specwindow , verbose = True ) : lspres = pgen_lsp ( times , mags , errs , magsarefluxes = magsarefluxes , startp = startp , endp = endp , autofreq = autofreq , nbestpeaks = nbestpeaks , periodepsilon = periodepsilon , stepsize = stepsize , nworkers = nworkers , sigclip = sigclip , glspfunc = glspfunc , verbose = verbose ) lspres [ 'method' ] = 'win' if lspres [ 'lspvals' ] is not None : lspmax = npnanmax ( lspres [ 'lspvals' ] ) if npisfinite ( lspmax ) : lspres [ 'lspvals' ] = lspres [ 'lspvals' ] / lspmax lspres [ 'nbestlspvals' ] = [ x / lspmax for x in lspres [ 'nbestlspvals' ] ] lspres [ 'bestlspval' ] = lspres [ 'bestlspval' ] / lspmax return lspres
6769	def install_apt ( self , fn = None , package_name = None , update = 0 , list_only = 0 ) : r = self . local_renderer assert self . genv [ ROLE ] apt_req_fqfn = fn or ( self . env . apt_requirments_fn and self . find_template ( self . env . apt_requirments_fn ) ) if not apt_req_fqfn : return [ ] assert os . path . isfile ( apt_req_fqfn ) lines = list ( self . env . apt_packages or [ ] ) for _ in open ( apt_req_fqfn ) . readlines ( ) : if _ . strip ( ) and not _ . strip ( ) . startswith ( '#' ) and ( not package_name or _ . strip ( ) == package_name ) : lines . extend ( _pkg . strip ( ) for _pkg in _ . split ( ' ' ) if _pkg . strip ( ) ) if list_only : return lines tmp_fn = r . write_temp_file ( '\n' . join ( lines ) ) apt_req_fqfn = tmp_fn if not self . genv . is_local : r . put ( local_path = tmp_fn , remote_path = tmp_fn ) apt_req_fqfn = self . genv . put_remote_path r . sudo ( 'DEBIAN_FRONTEND=noninteractive apt-get -yq update --fix-missing' ) r . sudo ( 'DEBIAN_FRONTEND=noninteractive apt-get -yq install `cat "%s" | tr "\\n" " "`' % apt_req_fqfn )
3865	async def leave ( self ) : is_group_conversation = ( self . _conversation . type == hangouts_pb2 . CONVERSATION_TYPE_GROUP ) try : if is_group_conversation : await self . _client . remove_user ( hangouts_pb2 . RemoveUserRequest ( request_header = self . _client . get_request_header ( ) , event_request_header = self . _get_event_request_header ( ) , ) ) else : await self . _client . delete_conversation ( hangouts_pb2 . DeleteConversationRequest ( request_header = self . _client . get_request_header ( ) , conversation_id = hangouts_pb2 . ConversationId ( id = self . id_ ) , delete_upper_bound_timestamp = parsers . to_timestamp ( datetime . datetime . now ( tz = datetime . timezone . utc ) ) ) ) except exceptions . NetworkError as e : logger . warning ( 'Failed to leave conversation: {}' . format ( e ) ) raise
5364	def stderr ( self ) : if self . _streaming : stderr = [ ] while not self . __stderr . empty ( ) : try : line = self . __stderr . get_nowait ( ) stderr . append ( line ) except : pass else : stderr = self . __stderr return stderr
9978	def find_funcdef ( source ) : try : module_node = compile ( source , "<string>" , mode = "exec" , flags = ast . PyCF_ONLY_AST ) except SyntaxError : return find_funcdef ( fix_lamdaline ( source ) ) for node in ast . walk ( module_node ) : if isinstance ( node , ast . FunctionDef ) or isinstance ( node , ast . Lambda ) : return node raise ValueError ( "function definition not found" )
4164	def embed_code_links ( app , exception ) : if exception is not None : return if not app . builder . config . plot_gallery : return if app . builder . name not in [ 'html' , 'readthedocs' ] : return print ( 'Embedding documentation hyperlinks in examples..' ) gallery_conf = app . config . sphinx_gallery_conf gallery_dirs = gallery_conf [ 'gallery_dirs' ] if not isinstance ( gallery_dirs , list ) : gallery_dirs = [ gallery_dirs ] for gallery_dir in gallery_dirs : _embed_code_links ( app , gallery_conf , gallery_dir )
5839	def submit_predict_request ( self , data_view_id , candidates , prediction_source = 'scalar' , use_prior = True ) : data = { "prediction_source" : prediction_source , "use_prior" : use_prior , "candidates" : candidates } failure_message = "Configuration creation failed" post_url = 'v1/data_views/' + str ( data_view_id ) + '/predict/submit' return self . _get_success_json ( self . _post_json ( post_url , data , failure_message = failure_message ) ) [ 'data' ] [ 'uid' ]
11319	def update_dois ( self ) : dois = record_get_field_instances ( self . record , '024' , ind1 = "7" ) all_dois = { } for field in dois : subs = field_get_subfield_instances ( field ) subs_dict = dict ( subs ) if subs_dict . get ( 'a' ) : if subs_dict [ 'a' ] in all_dois : record_delete_field ( self . record , tag = '024' , ind1 = '7' , field_position_global = field [ 4 ] ) continue all_dois [ subs_dict [ 'a' ] ] = field
9227	def NextPage ( gh ) : header = dict ( gh . getheaders ( ) ) if 'Link' in header : parts = header [ 'Link' ] . split ( ',' ) for part in parts : subparts = part . split ( ';' ) sub = subparts [ 1 ] . split ( '=' ) if sub [ 0 ] . strip ( ) == 'rel' : if sub [ 1 ] == '"next"' : page = int ( re . match ( r'.*page=(\d+).*' , subparts [ 0 ] , re . IGNORECASE | re . DOTALL | re . UNICODE ) . groups ( ) [ 0 ] ) return page return 0
2545	def add_review_comment ( self , doc , comment ) : if len ( doc . reviews ) != 0 : if not self . review_comment_set : self . review_comment_set = True doc . reviews [ - 1 ] . comment = comment return True else : raise CardinalityError ( 'ReviewComment' ) else : raise OrderError ( 'ReviewComment' )
6743	def render_to_file ( template , fn = None , extra = None , ** kwargs ) : import tempfile dryrun = get_dryrun ( kwargs . get ( 'dryrun' ) ) append_newline = kwargs . pop ( 'append_newline' , True ) style = kwargs . pop ( 'style' , 'cat' ) formatter = kwargs . pop ( 'formatter' , None ) content = render_to_string ( template , extra = extra ) if append_newline and not content . endswith ( '\n' ) : content += '\n' if formatter and callable ( formatter ) : content = formatter ( content ) if dryrun : if not fn : fd , fn = tempfile . mkstemp ( ) fout = os . fdopen ( fd , 'wt' ) fout . close ( ) else : if fn : fout = open ( fn , 'w' ) else : fd , fn = tempfile . mkstemp ( ) fout = os . fdopen ( fd , 'wt' ) fout . write ( content ) fout . close ( ) assert fn if style == 'cat' : cmd = 'cat <<EOF > %s\n%s\nEOF' % ( fn , content ) elif style == 'echo' : cmd = 'echo -e %s > %s' % ( shellquote ( content ) , fn ) else : raise NotImplementedError if BURLAP_COMMAND_PREFIX : print ( '%s run: %s' % ( render_command_prefix ( ) , cmd ) ) else : print ( cmd ) return fn
2321	def get_default ( self , * args , ** kwargs ) : def retrieve_param ( i ) : try : return self . __getattribute__ ( i ) except AttributeError : if i == "device" : return self . default_device else : return self . __getattribute__ ( i . upper ( ) ) if len ( args ) == 0 : if len ( kwargs ) == 1 and kwargs [ list ( kwargs . keys ( ) ) [ 0 ] ] is not None : return kwargs [ list ( kwargs . keys ( ) ) [ 0 ] ] elif len ( kwargs ) == 1 : return retrieve_param ( list ( kwargs . keys ( ) ) [ 0 ] ) else : raise TypeError ( "As dict is unordered, it is impossible to give" "the parameters in the correct order." ) else : out = [ ] for i in args : if i [ 1 ] is None : out . append ( retrieve_param ( i [ 0 ] ) ) else : out . append ( i [ 1 ] ) return out
8712	def file_list ( self ) : log . info ( 'Listing files' ) res = self . __exchange ( LIST_FILES ) res = res . split ( '\r\n' ) res = res [ 1 : - 1 ] files = [ ] for line in res : files . append ( line . split ( '\t' ) ) return files
6520	def files ( self , filters = None ) : filters = compile_masks ( filters or [ r'.*' ] ) for files in itervalues ( self . _found ) : for file_ in files : relpath = text_type ( Path ( file_ ) . relative_to ( self . base_path ) ) if matches_masks ( relpath , filters ) : yield file_
8879	def fit ( self , X , y = None ) : X = check_array ( X ) self . tree = BallTree ( X , leaf_size = self . leaf_size , metric = self . metric ) dist_train = self . tree . query ( X , k = 2 ) [ 0 ] if self . threshold == 'auto' : self . threshold_value = 0.5 * sqrt ( var ( dist_train [ : , 1 ] ) ) + mean ( dist_train [ : , 1 ] ) elif self . threshold == 'cv' : if y is None : raise ValueError ( "Y must be specified to find the optimal threshold." ) y = check_array ( y , accept_sparse = 'csc' , ensure_2d = False , dtype = None ) self . threshold_value = 0 score = 0 Y_pred , Y_true , AD = [ ] , [ ] , [ ] cv = KFold ( n_splits = 5 , random_state = 1 , shuffle = True ) for train_index , test_index in cv . split ( X ) : x_train = safe_indexing ( X , train_index ) x_test = safe_indexing ( X , test_index ) y_train = safe_indexing ( y , train_index ) y_test = safe_indexing ( y , test_index ) data_test = safe_indexing ( dist_train [ : , 1 ] , test_index ) if self . reg_model is None : reg_model = RandomForestRegressor ( n_estimators = 500 , random_state = 1 ) . fit ( x_train , y_train ) else : reg_model = clone ( self . reg_model ) . fit ( x_train , y_train ) Y_pred . append ( reg_model . predict ( x_test ) ) Y_true . append ( y_test ) AD . append ( data_test ) AD_ = unique ( hstack ( AD ) ) for z in AD_ : AD_new = hstack ( AD ) <= z if self . score == 'ba_ad' : val = balanced_accuracy_score_with_ad ( Y_true = hstack ( Y_true ) , Y_pred = hstack ( Y_pred ) , AD = AD_new ) elif self . score == 'rmse_ad' : val = rmse_score_with_ad ( Y_true = hstack ( Y_true ) , Y_pred = hstack ( Y_pred ) , AD = AD_new ) if val >= score : score = val self . threshold_value = z else : self . threshold_value = self . threshold return self
8813	def get_unused_ips ( session , used_ips_counts , ** kwargs ) : LOG . debug ( "Getting unused IPs..." ) with session . begin ( ) : query = session . query ( models . Subnet . segment_id , models . Subnet ) query = _filter ( query , ** kwargs ) query = query . group_by ( models . Subnet . segment_id , models . Subnet . id ) ret = defaultdict ( int ) for segment_id , subnet in query . all ( ) : net_size = netaddr . IPNetwork ( subnet . _cidr ) . size ip_policy = subnet [ "ip_policy" ] or { "size" : 0 } ret [ segment_id ] += net_size - ip_policy [ "size" ] for segment_id in used_ips_counts : ret [ segment_id ] -= used_ips_counts [ segment_id ] return ret
8017	async def websocket_disconnect ( self , message ) : self . closing = True await self . send_upstream ( message ) await super ( ) . websocket_disconnect ( message )
12202	def _from_jsonlines ( cls , lines , selector_handler = None , strict = False , debug = False ) : return cls ( json . loads ( "\n" . join ( [ l for l in lines if not cls . REGEX_COMMENT_LINE . match ( l ) ] ) ) , selector_handler = selector_handler , strict = strict , debug = debug )
5029	def transmit_content_metadata ( self , user ) : exporter = self . get_content_metadata_exporter ( user ) transmitter = self . get_content_metadata_transmitter ( ) transmitter . transmit ( exporter . export ( ) )
2574	def handle_app_update ( self , task_id , future , memo_cbk = False ) : if not self . tasks [ task_id ] [ 'app_fu' ] . done ( ) : logger . error ( "Internal consistency error: app_fu is not done for task {}" . format ( task_id ) ) if not self . tasks [ task_id ] [ 'app_fu' ] == future : logger . error ( "Internal consistency error: callback future is not the app_fu in task structure, for task {}" . format ( task_id ) ) if not memo_cbk : self . memoizer . update_memo ( task_id , self . tasks [ task_id ] , future ) if self . checkpoint_mode == 'task_exit' : self . checkpoint ( tasks = [ task_id ] ) if ( self . tasks [ task_id ] [ 'app_fu' ] and self . tasks [ task_id ] [ 'app_fu' ] . done ( ) and self . tasks [ task_id ] [ 'app_fu' ] . exception ( ) is None and self . tasks [ task_id ] [ 'executor' ] != 'data_manager' and self . tasks [ task_id ] [ 'func_name' ] != '_ftp_stage_in' and self . tasks [ task_id ] [ 'func_name' ] != '_http_stage_in' ) : for dfu in self . tasks [ task_id ] [ 'app_fu' ] . outputs : f = dfu . file_obj if isinstance ( f , File ) and f . is_remote ( ) : self . data_manager . stage_out ( f , self . tasks [ task_id ] [ 'executor' ] ) return
13339	def transpose ( a , axes = None ) : if isinstance ( a , np . ndarray ) : return np . transpose ( a , axes ) elif isinstance ( a , RemoteArray ) : return a . transpose ( * axes ) elif isinstance ( a , Remote ) : return _remote_to_array ( a ) . transpose ( * axes ) elif isinstance ( a , DistArray ) : if axes is None : axes = range ( a . ndim - 1 , - 1 , - 1 ) axes = list ( axes ) if len ( set ( axes ) ) < len ( axes ) : raise ValueError ( "repeated axis in transpose" ) if sorted ( axes ) != list ( range ( a . ndim ) ) : raise ValueError ( "axes don't match array" ) distaxis = a . _distaxis new_distaxis = axes . index ( distaxis ) new_subarrays = [ ra . transpose ( * axes ) for ra in a . _subarrays ] return DistArray ( new_subarrays , new_distaxis ) else : return np . transpose ( a , axes )
8824	def context ( self ) : if not self . _context : self . _context = context . get_admin_context ( ) return self . _context
4628	def get_private ( self ) : encoded = "%s %d" % ( self . brainkey , self . sequence ) a = _bytes ( encoded ) s = hashlib . sha256 ( hashlib . sha512 ( a ) . digest ( ) ) . digest ( ) return PrivateKey ( hexlify ( s ) . decode ( "ascii" ) , prefix = self . prefix )
3903	def _show_menu ( self ) : current_widget = self . _tabbed_window . get_current_widget ( ) if hasattr ( current_widget , 'get_menu_widget' ) : menu_widget = current_widget . get_menu_widget ( self . _hide_menu ) overlay = urwid . Overlay ( menu_widget , self . _tabbed_window , align = 'center' , width = ( 'relative' , 80 ) , valign = 'middle' , height = ( 'relative' , 80 ) ) self . _urwid_loop . widget = overlay
6503	def find_matches ( strings , words , length_hoped ) : lower_words = [ w . lower ( ) for w in words ] def has_match ( string ) : lower_string = string . lower ( ) for test_word in lower_words : if test_word in lower_string : return True return False shortened_strings = [ textwrap . wrap ( s ) for s in strings ] short_string_list = list ( chain . from_iterable ( shortened_strings ) ) matches = [ ms for ms in short_string_list if has_match ( ms ) ] cumulative_len = 0 break_at = None for idx , match in enumerate ( matches ) : cumulative_len += len ( match ) if cumulative_len >= length_hoped : break_at = idx break return matches [ 0 : break_at ]
6842	def force_stop ( self ) : r = self . local_renderer with self . settings ( warn_only = True ) : r . sudo ( 'pkill -9 -f celery' ) r . sudo ( 'rm -f /tmp/celery*.pid' )
3671	def identify_phase ( T , P , Tm = None , Tb = None , Tc = None , Psat = None ) : r if Tm and T <= Tm : return 's' elif Tc and T >= Tc : return 'g' elif Psat : if P <= Psat : return 'g' elif P > Psat : return 'l' elif Tb : if 9E4 < P < 1.1E5 : if T < Tb : return 'l' else : return 'g' elif P > 1.1E5 and T <= Tb : return 'l' else : return None else : return None
7419	def sample_cleanup ( data , sample ) : umap1file = os . path . join ( data . dirs . edits , sample . name + "-tmp-umap1.fastq" ) umap2file = os . path . join ( data . dirs . edits , sample . name + "-tmp-umap2.fastq" ) unmapped = os . path . join ( data . dirs . refmapping , sample . name + "-unmapped.bam" ) samplesam = os . path . join ( data . dirs . refmapping , sample . name + ".sam" ) split1 = os . path . join ( data . dirs . edits , sample . name + "-split1.fastq" ) split2 = os . path . join ( data . dirs . edits , sample . name + "-split2.fastq" ) refmap_derep = os . path . join ( data . dirs . edits , sample . name + "-refmap_derep.fastq" ) for f in [ umap1file , umap2file , unmapped , samplesam , split1 , split2 , refmap_derep ] : try : os . remove ( f ) except : pass
8662	def migrate ( src_path , src_passphrase , src_backend , dst_path , dst_passphrase , dst_backend ) : src_storage = STORAGE_MAPPING [ src_backend ] ( ** _parse_path_string ( src_path ) ) dst_storage = STORAGE_MAPPING [ dst_backend ] ( ** _parse_path_string ( dst_path ) ) src_stash = Stash ( src_storage , src_passphrase ) dst_stash = Stash ( dst_storage , dst_passphrase ) keys = src_stash . export ( ) dst_stash . load ( src_passphrase , keys = keys )
13284	def list_from_document ( cls , doc ) : objs = [ ] for feu in doc . xpath ( '//FEU' ) : detail_els = feu . xpath ( 'event-element-details/event-element-detail' ) for idx , detail in enumerate ( detail_els ) : objs . append ( cls ( feu , detail , id_suffix = idx , number_in_group = len ( detail_els ) ) ) return objs
4916	def entitlements ( self , request , pk = None ) : enterprise_customer_user = self . get_object ( ) instance = { "entitlements" : enterprise_customer_user . entitlements } serializer = serializers . EnterpriseCustomerUserEntitlementSerializer ( instance , context = { 'request' : request } ) return Response ( serializer . data )
913	def read ( cls , proto ) : instance = object . __new__ ( cls ) super ( PreviousValueModel , instance ) . __init__ ( proto = proto . modelBase ) instance . _logger = opf_utils . initLogger ( instance ) if len ( proto . predictedField ) : instance . _predictedField = proto . predictedField else : instance . _predictedField = None instance . _fieldNames = list ( proto . fieldNames ) instance . _fieldTypes = list ( proto . fieldTypes ) instance . _predictionSteps = list ( proto . predictionSteps ) return instance
5543	def clip ( self , array , geometries , inverted = False , clip_buffer = 0 ) : return commons_clip . clip_array_with_vector ( array , self . tile . affine , geometries , inverted = inverted , clip_buffer = clip_buffer * self . tile . pixel_x_size )
12103	def summary ( self ) : print ( "Type: %s" % self . __class__ . __name__ ) print ( "Batch Name: %r" % self . batch_name ) if self . tag : print ( "Tag: %s" % self . tag ) print ( "Root directory: %r" % self . get_root_directory ( ) ) print ( "Maximum concurrency: %s" % self . max_concurrency ) if self . description : print ( "Description: %s" % self . description )
3127	def update ( self , template_id , data ) : if 'name' not in data : raise KeyError ( 'The template must have a name' ) if 'html' not in data : raise KeyError ( 'The template must have html' ) self . template_id = template_id return self . _mc_client . _patch ( url = self . _build_path ( template_id ) , data = data )
4471	def _transform ( self , jam , state ) : if not hasattr ( jam . sandbox , 'muda' ) : raise RuntimeError ( 'No muda state found in jams sandbox.' ) jam_w = copy . deepcopy ( jam ) jam_w . sandbox . muda [ 'history' ] . append ( { 'transformer' : self . __serialize__ , 'state' : state } ) if hasattr ( self , 'audio' ) : self . audio ( jam_w . sandbox . muda , state ) if hasattr ( self , 'metadata' ) : self . metadata ( jam_w . file_metadata , state ) for query , function_name in six . iteritems ( self . dispatch ) : function = getattr ( self , function_name ) for matched_annotation in jam_w . search ( namespace = query ) : function ( matched_annotation , state ) return jam_w
9972	def read_range ( filepath , range_expr , sheet = None , dict_generator = None ) : def default_generator ( cells ) : for row_ind , row in enumerate ( cells ) : for col_ind , cell in enumerate ( row ) : yield ( row_ind , col_ind ) , cell . value book = opxl . load_workbook ( filepath , data_only = True ) if _is_range_address ( range_expr ) : sheet_names = [ name . upper ( ) for name in book . sheetnames ] index = sheet_names . index ( sheet . upper ( ) ) cells = book . worksheets [ index ] [ range_expr ] else : cells = _get_namedrange ( book , range_expr , sheet ) if isinstance ( cells , opxl . cell . Cell ) : return cells . value if dict_generator is None : dict_generator = default_generator gen = dict_generator ( cells ) return { keyval [ 0 ] : keyval [ 1 ] for keyval in gen }
10082	def _prepare_edit ( self , record ) : data = record . dumps ( ) data [ '_deposit' ] [ 'pid' ] [ 'revision_id' ] = record . revision_id data [ '_deposit' ] [ 'status' ] = 'draft' data [ '$schema' ] = self . build_deposit_schema ( record ) return data
12447	def render_to_string ( self ) : values = '' for key , value in self . items ( ) : values += '{}={};' . format ( key , value ) return values
4550	def fill_round_rect ( setter , x , y , w , h , r , color = None , aa = False ) : fill_rect ( setter , x + r , y , w - 2 * r , h , color , aa ) _fill_circle_helper ( setter , x + w - r - 1 , y + r , r , 1 , h - 2 * r - 1 , color , aa ) _fill_circle_helper ( setter , x + r , y + r , r , 2 , h - 2 * r - 1 , color , aa )
6351	def _phonetic_numbers ( self , phonetic ) : phonetic_array = phonetic . split ( '-' ) result = ' ' . join ( [ self . _pnums_with_leading_space ( i ) [ 1 : ] for i in phonetic_array ] ) return result
11012	def write ( context ) : config = context . obj title = click . prompt ( 'Title' ) author = click . prompt ( 'Author' , default = config . get ( 'DEFAULT_AUTHOR' ) ) slug = slugify ( title ) creation_date = datetime . now ( ) basename = '{:%Y-%m-%d}_{}.md' . format ( creation_date , slug ) meta = ( ( 'Title' , title ) , ( 'Date' , '{:%Y-%m-%d %H:%M}:00' . format ( creation_date ) ) , ( 'Modified' , '{:%Y-%m-%d %H:%M}:00' . format ( creation_date ) ) , ( 'Author' , author ) , ) file_content = '' for key , value in meta : file_content += '{}: {}\n' . format ( key , value ) file_content += '\n\n' file_content += 'Text...\n\n' file_content += '![image description]({filename}/images/my-photo.jpg)\n\n' file_content += 'Text...\n\n' os . makedirs ( config [ 'CONTENT_DIR' ] , exist_ok = True ) path = os . path . join ( config [ 'CONTENT_DIR' ] , basename ) with click . open_file ( path , 'w' ) as f : f . write ( file_content ) click . echo ( path ) click . launch ( path )
6918	def _get_acf_peakheights ( lags , acf , npeaks = 20 , searchinterval = 1 ) : maxinds = argrelmax ( acf , order = searchinterval ) [ 0 ] maxacfs = acf [ maxinds ] maxlags = lags [ maxinds ] mininds = argrelmin ( acf , order = searchinterval ) [ 0 ] minacfs = acf [ mininds ] minlags = lags [ mininds ] relpeakheights = npzeros ( npeaks ) relpeaklags = npzeros ( npeaks , dtype = npint64 ) peakindices = npzeros ( npeaks , dtype = npint64 ) for peakind , mxi in enumerate ( maxinds [ : npeaks ] ) : if npall ( mxi < mininds ) : continue leftminind = mininds [ mininds < mxi ] [ - 1 ] rightminind = mininds [ mininds > mxi ] [ 0 ] relpeakheights [ peakind ] = ( acf [ mxi ] - ( acf [ leftminind ] + acf [ rightminind ] ) / 2.0 ) relpeaklags [ peakind ] = lags [ mxi ] peakindices [ peakind ] = peakind if relpeakheights [ 0 ] > relpeakheights [ 1 ] : bestlag = relpeaklags [ 0 ] bestpeakheight = relpeakheights [ 0 ] bestpeakindex = peakindices [ 0 ] else : bestlag = relpeaklags [ 1 ] bestpeakheight = relpeakheights [ 1 ] bestpeakindex = peakindices [ 1 ] return { 'maxinds' : maxinds , 'maxacfs' : maxacfs , 'maxlags' : maxlags , 'mininds' : mininds , 'minacfs' : minacfs , 'minlags' : minlags , 'relpeakheights' : relpeakheights , 'relpeaklags' : relpeaklags , 'peakindices' : peakindices , 'bestlag' : bestlag , 'bestpeakheight' : bestpeakheight , 'bestpeakindex' : bestpeakindex }
707	def _engineServicesRunning ( ) : process = subprocess . Popen ( [ "ps" , "aux" ] , stdout = subprocess . PIPE ) stdout = process . communicate ( ) [ 0 ] result = process . returncode if result != 0 : raise RuntimeError ( "Unable to check for running client job manager" ) running = False for line in stdout . split ( "\n" ) : if "python" in line and "clientjobmanager.client_job_manager" in line : running = True break return running
12912	def append ( self , item ) : if self . meta_type == 'dict' : raise AssertionError ( 'Cannot append to object of `dict` base type!' ) if self . meta_type == 'list' : self . _list . append ( item ) return
10719	def x10_command ( self , house_code , unit_number , state ) : house_code = normalize_housecode ( house_code ) if unit_number is not None : unit_number = normalize_unitnumber ( unit_number ) return self . _x10_command ( house_code , unit_number , state )
7303	def set_mongonaut_base ( self ) : if hasattr ( self , "app_label" ) : return None self . app_label = self . kwargs . get ( 'app_label' ) self . document_name = self . kwargs . get ( 'document_name' ) self . models_name = self . kwargs . get ( 'models_name' , 'models' ) self . model_name = "{0}.{1}" . format ( self . app_label , self . models_name ) self . models = import_module ( self . model_name )
12777	def resorted ( values ) : if not values : return values values = sorted ( values ) first_word = next ( ( cnt for cnt , val in enumerate ( values ) if val and not val [ 0 ] . isdigit ( ) ) , None ) if first_word is None : return values words = values [ first_word : ] numbers = values [ : first_word ] return words + numbers
680	def getAllRecords ( self ) : values = [ ] numRecords = self . fields [ 0 ] . numRecords assert ( all ( field . numRecords == numRecords for field in self . fields ) ) for x in range ( numRecords ) : values . append ( self . getRecord ( x ) ) return values
7884	def _make_ns_declarations ( declarations , declared_prefixes ) : result = [ ] for namespace , prefix in declarations . items ( ) : if prefix : result . append ( u' xmlns:{0}={1}' . format ( prefix , quoteattr ( namespace ) ) ) else : result . append ( u' xmlns={1}' . format ( prefix , quoteattr ( namespace ) ) ) for d_namespace , d_prefix in declared_prefixes . items ( ) : if ( not prefix and not d_prefix ) or d_prefix == prefix : if namespace != d_namespace : del declared_prefixes [ d_namespace ] return u" " . join ( result )
3183	def update ( self , store_id , data ) : self . store_id = store_id return self . _mc_client . _patch ( url = self . _build_path ( store_id ) , data = data )
2821	def convert_dropout ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting dropout ...' ) if names == 'short' : tf_name = 'DO' + random_string ( 6 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) dropout = keras . layers . Dropout ( rate = params [ 'ratio' ] , name = tf_name ) layers [ scope_name ] = dropout ( layers [ inputs [ 0 ] ] )
2322	def read_causal_pairs ( filename , scale = True , ** kwargs ) : def convert_row ( row , scale ) : a = row [ "A" ] . split ( " " ) b = row [ "B" ] . split ( " " ) if a [ 0 ] == "" : a . pop ( 0 ) b . pop ( 0 ) if a [ - 1 ] == "" : a . pop ( - 1 ) b . pop ( - 1 ) a = array ( [ float ( i ) for i in a ] ) b = array ( [ float ( i ) for i in b ] ) if scale : a = scaler ( a ) b = scaler ( b ) return row [ 'SampleID' ] , a , b if isinstance ( filename , str ) : data = read_csv ( filename , ** kwargs ) elif isinstance ( filename , DataFrame ) : data = filename else : raise TypeError ( "Type not supported." ) conv_data = [ ] for idx , row in data . iterrows ( ) : conv_data . append ( convert_row ( row , scale ) ) df = DataFrame ( conv_data , columns = [ 'SampleID' , 'A' , 'B' ] ) df = df . set_index ( "SampleID" ) return df
12355	def delete ( self , wait = True ) : resp = self . parent . delete ( self . id ) if wait : self . wait ( ) return resp
12638	def merge_groups ( self , indices ) : try : merged = merge_dict_of_lists ( self . dicom_groups , indices , pop_later = True , copy = True ) self . dicom_groups = merged except IndexError : raise IndexError ( 'Index out of range to merge DICOM groups.' )
1480	def _start_processes ( self , commands ) : Log . info ( "Start processes" ) processes_to_monitor = { } for ( name , command ) in commands . items ( ) : p = self . _run_process ( name , command ) processes_to_monitor [ p . pid ] = ProcessInfo ( p , name , command ) log_pid_for_process ( name , p . pid ) with self . process_lock : self . processes_to_monitor . update ( processes_to_monitor )
10870	def calc_pts_lag ( npts = 20 ) : scl = { 15 : 0.072144 , 20 : 0.051532 , 25 : 0.043266 } [ npts ] pts0 , wts0 = np . polynomial . laguerre . laggauss ( npts ) pts = np . sinh ( pts0 * scl ) wts = scl * wts0 * np . cosh ( pts0 * scl ) * np . exp ( pts0 ) return pts , wts
3013	def locked_delete ( self ) : filters = { self . key_name : self . key_value } self . session . query ( self . model_class ) . filter_by ( ** filters ) . delete ( )
10181	def _events_process ( event_types = None , eager = False ) : event_types = event_types or list ( current_stats . enabled_events ) if eager : process_events . apply ( ( event_types , ) , throw = True ) click . secho ( 'Events processed successfully.' , fg = 'green' ) else : process_events . delay ( event_types ) click . secho ( 'Events processing task sent...' , fg = 'yellow' )
12160	def parent ( groups , ID ) : if ID in groups . keys ( ) : return ID if not ID in groups . keys ( ) : for actualParent in groups . keys ( ) : if ID in groups [ actualParent ] : return actualParent return None
872	def _setPath ( cls ) : cls . _path = os . path . join ( os . environ [ 'NTA_DYNAMIC_CONF_DIR' ] , cls . customFileName )
3590	def get_provider ( ) : global _provider if _provider is None : if sys . platform . startswith ( 'linux' ) : from . bluez_dbus . provider import BluezProvider _provider = BluezProvider ( ) elif sys . platform == 'darwin' : from . corebluetooth . provider import CoreBluetoothProvider _provider = CoreBluetoothProvider ( ) else : raise RuntimeError ( 'Sorry the {0} platform is not supported by the BLE library!' . format ( sys . platform ) ) return _provider
12914	def write_json ( self , fh , pretty = True ) : sjson = json . JSONEncoder ( ) . encode ( self . json ( ) ) if pretty : json . dump ( json . loads ( sjson ) , fh , sort_keys = True , indent = 4 ) else : json . dump ( json . loads ( sjson ) , fh ) return
12457	def iteritems ( data , ** kwargs ) : return iter ( data . items ( ** kwargs ) ) if IS_PY3 else data . iteritems ( ** kwargs )
13490	def reconcile ( self , server ) : if not self . challenge . exists ( server ) : raise Exception ( 'Challenge does not exist on server' ) existing = MapRouletteTaskCollection . from_server ( server , self . challenge ) same = [ ] new = [ ] changed = [ ] deleted = [ ] for task in self . tasks : if task . identifier in [ existing_task . identifier for existing_task in existing . tasks ] : if task == existing . get_by_identifier ( task . identifier ) : same . append ( task ) else : changed . append ( task ) else : new . append ( task ) for task in existing . tasks : if task . identifier not in [ task . identifier for task in self . tasks ] : deleted . append ( task ) if new : newCollection = MapRouletteTaskCollection ( self . challenge , tasks = new ) newCollection . create ( server ) if changed : changedCollection = MapRouletteTaskCollection ( self . challenge , tasks = changed ) changedCollection . update ( server ) if deleted : deletedCollection = MapRouletteTaskCollection ( self . challenge , tasks = deleted ) for task in deletedCollection . tasks : task . status = 'deleted' deletedCollection . update ( server ) return { 'same' : same , 'new' : new , 'changed' : changed , 'deleted' : deleted }
8629	def create_hireme_project ( session , title , description , currency , budget , jobs , hireme_initial_bid ) : jobs . append ( create_job_object ( id = 417 ) ) project_data = { 'title' : title , 'description' : description , 'currency' : currency , 'budget' : budget , 'jobs' : jobs , 'hireme' : True , 'hireme_initial_bid' : hireme_initial_bid } response = make_post_request ( session , 'projects' , json_data = project_data ) json_data = response . json ( ) if response . status_code == 200 : project_data = json_data [ 'result' ] p = Project ( project_data ) p . url = urljoin ( session . url , 'projects/%s' % p . seo_url ) return p else : raise ProjectNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] , )
3608	def put_async ( self , url , name , data , callback = None , params = None , headers = None ) : if name is None : name = '' params = params or { } headers = headers or { } endpoint = self . _build_endpoint_url ( url , name ) self . _authenticate ( params , headers ) data = json . dumps ( data , cls = JSONEncoder ) process_pool . apply_async ( make_put_request , args = ( endpoint , data , params , headers ) , callback = callback )
8345	def find ( self , name = None , attrs = { } , recursive = True , text = None , ** kwargs ) : r = None l = self . findAll ( name , attrs , recursive , text , 1 , ** kwargs ) if l : r = l [ 0 ] return r
3103	def do_GET ( self ) : self . send_response ( http_client . OK ) self . send_header ( 'Content-type' , 'text/html' ) self . end_headers ( ) parts = urllib . parse . urlparse ( self . path ) query = _helpers . parse_unique_urlencoded ( parts . query ) self . server . query_params = query self . wfile . write ( b'<html><head><title>Authentication Status</title></head>' ) self . wfile . write ( b'<body><p>The authentication flow has completed.</p>' ) self . wfile . write ( b'</body></html>' )
5228	def load_info ( cat ) : res = _load_yaml_ ( f'{PKG_PATH}/markets/{cat}.yml' ) root = os . environ . get ( 'BBG_ROOT' , '' ) . replace ( '\\' , '/' ) if not root : return res for cat , ovrd in _load_yaml_ ( f'{root}/markets/{cat}.yml' ) . items ( ) : if isinstance ( ovrd , dict ) : if cat in res : res [ cat ] . update ( ovrd ) else : res [ cat ] = ovrd if isinstance ( ovrd , list ) and isinstance ( res [ cat ] , list ) : res [ cat ] += ovrd return res
2558	def clean_pair ( cls , attribute , value ) : attribute = cls . clean_attribute ( attribute ) if value is True : value = attribute if value is False : value = "false" return ( attribute , value )
10215	def rank_subgraph_by_node_filter ( graph : BELGraph , node_predicates : Union [ NodePredicate , Iterable [ NodePredicate ] ] , annotation : str = 'Subgraph' , reverse : bool = True , ) -> List [ Tuple [ str , int ] ] : r1 = group_nodes_by_annotation_filtered ( graph , node_predicates = node_predicates , annotation = annotation ) r2 = count_dict_values ( r1 ) return sorted ( r2 . items ( ) , key = itemgetter ( 1 ) , reverse = reverse )
6623	def _getTarball ( url , into_directory , cache_key , origin_info = None ) : try : access_common . unpackFromCache ( cache_key , into_directory ) except KeyError as e : tok = settings . getProperty ( 'github' , 'authtoken' ) headers = { } if tok is not None : headers [ 'Authorization' ] = 'token ' + str ( tok ) logger . debug ( 'GET %s' , url ) response = requests . get ( url , allow_redirects = True , stream = True , headers = headers ) response . raise_for_status ( ) logger . debug ( 'getting file: %s' , url ) logger . debug ( 'headers: %s' , response . headers ) response . raise_for_status ( ) access_common . unpackTarballStream ( stream = response , into_directory = into_directory , hash = { } , cache_key = cache_key , origin_info = origin_info )
6730	def init_env ( ) : env . ROLES_DIR = ROLE_DIR env . services = [ ] env . confirm_deployment = False env . is_local = None env . base_config_dir = '.' env . src_dir = 'src' env . sites = { } env [ SITE ] = None env [ ROLE ] = None env . hosts_retriever = None env . hosts_retrievers = type ( env ) ( ) env . hostname_translator = 'default' env . hostname_translators = type ( env ) ( ) env . hostname_translators . default = lambda hostname : hostname env . default_site = None env . available_sites = [ ] env . available_sites_by_host = { } env . disk_usage_command = "df -H | grep -vE '^Filesystem|tmpfs|cdrom|none' | awk '{print $5 " " $1}'" env . burlap_data_dir = '.burlap' env . setdefault ( 'roledefs' , { } ) env . setdefault ( 'roles' , [ ] ) env . setdefault ( 'hosts' , [ ] ) env . setdefault ( 'exclude_hosts' , [ ] )
2729	def get_kernel_available ( self ) : kernels = list ( ) data = self . get_data ( "droplets/%s/kernels/" % self . id ) while True : for jsond in data [ u'kernels' ] : kernel = Kernel ( ** jsond ) kernel . token = self . token kernels . append ( kernel ) try : url = data [ u'links' ] [ u'pages' ] . get ( u'next' ) if not url : break data = self . get_data ( url ) except KeyError : break return kernels
628	def _orderForCoordinate ( cls , coordinate ) : seed = cls . _hashCoordinate ( coordinate ) rng = Random ( seed ) return rng . getReal64 ( )
54	def shift ( self , x = 0 , y = 0 ) : keypoints = [ keypoint . shift ( x = x , y = y ) for keypoint in self . keypoints ] return self . deepcopy ( keypoints )
11223	def dump_set ( self , obj , class_name = set_class_name ) : return { "$" + class_name : [ self . _json_convert ( item ) for item in obj ] }
3081	def xsrf_secret_key ( ) : secret = memcache . get ( XSRF_MEMCACHE_ID , namespace = OAUTH2CLIENT_NAMESPACE ) if not secret : model = SiteXsrfSecretKey . get_or_insert ( key_name = 'site' ) if not model . secret : model . secret = _generate_new_xsrf_secret_key ( ) model . put ( ) secret = model . secret memcache . add ( XSRF_MEMCACHE_ID , secret , namespace = OAUTH2CLIENT_NAMESPACE ) return str ( secret )
6038	def yticks ( self ) : return np . linspace ( np . min ( self [ : , 0 ] ) , np . max ( self [ : , 0 ] ) , 4 )
11265	def readline ( prev , filename = None , mode = 'r' , trim = str . rstrip , start = 1 , end = sys . maxsize ) : if prev is None : if filename is None : raise Exception ( 'No input available for readline.' ) elif is_str_type ( filename ) : file_list = [ filename , ] else : file_list = filename else : file_list = prev for fn in file_list : if isinstance ( fn , file_type ) : fd = fn else : fd = open ( fn , mode ) try : if start <= 1 and end == sys . maxsize : for line in fd : yield trim ( line ) else : for line_no , line in enumerate ( fd , 1 ) : if line_no < start : continue yield trim ( line ) if line_no >= end : break finally : if fd != fn : fd . close ( )
8296	def render ( self , size , frame , drawqueue ) : r_context = self . create_rcontext ( size , frame ) drawqueue . render ( r_context ) self . rendering_finished ( size , frame , r_context ) return r_context
9597	def save_screenshot ( self , filename , quietly = False ) : imgData = self . take_screenshot ( ) try : with open ( filename , "wb" ) as f : f . write ( b64decode ( imgData . encode ( 'ascii' ) ) ) except IOError as err : if not quietly : raise err
8181	def remove_node ( self , id ) : if self . has_key ( id ) : n = self [ id ] self . nodes . remove ( n ) del self [ id ] for e in list ( self . edges ) : if n in ( e . node1 , e . node2 ) : if n in e . node1 . links : e . node1 . links . remove ( n ) if n in e . node2 . links : e . node2 . links . remove ( n ) self . edges . remove ( e )
2429	def reset_document ( self ) : self . doc_version_set = False self . doc_comment_set = False self . doc_namespace_set = False self . doc_data_lics_set = False self . doc_name_set = False self . doc_spdx_id_set = False
4324	def convert ( self , samplerate = None , n_channels = None , bitdepth = None ) : bitdepths = [ 8 , 16 , 24 , 32 , 64 ] if bitdepth is not None : if bitdepth not in bitdepths : raise ValueError ( "bitdepth must be one of {}." . format ( str ( bitdepths ) ) ) self . output_format . extend ( [ '-b' , '{}' . format ( bitdepth ) ] ) if n_channels is not None : if not isinstance ( n_channels , int ) or n_channels <= 0 : raise ValueError ( "n_channels must be a positive integer." ) self . output_format . extend ( [ '-c' , '{}' . format ( n_channels ) ] ) if samplerate is not None : if not is_number ( samplerate ) or samplerate <= 0 : raise ValueError ( "samplerate must be a positive number." ) self . rate ( samplerate ) return self
11376	def _normalize_issue_dir_with_dtd ( self , path ) : if exists ( join ( path , 'resolved_issue.xml' ) ) : return issue_xml_content = open ( join ( path , 'issue.xml' ) ) . read ( ) sis = [ 'si510.dtd' , 'si520.dtd' , 'si540.dtd' ] tmp_extracted = 0 for si in sis : if si in issue_xml_content : self . _extract_correct_dtd_package ( si . split ( '.' ) [ 0 ] , path ) tmp_extracted = 1 if not tmp_extracted : message = "It looks like the path " + path message += " does not contain an si510, si520 or si540 in issue.xml file" self . logger . error ( message ) raise ValueError ( message ) command = [ "xmllint" , "--format" , "--loaddtd" , join ( path , 'issue.xml' ) , "--output" , join ( path , 'resolved_issue.xml' ) ] dummy , dummy , cmd_err = run_shell_command ( command ) if cmd_err : message = "Error in cleaning %s: %s" % ( join ( path , 'issue.xml' ) , cmd_err ) self . logger . error ( message ) raise ValueError ( message )
10374	def get_cutoff ( value : float , cutoff : Optional [ float ] = None ) -> int : cutoff = cutoff if cutoff is not None else 0 if value > cutoff : return 1 if value < ( - 1 * cutoff ) : return - 1 return 0
13291	def get_variables_by_attributes ( self , ** kwargs ) : vs = [ ] has_value_flag = False for vname in self . variables : var = self . variables [ vname ] for k , v in kwargs . items ( ) : if callable ( v ) : has_value_flag = v ( getattr ( var , k , None ) ) if has_value_flag is False : break elif hasattr ( var , k ) and getattr ( var , k ) == v : has_value_flag = True else : has_value_flag = False break if has_value_flag is True : vs . append ( self . variables [ vname ] ) return vs
2745	def create ( self ) : input_params = { "name" : self . name , "public_key" : self . public_key , } data = self . get_data ( "account/keys/" , type = POST , params = input_params ) if data : self . id = data [ 'ssh_key' ] [ 'id' ]
7085	def precess_coordinates ( ra , dec , epoch_one , epoch_two , jd = None , mu_ra = 0.0 , mu_dec = 0.0 , outscalar = False ) : raproc , decproc = np . radians ( ra ) , np . radians ( dec ) if ( ( mu_ra != 0.0 ) and ( mu_dec != 0.0 ) and jd ) : jd_epoch_one = JD2000 + ( epoch_one - epoch_two ) * 365.25 raproc = ( raproc + ( jd - jd_epoch_one ) * mu_ra * MAS_P_YR_TO_RAD_P_DAY / np . cos ( decproc ) ) decproc = decproc + ( jd - jd_epoch_one ) * mu_dec * MAS_P_YR_TO_RAD_P_DAY ca = np . cos ( raproc ) cd = np . cos ( decproc ) sa = np . sin ( raproc ) sd = np . sin ( decproc ) if epoch_one != epoch_two : t1 = 1.0e-3 * ( epoch_two - epoch_one ) t2 = 1.0e-3 * ( epoch_one - 2000.0 ) a = ( t1 * ARCSEC_TO_RADIANS * ( 23062.181 + t2 * ( 139.656 + 0.0139 * t2 ) + t1 * ( 30.188 - 0.344 * t2 + 17.998 * t1 ) ) ) b = t1 * t1 * ARCSEC_TO_RADIANS * ( 79.280 + 0.410 * t2 + 0.205 * t1 ) + a c = ( ARCSEC_TO_RADIANS * t1 * ( 20043.109 - t2 * ( 85.33 + 0.217 * t2 ) + t1 * ( - 42.665 - 0.217 * t2 - 41.833 * t2 ) ) ) sina , sinb , sinc = np . sin ( a ) , np . sin ( b ) , np . sin ( c ) cosa , cosb , cosc = np . cos ( a ) , np . cos ( b ) , np . cos ( c ) precmatrix = np . matrix ( [ [ cosa * cosb * cosc - sina * sinb , sina * cosb + cosa * sinb * cosc , cosa * sinc ] , [ - cosa * sinb - sina * cosb * cosc , cosa * cosb - sina * sinb * cosc , - sina * sinc ] , [ - cosb * sinc , - sinb * sinc , cosc ] ] ) precmatrix = precmatrix . transpose ( ) x = ( np . matrix ( [ cd * ca , cd * sa , sd ] ) ) . transpose ( ) x2 = precmatrix * x outra = np . arctan2 ( x2 [ 1 ] , x2 [ 0 ] ) outdec = np . arcsin ( x2 [ 2 ] ) outradeg = np . rad2deg ( outra ) outdecdeg = np . rad2deg ( outdec ) if outradeg < 0.0 : outradeg = outradeg + 360.0 if outscalar : return float ( outradeg ) , float ( outdecdeg ) else : return outradeg , outdecdeg else : return np . degrees ( raproc ) , np . degrees ( decproc )
2851	def _mpsse_sync ( self , max_retries = 10 ) : self . _write ( '\xAB' ) tries = 0 sync = False while not sync : data = self . _poll_read ( 2 ) if data == '\xFA\xAB' : sync = True tries += 1 if tries >= max_retries : raise RuntimeError ( 'Could not synchronize with FT232H!' )
11607	def social_widget_render ( parser , token ) : bits = token . split_contents ( ) tag_name = bits [ 0 ] if len ( bits ) < 2 : raise TemplateSyntaxError ( "'%s' takes at least one argument" % tag_name ) args = [ ] kwargs = { } bits = bits [ 1 : ] if len ( bits ) : for bit in bits : match = kwarg_re . match ( bit ) if not match : raise TemplateSyntaxError ( "Malformed arguments to %s tag" % tag_name ) name , value = match . groups ( ) if name : name = name . replace ( '-' , '_' ) kwargs [ name ] = parser . compile_filter ( value ) else : args . append ( parser . compile_filter ( value ) ) return SocialWidgetNode ( args , kwargs )
12597	def _check_xl_path ( xl_path : str ) : xl_path = op . abspath ( op . expanduser ( xl_path ) ) if not op . isfile ( xl_path ) : raise IOError ( "Could not find file in {}." . format ( xl_path ) ) return xl_path , _use_openpyxl_or_xlrf ( xl_path )
8490	def start_watching ( self ) : if self . watcher and self . watcher . is_alive ( ) : return self . watcher = Watcher ( ) self . watcher . start ( )
8738	def get_ports_count ( context , filters = None ) : LOG . info ( "get_ports_count for tenant %s filters %s" % ( context . tenant_id , filters ) ) return db_api . port_count_all ( context , join_security_groups = True , ** filters )
12686	def find ( self , * args ) : curr_node = self . __root return self . __traverse ( curr_node , 0 , * args )
7396	def get_publication_list ( context , list , template = 'publications/publications.html' ) : list = List . objects . filter ( list__iexact = list ) if not list : return '' list = list [ 0 ] publications = list . publication_set . all ( ) publications = publications . order_by ( '-year' , '-month' , '-id' ) if not publications : return '' populate ( publications ) return render_template ( template , context [ 'request' ] , { 'list' : list , 'publications' : publications } )
7269	def attribute ( * args , ** kw ) : return operator ( kind = Operator . Type . ATTRIBUTE , * args , ** kw )
5805	def get_dh_params_length ( server_handshake_bytes ) : output = None dh_params_bytes = None for record_type , _ , record_data in parse_tls_records ( server_handshake_bytes ) : if record_type != b'\x16' : continue for message_type , message_data in parse_handshake_messages ( record_data ) : if message_type == b'\x0c' : dh_params_bytes = message_data break if dh_params_bytes : break if dh_params_bytes : output = int_from_bytes ( dh_params_bytes [ 0 : 2 ] ) * 8 return output
11038	def maybe_key_vault ( client , mount_path ) : d = client . read_kv2 ( 'client_key' , mount_path = mount_path ) def get_or_create_key ( client_key ) : if client_key is not None : key_data = client_key [ 'data' ] [ 'data' ] key = _load_pem_private_key_bytes ( key_data [ 'key' ] . encode ( 'utf-8' ) ) return JWKRSA ( key = key ) else : key = generate_private_key ( u'rsa' ) key_data = { 'key' : _dump_pem_private_key_bytes ( key ) . decode ( 'utf-8' ) } d = client . create_or_update_kv2 ( 'client_key' , key_data , mount_path = mount_path ) return d . addCallback ( lambda _result : JWKRSA ( key = key ) ) return d . addCallback ( get_or_create_key )
13860	def is_date_type ( cls ) : if not isinstance ( cls , type ) : return False return issubclass ( cls , date ) and not issubclass ( cls , datetime )
13353	def status_job ( self , fn = None , name = None , timeout = 3 ) : if fn is None : def decorator ( fn ) : self . add_status_job ( fn , name , timeout ) return decorator else : self . add_status_job ( fn , name , timeout )
7216	def register ( self , task_json = None , json_filename = None ) : if not task_json and not json_filename : raise Exception ( "Both task json and filename can't be none." ) if task_json and json_filename : raise Exception ( "Both task json and filename can't be provided." ) if json_filename : task_json = json . load ( open ( json_filename , 'r' ) ) r = self . gbdx_connection . post ( self . _base_url , json = task_json ) raise_for_status ( r ) return r . text
7378	def process_keys ( func ) : @ wraps ( func ) def decorated ( self , k , * args ) : if not isinstance ( k , str ) : msg = "%s: key must be a string" % self . __class__ . __name__ raise ValueError ( msg ) if not k . startswith ( self . prefix ) : k = self . prefix + k return func ( self , k , * args ) return decorated
9008	def get_index_in_row ( self ) : expected_index = self . _cached_index_in_row instructions = self . _row . instructions if expected_index is not None and 0 <= expected_index < len ( instructions ) and instructions [ expected_index ] is self : return expected_index for index , instruction_in_row in enumerate ( instructions ) : if instruction_in_row is self : self . _cached_index_in_row = index return index return None
6222	def _update_yaw_and_pitch ( self ) : front = Vector3 ( [ 0.0 , 0.0 , 0.0 ] ) front . x = cos ( radians ( self . yaw ) ) * cos ( radians ( self . pitch ) ) front . y = sin ( radians ( self . pitch ) ) front . z = sin ( radians ( self . yaw ) ) * cos ( radians ( self . pitch ) ) self . dir = vector . normalise ( front ) self . right = vector . normalise ( vector3 . cross ( self . dir , self . _up ) ) self . up = vector . normalise ( vector3 . cross ( self . right , self . dir ) )
120	def terminate ( self ) : if not self . join_signal . is_set ( ) : self . join_signal . set ( ) time . sleep ( 0.01 ) if self . main_worker_thread . is_alive ( ) : self . main_worker_thread . join ( ) if self . threaded : for worker in self . workers : if worker . is_alive ( ) : worker . join ( ) else : for worker in self . workers : if worker . is_alive ( ) : worker . terminate ( ) worker . join ( ) while not self . all_finished ( ) : time . sleep ( 0.001 ) if self . queue . full ( ) : self . queue . get ( ) self . queue . put ( pickle . dumps ( None , protocol = - 1 ) ) time . sleep ( 0.01 ) while True : try : self . _queue_internal . get ( timeout = 0.005 ) except QueueEmpty : break if not self . _queue_internal . _closed : self . _queue_internal . close ( ) if not self . queue . _closed : self . queue . close ( ) self . _queue_internal . join_thread ( ) self . queue . join_thread ( ) time . sleep ( 0.025 )
36	def get_local_rank_size ( comm ) : this_node = platform . node ( ) ranks_nodes = comm . allgather ( ( comm . Get_rank ( ) , this_node ) ) node2rankssofar = defaultdict ( int ) local_rank = None for ( rank , node ) in ranks_nodes : if rank == comm . Get_rank ( ) : local_rank = node2rankssofar [ node ] node2rankssofar [ node ] += 1 assert local_rank is not None return local_rank , node2rankssofar [ this_node ]
1680	def SetVerboseLevel ( self , level ) : last_verbose_level = self . verbose_level self . verbose_level = level return last_verbose_level
6073	def mass_within_circle_in_units ( self , radius , unit_mass = 'angular' , kpc_per_arcsec = None , critical_surface_density = None ) : if self . has_mass_profile : return sum ( map ( lambda p : p . mass_within_circle_in_units ( radius = radius , unit_mass = unit_mass , kpc_per_arcsec = kpc_per_arcsec , critical_surface_density = critical_surface_density ) , self . mass_profiles ) ) else : return None
10762	def _wait_for_connection ( self , port ) : connected = False max_tries = 10 num_tries = 0 wait_time = 0.5 while not connected or num_tries >= max_tries : time . sleep ( wait_time ) try : af = socket . AF_INET addr = ( '127.0.0.1' , port ) sock = socket . socket ( af , socket . SOCK_STREAM ) sock . connect ( addr ) except socket . error : if sock : sock . close ( ) num_tries += 1 continue connected = True if not connected : print ( "Error connecting to sphinx searchd" , file = sys . stderr )
674	def _loadDummyModelParameters ( self , params ) : for key , value in params . iteritems ( ) : if type ( value ) == list : index = self . modelIndex % len ( params [ key ] ) self . _params [ key ] = params [ key ] [ index ] else : self . _params [ key ] = params [ key ]
2291	def orient_directed_graph ( self , data , dag , alg = 'HC' ) : alg_dic = { 'HC' : hill_climbing , 'HCr' : hill_climbing_with_removal , 'tabu' : tabu_search , 'EHC' : exploratory_hill_climbing } return alg_dic [ alg ] ( data , dag , nh = self . nh , nb_runs = self . nb_runs , gpu = self . gpu , nb_jobs = self . nb_jobs , lr = self . lr , train_epochs = self . train_epochs , test_epochs = self . test_epochs , verbose = self . verbose )
8593	def remove_snapshot ( self , snapshot_id ) : response = self . _perform_request ( url = '/snapshots/' + snapshot_id , method = 'DELETE' ) return response
1874	def MOVHPD ( cpu , dest , src ) : if src . size == 128 : assert dest . size == 64 dest . write ( Operators . EXTRACT ( src . read ( ) , 64 , 64 ) ) else : assert src . size == 64 and dest . size == 128 value = Operators . EXTRACT ( dest . read ( ) , 0 , 64 ) dest . write ( Operators . CONCAT ( 128 , src . read ( ) , value ) )
13228	def get_installation_token ( installation_id , integration_jwt ) : api_root = 'https://api.github.com' url = '{root}/installations/{id_:d}/access_tokens' . format ( api_root = api_root , id_ = installation_id ) headers = { 'Authorization' : 'Bearer {0}' . format ( integration_jwt . decode ( 'utf-8' ) ) , 'Accept' : 'application/vnd.github.machine-man-preview+json' } resp = requests . post ( url , headers = headers ) resp . raise_for_status ( ) return resp . json ( )
5979	def edge_pixels_from_mask ( mask ) : edge_pixel_total = total_edge_pixels_from_mask ( mask ) edge_pixels = np . zeros ( edge_pixel_total ) edge_index = 0 regular_index = 0 for y in range ( mask . shape [ 0 ] ) : for x in range ( mask . shape [ 1 ] ) : if not mask [ y , x ] : if mask [ y + 1 , x ] or mask [ y - 1 , x ] or mask [ y , x + 1 ] or mask [ y , x - 1 ] or mask [ y + 1 , x + 1 ] or mask [ y + 1 , x - 1 ] or mask [ y - 1 , x + 1 ] or mask [ y - 1 , x - 1 ] : edge_pixels [ edge_index ] = regular_index edge_index += 1 regular_index += 1 return edge_pixels
3026	def save_to_well_known_file ( credentials , well_known_file = None ) : if well_known_file is None : well_known_file = _get_well_known_file ( ) config_dir = os . path . dirname ( well_known_file ) if not os . path . isdir ( config_dir ) : raise OSError ( 'Config directory does not exist: {0}' . format ( config_dir ) ) credentials_data = credentials . serialization_data _save_private_file ( well_known_file , credentials_data )
3254	def delete_granule ( self , coverage , store , granule_id , workspace = None ) : params = dict ( ) workspace_name = workspace if isinstance ( store , basestring ) : store_name = store else : store_name = store . name workspace_name = store . workspace . name if workspace_name is None : raise ValueError ( "Must specify workspace" ) url = build_url ( self . service_url , [ "workspaces" , workspace_name , "coveragestores" , store_name , "coverages" , coverage , "index/granules" , granule_id , ".json" ] , params ) headers = { "Content-type" : "application/json" , "Accept" : "application/json" } resp = self . http_request ( url , method = 'delete' , headers = headers ) if resp . status_code != 200 : FailedRequestError ( 'Failed to delete granule from mosaic {} : {}, {}' . format ( store , resp . status_code , resp . text ) ) self . _cache . clear ( ) return None
8391	def check_visitors ( cls ) : for name in dir ( cls ) : if name . startswith ( "visit_" ) : if name [ 6 : ] not in CLASS_NAMES : raise Exception ( u"Method {} doesn't correspond to a node class" . format ( name ) ) return cls
3067	def clean_headers ( headers ) : clean = { } try : for k , v in six . iteritems ( headers ) : if not isinstance ( k , six . binary_type ) : k = str ( k ) if not isinstance ( v , six . binary_type ) : v = str ( v ) clean [ _helpers . _to_bytes ( k ) ] = _helpers . _to_bytes ( v ) except UnicodeEncodeError : from oauth2client . client import NonAsciiHeaderError raise NonAsciiHeaderError ( k , ': ' , v ) return clean
2399	def initialize_dictionaries ( self , e_set , max_feats2 = 200 ) : if ( hasattr ( e_set , '_type' ) ) : if ( e_set . _type == "train" ) : nvocab = util_functions . get_vocab ( e_set . _text , e_set . _score , max_feats2 = max_feats2 ) svocab = util_functions . get_vocab ( e_set . _clean_stem_text , e_set . _score , max_feats2 = max_feats2 ) self . _normal_dict = CountVectorizer ( ngram_range = ( 1 , 2 ) , vocabulary = nvocab ) self . _stem_dict = CountVectorizer ( ngram_range = ( 1 , 2 ) , vocabulary = svocab ) self . dict_initialized = True self . _mean_spelling_errors = sum ( e_set . _spelling_errors ) / float ( len ( e_set . _spelling_errors ) ) self . _spell_errors_per_character = sum ( e_set . _spelling_errors ) / float ( sum ( [ len ( t ) for t in e_set . _text ] ) ) good_pos_tags , bad_pos_positions = self . _get_grammar_errors ( e_set . _pos , e_set . _text , e_set . _tokens ) self . _grammar_errors_per_character = ( sum ( good_pos_tags ) / float ( sum ( [ len ( t ) for t in e_set . _text ] ) ) ) bag_feats = self . gen_bag_feats ( e_set ) f_row_sum = numpy . sum ( bag_feats [ : , : ] ) self . _mean_f_prop = f_row_sum / float ( sum ( [ len ( t ) for t in e_set . _text ] ) ) ret = "ok" else : raise util_functions . InputError ( e_set , "needs to be an essay set of the train type." ) else : raise util_functions . InputError ( e_set , "wrong input. need an essay set object" ) return ret
710	def _backupFile ( filePath ) : assert os . path . exists ( filePath ) stampNum = 0 ( prefix , suffix ) = os . path . splitext ( filePath ) while True : backupPath = "%s.%d%s" % ( prefix , stampNum , suffix ) stampNum += 1 if not os . path . exists ( backupPath ) : break shutil . copyfile ( filePath , backupPath ) return backupPath
10677	def Cp ( self , T ) : result = 0.0 for c , e in zip ( self . _coefficients , self . _exponents ) : result += c * T ** e return result
5787	def _advapi32_encrypt ( cipher , key , data , iv , padding ) : context_handle = None key_handle = None try : context_handle , key_handle = _advapi32_create_handles ( cipher , key , iv ) out_len = new ( advapi32 , 'DWORD *' , len ( data ) ) res = advapi32 . CryptEncrypt ( key_handle , null ( ) , True , 0 , null ( ) , out_len , 0 ) handle_error ( res ) buffer_len = deref ( out_len ) buffer = buffer_from_bytes ( buffer_len ) write_to_buffer ( buffer , data ) pointer_set ( out_len , len ( data ) ) res = advapi32 . CryptEncrypt ( key_handle , null ( ) , True , 0 , buffer , out_len , buffer_len ) handle_error ( res ) output = bytes_from_buffer ( buffer , deref ( out_len ) ) if cipher == 'aes' and not padding : if output [ - 16 : ] != ( b'\x10' * 16 ) : raise ValueError ( 'Invalid padding generated by OS crypto library' ) output = output [ : - 16 ] return output finally : if key_handle : advapi32 . CryptDestroyKey ( key_handle ) if context_handle : close_context_handle ( context_handle )
3378	def assert_optimal ( model , message = 'optimization failed' ) : status = model . solver . status if status != OPTIMAL : exception_cls = OPTLANG_TO_EXCEPTIONS_DICT . get ( status , OptimizationError ) raise exception_cls ( "{} ({})" . format ( message , status ) )
10600	def _url ( self , endpoint , url_data = None , parameters = None ) : try : url = '%s/%s' % ( self . base_url , self . endpoints [ endpoint ] ) except KeyError : raise EndPointDoesNotExist ( endpoint ) if url_data : url = url % url_data if parameters : url = '%s?%s' % ( url , urllib . urlencode ( parameters , True ) ) return url
8745	def get_floatingips ( context , filters = None , fields = None , sorts = [ 'id' ] , limit = None , marker = None , page_reverse = False ) : LOG . info ( 'get_floatingips for tenant %s filters %s fields %s' % ( context . tenant_id , filters , fields ) ) floating_ips = _get_ips_by_type ( context , ip_types . FLOATING , filters = filters , fields = fields ) return [ v . _make_floating_ip_dict ( flip ) for flip in floating_ips ]
9464	def conference_undeaf ( self , call_params ) : path = '/' + self . api_version + '/ConferenceUndeaf/' method = 'POST' return self . request ( path , method , call_params )
4186	def window_taylor ( N , nbar = 4 , sll = - 30 ) : B = 10 ** ( - sll / 20 ) A = log ( B + sqrt ( B ** 2 - 1 ) ) / pi s2 = nbar ** 2 / ( A ** 2 + ( nbar - 0.5 ) ** 2 ) ma = arange ( 1 , nbar ) def calc_Fm ( m ) : numer = ( - 1 ) ** ( m + 1 ) * prod ( 1 - m ** 2 / s2 / ( A ** 2 + ( ma - 0.5 ) ** 2 ) ) denom = 2 * prod ( [ 1 - m ** 2 / j ** 2 for j in ma if j != m ] ) return numer / denom Fm = array ( [ calc_Fm ( m ) for m in ma ] ) def W ( n ) : return 2 * np . sum ( Fm * cos ( 2 * pi * ma * ( n - N / 2 + 1 / 2 ) / N ) ) + 1 w = array ( [ W ( n ) for n in range ( N ) ] ) scale = W ( ( N - 1 ) / 2 ) w /= scale return w
7034	def import_apikey ( lcc_server , apikey_text_json ) : USERHOME = os . path . expanduser ( '~' ) APIKEYFILE = os . path . join ( USERHOME , '.astrobase' , 'lccs' , 'apikey-%s' % lcc_server . replace ( 'https://' , 'https-' ) . replace ( 'http://' , 'http-' ) ) respdict = json . loads ( apikey_text_json ) apikey = respdict [ 'apikey' ] expires = respdict [ 'expires' ] if not os . path . exists ( os . path . dirname ( APIKEYFILE ) ) : os . makedirs ( os . path . dirname ( APIKEYFILE ) ) with open ( APIKEYFILE , 'w' ) as outfd : outfd . write ( '%s %s\n' % ( apikey , expires ) ) os . chmod ( APIKEYFILE , 0o100600 ) LOGINFO ( 'key fetched successfully from: %s. expires on: %s' % ( lcc_server , expires ) ) LOGINFO ( 'written to: %s' % APIKEYFILE ) return apikey , expires
9532	def dumps ( obj , key = None , salt = 'django.core.signing' , serializer = JSONSerializer , compress = False ) : data = serializer ( ) . dumps ( obj ) is_compressed = False if compress : compressed = zlib . compress ( data ) if len ( compressed ) < ( len ( data ) - 1 ) : data = compressed is_compressed = True base64d = b64_encode ( data ) if is_compressed : base64d = b'.' + base64d return TimestampSigner ( key , salt = salt ) . sign ( base64d )
12187	async def handle_message ( self , message , filters ) : data = self . _unpack_message ( message ) logger . debug ( data ) if data . get ( 'type' ) == 'error' : raise SlackApiError ( data . get ( 'error' , { } ) . get ( 'msg' , str ( data ) ) ) elif self . message_is_to_me ( data ) : text = data [ 'text' ] [ len ( self . address_as ) : ] . strip ( ) if text == 'help' : return self . _respond ( channel = data [ 'channel' ] , text = self . _instruction_list ( filters ) , ) elif text == 'version' : return self . _respond ( channel = data [ 'channel' ] , text = self . VERSION , ) for _filter in filters : if _filter . matches ( data ) : logger . debug ( 'Response triggered' ) async for response in _filter : self . _respond ( channel = data [ 'channel' ] , text = response )
12321	def send ( self , send_email = True ) : url = str ( self . api . base_url + '{code}/status/' ) . format ( code = self . code ) payload = { 'mark_as_sent' : True , 'send_email' : send_email , } stat = self . api . connection . make_put ( url , payload )
9095	def _add_annotation_to_graph ( self , graph : BELGraph ) -> None : if 'bio2bel' not in graph . annotation_list : graph . annotation_list [ 'bio2bel' ] = set ( ) graph . annotation_list [ 'bio2bel' ] . add ( self . module_name )
9031	def _expand_produced_mesh ( self , mesh , mesh_index , row_position , passed ) : if not mesh . is_consumed ( ) : return row = mesh . consuming_row position = Point ( row_position . x - mesh . index_in_consuming_row + mesh_index , row_position . y + INSTRUCTION_HEIGHT ) self . _expand ( row , position , passed )
1383	def unregister_watch ( self , uid ) : Log . info ( "Unregister a watch with uid: " + str ( uid ) ) self . watches . pop ( uid , None )
12740	def _parse_corporations ( self , datafield , subfield , roles = [ "any" ] ) : if len ( datafield ) != 3 : raise ValueError ( "datafield parameter have to be exactly 3 chars long!" ) if len ( subfield ) != 1 : raise ValueError ( "Bad subfield specification - subield have to be 3 chars long!" ) parsed_corporations = [ ] for corporation in self . get_subfields ( datafield , subfield ) : other_subfields = corporation . other_subfields if "4" in other_subfields and roles != [ "any" ] : corp_roles = other_subfields [ "4" ] relevant = any ( map ( lambda role : role in roles , corp_roles ) ) if not relevant : continue name = "" place = "" date = "" name = corporation if "c" in other_subfields : place = "," . join ( other_subfields [ "c" ] ) if "d" in other_subfields : date = "," . join ( other_subfields [ "d" ] ) parsed_corporations . append ( Corporation ( name , place , date ) ) return parsed_corporations
1911	def GetNBits ( value , nbits ) : if isinstance ( value , int ) : return Operators . EXTRACT ( value , 0 , nbits ) elif isinstance ( value , BitVec ) : if value . size < nbits : return Operators . ZEXTEND ( value , nbits ) else : return Operators . EXTRACT ( value , 0 , nbits )
3364	def load_yaml_model ( filename ) : if isinstance ( filename , string_types ) : with io . open ( filename , "r" ) as file_handle : return model_from_dict ( yaml . load ( file_handle ) ) else : return model_from_dict ( yaml . load ( filename ) )
2908	def _find_child_of ( self , parent_task_spec ) : if self . parent is None : return self if self . parent . task_spec == parent_task_spec : return self return self . parent . _find_child_of ( parent_task_spec )
12215	def get_frame_locals ( stepback = 0 ) : with Frame ( stepback = stepback ) as frame : locals_dict = frame . f_locals return locals_dict
616	def expGenerator ( args ) : parser = OptionParser ( ) parser . set_usage ( "%prog [options] --description='{json object with args}'\n" + "%prog [options] --descriptionFromFile='{filename}'\n" + "%prog [options] --showSchema" ) parser . add_option ( "--description" , dest = "description" , help = "Tells ExpGenerator to generate an experiment description.py and " "permutations.py file using the given JSON formatted experiment " "description string." ) parser . add_option ( "--descriptionFromFile" , dest = 'descriptionFromFile' , help = "Tells ExpGenerator to open the given filename and use it's " "contents as the JSON formatted experiment description." ) parser . add_option ( "--claDescriptionTemplateFile" , dest = 'claDescriptionTemplateFile' , default = 'claDescriptionTemplate.tpl' , help = "The file containing the template description file for " " ExpGenerator [default: %default]" ) parser . add_option ( "--showSchema" , action = "store_true" , dest = "showSchema" , help = "Prints the JSON schemas for the --description arg." ) parser . add_option ( "--version" , dest = 'version' , default = 'v2' , help = "Generate the permutations file for this version of hypersearch." " Possible choices are 'v1' and 'v2' [default: %default]." ) parser . add_option ( "--outDir" , dest = "outDir" , default = None , help = "Where to generate experiment. If not specified, " "then a temp directory will be created" ) ( options , remainingArgs ) = parser . parse_args ( args ) if len ( remainingArgs ) > 0 : raise _InvalidCommandArgException ( _makeUsageErrorStr ( "Unexpected command-line args: <%s>" % ( ' ' . join ( remainingArgs ) , ) , parser . get_usage ( ) ) ) activeOptions = filter ( lambda x : getattr ( options , x ) != None , ( 'description' , 'showSchema' ) ) if len ( activeOptions ) > 1 : raise _InvalidCommandArgException ( _makeUsageErrorStr ( ( "The specified command options are " + "mutually-exclusive: %s" ) % ( activeOptions , ) , parser . get_usage ( ) ) ) if options . showSchema : _handleShowSchemaOption ( ) elif options . description : _handleDescriptionOption ( options . description , options . outDir , parser . get_usage ( ) , hsVersion = options . version , claDescriptionTemplateFile = options . claDescriptionTemplateFile ) elif options . descriptionFromFile : _handleDescriptionFromFileOption ( options . descriptionFromFile , options . outDir , parser . get_usage ( ) , hsVersion = options . version , claDescriptionTemplateFile = options . claDescriptionTemplateFile ) else : raise _InvalidCommandArgException ( _makeUsageErrorStr ( "Error in validating command options. No option " "provided:\n" , parser . get_usage ( ) ) )
12670	def create ( _ ) : endpoint = client_endpoint ( ) if not endpoint : raise CLIError ( "Connection endpoint not found. " "Before running sfctl commands, connect to a cluster using " "the 'sfctl cluster select' command." ) no_verify = no_verify_setting ( ) if security_type ( ) == 'aad' : auth = AdalAuthentication ( no_verify ) else : cert = cert_info ( ) ca_cert = ca_cert_info ( ) auth = ClientCertAuthentication ( cert , ca_cert , no_verify ) return ServiceFabricClientAPIs ( auth , base_url = endpoint )
2861	def _transaction_end ( self ) : self . _command . append ( '\x87' ) self . _ft232h . _write ( '' . join ( self . _command ) ) return bytearray ( self . _ft232h . _poll_read ( self . _expected ) )
7320	def make_message_multipart ( message ) : if not message . is_multipart ( ) : multipart_message = email . mime . multipart . MIMEMultipart ( 'alternative' ) for header_key in set ( message . keys ( ) ) : values = message . get_all ( header_key , failobj = [ ] ) for value in values : multipart_message [ header_key ] = value original_text = message . get_payload ( ) multipart_message . attach ( email . mime . text . MIMEText ( original_text ) ) message = multipart_message message = _create_boundary ( message ) return message
2115	def status ( self , pk = None , detail = False , ** kwargs ) : job = self . last_job_data ( pk , ** kwargs ) if detail : return job return { 'elapsed' : job [ 'elapsed' ] , 'failed' : job [ 'failed' ] , 'status' : job [ 'status' ] , }
4682	def getKeyType ( self , account , pub ) : for authority in [ "owner" , "active" ] : for key in account [ authority ] [ "key_auths" ] : if str ( pub ) == key [ 0 ] : return authority if str ( pub ) == account [ "options" ] [ "memo_key" ] : return "memo" return None
8645	def get_track_by_id ( session , track_id , track_point_limit = None , track_point_offset = None ) : tracking_data = { } if track_point_limit : tracking_data [ 'track_point_limit' ] = track_point_limit if track_point_offset : tracking_data [ 'track_point_offset' ] = track_point_offset response = make_get_request ( session , 'tracks/{}' . format ( track_id ) , params_data = tracking_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise TrackNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
5488	def json_rpc_format ( self ) : error = { 'name' : text_type ( self . __class__ . __name__ ) , 'code' : self . code , 'message' : '{0}' . format ( text_type ( self . message ) ) , 'data' : self . data } if current_app . config [ 'DEBUG' ] : import sys , traceback error [ 'stack' ] = traceback . format_exc ( ) error [ 'executable' ] = sys . executable return error
12217	def print_file_info ( ) : tpl = TableLogger ( columns = 'file,created,modified,size' ) for f in os . listdir ( '.' ) : size = os . stat ( f ) . st_size date_created = datetime . fromtimestamp ( os . path . getctime ( f ) ) date_modified = datetime . fromtimestamp ( os . path . getmtime ( f ) ) tpl ( f , date_created , date_modified , size )
6600	def open ( self ) : self . path = self . _prepare_dir ( self . topdir ) self . _copy_executable ( area_path = self . path ) self . _save_logging_levels ( area_path = self . path ) self . _put_python_modules ( modules = self . python_modules , area_path = self . path )
3819	async def add_user ( self , add_user_request ) : response = hangouts_pb2 . AddUserResponse ( ) await self . _pb_request ( 'conversations/adduser' , add_user_request , response ) return response
1451	def incr ( self , key , to_add = 1 ) : if key not in self . value : self . value [ key ] = CountMetric ( ) self . value [ key ] . incr ( to_add )
2752	def get_all_domains ( self ) : data = self . get_data ( "domains/" ) domains = list ( ) for jsoned in data [ 'domains' ] : domain = Domain ( ** jsoned ) domain . token = self . token domains . append ( domain ) return domains
8438	def _patched_run_hook ( hook_name , project_dir , context ) : if hook_name == 'post_gen_project' : with temple . utils . cd ( project_dir ) : temple . utils . write_temple_config ( context [ 'cookiecutter' ] , context [ 'template' ] , context [ 'version' ] ) return cc_hooks . run_hook ( hook_name , project_dir , context )
5645	def make_views ( cls , conn ) : conn . execute ( 'DROP VIEW IF EXISTS main.day_trips' ) conn . execute ( 'CREATE VIEW day_trips AS ' 'SELECT day_trips2.*, trips.* ' 'FROM day_trips2 JOIN trips USING (trip_I);' ) conn . commit ( ) conn . execute ( 'DROP VIEW IF EXISTS main.day_stop_times' ) conn . execute ( 'CREATE VIEW day_stop_times AS ' 'SELECT day_trips2.*, trips.*, stop_times.*, ' 'day_trips2.day_start_ut+stop_times.arr_time_ds AS arr_time_ut, ' 'day_trips2.day_start_ut+stop_times.dep_time_ds AS dep_time_ut ' 'FROM day_trips2 ' 'JOIN trips USING (trip_I) ' 'JOIN stop_times USING (trip_I)' ) conn . commit ( )
4685	def unlock_wallet ( self , * args , ** kwargs ) : self . blockchain . wallet . unlock ( * args , ** kwargs ) return self
8522	def add_int ( self , name , min , max , warp = None ) : min , max = map ( int , ( min , max ) ) if max < min : raise ValueError ( 'variable %s: max < min error' % name ) if warp not in ( None , 'log' ) : raise ValueError ( 'variable %s: warp=%s is not supported. use ' 'None or "log",' % ( name , warp ) ) if min <= 0 and warp == 'log' : raise ValueError ( 'variable %s: log-warping requires min > 0' ) self . variables [ name ] = IntVariable ( name , min , max , warp )
1796	def XADD ( cpu , dest , src ) : MASK = ( 1 << dest . size ) - 1 SIGN_MASK = 1 << ( dest . size - 1 ) arg0 = dest . read ( ) arg1 = src . read ( ) temp = ( arg1 + arg0 ) & MASK src . write ( arg0 ) dest . write ( temp ) tempCF = Operators . OR ( Operators . ULT ( temp , arg0 ) , Operators . ULT ( temp , arg1 ) ) cpu . CF = tempCF cpu . AF = ( ( arg0 ^ arg1 ) ^ temp ) & 0x10 != 0 cpu . ZF = temp == 0 cpu . SF = ( temp & SIGN_MASK ) != 0 cpu . OF = ( ( ( arg0 ^ arg1 ^ SIGN_MASK ) & ( temp ^ arg1 ) ) & SIGN_MASK ) != 0 cpu . PF = cpu . _calculate_parity_flag ( temp )
13793	def handle_add_fun ( self , function_name ) : function_name = function_name . strip ( ) try : function = get_function ( function_name ) except Exception , exc : self . wfile . write ( js_error ( exc ) + NEWLINE ) return if not getattr ( function , 'view_decorated' , None ) : self . functions [ function_name ] = ( self . function_counter , function ) else : self . functions [ function_name ] = ( self . function_counter , function ( self . log ) ) self . function_counter += 1 return True
8598	def list_shares ( self , group_id , depth = 1 ) : response = self . _perform_request ( '/um/groups/%s/shares?depth=%s' % ( group_id , str ( depth ) ) ) return response
1766	def decode_instruction ( self , pc ) : if pc in self . _instruction_cache : return self . _instruction_cache [ pc ] text = b'' for address in range ( pc , pc + self . max_instr_width ) : if not self . memory . access_ok ( address , 'x' ) : break c = self . memory [ address ] if issymbolic ( c ) : if isinstance ( self . memory , LazySMemory ) : try : vals = visitors . simplify_array_select ( c ) c = bytes ( [ vals [ 0 ] ] ) except visitors . ArraySelectSimplifier . ExpressionNotSimple : c = struct . pack ( 'B' , solver . get_value ( self . memory . constraints , c ) ) elif isinstance ( c , Constant ) : c = bytes ( [ c . value ] ) else : logger . error ( 'Concretize executable memory %r %r' , c , text ) raise ConcretizeMemory ( self . memory , address = pc , size = 8 * self . max_instr_width , policy = 'INSTRUCTION' ) text += c code = text . ljust ( self . max_instr_width , b'\x00' ) try : insn = self . disasm . disassemble_instruction ( code , pc ) except StopIteration as e : raise DecodeException ( pc , code ) if not self . memory . access_ok ( slice ( pc , pc + insn . size ) , 'x' ) : logger . info ( "Trying to execute instructions from non-executable memory" ) raise InvalidMemoryAccess ( pc , 'x' ) insn . operands = self . _wrap_operands ( insn . operands ) self . _instruction_cache [ pc ] = insn return insn
2050	def _swap_mode ( self ) : assert self . mode in ( cs . CS_MODE_ARM , cs . CS_MODE_THUMB ) if self . mode == cs . CS_MODE_ARM : self . mode = cs . CS_MODE_THUMB else : self . mode = cs . CS_MODE_ARM
571	def runModelGivenBaseAndParams ( modelID , jobID , baseDescription , params , predictedField , reportKeys , optimizeKey , jobsDAO , modelCheckpointGUID , logLevel = None , predictionCacheMaxRecords = None ) : from nupic . swarming . ModelRunner import OPFModelRunner logger = logging . getLogger ( 'com.numenta.nupic.hypersearch.utils' ) experimentDir = tempfile . mkdtemp ( ) try : logger . info ( "Using experiment directory: %s" % ( experimentDir ) ) paramsFilePath = os . path . join ( experimentDir , 'description.py' ) paramsFile = open ( paramsFilePath , 'wb' ) paramsFile . write ( _paramsFileHead ( ) ) items = params . items ( ) items . sort ( ) for ( key , value ) in items : quotedKey = _quoteAndEscape ( key ) if isinstance ( value , basestring ) : paramsFile . write ( " %s : '%s',\n" % ( quotedKey , value ) ) else : paramsFile . write ( " %s : %s,\n" % ( quotedKey , value ) ) paramsFile . write ( _paramsFileTail ( ) ) paramsFile . close ( ) baseParamsFile = open ( os . path . join ( experimentDir , 'base.py' ) , 'wb' ) baseParamsFile . write ( baseDescription ) baseParamsFile . close ( ) fd = open ( paramsFilePath ) expDescription = fd . read ( ) fd . close ( ) jobsDAO . modelSetFields ( modelID , { 'genDescription' : expDescription } ) try : runner = OPFModelRunner ( modelID = modelID , jobID = jobID , predictedField = predictedField , experimentDir = experimentDir , reportKeyPatterns = reportKeys , optimizeKeyPattern = optimizeKey , jobsDAO = jobsDAO , modelCheckpointGUID = modelCheckpointGUID , logLevel = logLevel , predictionCacheMaxRecords = predictionCacheMaxRecords ) signal . signal ( signal . SIGINT , runner . handleWarningSignal ) ( completionReason , completionMsg ) = runner . run ( ) except InvalidConnectionException : raise except Exception , e : ( completionReason , completionMsg ) = _handleModelRunnerException ( jobID , modelID , jobsDAO , experimentDir , logger , e ) finally : shutil . rmtree ( experimentDir ) signal . signal ( signal . SIGINT , signal . default_int_handler ) return ( completionReason , completionMsg )
8069	def randomChildElement ( self , node ) : choices = [ e for e in node . childNodes if e . nodeType == e . ELEMENT_NODE ] chosen = random . choice ( choices ) if _debug : sys . stderr . write ( '%s available choices: %s\n' % ( len ( choices ) , [ e . toxml ( ) for e in choices ] ) ) sys . stderr . write ( 'Chosen: %s\n' % chosen . toxml ( ) ) return chosen
8509	def fit ( self , X , y = None ) : from pylearn2 . config import yaml_parse from pylearn2 . train import Train params = self . get_params ( ) yaml_string = Template ( self . yaml_string ) . substitute ( params ) self . trainer = yaml_parse . load ( yaml_string ) assert isinstance ( self . trainer , Train ) if self . trainer . dataset is not None : raise ValueError ( 'Train YAML database must evaluate to None.' ) self . trainer . dataset = self . _get_dataset ( X , y ) if ( hasattr ( self . trainer . algorithm , 'monitoring_dataset' ) and self . trainer . algorithm . monitoring_dataset is not None ) : monitoring_dataset = self . trainer . algorithm . monitoring_dataset if len ( monitoring_dataset ) == 1 and '' in monitoring_dataset : monitoring_dataset [ '' ] = self . trainer . dataset else : monitoring_dataset [ 'train' ] = self . trainer . dataset self . trainer . algorithm . _set_monitoring_dataset ( monitoring_dataset ) else : self . trainer . algorithm . _set_monitoring_dataset ( self . trainer . dataset ) self . trainer . main_loop ( )
6970	def _old_epd_magseries ( times , mags , errs , fsv , fdv , fkv , xcc , ycc , bgv , bge , epdsmooth_windowsize = 21 , epdsmooth_sigclip = 3.0 , epdsmooth_func = smooth_magseries_signal_medfilt , epdsmooth_extraparams = None ) : finiteind = np . isfinite ( mags ) mags_median = np . median ( mags [ finiteind ] ) mags_stdev = np . nanstd ( mags ) if epdsmooth_sigclip : excludeind = abs ( mags - mags_median ) < epdsmooth_sigclip * mags_stdev finalind = finiteind & excludeind else : finalind = finiteind final_mags = mags [ finalind ] final_len = len ( final_mags ) if isinstance ( epdsmooth_extraparams , dict ) : smoothedmags = epdsmooth_func ( final_mags , epdsmooth_windowsize , ** epdsmooth_extraparams ) else : smoothedmags = epdsmooth_func ( final_mags , epdsmooth_windowsize ) epdmatrix = np . c_ [ fsv [ finalind ] ** 2.0 , fsv [ finalind ] , fdv [ finalind ] ** 2.0 , fdv [ finalind ] , fkv [ finalind ] ** 2.0 , fkv [ finalind ] , np . ones ( final_len ) , fsv [ finalind ] * fdv [ finalind ] , fsv [ finalind ] * fkv [ finalind ] , fdv [ finalind ] * fkv [ finalind ] , np . sin ( 2 * np . pi * xcc [ finalind ] ) , np . cos ( 2 * np . pi * xcc [ finalind ] ) , np . sin ( 2 * np . pi * ycc [ finalind ] ) , np . cos ( 2 * np . pi * ycc [ finalind ] ) , np . sin ( 4 * np . pi * xcc [ finalind ] ) , np . cos ( 4 * np . pi * xcc [ finalind ] ) , np . sin ( 4 * np . pi * ycc [ finalind ] ) , np . cos ( 4 * np . pi * ycc [ finalind ] ) , bgv [ finalind ] , bge [ finalind ] ] try : coeffs , residuals , rank , singulars = lstsq ( epdmatrix , smoothedmags , rcond = None ) if DEBUG : print ( 'coeffs = %s, residuals = %s' % ( coeffs , residuals ) ) retdict = { 'times' : times , 'mags' : ( mags_median + _old_epd_diffmags ( coeffs , fsv , fdv , fkv , xcc , ycc , bgv , bge , mags ) ) , 'errs' : errs , 'fitcoeffs' : coeffs , 'residuals' : residuals } return retdict except Exception as e : LOGEXCEPTION ( 'EPD solution did not converge' ) retdict = { 'times' : times , 'mags' : np . full_like ( mags , np . nan ) , 'errs' : errs , 'fitcoeffs' : coeffs , 'residuals' : residuals } return retdict
1177	def search ( self , string , pos = 0 , endpos = sys . maxint ) : state = _State ( string , pos , endpos , self . flags ) if state . search ( self . _code ) : return SRE_Match ( self , state ) else : return None
4461	def transform ( self , jam ) : yield jam for jam_out in self . transformer . transform ( jam ) : yield jam_out
1851	def RCL ( cpu , dest , src ) : OperandSize = dest . size count = src . read ( ) countMask = { 8 : 0x1f , 16 : 0x1f , 32 : 0x1f , 64 : 0x3f } [ OperandSize ] tempCount = Operators . ZEXTEND ( ( count & countMask ) % ( src . size + 1 ) , OperandSize ) value = dest . read ( ) if isinstance ( tempCount , int ) and tempCount == 0 : new_val = value dest . write ( new_val ) else : carry = Operators . ITEBV ( OperandSize , cpu . CF , 1 , 0 ) right = value >> ( OperandSize - tempCount ) new_val = ( value << tempCount ) | ( carry << ( tempCount - 1 ) ) | ( right >> 1 ) dest . write ( new_val ) def sf ( v , size ) : return ( v & ( 1 << ( size - 1 ) ) ) != 0 cpu . CF = sf ( value << ( tempCount - 1 ) , OperandSize ) cpu . OF = Operators . ITE ( tempCount == 1 , sf ( new_val , OperandSize ) != cpu . CF , cpu . OF )
9587	def isarray ( array , test , dim = 2 ) : if dim > 1 : return all ( isarray ( array [ i ] , test , dim - 1 ) for i in range ( len ( array ) ) ) return all ( test ( i ) for i in array )
9578	def read_numeric_array ( fd , endian , header , data_etypes ) : if header [ 'is_complex' ] : raise ParseError ( 'Complex arrays are not supported' ) data = read_elements ( fd , endian , data_etypes ) if not isinstance ( data , Sequence ) : return data rowcount = header [ 'dims' ] [ 0 ] colcount = header [ 'dims' ] [ 1 ] array = [ list ( data [ c * rowcount + r ] for c in range ( colcount ) ) for r in range ( rowcount ) ] return squeeze ( array )
13660	def subroute ( self , * components ) : def _factory ( f ) : self . _addRoute ( f , subroute ( * components ) ) return f return _factory
3343	def calc_base64 ( s ) : s = compat . to_bytes ( s ) s = compat . base64_encodebytes ( s ) . strip ( ) return compat . to_native ( s )
6554	def flip_variable ( self , v ) : try : idx = self . variables . index ( v ) except ValueError : raise ValueError ( "variable {} is not a variable in constraint {}" . format ( v , self . name ) ) if self . vartype is dimod . BINARY : original_func = self . func def func ( * args ) : new_args = list ( args ) new_args [ idx ] = 1 - new_args [ idx ] return original_func ( * new_args ) self . func = func self . configurations = frozenset ( config [ : idx ] + ( 1 - config [ idx ] , ) + config [ idx + 1 : ] for config in self . configurations ) else : original_func = self . func def func ( * args ) : new_args = list ( args ) new_args [ idx ] = - new_args [ idx ] return original_func ( * new_args ) self . func = func self . configurations = frozenset ( config [ : idx ] + ( - config [ idx ] , ) + config [ idx + 1 : ] for config in self . configurations ) self . name = '{} ({} flipped)' . format ( self . name , v )
11351	def merge_from_list ( self , list_args ) : def xs ( name , parser_args , list_args ) : for args , kwargs in list_args : if len ( set ( args ) & parser_args ) > 0 : yield args , kwargs else : if 'dest' in kwargs : if kwargs [ 'dest' ] == name : yield args , kwargs for args , kwargs in xs ( self . name , self . parser_args , list_args ) : self . merge_args ( args ) self . merge_kwargs ( kwargs )
3025	def _save_private_file ( filename , json_contents ) : temp_filename = tempfile . mktemp ( ) file_desc = os . open ( temp_filename , os . O_WRONLY | os . O_CREAT , 0o600 ) with os . fdopen ( file_desc , 'w' ) as file_handle : json . dump ( json_contents , file_handle , sort_keys = True , indent = 2 , separators = ( ',' , ': ' ) ) shutil . move ( temp_filename , filename )
9976	def alter_freevars ( func , globals_ = None , ** vars ) : if globals_ is None : globals_ = func . __globals__ frees = tuple ( vars . keys ( ) ) oldlocs = func . __code__ . co_names newlocs = tuple ( name for name in oldlocs if name not in frees ) code = _alter_code ( func . __code__ , co_freevars = frees , co_names = newlocs , co_flags = func . __code__ . co_flags | inspect . CO_NESTED ) closure = _create_closure ( * vars . values ( ) ) return FunctionType ( code , globals_ , closure = closure )
10900	def check_consistency ( self ) : error = False regex = re . compile ( '([a-zA-Z_][a-zA-Z0-9_]*)' ) if 'full' not in self . modelstr : raise ModelError ( 'Model must contain a `full` key describing ' 'the entire image formation' ) for name , eq in iteritems ( self . modelstr ) : var = regex . findall ( eq ) for v in var : v = re . sub ( r"^d" , '' , v ) if v not in self . varmap : log . error ( "Variable '%s' (eq. '%s': '%s') not found in category map %r" % ( v , name , eq , self . varmap ) ) error = True if error : raise ModelError ( 'Inconsistent varmap and modelstr descriptions' )
2943	def validate ( self ) : results = [ ] from . . specs import Join def recursive_find_loop ( task , history ) : current = history [ : ] current . append ( task ) if isinstance ( task , Join ) : if task in history : msg = "Found loop with '%s': %s then '%s' again" % ( task . name , '->' . join ( [ p . name for p in history ] ) , task . name ) raise Exception ( msg ) for predecessor in task . inputs : recursive_find_loop ( predecessor , current ) for parent in task . inputs : recursive_find_loop ( parent , current ) for task_id , task in list ( self . task_specs . items ( ) ) : try : recursive_find_loop ( task , [ ] ) except Exception as exc : results . append ( exc . __str__ ( ) ) if not task . inputs and task . name not in [ 'Start' , 'Root' ] : if task . outputs : results . append ( "Task '%s' is disconnected (no inputs)" % task . name ) else : LOG . debug ( "Task '%s' is not being used" % task . name ) return results
3994	def _load_ssh_auth_pre_yosemite ( ) : for process in psutil . process_iter ( ) : if process . name ( ) == 'ssh-agent' : ssh_auth_sock = subprocess . check_output ( [ 'launchctl' , 'bsexec' , str ( process . pid ) , 'launchctl' , 'getenv' , 'SSH_AUTH_SOCK' ] ) . rstrip ( ) if ssh_auth_sock : _set_ssh_auth_sock ( ssh_auth_sock ) break else : daemon_warnings . warn ( 'ssh' , 'No running ssh-agent found linked to SSH_AUTH_SOCK' )
6645	def _mergeDictionaries ( * args ) : result = type ( args [ 0 ] ) ( ) for k , v in itertools . chain ( * [ x . items ( ) for x in args ] ) : if not k in result : result [ k ] = v elif isinstance ( result [ k ] , dict ) and isinstance ( v , dict ) : result [ k ] = _mergeDictionaries ( result [ k ] , v ) return result
7933	def _connect ( self , server = None , port = None ) : if self . me . node or self . me . resource : raise Value ( "Component JID may have only domain defined" ) if not server : server = self . server if not port : port = self . port if not server or not port : raise ValueError ( "Server or port not given" ) Stream . _connect ( self , server , port , None , self . me )
1684	def Begin ( self , function_name ) : self . in_a_function = True self . lines_in_function = 0 self . current_function = function_name
11590	def _rc_smove ( self , src , dst , value ) : if self . type ( src ) != b ( "set" ) : return self . smove ( src + "{" + src + "}" , dst , value ) if self . type ( dst ) != b ( "set" ) : return self . smove ( dst + "{" + dst + "}" , src , value ) if self . srem ( src , value ) : return 1 if self . sadd ( dst , value ) else 0 return 0
2236	def timestamp ( method = 'iso8601' ) : if method == 'iso8601' : tz_hour = time . timezone // 3600 utc_offset = str ( tz_hour ) if tz_hour < 0 else '+' + str ( tz_hour ) stamp = time . strftime ( '%Y-%m-%dT%H%M%S' ) + utc_offset return stamp else : raise ValueError ( 'only iso8601 is accepted for now' )
5057	def build_notification_message ( template_context , template_configuration = None ) : if ( template_configuration is not None and template_configuration . html_template and template_configuration . plaintext_template ) : plain_msg , html_msg = template_configuration . render_all_templates ( template_context ) else : plain_msg = render_to_string ( 'enterprise/emails/user_notification.txt' , template_context ) html_msg = render_to_string ( 'enterprise/emails/user_notification.html' , template_context ) return plain_msg , html_msg
9303	def get_request_date ( cls , req ) : date = None for header in [ 'x-amz-date' , 'date' ] : if header not in req . headers : continue try : date_str = cls . parse_date ( req . headers [ header ] ) except DateFormatError : continue try : date = datetime . datetime . strptime ( date_str , '%Y-%m-%d' ) . date ( ) except ValueError : continue else : break return date
10877	def calculate_linescan_psf ( x , y , z , normalize = False , kfki = 0.889 , zint = 100. , polar_angle = 0. , wrap = True , ** kwargs ) : if wrap : xpts = vec_to_halfvec ( x ) ypts = vec_to_halfvec ( y ) x3 , y3 , z3 = np . meshgrid ( xpts , ypts , z , indexing = 'ij' ) else : x3 , y3 , z3 = np . meshgrid ( x , y , z , indexing = 'ij' ) rho3 = np . sqrt ( x3 * x3 + y3 * y3 ) if wrap : y2 , z2 = np . meshgrid ( ypts , z , indexing = 'ij' ) hilm0 = calculate_linescan_ilm_psf ( y2 , z2 , zint = zint , polar_angle = polar_angle , ** kwargs ) if ypts [ 0 ] == 0 : hilm = np . append ( hilm0 [ - 1 : 0 : - 1 ] , hilm0 , axis = 0 ) else : hilm = np . append ( hilm0 [ : : - 1 ] , hilm0 , axis = 0 ) else : y2 , z2 = np . meshgrid ( y , z , indexing = 'ij' ) hilm = calculate_linescan_ilm_psf ( y2 , z2 , zint = zint , polar_angle = polar_angle , ** kwargs ) if wrap : func = lambda * args : get_hsym_asym ( rho3 * kfki , z3 * kfki , zint = kfki * zint , get_hdet = True , ** kwargs ) [ 0 ] hdet = wrap_and_calc_psf ( xpts , ypts , z , func ) else : hdet , toss = get_hsym_asym ( rho3 * kfki , z3 * kfki , zint = kfki * zint , get_hdet = True , ** kwargs ) if normalize : hilm /= hilm . sum ( ) hdet /= hdet . sum ( ) for a in range ( x . size ) : hdet [ a ] *= hilm return hdet if normalize else hdet / hdet . sum ( )
6619	def poll ( self ) : finished_procs = [ p for p in self . running_procs if p . poll ( ) is not None ] self . running_procs = collections . deque ( [ p for p in self . running_procs if p not in finished_procs ] ) for proc in finished_procs : stdout , stderr = proc . communicate ( ) finished_pids = [ p . pid for p in finished_procs ] self . finished_pids . extend ( finished_pids ) logger = logging . getLogger ( __name__ ) messages = 'Running: {}, Finished: {}' . format ( len ( self . running_procs ) , len ( self . finished_pids ) ) logger . info ( messages ) return finished_pids
7776	def __from_xml ( self , value ) : n = value . children vns = get_node_ns ( value ) while n : if n . type != 'element' : n = n . next continue ns = get_node_ns ( n ) if ( ns and vns and ns . getContent ( ) != vns . getContent ( ) ) : n = n . next continue if n . name == 'POBOX' : self . pobox = unicode ( n . getContent ( ) , "utf-8" , "replace" ) elif n . name in ( 'EXTADR' , 'EXTADD' ) : self . extadr = unicode ( n . getContent ( ) , "utf-8" , "replace" ) elif n . name == 'STREET' : self . street = unicode ( n . getContent ( ) , "utf-8" , "replace" ) elif n . name == 'LOCALITY' : self . locality = unicode ( n . getContent ( ) , "utf-8" , "replace" ) elif n . name == 'REGION' : self . region = unicode ( n . getContent ( ) , "utf-8" , "replace" ) elif n . name == 'PCODE' : self . pcode = unicode ( n . getContent ( ) , "utf-8" , "replace" ) elif n . name == 'CTRY' : self . ctry = unicode ( n . getContent ( ) , "utf-8" , "replace" ) elif n . name in ( "HOME" , "WORK" , "POSTAL" , "PARCEL" , "DOM" , "INTL" , "PREF" ) : self . type . append ( n . name . lower ( ) ) n = n . next if self . type == [ ] : self . type = [ "intl" , "postal" , "parcel" , "work" ] elif "dom" in self . type and "intl" in self . type : raise ValueError ( "Both 'dom' and 'intl' specified in vcard ADR" )
12778	def render ( self , dt ) : for frame in self . _frozen : for body in frame : self . draw_body ( body ) for body in self . world . bodies : self . draw_body ( body ) if hasattr ( self . world , 'markers' ) : window . glColor4f ( 0.9 , 0.1 , 0.1 , 0.9 ) window . glLineWidth ( 3 ) for j in self . world . markers . joints . values ( ) : window . glBegin ( window . GL_LINES ) window . glVertex3f ( * j . getAnchor ( ) ) window . glVertex3f ( * j . getAnchor2 ( ) ) window . glEnd ( )
8363	def get_key_map ( self ) : kdict = { } for gdk_name in dir ( Gdk ) : nb_name = gdk_name . upper ( ) kdict [ nb_name ] = getattr ( Gdk , gdk_name ) return kdict
12035	def sweepYfiltered ( self ) : assert self . kernel is not None return swhlab . common . convolve ( self . sweepY , self . kernel )
12745	def pid ( kp = 0. , ki = 0. , kd = 0. , smooth = 0.1 ) : r state = dict ( p = 0 , i = 0 , d = 0 ) def control ( error , dt = 1 ) : state [ 'd' ] = smooth * state [ 'd' ] + ( 1 - smooth ) * ( error - state [ 'p' ] ) / dt state [ 'i' ] += error * dt state [ 'p' ] = error return kp * state [ 'p' ] + ki * state [ 'i' ] + kd * state [ 'd' ] return control
6075	def einstein_radius_in_units ( self , unit_length = 'arcsec' , kpc_per_arcsec = None ) : if self . has_mass_profile : return sum ( map ( lambda p : p . einstein_radius_in_units ( unit_length = unit_length , kpc_per_arcsec = kpc_per_arcsec ) , self . mass_profiles ) ) else : return None
6240	def render_lights_debug ( self , camera_matrix , projection ) : self . ctx . enable ( moderngl . BLEND ) self . ctx . blend_func = moderngl . SRC_ALPHA , moderngl . ONE_MINUS_SRC_ALPHA for light in self . point_lights : m_mv = matrix44 . multiply ( light . matrix , camera_matrix ) light_size = light . radius self . debug_shader [ "m_proj" ] . write ( projection . tobytes ( ) ) self . debug_shader [ "m_mv" ] . write ( m_mv . astype ( 'f4' ) . tobytes ( ) ) self . debug_shader [ "size" ] . value = light_size self . unit_cube . render ( self . debug_shader , mode = moderngl . LINE_STRIP ) self . ctx . disable ( moderngl . BLEND )
11254	def attrs ( prev , attr_names ) : for obj in prev : attr_values = [ ] for name in attr_names : if hasattr ( obj , name ) : attr_values . append ( getattr ( obj , name ) ) yield attr_values
248	def make_transaction_frame ( transactions ) : transaction_list = [ ] for dt in transactions . index : txns = transactions . loc [ dt ] if len ( txns ) == 0 : continue for txn in txns : txn = map_transaction ( txn ) transaction_list . append ( txn ) df = pd . DataFrame ( sorted ( transaction_list , key = lambda x : x [ 'dt' ] ) ) df [ 'txn_dollars' ] = - df [ 'amount' ] * df [ 'price' ] df . index = list ( map ( pd . Timestamp , df . dt . values ) ) return df
2404	def gen_prompt_feats ( self , e_set ) : prompt_toks = nltk . word_tokenize ( e_set . _prompt ) expand_syns = [ ] for word in prompt_toks : synonyms = util_functions . get_wordnet_syns ( word ) expand_syns . append ( synonyms ) expand_syns = list ( chain . from_iterable ( expand_syns ) ) prompt_overlap = [ ] prompt_overlap_prop = [ ] for j in e_set . _tokens : tok_length = len ( j ) if ( tok_length == 0 ) : tok_length = 1 prompt_overlap . append ( len ( [ i for i in j if i in prompt_toks ] ) ) prompt_overlap_prop . append ( prompt_overlap [ len ( prompt_overlap ) - 1 ] / float ( tok_length ) ) expand_overlap = [ ] expand_overlap_prop = [ ] for j in e_set . _tokens : tok_length = len ( j ) if ( tok_length == 0 ) : tok_length = 1 expand_overlap . append ( len ( [ i for i in j if i in expand_syns ] ) ) expand_overlap_prop . append ( expand_overlap [ len ( expand_overlap ) - 1 ] / float ( tok_length ) ) prompt_arr = numpy . array ( ( prompt_overlap , prompt_overlap_prop , expand_overlap , expand_overlap_prop ) ) . transpose ( ) return prompt_arr . copy ( )
10760	def from_rectilinear ( cls , x , y , z , formatter = numpy_formatter ) : x = np . asarray ( x , dtype = np . float64 ) y = np . asarray ( y , dtype = np . float64 ) z = np . ma . asarray ( z , dtype = np . float64 ) if x . ndim != 1 : raise TypeError ( "'x' must be a 1D array but is a {:d}D array" . format ( x . ndim ) ) if y . ndim != 1 : raise TypeError ( "'y' must be a 1D array but is a {:d}D array" . format ( y . ndim ) ) if z . ndim != 2 : raise TypeError ( "'z' must be a 2D array but it a {:d}D array" . format ( z . ndim ) ) if x . size != z . shape [ 1 ] : raise TypeError ( ( "the length of 'x' must be equal to the number of columns in " "'z' but the length of 'x' is {:d} and 'z' has {:d} " "columns" ) . format ( x . size , z . shape [ 1 ] ) ) if y . size != z . shape [ 0 ] : raise TypeError ( ( "the length of 'y' must be equal to the number of rows in " "'z' but the length of 'y' is {:d} and 'z' has {:d} " "rows" ) . format ( y . size , z . shape [ 0 ] ) ) y , x = np . meshgrid ( y , x , indexing = 'ij' ) return cls ( x , y , z , formatter )
1560	def register_metric ( self , name , metric , time_bucket_in_sec ) : collector = self . get_metrics_collector ( ) collector . register_metric ( name , metric , time_bucket_in_sec )
4786	def ends_with ( self , suffix ) : if suffix is None : raise TypeError ( 'given suffix arg must not be none' ) if isinstance ( self . val , str_types ) : if not isinstance ( suffix , str_types ) : raise TypeError ( 'given suffix arg must be a string' ) if len ( suffix ) == 0 : raise ValueError ( 'given suffix arg must not be empty' ) if not self . val . endswith ( suffix ) : self . _err ( 'Expected <%s> to end with <%s>, but did not.' % ( self . val , suffix ) ) elif isinstance ( self . val , Iterable ) : if len ( self . val ) == 0 : raise ValueError ( 'val must not be empty' ) last = None for last in self . val : pass if last != suffix : self . _err ( 'Expected %s to end with <%s>, but did not.' % ( self . val , suffix ) ) else : raise TypeError ( 'val is not a string or iterable' ) return self
150	def deepcopy ( self ) : polys = [ poly . deepcopy ( ) for poly in self . polygons ] return PolygonsOnImage ( polys , tuple ( self . shape ) )
13788	def open ( name = None , fileobj = None , closefd = True ) : return Guesser ( ) . open ( name = name , fileobj = fileobj , closefd = closefd )
10471	def _queueEvent ( self , event , args ) : if not hasattr ( self , 'eventList' ) : self . eventList = deque ( [ ( event , args ) ] ) return self . eventList . append ( ( event , args ) )
1058	def remove_extension ( module , name , code ) : key = ( module , name ) if ( _extension_registry . get ( key ) != code or _inverted_registry . get ( code ) != key ) : raise ValueError ( "key %s is not registered with code %s" % ( key , code ) ) del _extension_registry [ key ] del _inverted_registry [ code ] if code in _extension_cache : del _extension_cache [ code ]
9593	def switch_to_frame ( self , frame_reference = None ) : if frame_reference is not None and type ( frame_reference ) not in [ int , WebElement ] : raise TypeError ( 'Type of frame_reference must be None or int or WebElement' ) self . _execute ( Command . SWITCH_TO_FRAME , { 'id' : frame_reference } )
12471	def get_abspath ( folderpath ) : if not op . exists ( folderpath ) : raise FolderNotFound ( folderpath ) return op . abspath ( folderpath )
10946	def do_internal_run ( self ) : if not self . save_J : raise RuntimeError ( 'self.save_J=True required for do_internal_run()' ) if not np . all ( self . _has_saved_J ) : raise RuntimeError ( 'J, JTJ have not been pre-computed. Call do_run_1 or do_run_2' ) self . _do_run ( mode = 'internal' )
12300	def search ( self , what , name = None , version = None ) : filtered = { } if what is None : whats = list ( self . plugins . keys ( ) ) elif what is not None : if what not in self . plugins : raise Exception ( "Unknown class of plugins" ) whats = [ what ] for what in whats : if what not in filtered : filtered [ what ] = [ ] for key in self . plugins [ what ] . keys ( ) : ( k_name , k_version ) = key if name is not None and k_name != name : continue if version is not None and k_version != version : continue if self . plugins [ what ] [ key ] . enable == 'n' : continue filtered [ what ] . append ( key ) return filtered
10394	def calculate_average_score_by_annotation ( graph : BELGraph , annotation : str , key : Optional [ str ] = None , runs : Optional [ int ] = None , use_tqdm : bool = False , ) -> Mapping [ str , float ] : candidate_mechanisms = generate_bioprocess_mechanisms ( graph , key = key ) scores : Mapping [ BaseEntity , Tuple ] = calculate_average_scores_on_subgraphs ( subgraphs = candidate_mechanisms , key = key , runs = runs , use_tqdm = use_tqdm , ) subgraph_bp : Mapping [ str , List [ BaseEntity ] ] = defaultdict ( list ) subgraphs : Mapping [ str , BELGraph ] = get_subgraphs_by_annotation ( graph , annotation ) for annotation_value , subgraph in subgraphs . items ( ) : subgraph_bp [ annotation_value ] . extend ( get_nodes_by_function ( subgraph , BIOPROCESS ) ) return { annotation_value : np . average ( scores [ bp ] [ 0 ] for bp in bps ) for annotation_value , bps in subgraph_bp . items ( ) }
472	def build_reverse_dictionary ( word_to_id ) : reverse_dictionary = dict ( zip ( word_to_id . values ( ) , word_to_id . keys ( ) ) ) return reverse_dictionary
5128	def transitions ( self , return_matrix = True ) : if return_matrix : mat = np . zeros ( ( self . nV , self . nV ) ) for v in self . g . nodes ( ) : ind = [ e [ 1 ] for e in sorted ( self . g . out_edges ( v ) ) ] mat [ v , ind ] = self . _route_probs [ v ] else : mat = { k : { e [ 1 ] : p for e , p in zip ( sorted ( self . g . out_edges ( k ) ) , value ) } for k , value in enumerate ( self . _route_probs ) } return mat
11414	def record_get_subfields ( rec , tag , field_position_global = None , field_position_local = None ) : field = record_get_field ( rec , tag , field_position_global = field_position_global , field_position_local = field_position_local ) return field [ 0 ]
224	async def receive ( self ) -> Message : if self . client_state == WebSocketState . CONNECTING : message = await self . _receive ( ) message_type = message [ "type" ] assert message_type == "websocket.connect" self . client_state = WebSocketState . CONNECTED return message elif self . client_state == WebSocketState . CONNECTED : message = await self . _receive ( ) message_type = message [ "type" ] assert message_type in { "websocket.receive" , "websocket.disconnect" } if message_type == "websocket.disconnect" : self . client_state = WebSocketState . DISCONNECTED return message else : raise RuntimeError ( 'Cannot call "receive" once a disconnect message has been received.' )
4088	def asyncSlot ( * args ) : def outer_decorator ( fn ) : @ Slot ( * args ) @ functools . wraps ( fn ) def wrapper ( * args , ** kwargs ) : asyncio . ensure_future ( fn ( * args , ** kwargs ) ) return wrapper return outer_decorator
1741	def add_inputs ( self , xs ) : states = [ ] cur = self for x in xs : cur = cur . add_input ( x ) states . append ( cur ) return states
9610	def execute ( self , command , data = { } ) : method , uri = command try : path = self . _formatter . format_map ( uri , data ) body = self . _formatter . get_unused_kwargs ( ) url = "{0}{1}" . format ( self . _url , path ) return self . _request ( method , url , body ) except KeyError as err : LOGGER . debug ( 'Endpoint {0} is missing argument {1}' . format ( uri , err ) ) raise
1880	def PSRLQ ( cpu , dest , src ) : count = src . read ( ) count = Operators . ITEBV ( src . size , Operators . UGT ( count , 63 ) , 64 , count ) count = Operators . EXTRACT ( count , 0 , 64 ) if dest . size == 64 : dest . write ( dest . read ( ) >> count ) else : hi = Operators . EXTRACT ( dest . read ( ) , 64 , 64 ) >> count low = Operators . EXTRACT ( dest . read ( ) , 0 , 64 ) >> count dest . write ( Operators . CONCAT ( 128 , hi , low ) )
12451	def deref ( self , data ) : deref = copy . deepcopy ( jsonref . JsonRef . replace_refs ( data ) ) self . write_template ( deref , filename = 'swagger.json' ) return deref
10064	def process_schema ( value ) : schemas = current_app . extensions [ 'invenio-jsonschemas' ] . schemas try : return schemas [ value ] except KeyError : raise click . BadParameter ( 'Unknown schema {0}. Please use one of:\n {1}' . format ( value , '\n' . join ( schemas . keys ( ) ) ) )
8716	def node_heap ( self ) : log . info ( 'Heap' ) res = self . __exchange ( 'print(node.heap())' ) log . info ( res ) return int ( res . split ( '\r\n' ) [ 1 ] )
7822	def _make_response ( self , nonce , salt , iteration_count ) : self . _salted_password = self . Hi ( self . Normalize ( self . password ) , salt , iteration_count ) self . password = None if self . channel_binding : channel_binding = b"c=" + standard_b64encode ( self . _gs2_header + self . _cb_data ) else : channel_binding = b"c=" + standard_b64encode ( self . _gs2_header ) client_final_message_without_proof = ( channel_binding + b",r=" + nonce ) client_key = self . HMAC ( self . _salted_password , b"Client Key" ) stored_key = self . H ( client_key ) auth_message = ( self . _client_first_message_bare + b"," + self . _server_first_message + b"," + client_final_message_without_proof ) self . _auth_message = auth_message client_signature = self . HMAC ( stored_key , auth_message ) client_proof = self . XOR ( client_key , client_signature ) proof = b"p=" + standard_b64encode ( client_proof ) client_final_message = ( client_final_message_without_proof + b"," + proof ) return Response ( client_final_message )
1668	def ShouldCheckNamespaceIndentation ( nesting_state , is_namespace_indent_item , raw_lines_no_comments , linenum ) : is_forward_declaration = IsForwardClassDeclaration ( raw_lines_no_comments , linenum ) if not ( is_namespace_indent_item or is_forward_declaration ) : return False if IsMacroDefinition ( raw_lines_no_comments , linenum ) : return False return IsBlockInNameSpace ( nesting_state , is_forward_declaration )
10572	def walk_depth ( path , max_depth = float ( 'inf' ) ) : start_level = os . path . abspath ( path ) . count ( os . path . sep ) for dir_entry in os . walk ( path ) : root , dirs , _ = dir_entry level = root . count ( os . path . sep ) - start_level yield dir_entry if level >= max_depth : dirs [ : ] = [ ]
8772	def _remove_default_tz_bindings ( self , context , network_id ) : default_tz = CONF . NVP . default_tz if not default_tz : LOG . warn ( "additional_default_tz_types specified, " "but no default_tz. Skipping " "_remove_default_tz_bindings()." ) return if not network_id : LOG . warn ( "neutron network_id not specified, skipping " "_remove_default_tz_bindings()" ) return for net_type in CONF . NVP . additional_default_tz_types : if net_type in TZ_BINDINGS : binding = TZ_BINDINGS [ net_type ] binding . remove ( context , default_tz , network_id ) else : LOG . warn ( "Unknown default tz type %s" % ( net_type ) )
12056	def ftp_folder_match ( ftp , localFolder , deleteStuff = True ) : for fname in glob . glob ( localFolder + "/*.*" ) : ftp_upload ( ftp , fname ) return
11947	def jocker ( test_options = None ) : version = ver_check ( ) options = test_options or docopt ( __doc__ , version = version ) _set_global_verbosity_level ( options . get ( '--verbose' ) ) jocker_lgr . debug ( options ) jocker_run ( options )
10691	def rgb_to_hex ( rgb ) : r , g , b = rgb return "#{0}{1}{2}" . format ( hex ( int ( r ) ) [ 2 : ] . zfill ( 2 ) , hex ( int ( g ) ) [ 2 : ] . zfill ( 2 ) , hex ( int ( b ) ) [ 2 : ] . zfill ( 2 ) )
7675	def pprint_jobject ( obj , ** kwargs ) : obj_simple = { k : v for k , v in six . iteritems ( obj . __json__ ) if v } string = json . dumps ( obj_simple , ** kwargs ) string = re . sub ( r'[{}"]' , '' , string ) string = re . sub ( r',\n' , '\n' , string ) string = re . sub ( r'^\s*$' , '' , string ) return string
3936	def set ( self , refresh_token ) : logger . info ( 'Saving refresh_token to %s' , repr ( self . _filename ) ) try : with open ( self . _filename , 'w' ) as f : f . write ( refresh_token ) except IOError as e : logger . warning ( 'Failed to save refresh_token: %s' , e )
2831	def set_training ( model , mode ) : if mode is None : yield return old_mode = model . training if old_mode != mode : model . train ( mode ) try : yield finally : if old_mode != mode : model . train ( old_mode )
7555	def store_random ( self ) : with h5py . File ( self . database . input , 'a' ) as io5 : fillsets = io5 [ "quartets" ] qiter = itertools . combinations ( xrange ( len ( self . samples ) ) , 4 ) rand = np . arange ( 0 , n_choose_k ( len ( self . samples ) , 4 ) ) np . random . shuffle ( rand ) rslice = rand [ : self . params . nquartets ] rss = np . sort ( rslice ) riter = iter ( rss ) del rand , rslice print ( self . _chunksize ) rando = riter . next ( ) tmpr = np . zeros ( ( self . params . nquartets , 4 ) , dtype = np . uint16 ) tidx = 0 while 1 : try : for i , j in enumerate ( qiter ) : if i == rando : tmpr [ tidx ] = j tidx += 1 rando = riter . next ( ) if not i % self . _chunksize : print ( min ( i , self . params . nquartets ) ) except StopIteration : break fillsets [ : ] = tmpr del tmpr
946	def _isCheckpointDir ( checkpointDir ) : lastSegment = os . path . split ( checkpointDir ) [ 1 ] if lastSegment [ 0 ] == '.' : return False if not checkpointDir . endswith ( g_defaultCheckpointExtension ) : return False if not os . path . isdir ( checkpointDir ) : return False return True
13442	def cmd_init_pull_from_cloud ( args ) : ( lcat , ccat ) = ( args . local_catalog , args . cloud_catalog ) logging . info ( "[init-pull-from-cloud]: %s => %s" % ( ccat , lcat ) ) if isfile ( lcat ) : args . error ( "[init-pull-from-cloud] The local catalog already exist: %s" % lcat ) if not isfile ( ccat ) : args . error ( "[init-pull-from-cloud] The cloud catalog does not exist: %s" % ccat ) ( lmeta , cmeta ) = ( "%s.lrcloud" % lcat , "%s.lrcloud" % ccat ) if isfile ( lmeta ) : args . error ( "[init-pull-from-cloud] The local meta-data already exist: %s" % lmeta ) if not isfile ( cmeta ) : args . error ( "[init-pull-from-cloud] The cloud meta-data does not exist: %s" % cmeta ) logging . info ( "Locking local catalog: %s" % ( lcat ) ) if not lock_file ( lcat ) : raise RuntimeError ( "The catalog %s is locked!" % lcat ) util . copy ( ccat , lcat ) cloudDAG = ChangesetDAG ( ccat ) path = cloudDAG . path ( cloudDAG . root . hash , cloudDAG . leafs [ 0 ] . hash ) util . apply_changesets ( args , path , lcat ) mfile = MetaFile ( lmeta ) utcnow = datetime . utcnow ( ) . strftime ( DATETIME_FORMAT ) [ : - 4 ] mfile [ 'catalog' ] [ 'hash' ] = hashsum ( lcat ) mfile [ 'catalog' ] [ 'modification_utc' ] = utcnow mfile [ 'catalog' ] [ 'filename' ] = lcat mfile [ 'last_push' ] [ 'filename' ] = cloudDAG . leafs [ 0 ] . mfile [ 'changeset' ] [ 'filename' ] mfile [ 'last_push' ] [ 'hash' ] = cloudDAG . leafs [ 0 ] . mfile [ 'changeset' ] [ 'hash' ] mfile [ 'last_push' ] [ 'modification_utc' ] = cloudDAG . leafs [ 0 ] . mfile [ 'changeset' ] [ 'modification_utc' ] mfile . flush ( ) if not args . no_smart_previews : copy_smart_previews ( lcat , ccat , local2cloud = False ) logging . info ( "Unlocking local catalog: %s" % ( lcat ) ) unlock_file ( lcat ) logging . info ( "[init-pull-from-cloud]: Success!" )
13224	def main ( ) : parser = argparse . ArgumentParser ( description = 'Discover and ingest metadata from document sources, ' 'including lsstdoc-based LaTeX documents and ' 'reStructuredText-based technotes. Metadata can be ' 'upserted into the LSST Projectmeta MongoDB.' ) parser . add_argument ( '--ltd-product' , dest = 'ltd_product_url' , help = 'URL of an LSST the Docs product ' '(https://keeper.lsst.codes/products/<slug>). If provided, ' 'only this document will be ingested.' ) parser . add_argument ( '--github-token' , help = 'GitHub personal access token.' ) parser . add_argument ( '--mongodb-uri' , help = 'MongoDB connection URI. If provided, metadata will be loaded ' 'into the Projectmeta database. Omit this argument to just ' 'test the ingest pipeline.' ) parser . add_argument ( '--mongodb-db' , default = 'lsstprojectmeta' , help = 'Name of MongoDB database' ) parser . add_argument ( '--mongodb-collection' , default = 'resources' , help = 'Name of the MongoDB collection for projectmeta resources' ) args = parser . parse_args ( ) stream_handler = logging . StreamHandler ( ) stream_formatter = logging . Formatter ( '%(asctime)s %(levelname)8s %(name)s | %(message)s' ) stream_handler . setFormatter ( stream_formatter ) root_logger = logging . getLogger ( ) root_logger . addHandler ( stream_handler ) root_logger . setLevel ( logging . WARNING ) app_logger = logging . getLogger ( 'lsstprojectmeta' ) app_logger . setLevel ( logging . DEBUG ) if args . mongodb_uri is not None : mongo_client = AsyncIOMotorClient ( args . mongodb_uri , ssl = True ) collection = mongo_client [ args . mongodb_db ] [ args . mongodb_collection ] else : collection = None loop = asyncio . get_event_loop ( ) if args . ltd_product_url is not None : loop . run_until_complete ( run_single_ltd_doc ( args . ltd_product_url , args . github_token , collection ) ) else : loop . run_until_complete ( run_bulk_etl ( args . github_token , collection ) )
8219	def do_unfullscreen ( self , widget ) : self . unfullscreen ( ) self . is_fullscreen = False self . bot . _screen_ratio = None
12478	def get_sys_path ( rcpath , app_name , section_name = None ) : if op . exists ( rcpath ) : return op . realpath ( op . expanduser ( rcpath ) ) try : settings = rcfile ( app_name , section_name ) except : raise try : sys_path = op . expanduser ( settings [ rcpath ] ) except KeyError : raise IOError ( 'Could not find an existing variable with name {0} in' ' section {1} of {2}rc config setup. Maybe it is a ' ' folder that could not be found.' . format ( rcpath , section_name , app_name ) ) else : if not op . exists ( sys_path ) : raise IOError ( 'Could not find the path {3} indicated by the ' 'variable {0} in section {1} of {2}rc config ' 'setup.' . format ( rcpath , section_name , app_name , sys_path ) ) return op . realpath ( op . expanduser ( sys_path ) )
7723	def get_password ( self ) : for child in xml_element_iter ( self . xmlnode . children ) : if get_node_ns_uri ( child ) == MUC_NS and child . name == "password" : return from_utf8 ( child . getContent ( ) ) return None
1769	def concrete_emulate ( self , insn ) : if not self . emu : self . emu = ConcreteUnicornEmulator ( self ) self . emu . _stop_at = self . _break_unicorn_at try : self . emu . emulate ( insn ) except unicorn . UcError as e : if e . errno == unicorn . UC_ERR_INSN_INVALID : text_bytes = ' ' . join ( '%02x' % x for x in insn . bytes ) logger . error ( "Unimplemented instruction: 0x%016x:\t%s\t%s\t%s" , insn . address , text_bytes , insn . mnemonic , insn . op_str ) raise InstructionEmulationError ( str ( e ) )
2484	def create_checksum_node ( self , chksum ) : chksum_node = BNode ( ) type_triple = ( chksum_node , RDF . type , self . spdx_namespace . Checksum ) self . graph . add ( type_triple ) algorithm_triple = ( chksum_node , self . spdx_namespace . algorithm , Literal ( chksum . identifier ) ) self . graph . add ( algorithm_triple ) value_triple = ( chksum_node , self . spdx_namespace . checksumValue , Literal ( chksum . value ) ) self . graph . add ( value_triple ) return chksum_node
12957	def _get_key_for_index ( self , indexedField , val ) : if hasattr ( indexedField , 'toIndex' ) : val = indexedField . toIndex ( val ) else : val = self . fields [ indexedField ] . toIndex ( val ) return '' . join ( [ INDEXED_REDIS_PREFIX , self . keyName , ':idx:' , indexedField , ':' , val ] )
1348	def write_success_response ( self , result ) : response = self . make_success_response ( result ) now = time . time ( ) spent = now - self . basehandler_starttime response [ constants . RESPONSE_KEY_EXECUTION_TIME ] = spent self . write_json_response ( response )
9032	def _place_row ( self , row , position ) : self . _rows_in_grid [ row ] = RowInGrid ( row , position )
8090	def textheight ( self , txt , width = None ) : w = width return self . textmetrics ( txt , width = w ) [ 1 ]
13087	def config_dir ( self ) : home = expanduser ( '~' ) config_dir = os . path . join ( home , '.jackal' ) return config_dir
6492	def _process_facet_terms ( facet_terms ) : elastic_facets = { } for facet in facet_terms : facet_term = { "field" : facet } if facet_terms [ facet ] : for facet_option in facet_terms [ facet ] : facet_term [ facet_option ] = facet_terms [ facet ] [ facet_option ] elastic_facets [ facet ] = { "terms" : facet_term } return elastic_facets
9218	def _smixins ( self , name ) : return ( self . _mixins [ name ] if name in self . _mixins else False )
1531	def get_pplan ( self , topologyName , callback = None ) : if callback : self . pplan_watchers [ topologyName ] . append ( callback ) else : pplan_path = self . get_pplan_path ( topologyName ) with open ( pplan_path ) as f : data = f . read ( ) pplan = PhysicalPlan ( ) pplan . ParseFromString ( data ) return pplan
3874	def _add_conversation ( self , conversation , events = [ ] , event_cont_token = None ) : conv_id = conversation . conversation_id . id logger . debug ( 'Adding new conversation: {}' . format ( conv_id ) ) conv = Conversation ( self . _client , self . _user_list , conversation , events , event_cont_token ) self . _conv_dict [ conv_id ] = conv return conv
3357	def _extend_nocheck ( self , iterable ) : current_length = len ( self ) list . extend ( self , iterable ) _dict = self . _dict if current_length is 0 : self . _generate_index ( ) return for i , obj in enumerate ( islice ( self , current_length , None ) , current_length ) : _dict [ obj . id ] = i
6202	def merge_da ( ts_d , ts_par_d , ts_a , ts_par_a ) : ts = np . hstack ( [ ts_d , ts_a ] ) ts_par = np . hstack ( [ ts_par_d , ts_par_a ] ) a_ch = np . hstack ( [ np . zeros ( ts_d . shape [ 0 ] , dtype = bool ) , np . ones ( ts_a . shape [ 0 ] , dtype = bool ) ] ) index_sort = ts . argsort ( ) return ts [ index_sort ] , a_ch [ index_sort ] , ts_par [ index_sort ]
846	def _getDistances ( self , inputPattern , partitionId = None ) : if not self . _finishedLearning : self . finishLearning ( ) self . _finishedLearning = True if self . _vt is not None and len ( self . _vt ) > 0 : inputPattern = numpy . dot ( self . _vt , inputPattern - self . _mean ) sparseInput = self . _sparsifyVector ( inputPattern ) dist = self . _calcDistance ( sparseInput ) if self . _specificIndexTraining : dist [ numpy . array ( self . _categoryList ) == - 1 ] = numpy . inf if partitionId is not None : dist [ self . _partitionIdMap . get ( partitionId , [ ] ) ] = numpy . inf return dist
11475	def renew_token ( ) : session . token = session . communicator . login_with_api_key ( session . email , session . api_key , application = session . application ) if len ( session . token ) < 10 : one_time_pass = getpass . getpass ( 'One-Time Password: ' ) session . token = session . communicator . mfa_otp_login ( session . token , one_time_pass ) return session . token
6776	def _configure_users ( self , site = None , full = 0 , only_data = 0 ) : site = site or ALL full = int ( full ) if full and not only_data : packager = self . get_satchel ( 'packager' ) packager . install_required ( type = SYSTEM , service = self . name ) r = self . local_renderer params = self . get_user_vhosts ( site = site ) with settings ( warn_only = True ) : self . add_admin_user ( ) params = sorted ( list ( params ) ) if not only_data : for user , password , vhost in params : r . env . broker_user = user r . env . broker_password = password r . env . broker_vhost = vhost with settings ( warn_only = True ) : r . sudo ( 'rabbitmqctl add_user {broker_user} {broker_password}' ) r . sudo ( 'rabbitmqctl add_vhost {broker_vhost}' ) r . sudo ( 'rabbitmqctl set_permissions -p {broker_vhost} {broker_user} ".*" ".*" ".*"' ) r . sudo ( 'rabbitmqctl set_permissions -p {broker_vhost} {admin_username} ".*" ".*" ".*"' ) return params
5138	def process_file ( self , file ) : if sys . version_info [ 0 ] >= 3 : nxt = file . __next__ else : nxt = file . next for token in tokenize . generate_tokens ( nxt ) : self . process_token ( * token ) self . make_index ( )
5786	def _raw_write ( self ) : data_available = libssl . BIO_ctrl_pending ( self . _wbio ) if data_available == 0 : return b'' to_read = min ( self . _buffer_size , data_available ) read = libssl . BIO_read ( self . _wbio , self . _bio_write_buffer , to_read ) to_write = bytes_from_buffer ( self . _bio_write_buffer , read ) output = to_write while len ( to_write ) : raise_disconnect = False try : sent = self . _socket . send ( to_write ) except ( socket_ . error ) as e : if e . errno == 104 or e . errno == 32 : raise_disconnect = True else : raise if raise_disconnect : raise_disconnection ( ) to_write = to_write [ sent : ] if len ( to_write ) : self . select_write ( ) return output
7932	def connect ( self , server = None , port = None ) : self . lock . acquire ( ) try : self . _connect ( server , port ) finally : self . lock . release ( )
3091	def locked_delete ( self ) : if self . _cache : self . _cache . delete ( self . _key_name ) self . _delete_entity ( )
6809	def configure_camera ( self ) : r = self . local_renderer if self . env . camera_enabled : r . pc ( 'Enabling camera.' ) r . enable_attr ( filename = '/boot/config.txt' , key = 'start_x' , value = 1 , use_sudo = True , ) r . enable_attr ( filename = '/boot/config.txt' , key = 'gpu_mem' , value = r . env . gpu_mem , use_sudo = True , ) r . run ( 'cd ~; git clone https://github.com/raspberrypi/userland.git; cd userland; ./buildme' ) r . run ( 'touch ~/.bash_aliases' ) r . append ( r'PATH=$PATH:/opt/vc/bin\nexport PATH' , '~/.bash_aliases' ) r . append ( r'LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/vc/lib\nexport LD_LIBRARY_PATH' , '~/.bash_aliases' ) r . run ( 'source ~/.bashrc' ) r . sudo ( 'ldconfig' ) r . sudo ( "echo 'SUBSYSTEM==\"vchiq\",GROUP=\"video\",MODE=\"0660\"' > /etc/udev/rules.d/10-vchiq-permissions.rules" ) r . sudo ( "usermod -a -G video {user}" ) r . reboot ( wait = 300 , timeout = 60 ) self . test_camera ( ) else : r . disable_attr ( filename = '/boot/config.txt' , key = 'start_x' , use_sudo = True , ) r . disable_attr ( filename = '/boot/config.txt' , key = 'gpu_mem' , use_sudo = True , ) r . reboot ( wait = 300 , timeout = 60 )
10001	def get_nodes_with ( self , obj ) : result = set ( ) if nx . __version__ [ 0 ] == "1" : nodes = self . nodes_iter ( ) else : nodes = self . nodes for node in nodes : if node [ OBJ ] == obj : result . add ( node ) return result
930	def _createAggregateRecord ( self ) : record = [ ] for i , ( fieldIdx , aggFP , paramIdx ) in enumerate ( self . _fields ) : if aggFP is None : continue values = self . _slice [ i ] refIndex = None if paramIdx is not None : record . append ( aggFP ( values , self . _slice [ paramIdx ] ) ) else : record . append ( aggFP ( values ) ) return record
2141	def parse_kv ( var_string ) : return_dict = { } if var_string is None : return { } fix_encoding_26 = False if sys . version_info < ( 2 , 7 ) and '\x00' in shlex . split ( u'a' ) [ 0 ] : fix_encoding_26 = True is_unicode = False if fix_encoding_26 or not isinstance ( var_string , str ) : if isinstance ( var_string , six . text_type ) : var_string = var_string . encode ( 'UTF-8' ) is_unicode = True else : var_string = str ( var_string ) for token in shlex . split ( var_string ) : if ( is_unicode ) : token = token . decode ( 'UTF-8' ) if fix_encoding_26 : token = six . text_type ( token ) if '=' in token : ( k , v ) = token . split ( '=' , 1 ) if len ( k ) == 0 or len ( v ) == 0 : raise Exception try : return_dict [ k ] = ast . literal_eval ( v ) except Exception : return_dict [ k ] = v else : raise Exception return return_dict
2955	def update ( self , containers ) : self . _containers = deepcopy ( containers ) self . __write ( containers , initialize = False )
6781	def get_current_thumbprint ( self , components = None ) : components = str_to_component_list ( components ) if self . verbose : print ( 'deploy.get_current_thumbprint.components:' , components ) manifest_data = { } for component_name , func in sorted ( manifest_recorder . items ( ) ) : self . vprint ( 'Checking thumbprint for component %s...' % component_name ) manifest_key = assert_valid_satchel ( component_name ) service_name = clean_service_name ( component_name ) if service_name not in self . genv . services : self . vprint ( 'Skipping unused component:' , component_name ) continue elif components and service_name not in components : self . vprint ( 'Skipping non-matching component:' , component_name ) continue try : self . vprint ( 'Retrieving manifest for %s...' % component_name ) manifest_data [ manifest_key ] = func ( ) if self . verbose : pprint ( manifest_data [ manifest_key ] , indent = 4 ) except exceptions . AbortDeployment as e : raise return manifest_data
1045	def float_pack ( x , size ) : if size == 8 : MIN_EXP = - 1021 MAX_EXP = 1024 MANT_DIG = 53 BITS = 64 elif size == 4 : MIN_EXP = - 125 MAX_EXP = 128 MANT_DIG = 24 BITS = 32 else : raise ValueError ( "invalid size value" ) sign = math . copysign ( 1.0 , x ) < 0.0 if math . isinf ( x ) : mant = 0 exp = MAX_EXP - MIN_EXP + 2 elif math . isnan ( x ) : mant = 1 << ( MANT_DIG - 2 ) exp = MAX_EXP - MIN_EXP + 2 elif x == 0.0 : mant = 0 exp = 0 else : m , e = math . frexp ( abs ( x ) ) exp = e - ( MIN_EXP - 1 ) if exp > 0 : mant = round_to_nearest ( m * ( 1 << MANT_DIG ) ) mant -= 1 << MANT_DIG - 1 else : if exp + MANT_DIG - 1 >= 0 : mant = round_to_nearest ( m * ( 1 << exp + MANT_DIG - 1 ) ) else : mant = 0 exp = 0 assert 0 <= mant <= 1 << MANT_DIG - 1 if mant == 1 << MANT_DIG - 1 : mant = 0 exp += 1 if exp >= MAX_EXP - MIN_EXP + 2 : raise OverflowError ( "float too large to pack in this format" ) assert 0 <= mant < 1 << MANT_DIG - 1 assert 0 <= exp <= MAX_EXP - MIN_EXP + 2 assert 0 <= sign <= 1 return ( ( sign << BITS - 1 ) | ( exp << MANT_DIG - 1 ) ) | mant
8511	def load ( self ) : from pylearn2 . config import yaml_parse from pylearn2 . datasets import Dataset dataset = yaml_parse . load ( self . yaml_string ) assert isinstance ( dataset , Dataset ) data = dataset . iterator ( mode = 'sequential' , num_batches = 1 , data_specs = dataset . data_specs , return_tuple = True ) . next ( ) if len ( data ) == 2 : X , y = data y = np . squeeze ( y ) if self . one_hot : y = np . argmax ( y , axis = 1 ) else : X = data y = None return X , y
3914	def _on_typing ( self , typing_message ) : self . _typing_statuses [ typing_message . user_id ] = typing_message . status self . _update ( )
12698	def _parse_control_fields ( self , fields , tag_id = "tag" ) : for field in fields : params = field . params if tag_id not in params : continue self . controlfields [ params [ tag_id ] ] = field . getContent ( ) . strip ( )
6425	def tanimoto_coeff ( self , src , tar , qval = 2 ) : coeff = self . sim ( src , tar , qval ) if coeff != 0 : return log ( coeff , 2 ) return float ( '-inf' )
10947	def reset ( self ) : inds = list ( range ( self . state . obj_get_positions ( ) . shape [ 0 ] ) ) self . _rad_nms = self . state . param_particle_rad ( inds ) self . _pos_nms = self . state . param_particle_pos ( inds ) self . _initial_rad = np . copy ( self . state . state [ self . _rad_nms ] ) self . _initial_pos = np . copy ( self . state . state [ self . _pos_nms ] ) . reshape ( ( - 1 , 3 ) ) self . param_vals [ self . rscale_mask ] = 0
623	def indexFromCoordinates ( coordinates , dimensions ) : index = 0 for i , dimension in enumerate ( dimensions ) : index *= dimension index += coordinates [ i ] return index
10858	def get_update_tile ( self , params , values ) : doglobal , particles = self . _update_type ( params ) if doglobal : return self . shape . copy ( ) values0 = self . get_values ( params ) tiles0 = [ self . _tile ( n ) for n in particles ] self . set_values ( params , values ) tiles1 = [ self . _tile ( n ) for n in particles ] self . set_values ( params , values0 ) return Tile . boundingtile ( tiles0 + tiles1 )
9429	def extract ( self , member , path = None , pwd = None ) : if isinstance ( member , RarInfo ) : member = member . filename if path is None : path = os . getcwd ( ) self . _extract_members ( [ member ] , path , pwd ) return os . path . join ( path , member )
11365	def create_logger ( name , filename = None , logging_level = logging . DEBUG ) : logger = logging . getLogger ( name ) formatter = logging . Formatter ( ( '%(asctime)s - %(name)s - ' '%(levelname)-8s - %(message)s' ) ) if filename : fh = logging . FileHandler ( filename = filename ) fh . setFormatter ( formatter ) logger . addHandler ( fh ) ch = logging . StreamHandler ( ) ch . setFormatter ( formatter ) logger . addHandler ( ch ) logger . setLevel ( logging_level ) return logger
10645	def create ( dataset , symbol , degree ) : x_vals = dataset . data [ 'T' ] . tolist ( ) y_vals = dataset . data [ symbol ] . tolist ( ) coeffs = np . polyfit ( x_vals , y_vals , degree ) result = PolynomialModelT ( dataset . material , dataset . names_dict [ symbol ] , symbol , dataset . display_symbols_dict [ symbol ] , dataset . units_dict [ symbol ] , None , [ dataset . name ] , coeffs ) result . state_schema [ 'T' ] [ 'min' ] = float ( min ( x_vals ) ) result . state_schema [ 'T' ] [ 'max' ] = float ( max ( x_vals ) ) return result
7164	def add_intent ( self , name , lines , reload_cache = False ) : self . intents . add ( name , lines , reload_cache ) self . padaos . add_intent ( name , lines ) self . must_train = True
11115	def save ( self ) : repoInfoPath = os . path . join ( self . __path , ".pyrepinfo" ) try : fdinfo = open ( repoInfoPath , 'wb' ) except Exception as e : raise Exception ( "unable to open repository info for saving (%s)" % e ) try : pickle . dump ( self , fdinfo , protocol = 2 ) except Exception as e : fdinfo . flush ( ) os . fsync ( fdinfo . fileno ( ) ) fdinfo . close ( ) raise Exception ( "Unable to save repository info (%s)" % e ) finally : fdinfo . flush ( ) os . fsync ( fdinfo . fileno ( ) ) fdinfo . close ( ) repoTimePath = os . path . join ( self . __path , ".pyrepstate" ) try : self . __state = ( "%.6f" % time . time ( ) ) . encode ( ) with open ( repoTimePath , 'wb' ) as fdtime : fdtime . write ( self . __state ) fdtime . flush ( ) os . fsync ( fdtime . fileno ( ) ) except Exception as e : raise Exception ( "unable to open repository time stamp for saving (%s)" % e )
7532	def trackjobs ( func , results , spacer ) : LOGGER . info ( "inside trackjobs of %s" , func ) asyncs = [ ( i , results [ i ] ) for i in results if i . split ( "-" , 2 ) [ 0 ] == func ] start = time . time ( ) while 1 : ready = [ i [ 1 ] . ready ( ) for i in asyncs ] elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) printstr = " {} | {} | s3 |" . format ( PRINTSTR [ func ] , elapsed ) progressbar ( len ( ready ) , sum ( ready ) , printstr , spacer = spacer ) time . sleep ( 0.1 ) if len ( ready ) == sum ( ready ) : print ( "" ) break sfails = [ ] errmsgs = [ ] for job in asyncs : if not job [ 1 ] . successful ( ) : sfails . append ( job [ 0 ] ) errmsgs . append ( job [ 1 ] . result ( ) ) return func , sfails , errmsgs
13397	def get_reference_to_class ( cls , class_or_class_name ) : if isinstance ( class_or_class_name , type ) : return class_or_class_name elif isinstance ( class_or_class_name , string_types ) : if ":" in class_or_class_name : mod_name , class_name = class_or_class_name . split ( ":" ) if not mod_name in sys . modules : __import__ ( mod_name ) mod = sys . modules [ mod_name ] return mod . __dict__ [ class_name ] else : return cls . load_class_from_locals ( class_or_class_name ) else : msg = "Unexpected Type '%s'" % type ( class_or_class_name ) raise InternalCashewException ( msg )
782	def jobReactivateRunningJobs ( self ) : with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET _eng_cjm_conn_id=%%s, ' ' _eng_allocate_new_workers=TRUE ' ' WHERE status=%%s ' % ( self . jobsTableName , ) conn . cursor . execute ( query , [ self . _connectionID , self . STATUS_RUNNING ] ) return
12284	def rootdir ( self , username , reponame , create = True ) : path = os . path . join ( self . workspace , 'datasets' , username , reponame ) if create : try : os . makedirs ( path ) except : pass return path
12682	def row ( self , idx ) : return DataFrameRow ( idx , [ x [ idx ] for x in self ] , self . colnames )
12125	def pprint_args ( self , pos_args , keyword_args , infix_operator = None , extra_params = { } ) : if infix_operator and not ( len ( pos_args ) == 2 and keyword_args == [ ] ) : raise Exception ( 'Infix format requires exactly two' ' positional arguments and no keywords' ) ( kwargs , _ , _ , _ ) = self . _pprint_args self . _pprint_args = ( keyword_args + kwargs , pos_args , infix_operator , extra_params )
2040	def RETURN ( self , offset , size ) : data = self . read_buffer ( offset , size ) raise EndTx ( 'RETURN' , data )
11522	def add_condor_job ( self , token , batchmaketaskid , jobdefinitionfilename , outputfilename , errorfilename , logfilename , postfilename ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'batchmaketaskid' ] = batchmaketaskid parameters [ 'jobdefinitionfilename' ] = jobdefinitionfilename parameters [ 'outputfilename' ] = outputfilename parameters [ 'errorfilename' ] = errorfilename parameters [ 'logfilename' ] = logfilename parameters [ 'postfilename' ] = postfilename response = self . request ( 'midas.batchmake.add.condor.job' , parameters ) return response
911	def advance ( self ) : hasMore = True try : self . __iter . next ( ) except StopIteration : self . __iter = None hasMore = False return hasMore
10235	def reaction_cartesian_expansion ( graph : BELGraph , accept_unqualified_edges : bool = True ) -> None : for u , v , d in list ( graph . edges ( data = True ) ) : if CITATION not in d and accept_unqualified_edges : _reaction_cartesion_expansion_unqualified_helper ( graph , u , v , d ) continue if isinstance ( u , Reaction ) and isinstance ( v , Reaction ) : catalysts = _get_catalysts_in_reaction ( u ) | _get_catalysts_in_reaction ( v ) for reactant , product in chain ( itt . product ( u . reactants , u . products ) , itt . product ( v . reactants , v . products ) ) : if reactant in catalysts or product in catalysts : continue graph . add_increases ( reactant , product , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) for product , reactant in itt . product ( u . products , u . reactants ) : if reactant in catalysts or product in catalysts : continue graph . add_qualified_edge ( product , reactant , relation = d [ RELATION ] , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) elif isinstance ( u , Reaction ) : catalysts = _get_catalysts_in_reaction ( u ) for product in u . products : if product in catalysts : continue if v not in u . products and v not in u . reactants : graph . add_increases ( product , v , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) for reactant in u . reactants : graph . add_increases ( reactant , product , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) elif isinstance ( v , Reaction ) : for reactant in v . reactants : catalysts = _get_catalysts_in_reaction ( v ) if reactant in catalysts : continue if u not in v . products and u not in v . reactants : graph . add_increases ( u , reactant , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) for product in v . products : graph . add_increases ( reactant , product , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) _remove_reaction_nodes ( graph )
1554	def _add_in_streams ( self , bolt ) : if self . inputs is None : return input_dict = self . _sanitize_inputs ( ) for global_streamid , gtype in input_dict . items ( ) : in_stream = bolt . inputs . add ( ) in_stream . stream . CopyFrom ( self . _get_stream_id ( global_streamid . component_id , global_streamid . stream_id ) ) if isinstance ( gtype , Grouping . FIELDS ) : in_stream . gtype = gtype . gtype in_stream . grouping_fields . CopyFrom ( self . _get_stream_schema ( gtype . fields ) ) elif isinstance ( gtype , Grouping . CUSTOM ) : in_stream . gtype = gtype . gtype in_stream . custom_grouping_object = gtype . python_serialized in_stream . type = topology_pb2 . CustomGroupingObjectType . Value ( "PYTHON_OBJECT" ) else : in_stream . gtype = gtype
8891	def deserialize ( cls , dict_model ) : kwargs = { } for f in cls . _meta . concrete_fields : if f . attname in dict_model : kwargs [ f . attname ] = dict_model [ f . attname ] return cls ( ** kwargs )
8309	def pangocairo_create_context ( cr ) : try : return PangoCairo . create_context ( cr ) except KeyError as e : if e . args == ( 'could not find foreign type Context' , ) : raise ShoebotInstallError ( "Error creating PangoCairo missing dependency: python-gi-cairo" ) else : raise
9337	def wait ( self ) : e , r = self . result . get ( ) self . slave . join ( ) self . slave = None self . result = None if isinstance ( e , Exception ) : raise SlaveException ( e , r ) return r
11750	def _register_blueprint ( self , app , bp , bundle_path , child_path , description ) : base_path = sanitize_path ( self . _journey_path + bundle_path + child_path ) app . register_blueprint ( bp , url_prefix = base_path ) return { 'name' : bp . name , 'path' : child_path , 'import_name' : bp . import_name , 'description' : description , 'routes' : self . get_blueprint_routes ( app , base_path ) }
4957	def parse_csv ( file_stream , expected_columns = None ) : reader = unicodecsv . DictReader ( file_stream , encoding = "utf-8" ) if expected_columns and set ( expected_columns ) - set ( reader . fieldnames ) : raise ValidationError ( ValidationMessages . MISSING_EXPECTED_COLUMNS . format ( expected_columns = ", " . join ( expected_columns ) , actual_columns = ", " . join ( reader . fieldnames ) ) ) for row in reader : yield row
5235	def filter_by_dates ( files_or_folders : list , date_fmt = DATE_FMT ) -> list : r = re . compile ( f'.*{date_fmt}.*' ) return list ( filter ( lambda vv : r . match ( vv . replace ( '\\' , '/' ) . split ( '/' ) [ - 1 ] ) is not None , files_or_folders , ) )
11157	def print_big_dir_and_big_file ( self , top_n = 5 ) : self . assert_is_dir_and_exists ( ) size_table1 = sorted ( [ ( p , p . dirsize ) for p in self . select_dir ( recursive = False ) ] , key = lambda x : x [ 1 ] , reverse = True , ) for p1 , size1 in size_table1 [ : top_n ] : print ( "{:<9} {:<9}" . format ( repr_data_size ( size1 ) , p1 . abspath ) ) size_table2 = sorted ( [ ( p , p . size ) for p in p1 . select_file ( recursive = True ) ] , key = lambda x : x [ 1 ] , reverse = True , ) for p2 , size2 in size_table2 [ : top_n ] : print ( " {:<9} {:<9}" . format ( repr_data_size ( size2 ) , p2 . abspath ) )
7134	def run_cell ( self , cell ) : globals = self . ipy_shell . user_global_ns locals = self . ipy_shell . user_ns globals . update ( { "__ipy_scope__" : None , } ) try : with redirect_stdout ( self . stdout ) : self . run ( cell , globals , locals ) except : self . code_error = True if self . options . debug : raise BdbQuit finally : self . finalize ( )
13597	def get_state ( self ) : return [ os . path . join ( dp , f ) for dp , _ , fn in os . walk ( self . dir ) for f in fn ]
5355	def convert_from_eclipse ( self , eclipse_projects ) : projects = { } projects [ 'unknown' ] = { "gerrit" : [ "git.eclipse.org" ] , "bugzilla" : [ "https://bugs.eclipse.org/bugs/" ] } projects = compose_title ( projects , eclipse_projects ) projects = compose_projects_json ( projects , eclipse_projects ) return projects
7505	def _run_qmc ( self , boot ) : self . _tmp = os . path . join ( self . dirs , ".tmpwtre" ) cmd = [ ip . bins . qmc , "qrtt=" + self . files . qdump , "otre=" + self . _tmp ] proc = subprocess . Popen ( cmd , stderr = subprocess . STDOUT , stdout = subprocess . PIPE ) res = proc . communicate ( ) if proc . returncode : LOGGER . error ( res ) raise IPyradWarningExit ( res [ 1 ] ) with open ( self . _tmp ) as intree : tmp = ete3 . Tree ( intree . read ( ) . strip ( ) ) tmpwtre = self . _renamer ( tmp ) if boot : self . trees . boots = os . path . join ( self . dirs , self . name + ".boots" ) with open ( self . trees . boots , 'a' ) as outboot : outboot . write ( tmpwtre + "\n" ) else : self . trees . tree = os . path . join ( self . dirs , self . name + ".tree" ) with open ( self . trees . tree , 'w' ) as outtree : outtree . write ( tmpwtre ) self . _save ( )
874	def initStateFrom ( self , particleId , particleState , newBest ) : if newBest : ( bestResult , bestPosition ) = self . _resultsDB . getParticleBest ( particleId ) else : bestResult = bestPosition = None varStates = particleState [ 'varStates' ] for varName in varStates . keys ( ) : varState = copy . deepcopy ( varStates [ varName ] ) if newBest : varState [ 'bestResult' ] = bestResult if bestPosition is not None : varState [ 'bestPosition' ] = bestPosition [ varName ] self . permuteVars [ varName ] . setState ( varState )
3712	def calculate_P ( self , T , P , method ) : r if method == EOS : self . eos [ 0 ] = self . eos [ 0 ] . to_TP ( T = T , P = P ) Vm = self . eos [ 0 ] . V_g elif method == TSONOPOULOS_EXTENDED : B = BVirial_Tsonopoulos_extended ( T , self . Tc , self . Pc , self . omega , dipole = self . dipole ) Vm = ideal_gas ( T , P ) + B elif method == TSONOPOULOS : B = BVirial_Tsonopoulos ( T , self . Tc , self . Pc , self . omega ) Vm = ideal_gas ( T , P ) + B elif method == ABBOTT : B = BVirial_Abbott ( T , self . Tc , self . Pc , self . omega ) Vm = ideal_gas ( T , P ) + B elif method == PITZER_CURL : B = BVirial_Pitzer_Curl ( T , self . Tc , self . Pc , self . omega ) Vm = ideal_gas ( T , P ) + B elif method == CRC_VIRIAL : a1 , a2 , a3 , a4 , a5 = self . CRC_VIRIAL_coeffs t = 298.15 / T - 1. B = ( a1 + a2 * t + a3 * t ** 2 + a4 * t ** 3 + a5 * t ** 4 ) / 1E6 Vm = ideal_gas ( T , P ) + B elif method == IDEAL : Vm = ideal_gas ( T , P ) elif method == COOLPROP : Vm = 1. / PropsSI ( 'DMOLAR' , 'T' , T , 'P' , P , self . CASRN ) elif method in self . tabular_data : Vm = self . interpolate_P ( T , P , method ) return Vm
10011	def get ( vals , key , default_val = None ) : val = vals for part in key . split ( '.' ) : if isinstance ( val , dict ) : val = val . get ( part , None ) if val is None : return default_val else : return default_val return val
7153	def many ( prompt , * args , ** kwargs ) : def get_options ( options , chosen ) : return [ options [ i ] for i , c in enumerate ( chosen ) if c ] def get_verbose_options ( verbose_options , chosen ) : no , yes = ' ' , '✔' if sys . version_info < ( 3 , 3 ) : no , yes = ' ' , '@' opts = [ '{} {}' . format ( yes if c else no , verbose_options [ i ] ) for i , c in enumerate ( chosen ) ] return opts + [ '{}{}' . format ( ' ' , kwargs . get ( 'done' , 'done...' ) ) ] options , verbose_options = prepare_options ( args ) chosen = [ False ] * len ( options ) index = kwargs . get ( 'idx' , 0 ) default = kwargs . get ( 'default' , None ) if isinstance ( default , list ) : for idx in default : chosen [ idx ] = True if isinstance ( default , int ) : chosen [ default ] = True while True : try : index = one ( prompt , * get_verbose_options ( verbose_options , chosen ) , return_index = True , idx = index ) except QuestionnaireGoBack : if any ( chosen ) : raise QuestionnaireGoBack ( 0 ) else : raise QuestionnaireGoBack if index == len ( options ) : return get_options ( options , chosen ) chosen [ index ] = not chosen [ index ]
4937	def transform_description ( self , content_metadata_item ) : full_description = content_metadata_item . get ( 'full_description' ) or '' if 0 < len ( full_description ) <= self . LONG_STRING_LIMIT : return full_description return content_metadata_item . get ( 'short_description' ) or content_metadata_item . get ( 'title' ) or ''
12566	def get_dataset ( self , ds_name , mode = 'r' ) : if ds_name in self . _datasets : return self . _datasets [ ds_name ] else : return self . create_empty_dataset ( ds_name )
12606	def search_unique ( table , sample , unique_fields = None ) : if unique_fields is None : unique_fields = list ( sample . keys ( ) ) query = _query_data ( sample , field_names = unique_fields , operators = '__eq__' ) items = table . search ( query ) if len ( items ) == 1 : return items [ 0 ] if len ( items ) == 0 : return None raise MoreThanOneItemError ( 'Expected to find zero or one items, but found ' '{} items.' . format ( len ( items ) ) )
12274	def iso_reference_str2int ( n ) : n = n . upper ( ) numbers = [ ] for c in n : iso_reference_valid_char ( c ) if c in ISO_REFERENCE_VALID_NUMERIC : numbers . append ( c ) else : numbers . append ( str ( iso_reference_char2int ( c ) ) ) return int ( '' . join ( numbers ) )
183	def to_segmentation_map ( self , image_shape , size_lines = 1 , size_points = 0 , raise_if_out_of_image = False ) : from . segmaps import SegmentationMapOnImage return SegmentationMapOnImage ( self . draw_mask ( image_shape , size_lines = size_lines , size_points = size_points , raise_if_out_of_image = raise_if_out_of_image ) , shape = image_shape )
8108	def search ( q , start = 0 , wait = 10 , asynchronous = False , cached = False ) : service = GOOGLE_SEARCH return GoogleSearch ( q , start , service , "" , wait , asynchronous , cached )
9862	async def update_info ( self , * _ ) : query = gql ( ) res = await self . _execute ( query ) if res is None : return errors = res . get ( "errors" , [ ] ) if errors : msg = errors [ 0 ] . get ( "message" , "failed to login" ) _LOGGER . error ( msg ) raise InvalidLogin ( msg ) data = res . get ( "data" ) if not data : return viewer = data . get ( "viewer" ) if not viewer : return self . _name = viewer . get ( "name" ) homes = viewer . get ( "homes" , [ ] ) self . _home_ids = [ ] for _home in homes : home_id = _home . get ( "id" ) self . _all_home_ids += [ home_id ] subs = _home . get ( "subscriptions" ) if subs : status = subs [ 0 ] . get ( "status" , "ended" ) . lower ( ) if not home_id or status != "running" : continue self . _home_ids += [ home_id ]
2772	def load ( self ) : data = self . get_data ( 'load_balancers/%s' % self . id , type = GET ) load_balancer = data [ 'load_balancer' ] for attr in load_balancer . keys ( ) : if attr == 'health_check' : health_check = HealthCheck ( ** load_balancer [ 'health_check' ] ) setattr ( self , attr , health_check ) elif attr == 'sticky_sessions' : sticky_ses = StickySesions ( ** load_balancer [ 'sticky_sessions' ] ) setattr ( self , attr , sticky_ses ) elif attr == 'forwarding_rules' : rules = list ( ) for rule in load_balancer [ 'forwarding_rules' ] : rules . append ( ForwardingRule ( ** rule ) ) setattr ( self , attr , rules ) else : setattr ( self , attr , load_balancer [ attr ] ) return self
10992	def _calc_ilm_order ( imshape ) : zorder = int ( imshape [ 0 ] / 6.25 ) + 1 l_npts = int ( imshape [ 1 ] / 42.5 ) + 1 npts = ( ) for a in range ( l_npts ) : if a < 5 : npts += ( int ( imshape [ 2 ] * [ 59 , 39 , 29 , 19 , 14 ] [ a ] / 512. ) + 1 , ) else : npts += ( int ( imshape [ 2 ] * 11 / 512. ) + 1 , ) return npts , zorder
3607	def put ( self , url , name , data , params = None , headers = None , connection = None ) : assert name , 'Snapshot name must be specified' params = params or { } headers = headers or { } endpoint = self . _build_endpoint_url ( url , name ) self . _authenticate ( params , headers ) data = json . dumps ( data , cls = JSONEncoder ) return make_put_request ( endpoint , data , params , headers , connection = connection )
12357	def connect ( self , interactive = False ) : from poseidon . ssh import SSHClient rs = SSHClient ( self . ip_address , interactive = interactive ) return rs
8485	def load ( self , clear = False ) : if clear : self . settings = { } defer = [ ] for conf in pkg_resources . iter_entry_points ( 'pyconfig' ) : if conf . attrs : raise RuntimeError ( "config must be a module" ) mod_name = conf . module_name base_name = conf . name if conf . name != 'any' else None log . info ( "Loading module '%s'" , mod_name ) mod_dict = runpy . run_module ( mod_name ) if mod_dict . get ( 'deferred' , None ) is deferred : log . info ( "Deferring module '%s'" , mod_name ) mod_dict . pop ( 'deferred' ) defer . append ( ( mod_name , base_name , mod_dict ) ) continue self . _update ( mod_dict , base_name ) for mod_name , base_name , mod_dict in defer : log . info ( "Loading deferred module '%s'" , mod_name ) self . _update ( mod_dict , base_name ) if etcd ( ) . configured : mod_dict = etcd ( ) . load ( ) if mod_dict : self . _update ( mod_dict ) mod_dict = None try : mod_dict = runpy . run_module ( 'localconfig' ) except ImportError : pass except ValueError as err : if getattr ( err , 'message' ) != '__package__ set to non-string' : raise mod_name = 'localconfig' if sys . version_info < ( 2 , 7 ) : loader , code , fname = runpy . _get_module_details ( mod_name ) else : _ , loader , code , fname = runpy . _get_module_details ( mod_name ) mod_dict = runpy . _run_code ( code , { } , { } , mod_name , fname , loader , pkg_name = None ) if mod_dict : log . info ( "Loading module 'localconfig'" ) self . _update ( mod_dict ) self . call_reload_hooks ( )
13271	def unique_justseen ( iterable , key = None ) : "List unique elements, preserving order. Remember only the element just seen." try : from itertools import imap as map except ImportError : from builtins import map return map ( next , map ( operator . itemgetter ( 1 ) , itertools . groupby ( iterable , key ) ) )
8995	def relative_file ( self , module , file ) : path = self . _relative_to_absolute ( module , file ) return self . path ( path )
9923	def save ( self ) : try : email = models . EmailAddress . objects . get ( email = self . validated_data [ "email" ] , is_verified = False ) logger . debug ( "Resending verification email to %s" , self . validated_data [ "email" ] , ) email . send_confirmation ( ) except models . EmailAddress . DoesNotExist : logger . debug ( "Not resending verification email to %s because the address " "doesn't exist in the database." , self . validated_data [ "email" ] , )
12472	def get_extension ( filepath , check_if_exists = False , allowed_exts = ALLOWED_EXTS ) : if check_if_exists : if not op . exists ( filepath ) : raise IOError ( 'File not found: ' + filepath ) rest , ext = op . splitext ( filepath ) if ext in allowed_exts : alloweds = allowed_exts [ ext ] _ , ext2 = op . splitext ( rest ) if ext2 in alloweds : ext = ext2 + ext return ext
10271	def is_unweighted_source ( graph : BELGraph , node : BaseEntity , key : str ) -> bool : return graph . in_degree ( node ) == 0 and key not in graph . nodes [ node ]
7642	def _conversion ( target , source ) : def register ( func ) : __CONVERSION__ [ target ] [ source ] = func return func return register
8781	def create_locks ( context , network_ids , addresses ) : for address in addresses : address_model = None try : address_model = _find_or_create_address ( context , network_ids , address ) lock_holder = None if address_model . lock_id : lock_holder = db_api . lock_holder_find ( context , lock_id = address_model . lock_id , name = LOCK_NAME , scope = db_api . ONE ) if not lock_holder : LOG . info ( "Creating lock holder on IPAddress %s with id %s" , address_model . address_readable , address_model . id ) db_api . lock_holder_create ( context , address_model , name = LOCK_NAME , type = "ip_address" ) except Exception : LOG . exception ( "Failed to create lock holder on IPAddress %s" , address_model ) continue context . session . flush ( )
12673	def aggregate ( * args ) : if args and isinstance ( args [ 0 ] , dataframe . DataFrame ) : return args [ 0 ] . aggregate ( args [ 1 ] , args [ 2 ] , * args [ 3 : ] ) elif not args : raise ValueError ( "No arguments provided" ) else : return pipeable . Pipeable ( pipeable . PipingMethod . AGGREGATE , * args )
1448	def parse ( version ) : match = _REGEX . match ( version ) if match is None : raise ValueError ( '%s is not valid SemVer string' % version ) verinfo = match . groupdict ( ) verinfo [ 'major' ] = int ( verinfo [ 'major' ] ) verinfo [ 'minor' ] = int ( verinfo [ 'minor' ] ) verinfo [ 'patch' ] = int ( verinfo [ 'patch' ] ) return verinfo
2073	def convert_input ( X ) : if not isinstance ( X , pd . DataFrame ) : if isinstance ( X , list ) : X = pd . DataFrame ( X ) elif isinstance ( X , ( np . generic , np . ndarray ) ) : X = pd . DataFrame ( X ) elif isinstance ( X , csr_matrix ) : X = pd . DataFrame ( X . todense ( ) ) elif isinstance ( X , pd . Series ) : X = pd . DataFrame ( X ) else : raise ValueError ( 'Unexpected input type: %s' % ( str ( type ( X ) ) ) ) X = X . apply ( lambda x : pd . to_numeric ( x , errors = 'ignore' ) ) return X
1931	def update ( self , name : str , value = None , default = None , description : str = None ) : if name in self . _vars : description = description or self . _vars [ name ] . description default = default or self . _vars [ name ] . default elif name == 'name' : raise ConfigError ( "'name' is a reserved name for a group." ) v = _Var ( name , description = description , default = default , defined = False ) v . value = value self . _vars [ name ] = v
11264	def stdout ( prev , endl = '\n' , thru = False ) : for i in prev : sys . stdout . write ( str ( i ) + endl ) if thru : yield i
9893	def _uptime_plan9 ( ) : try : f = open ( '/dev/time' , 'r' ) s , ns , ct , cf = f . read ( ) . split ( ) f . close ( ) return float ( ct ) / float ( cf ) except ( IOError , ValueError ) : return None
1258	def get_savable_components ( self ) : components = self . get_components ( ) components = [ components [ name ] for name in sorted ( components ) ] return set ( filter ( lambda x : isinstance ( x , util . SavableComponent ) , components ) )
10358	def shuffle_node_data ( graph : BELGraph , key : str , percentage : Optional [ float ] = None ) -> BELGraph : percentage = percentage or 0.3 assert 0 < percentage <= 1 n = graph . number_of_nodes ( ) swaps = int ( percentage * n * ( n - 1 ) / 2 ) result : BELGraph = graph . copy ( ) for _ in range ( swaps ) : s , t = random . sample ( result . node , 2 ) result . nodes [ s ] [ key ] , result . nodes [ t ] [ key ] = result . nodes [ t ] [ key ] , result . nodes [ s ] [ key ] return result
4868	def to_representation ( self , instance ) : updated_course_run = copy . deepcopy ( instance ) enterprise_customer_catalog = self . context [ 'enterprise_customer_catalog' ] updated_course_run [ 'enrollment_url' ] = enterprise_customer_catalog . get_course_run_enrollment_url ( updated_course_run [ 'key' ] ) return updated_course_run
3396	def gapfill ( model , universal = None , lower_bound = 0.05 , penalties = None , demand_reactions = True , exchange_reactions = False , iterations = 1 ) : gapfiller = GapFiller ( model , universal = universal , lower_bound = lower_bound , penalties = penalties , demand_reactions = demand_reactions , exchange_reactions = exchange_reactions ) return gapfiller . fill ( iterations = iterations )
7549	def _set_debug_dict ( __loglevel__ ) : _lconfig . dictConfig ( { 'version' : 1 , 'disable_existing_loggers' : False , 'formatters' : { 'standard' : { 'format' : "%(asctime)s \t" + "pid=%(process)d \t" + "[%(filename)s]\t" + "%(levelname)s \t" + "%(message)s" } , } , 'handlers' : { __name__ : { 'level' : __loglevel__ , 'class' : 'logging.FileHandler' , 'filename' : __debugfile__ , 'formatter' : "standard" , 'mode' : 'a+' } } , 'loggers' : { __name__ : { 'handlers' : [ __name__ ] , 'level' : __loglevel__ , 'propogate' : True } } } )
2668	def sixteen_oscillator_two_stimulated_ensembles_grid ( ) : "Not accurate false due to spikes are observed" parameters = legion_parameters ( ) parameters . teta_x = - 1.1 template_dynamic_legion ( 16 , 2000 , 1500 , conn_type = conn_type . GRID_FOUR , params = parameters , stimulus = [ 1 , 1 , 1 , 0 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 1 , 1 ] )
9469	def conference_list ( self , call_params ) : path = '/' + self . api_version + '/ConferenceList/' method = 'POST' return self . request ( path , method , call_params )
11746	def init_app ( self , app ) : if len ( self . _attached_bundles ) == 0 : raise NoBundlesAttached ( "At least one bundle must be attached before initializing Journey" ) for bundle in self . _attached_bundles : processed_bundle = { 'path' : bundle . path , 'description' : bundle . description , 'blueprints' : [ ] } for ( bp , description ) in bundle . blueprints : blueprint = self . _register_blueprint ( app , bp , bundle . path , self . get_bp_path ( bp ) , description ) processed_bundle [ 'blueprints' ] . append ( blueprint ) self . _registered_bundles . append ( processed_bundle )
4816	def create_n_gram_df ( df , n_pad ) : n_pad_2 = int ( ( n_pad - 1 ) / 2 ) for i in range ( n_pad_2 ) : df [ 'char-{}' . format ( i + 1 ) ] = df [ 'char' ] . shift ( i + 1 ) df [ 'type-{}' . format ( i + 1 ) ] = df [ 'type' ] . shift ( i + 1 ) df [ 'char{}' . format ( i + 1 ) ] = df [ 'char' ] . shift ( - i - 1 ) df [ 'type{}' . format ( i + 1 ) ] = df [ 'type' ] . shift ( - i - 1 ) return df [ n_pad_2 : - n_pad_2 ]
773	def generateStats ( filename , maxSamples = None , ) : statsCollectorMapping = { 'float' : FloatStatsCollector , 'int' : IntStatsCollector , 'string' : StringStatsCollector , 'datetime' : DateTimeStatsCollector , 'bool' : BoolStatsCollector , } filename = resource_filename ( "nupic.datafiles" , filename ) print "*" * 40 print "Collecting statistics for file:'%s'" % ( filename , ) dataFile = FileRecordStream ( filename ) statsCollectors = [ ] for fieldName , fieldType , fieldSpecial in dataFile . getFields ( ) : statsCollector = statsCollectorMapping [ fieldType ] ( fieldName , fieldType , fieldSpecial ) statsCollectors . append ( statsCollector ) if maxSamples is None : maxSamples = 500000 for i in xrange ( maxSamples ) : record = dataFile . getNextRecord ( ) if record is None : break for i , value in enumerate ( record ) : statsCollectors [ i ] . addValue ( value ) stats = { } for statsCollector in statsCollectors : statsCollector . getStats ( stats ) if dataFile . getResetFieldIdx ( ) is not None : resetFieldName , _ , _ = dataFile . getFields ( ) [ dataFile . reset ] stats . pop ( resetFieldName ) if VERBOSITY > 0 : pprint . pprint ( stats ) return stats
6214	def load_gltf ( self ) : with open ( self . path ) as fd : self . meta = GLTFMeta ( self . path , json . load ( fd ) )
10176	def agg_iter ( self , lower_limit = None , upper_limit = None ) : lower_limit = lower_limit or self . get_bookmark ( ) . isoformat ( ) upper_limit = upper_limit or ( datetime . datetime . utcnow ( ) . replace ( microsecond = 0 ) . isoformat ( ) ) aggregation_data = { } self . agg_query = Search ( using = self . client , index = self . event_index ) . filter ( 'range' , timestamp = { 'gte' : self . _format_range_dt ( lower_limit ) , 'lte' : self . _format_range_dt ( upper_limit ) } ) for modifier in self . query_modifiers : self . agg_query = modifier ( self . agg_query ) hist = self . agg_query . aggs . bucket ( 'histogram' , 'date_histogram' , field = 'timestamp' , interval = self . aggregation_interval ) terms = hist . bucket ( 'terms' , 'terms' , field = self . aggregation_field , size = 0 ) top = terms . metric ( 'top_hit' , 'top_hits' , size = 1 , sort = { 'timestamp' : 'desc' } ) for dst , ( metric , src , opts ) in self . metric_aggregation_fields . items ( ) : terms . metric ( dst , metric , field = src , ** opts ) results = self . agg_query . execute ( ) index_name = None for interval in results . aggregations [ 'histogram' ] . buckets : interval_date = datetime . datetime . strptime ( interval [ 'key_as_string' ] , '%Y-%m-%dT%H:%M:%S' ) for aggregation in interval [ 'terms' ] . buckets : aggregation_data [ 'timestamp' ] = interval_date . isoformat ( ) aggregation_data [ self . aggregation_field ] = aggregation [ 'key' ] aggregation_data [ 'count' ] = aggregation [ 'doc_count' ] if self . metric_aggregation_fields : for f in self . metric_aggregation_fields : aggregation_data [ f ] = aggregation [ f ] [ 'value' ] doc = aggregation . top_hit . hits . hits [ 0 ] [ '_source' ] for destination , source in self . copy_fields . items ( ) : if isinstance ( source , six . string_types ) : aggregation_data [ destination ] = doc [ source ] else : aggregation_data [ destination ] = source ( doc , aggregation_data ) index_name = 'stats-{0}-{1}' . format ( self . event , interval_date . strftime ( self . index_name_suffix ) ) self . indices . add ( index_name ) yield dict ( _id = '{0}-{1}' . format ( aggregation [ 'key' ] , interval_date . strftime ( self . doc_id_suffix ) ) , _index = index_name , _type = self . aggregation_doc_type , _source = aggregation_data ) self . last_index_written = index_name
5441	def rewrite_uris ( self , raw_uri , file_provider ) : if file_provider == job_model . P_GCS : normalized , docker_path = _gcs_uri_rewriter ( raw_uri ) elif file_provider == job_model . P_LOCAL : normalized , docker_path = _local_uri_rewriter ( raw_uri ) else : raise ValueError ( 'File provider not supported: %r' % file_provider ) return normalized , os . path . join ( self . _relative_path , docker_path )
8093	def node_label ( s , node , alpha = 1.0 ) : if s . text : s . _ctx . font ( s . font ) s . _ctx . fontsize ( s . fontsize ) s . _ctx . nostroke ( ) s . _ctx . fill ( s . text . r , s . text . g , s . text . b , s . text . a * alpha ) try : p = node . _textpath except : txt = node . label try : txt = unicode ( txt ) except : try : txt = txt . decode ( "utf-8" ) except : pass dx , dy = 0 , 0 if s . align == 2 : dx = - s . _ctx . textwidth ( txt , s . textwidth ) / 2 dy = s . _ctx . textheight ( txt ) / 2 node . _textpath = s . _ctx . textpath ( txt , dx , dy , width = s . textwidth ) p = node . _textpath if s . depth : try : __colors . shadow ( dx = 2 , dy = 4 , blur = 5 , alpha = 0.3 * alpha ) except : pass s . _ctx . push ( ) s . _ctx . translate ( node . x , node . y ) s . _ctx . scale ( alpha ) s . _ctx . drawpath ( p . copy ( ) ) s . _ctx . pop ( )
217	def append ( self , key : str , value : str ) -> None : append_key = key . lower ( ) . encode ( "latin-1" ) append_value = value . encode ( "latin-1" ) self . _list . append ( ( append_key , append_value ) )
10811	def query_by_names ( cls , names ) : assert isinstance ( names , list ) return cls . query . filter ( cls . name . in_ ( names ) )
6898	def serial_periodicfeatures ( pfpkl_list , lcbasedir , outdir , starfeaturesdir = None , fourierorder = 5 , transitparams = ( - 0.01 , 0.1 , 0.1 ) , ebparams = ( - 0.2 , 0.3 , 0.7 , 0.5 ) , pdiff_threshold = 1.0e-4 , sidereal_threshold = 1.0e-4 , sampling_peak_multiplier = 5.0 , sampling_startp = None , sampling_endp = None , starfeatures = None , timecols = None , magcols = None , errcols = None , lcformat = 'hat-sql' , lcformatdir = None , sigclip = 10.0 , verbose = False , maxobjects = None ) : try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( fileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None if not os . path . exists ( outdir ) : os . makedirs ( outdir ) if maxobjects : pfpkl_list = pfpkl_list [ : maxobjects ] LOGINFO ( '%s periodfinding pickles to process' % len ( pfpkl_list ) ) if starfeaturesdir and os . path . exists ( starfeaturesdir ) : starfeatures_list = [ ] LOGINFO ( 'collecting starfeatures pickles...' ) for pfpkl in pfpkl_list : sfpkl1 = os . path . basename ( pfpkl ) . replace ( 'periodfinding' , 'starfeatures' ) sfpkl2 = sfpkl1 . replace ( '.gz' , '' ) sfpath1 = os . path . join ( starfeaturesdir , sfpkl1 ) sfpath2 = os . path . join ( starfeaturesdir , sfpkl2 ) if os . path . exists ( sfpath1 ) : starfeatures_list . append ( sfpkl1 ) elif os . path . exists ( sfpath2 ) : starfeatures_list . append ( sfpkl2 ) else : starfeatures_list . append ( None ) else : starfeatures_list = [ None for x in pfpkl_list ] kwargs = { 'fourierorder' : fourierorder , 'transitparams' : transitparams , 'ebparams' : ebparams , 'pdiff_threshold' : pdiff_threshold , 'sidereal_threshold' : sidereal_threshold , 'sampling_peak_multiplier' : sampling_peak_multiplier , 'sampling_startp' : sampling_startp , 'sampling_endp' : sampling_endp , 'timecols' : timecols , 'magcols' : magcols , 'errcols' : errcols , 'lcformat' : lcformat , 'lcformatdir' : lcformatdir , 'sigclip' : sigclip , 'verbose' : verbose } tasks = [ ( x , lcbasedir , outdir , y , kwargs ) for ( x , y ) in zip ( pfpkl_list , starfeatures_list ) ] LOGINFO ( 'processing periodfinding pickles...' ) for task in tqdm ( tasks ) : _periodicfeatures_worker ( task )
6539	def matches_masks ( target , masks ) : for mask in masks : if mask . search ( target ) : return True return False
9121	def dropbox_submission ( dropbox , request ) : try : data = dropbox_schema . deserialize ( request . POST ) except Exception : return HTTPFound ( location = request . route_url ( 'dropbox_form' ) ) dropbox . message = data . get ( 'message' ) if 'testing_secret' in dropbox . settings : dropbox . from_watchdog = is_equal ( unicode ( dropbox . settings [ 'test_submission_secret' ] ) , data . pop ( 'testing_secret' , u'' ) ) if data . get ( 'upload' ) is not None : dropbox . add_attachment ( data [ 'upload' ] ) dropbox . submit ( ) drop_url = request . route_url ( 'dropbox_view' , drop_id = dropbox . drop_id ) print ( "Created dropbox %s" % drop_url ) return HTTPFound ( location = drop_url )
3418	def load_matlab_model ( infile_path , variable_name = None , inf = inf ) : if not scipy_io : raise ImportError ( 'load_matlab_model requires scipy' ) data = scipy_io . loadmat ( infile_path ) possible_names = [ ] if variable_name is None : meta_vars = { "__globals__" , "__header__" , "__version__" } possible_names = sorted ( i for i in data if i not in meta_vars ) if len ( possible_names ) == 1 : variable_name = possible_names [ 0 ] if variable_name is not None : return from_mat_struct ( data [ variable_name ] , model_id = variable_name , inf = inf ) for possible_name in possible_names : try : return from_mat_struct ( data [ possible_name ] , model_id = possible_name , inf = inf ) except ValueError : pass raise IOError ( "no COBRA model found" )
8056	def do_escape_nl ( self , arg ) : if arg . lower ( ) == 'off' : self . escape_nl = False else : self . escape_nl = True
3062	def string_to_scopes ( scopes ) : if not scopes : return [ ] elif isinstance ( scopes , six . string_types ) : return scopes . split ( ' ' ) else : return scopes
5046	def _enroll_users ( cls , request , enterprise_customer , emails , mode , course_id = None , program_details = None , notify = True ) : pending_messages = [ ] if course_id : succeeded , pending , failed = cls . enroll_users_in_course ( enterprise_customer = enterprise_customer , course_id = course_id , course_mode = mode , emails = emails , ) all_successes = succeeded + pending if notify : enterprise_customer . notify_enrolled_learners ( catalog_api_user = request . user , course_id = course_id , users = all_successes , ) if succeeded : pending_messages . append ( cls . get_success_enrollment_message ( succeeded , course_id ) ) if failed : pending_messages . append ( cls . get_failed_enrollment_message ( failed , course_id ) ) if pending : pending_messages . append ( cls . get_pending_enrollment_message ( pending , course_id ) ) if program_details : succeeded , pending , failed = cls . enroll_users_in_program ( enterprise_customer = enterprise_customer , program_details = program_details , course_mode = mode , emails = emails , ) all_successes = succeeded + pending if notify : cls . notify_program_learners ( enterprise_customer = enterprise_customer , program_details = program_details , users = all_successes ) program_identifier = program_details . get ( 'title' , program_details . get ( 'uuid' , _ ( 'the program' ) ) ) if succeeded : pending_messages . append ( cls . get_success_enrollment_message ( succeeded , program_identifier ) ) if failed : pending_messages . append ( cls . get_failed_enrollment_message ( failed , program_identifier ) ) if pending : pending_messages . append ( cls . get_pending_enrollment_message ( pending , program_identifier ) ) cls . send_messages ( request , pending_messages )
12405	def reverse ( self ) : if self . _original_target_content : with open ( self . target , 'w' ) as fp : fp . write ( self . _original_target_content )
2911	def _find_ancestor_from_name ( self , name ) : if self . parent is None : return None if self . parent . get_name ( ) == name : return self . parent return self . parent . _find_ancestor_from_name ( name )
8567	def add_loadbalanced_nics ( self , datacenter_id , loadbalancer_id , nic_id ) : data = '{ "id": "' + nic_id + '" }' response = self . _perform_request ( url = '/datacenters/%s/loadbalancers/%s/balancednics' % ( datacenter_id , loadbalancer_id ) , method = 'POST' , data = data ) return response
3411	def add_moma ( model , solution = None , linear = True ) : r if 'moma_old_objective' in model . solver . variables : raise ValueError ( 'model is already adjusted for MOMA' ) if not linear : model . solver = sutil . choose_solver ( model , qp = True ) if solution is None : solution = pfba ( model ) prob = model . problem v = prob . Variable ( "moma_old_objective" ) c = prob . Constraint ( model . solver . objective . expression - v , lb = 0.0 , ub = 0.0 , name = "moma_old_objective_constraint" ) to_add = [ v , c ] model . objective = prob . Objective ( Zero , direction = "min" , sloppy = True ) obj_vars = [ ] for r in model . reactions : flux = solution . fluxes [ r . id ] if linear : components = sutil . add_absolute_expression ( model , r . flux_expression , name = "moma_dist_" + r . id , difference = flux , add = False ) to_add . extend ( components ) obj_vars . append ( components . variable ) else : dist = prob . Variable ( "moma_dist_" + r . id ) const = prob . Constraint ( r . flux_expression - dist , lb = flux , ub = flux , name = "moma_constraint_" + r . id ) to_add . extend ( [ dist , const ] ) obj_vars . append ( dist ** 2 ) model . add_cons_vars ( to_add ) if linear : model . objective . set_linear_coefficients ( { v : 1.0 for v in obj_vars } ) else : model . objective = prob . Objective ( add ( obj_vars ) , direction = "min" , sloppy = True )
7914	def get_int_range_validator ( start , stop ) : def validate_int_range ( value ) : value = int ( value ) if value >= start and value < stop : return value raise ValueError ( "Not in <{0},{1}) range" . format ( start , stop ) ) return validate_int_range
11104	def acquire_lock ( func ) : @ wraps ( func ) def wrapper ( self , * args , ** kwargs ) : with self . locker as r : acquired , code , _ = r if acquired : try : r = func ( self , * args , ** kwargs ) except Exception as err : e = str ( err ) else : e = None else : warnings . warn ( "code %s. Unable to aquire the lock when calling '%s'. You may try again!" % ( code , func . __name__ ) ) e = None r = None if e is not None : traceback . print_stack ( ) raise Exception ( e ) return r return wrapper
1833	def JCXZ ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . CX == 0 , target . read ( ) , cpu . PC )
12858	def from_date ( datetime_date ) : return BusinessDate . from_ymd ( datetime_date . year , datetime_date . month , datetime_date . day )
766	def clean ( s ) : lines = [ l . rstrip ( ) for l in s . split ( '\n' ) ] return '\n' . join ( lines )
7344	def get_data ( self , response ) : if self . _response_list : return response elif self . _response_key is None : if hasattr ( response , "items" ) : for key , data in response . items ( ) : if ( hasattr ( data , "__getitem__" ) and not hasattr ( data , "items" ) and len ( data ) > 0 and 'id' in data [ 0 ] ) : self . _response_key = key return data else : self . _response_list = True return response else : return response [ self . _response_key ] raise NoDataFound ( response = response , url = self . request . get_url ( ) )
10883	def patch_docs ( subclass , superclass ) : funcs0 = inspect . getmembers ( subclass , predicate = inspect . ismethod ) funcs1 = inspect . getmembers ( superclass , predicate = inspect . ismethod ) funcs1 = [ f [ 0 ] for f in funcs1 ] for name , func in funcs0 : if name . startswith ( '_' ) : continue if name not in funcs1 : continue if func . __doc__ is None : func = getattr ( subclass , name ) func . __func__ . __doc__ = getattr ( superclass , name ) . __func__ . __doc__
9420	def is_rarfile ( filename ) : mode = constants . RAR_OM_LIST_INCSPLIT archive = unrarlib . RAROpenArchiveDataEx ( filename , mode = mode ) try : handle = unrarlib . RAROpenArchiveEx ( ctypes . byref ( archive ) ) except unrarlib . UnrarException : return False unrarlib . RARCloseArchive ( handle ) return ( archive . OpenResult == constants . SUCCESS )
8414	def round_any ( x , accuracy , f = np . round ) : if not hasattr ( x , 'dtype' ) : x = np . asarray ( x ) return f ( x / accuracy ) * accuracy
2247	def _make_signature_key ( args , kwargs ) : kwitems = kwargs . items ( ) if ( sys . version_info . major , sys . version_info . minor ) < ( 3 , 7 ) : kwitems = sorted ( kwitems ) kwitems = tuple ( kwitems ) try : key = _hashable ( args ) , _hashable ( kwitems ) except TypeError : raise TypeError ( 'Signature is not hashable: args={} kwargs{}' . format ( args , kwargs ) ) return key
9340	def flatten_dtype ( dtype , _next = None ) : types = [ ] if _next is None : _next = [ 0 , '' ] primary = True else : primary = False prefix = _next [ 1 ] if dtype . names is None : for i in numpy . ndindex ( dtype . shape ) : if dtype . base == dtype : types . append ( ( '%s%s' % ( prefix , simplerepr ( i ) ) , dtype ) ) _next [ 0 ] += 1 else : _next [ 1 ] = '%s%s' % ( prefix , simplerepr ( i ) ) types . extend ( flatten_dtype ( dtype . base , _next ) ) else : for field in dtype . names : typ_fields = dtype . fields [ field ] if len ( prefix ) > 0 : _next [ 1 ] = prefix + '.' + field else : _next [ 1 ] = '' + field flat_dt = flatten_dtype ( typ_fields [ 0 ] , _next ) types . extend ( flat_dt ) _next [ 1 ] = prefix if primary : return numpy . dtype ( types ) else : return types
10263	def _collapse_edge_by_namespace ( graph : BELGraph , victim_namespaces : Strings , survivor_namespaces : str , relations : Strings ) -> None : relation_filter = build_relation_predicate ( relations ) source_namespace_filter = build_source_namespace_filter ( victim_namespaces ) target_namespace_filter = build_target_namespace_filter ( survivor_namespaces ) edge_predicates = [ relation_filter , source_namespace_filter , target_namespace_filter ] _collapse_edge_passing_predicates ( graph , edge_predicates = edge_predicates )
10448	def click ( self , window_name , object_name ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) size = self . _getobjectsize ( object_handle ) self . _grabfocus ( object_handle ) self . wait ( 0.5 ) self . generatemouseevent ( size [ 0 ] + size [ 2 ] / 2 , size [ 1 ] + size [ 3 ] / 2 , "b1c" ) return 1
13604	def system ( self , cmd , fake_code = False ) : try : if self . options . dry_run : def fake_system ( cmd ) : self . print_message ( cmd ) return fake_code return fake_system ( cmd ) except AttributeError : self . logger . warnning ( "fake mode enabled," "but you don't set '--dry-run' option " "in your argparser options" ) pass return os . system ( cmd )
10989	def link_zscale ( st ) : psf = st . get ( 'psf' ) psf . param_dict [ 'zscale' ] = psf . param_dict [ 'psf-zscale' ] psf . params [ psf . params . index ( 'psf-zscale' ) ] = 'zscale' psf . global_zscale = True psf . param_dict . pop ( 'psf-zscale' ) st . trigger_parameter_change ( ) st . reset ( )
101	def draw_text ( img , y , x , text , color = ( 0 , 255 , 0 ) , size = 25 ) : do_assert ( img . dtype in [ np . uint8 , np . float32 ] ) input_dtype = img . dtype if img . dtype == np . float32 : img = img . astype ( np . uint8 ) img = PIL_Image . fromarray ( img ) font = PIL_ImageFont . truetype ( DEFAULT_FONT_FP , size ) context = PIL_ImageDraw . Draw ( img ) context . text ( ( x , y ) , text , fill = tuple ( color ) , font = font ) img_np = np . asarray ( img ) if not img_np . flags [ "WRITEABLE" ] : try : img_np . setflags ( write = True ) except ValueError as ex : if "cannot set WRITEABLE flag to True of this array" in str ( ex ) : img_np = np . copy ( img_np ) if img_np . dtype != input_dtype : img_np = img_np . astype ( input_dtype ) return img_np
12230	def unpatch_locals ( depth = 3 ) : for name , locals_dict in traverse_local_prefs ( depth ) : if isinstance ( locals_dict [ name ] , PatchedLocal ) : locals_dict [ name ] = locals_dict [ name ] . val del get_frame_locals ( depth ) [ __PATCHED_LOCALS_SENTINEL ]
233	def plot_sector_exposures_gross ( gross_exposures , sector_dict = None , ax = None ) : if ax is None : ax = plt . gca ( ) if sector_dict is None : sector_names = SECTORS . values ( ) else : sector_names = sector_dict . values ( ) color_list = plt . cm . gist_rainbow ( np . linspace ( 0 , 1 , 11 ) ) ax . stackplot ( gross_exposures [ 0 ] . index , gross_exposures , labels = sector_names , colors = color_list , alpha = 0.8 , baseline = 'zero' ) ax . axhline ( 0 , color = 'k' , linestyle = '-' ) ax . set ( title = 'Gross exposure to sectors' , ylabel = 'Proportion of gross exposure \n in sectors' ) return ax
11587	def object ( self , infotype , key ) : "Return the encoding, idletime, or refcount about the key" redisent = self . redises [ self . _getnodenamefor ( key ) + '_slave' ] return getattr ( redisent , 'object' ) ( infotype , key )
8818	def get_networks ( context , limit = None , sorts = [ 'id' ] , marker = None , page_reverse = False , filters = None , fields = None ) : LOG . info ( "get_networks for tenant %s with filters %s, fields %s" % ( context . tenant_id , filters , fields ) ) filters = filters or { } nets = db_api . network_find ( context , limit , sorts , marker , page_reverse , join_subnets = True , ** filters ) or [ ] nets = [ v . _make_network_dict ( net , fields = fields ) for net in nets ] return nets
6248	def get_texture ( self , label : str ) -> Union [ moderngl . Texture , moderngl . TextureArray , moderngl . Texture3D , moderngl . TextureCube ] : return self . _project . get_texture ( label )
12757	def add_torques ( self , torques ) : j = 0 for joint in self . joints : joint . add_torques ( list ( torques [ j : j + joint . ADOF ] ) + [ 0 ] * ( 3 - joint . ADOF ) ) j += joint . ADOF
4986	def get_path_variables ( ** kwargs ) : enterprise_customer_uuid = kwargs . get ( 'enterprise_uuid' , '' ) course_run_id = kwargs . get ( 'course_id' , '' ) course_key = kwargs . get ( 'course_key' , '' ) program_uuid = kwargs . get ( 'program_uuid' , '' ) return enterprise_customer_uuid , course_run_id , course_key , program_uuid
3592	def encryptPassword ( self , login , passwd ) : binaryKey = b64decode ( config . GOOGLE_PUBKEY ) i = utils . readInt ( binaryKey , 0 ) modulus = utils . toBigInt ( binaryKey [ 4 : ] [ 0 : i ] ) j = utils . readInt ( binaryKey , i + 4 ) exponent = utils . toBigInt ( binaryKey [ i + 8 : ] [ 0 : j ] ) digest = hashes . Hash ( hashes . SHA1 ( ) , backend = default_backend ( ) ) digest . update ( binaryKey ) h = b'\x00' + digest . finalize ( ) [ 0 : 4 ] der_data = encode_dss_signature ( modulus , exponent ) publicKey = load_der_public_key ( der_data , backend = default_backend ( ) ) to_be_encrypted = login . encode ( ) + b'\x00' + passwd . encode ( ) ciphertext = publicKey . encrypt ( to_be_encrypted , padding . OAEP ( mgf = padding . MGF1 ( algorithm = hashes . SHA1 ( ) ) , algorithm = hashes . SHA1 ( ) , label = None ) ) return urlsafe_b64encode ( h + ciphertext )
4349	def vad ( self , location = 1 , normalize = True , activity_threshold = 7.0 , min_activity_duration = 0.25 , initial_search_buffer = 1.0 , max_gap = 0.25 , initial_pad = 0.0 ) : if location not in [ - 1 , 1 ] : raise ValueError ( "location must be -1 or 1." ) if not isinstance ( normalize , bool ) : raise ValueError ( "normalize muse be a boolean." ) if not is_number ( activity_threshold ) : raise ValueError ( "activity_threshold must be a number." ) if not is_number ( min_activity_duration ) or min_activity_duration < 0 : raise ValueError ( "min_activity_duration must be a positive number" ) if not is_number ( initial_search_buffer ) or initial_search_buffer < 0 : raise ValueError ( "initial_search_buffer must be a positive number" ) if not is_number ( max_gap ) or max_gap < 0 : raise ValueError ( "max_gap must be a positive number." ) if not is_number ( initial_pad ) or initial_pad < 0 : raise ValueError ( "initial_pad must be a positive number." ) effect_args = [ ] if normalize : effect_args . append ( 'norm' ) if location == - 1 : effect_args . append ( 'reverse' ) effect_args . extend ( [ 'vad' , '-t' , '{:f}' . format ( activity_threshold ) , '-T' , '{:f}' . format ( min_activity_duration ) , '-s' , '{:f}' . format ( initial_search_buffer ) , '-g' , '{:f}' . format ( max_gap ) , '-p' , '{:f}' . format ( initial_pad ) ] ) if location == - 1 : effect_args . append ( 'reverse' ) self . effects . extend ( effect_args ) self . effects_log . append ( 'vad' ) return self
4041	def _retrieve_data ( self , request = None ) : full_url = "%s%s" % ( self . endpoint , request ) self . self_link = request self . request = requests . get ( url = full_url , headers = self . default_headers ( ) ) self . request . encoding = "utf-8" try : self . request . raise_for_status ( ) except requests . exceptions . HTTPError : error_handler ( self . request ) return self . request
10224	def get_chaotic_pairs ( graph : BELGraph ) -> SetOfNodePairs : cg = get_causal_subgraph ( graph ) results = set ( ) for u , v , d in cg . edges ( data = True ) : if d [ RELATION ] not in CAUSAL_INCREASE_RELATIONS : continue if cg . has_edge ( v , u ) and any ( dd [ RELATION ] in CAUSAL_INCREASE_RELATIONS for dd in cg [ v ] [ u ] . values ( ) ) : results . add ( tuple ( sorted ( [ u , v ] , key = str ) ) ) return results
8199	def transform_from_local ( xp , yp , cphi , sphi , mx , my ) : x = xp * cphi - yp * sphi + mx y = xp * sphi + yp * cphi + my return ( x , y )
13229	def create_jwt ( integration_id , private_key_path ) : integration_id = int ( integration_id ) with open ( private_key_path , 'rb' ) as f : cert_bytes = f . read ( ) now = datetime . datetime . now ( ) expiration_time = now + datetime . timedelta ( minutes = 9 ) payload = { 'iat' : int ( now . timestamp ( ) ) , 'exp' : int ( expiration_time . timestamp ( ) ) , 'iss' : integration_id } return jwt . encode ( payload , cert_bytes , algorithm = 'RS256' )
12308	def get_files_to_commit ( autooptions ) : workingdir = autooptions [ 'working-directory' ] includes = autooptions [ 'track' ] [ 'includes' ] excludes = autooptions [ 'track' ] [ 'excludes' ] includes = r'|' . join ( [ fnmatch . translate ( x ) for x in includes ] ) excludes = r'|' . join ( [ fnmatch . translate ( x ) for x in excludes ] ) or r'$.' matched_files = [ ] for root , dirs , files in os . walk ( workingdir ) : dirs [ : ] = [ d for d in dirs if not re . match ( excludes , d ) ] files = [ f for f in files if not re . match ( excludes , f ) ] files = [ f for f in files if re . match ( includes , f ) ] files = [ os . path . join ( root , f ) for f in files ] matched_files . extend ( files ) return matched_files
10544	def update_task ( task ) : try : task_id = task . id task = _forbidden_attributes ( task ) res = _pybossa_req ( 'put' , 'task' , task_id , payload = task . data ) if res . get ( 'id' ) : return Task ( res ) else : return res except : raise
9626	def detail_view ( self , request , module , preview ) : try : preview = self . __previews [ module ] [ preview ] except KeyError : raise Http404 return preview . detail_view ( request )
7916	def get_arg_parser ( cls , settings = None , option_prefix = u'--' , add_help = False ) : parser = argparse . ArgumentParser ( add_help = add_help , prefix_chars = option_prefix [ 0 ] ) if settings is None : settings = cls . list_all ( basic = True ) if sys . version_info . major < 3 : from locale import getpreferredencoding encoding = getpreferredencoding ( ) def decode_string_option ( value ) : return value . decode ( encoding ) for name in settings : if name not in cls . _defs : logger . debug ( "get_arg_parser: ignoring unknown option {0}" . format ( name ) ) return setting = cls . _defs [ name ] if not setting . cmdline_help : logger . debug ( "get_arg_parser: option {0} has no cmdline" . format ( name ) ) return if sys . version_info . major < 3 : name = name . encode ( encoding , "replace" ) option = option_prefix + name . replace ( "_" , "-" ) dest = "pyxmpp2_" + name if setting . validator : opt_type = setting . validator elif setting . type is unicode and sys . version_info . major < 3 : opt_type = decode_string_option else : opt_type = setting . type if setting . default_d : default_s = setting . default_d if sys . version_info . major < 3 : default_s = default_s . encode ( encoding , "replace" ) elif setting . default is not None : default_s = repr ( setting . default ) else : default_s = None opt_help = setting . cmdline_help if sys . version_info . major < 3 : opt_help = opt_help . encode ( encoding , "replace" ) if default_s : opt_help += " (Default: {0})" . format ( default_s ) if opt_type is bool : opt_action = _YesNoAction else : opt_action = "store" parser . add_argument ( option , action = opt_action , default = setting . default , type = opt_type , help = opt_help , metavar = name . upper ( ) , dest = dest ) return parser
1955	def empty_platform ( cls , arch ) : platform = cls ( None ) platform . _init_cpu ( arch ) platform . _init_std_fds ( ) return platform
3272	def _init ( self ) : self . provider . _count_get_resource_inst_init += 1 tableName , primKey = self . provider . _split_path ( self . path ) display_type = "Unknown" displayTypeComment = "" contentType = "text/html" if tableName is None : display_type = "Database" elif primKey is None : display_type = "Database Table" else : contentType = "text/csv" if primKey == "_ENTIRE_CONTENTS" : display_type = "Database Table Contents" displayTypeComment = "CSV Representation of Table Contents" else : display_type = "Database Record" displayTypeComment = "Attributes available as properties" is_collection = primKey is None self . _cache = { "content_length" : None , "contentType" : contentType , "created" : time . time ( ) , "display_name" : self . name , "etag" : hashlib . md5 ( ) . update ( self . path ) . hexdigest ( ) , "modified" : None , "support_ranges" : False , "display_info" : { "type" : display_type , "typeComment" : displayTypeComment } , } if not is_collection : self . _cache [ "modified" ] = time . time ( ) _logger . debug ( "- % self . provider . _count_initConnection )
5759	def get_jenkins_job_urls ( rosdistro_name , jenkins_url , release_build_name , targets ) : urls = { } for target in targets : view_name = get_release_view_name ( rosdistro_name , release_build_name , target . os_name , target . os_code_name , target . arch ) base_url = jenkins_url + '/view/%s/job/%s__{pkg}__' % ( view_name , view_name ) if target . arch == 'source' : urls [ target ] = base_url + '%s_%s__source' % ( target . os_name , target . os_code_name ) else : urls [ target ] = base_url + '%s_%s_%s__binary' % ( target . os_name , target . os_code_name , target . arch ) return urls
5700	def _distribution ( gtfs , table , column ) : cur = gtfs . conn . cursor ( ) cur . execute ( 'SELECT {column}, count(*) ' 'FROM {table} GROUP BY {column} ' 'ORDER BY {column}' . format ( column = column , table = table ) ) return ' ' . join ( '%s:%s' % ( t , c ) for t , c in cur )
4445	def create_index ( self , fields , no_term_offsets = False , no_field_flags = False , stopwords = None ) : args = [ self . CREATE_CMD , self . index_name ] if no_term_offsets : args . append ( self . NOOFFSETS ) if no_field_flags : args . append ( self . NOFIELDS ) if stopwords is not None and isinstance ( stopwords , ( list , tuple , set ) ) : args += [ self . STOPWORDS , len ( stopwords ) ] if len ( stopwords ) > 0 : args += list ( stopwords ) args . append ( 'SCHEMA' ) args += list ( itertools . chain ( * ( f . redis_args ( ) for f in fields ) ) ) return self . redis . execute_command ( * args )
2422	def str_from_text ( text ) : REGEX = re . compile ( '<text>((.|\n)+)</text>' , re . UNICODE ) match = REGEX . match ( text ) if match : return match . group ( 1 ) else : return None
9049	def B ( self ) : return unvec ( self . _vecB . value , ( self . X . shape [ 1 ] , self . A . shape [ 0 ] ) )
1322	def MoveToCenter ( self ) -> bool : if self . IsTopLevel ( ) : rect = self . BoundingRectangle screenWidth , screenHeight = GetScreenSize ( ) x , y = ( screenWidth - rect . width ( ) ) // 2 , ( screenHeight - rect . height ( ) ) // 2 if x < 0 : x = 0 if y < 0 : y = 0 return SetWindowPos ( self . NativeWindowHandle , SWP . HWND_Top , x , y , 0 , 0 , SWP . SWP_NoSize ) return False
12437	def stream ( cls , response , sequence ) : iterator = iter ( sequence ) data = { 'chunk' : next ( iterator ) } response . streaming = True def streamer ( ) : while True : if response . asynchronous : yield data [ 'chunk' ] else : response . send ( data [ 'chunk' ] ) yield response . body response . body = None try : data [ 'chunk' ] = next ( iterator ) except StopIteration : break if not response . asynchronous : response . close ( ) return streamer ( )
3188	def create ( self , list_id , subscriber_hash , data ) : subscriber_hash = check_subscriber_hash ( subscriber_hash ) self . list_id = list_id self . subscriber_hash = subscriber_hash if 'note' not in data : raise KeyError ( 'The list member note must have a note' ) response = self . _mc_client . _post ( url = self . _build_path ( list_id , 'members' , subscriber_hash , 'notes' ) , data = data ) if response is not None : self . note_id = response [ 'id' ] else : self . note_id = None return response
3426	def medium ( self , medium ) : def set_active_bound ( reaction , bound ) : if reaction . reactants : reaction . lower_bound = - bound elif reaction . products : reaction . upper_bound = bound media_rxns = list ( ) exchange_rxns = frozenset ( self . exchanges ) for rxn_id , bound in iteritems ( medium ) : rxn = self . reactions . get_by_id ( rxn_id ) if rxn not in exchange_rxns : LOGGER . warn ( "%s does not seem to be an" " an exchange reaction. Applying bounds anyway." , rxn . id ) media_rxns . append ( rxn ) set_active_bound ( rxn , bound ) media_rxns = frozenset ( media_rxns ) for rxn in ( exchange_rxns - media_rxns ) : set_active_bound ( rxn , 0 )
12889	def handle_set ( self , item , value ) : doc = yield from self . call ( 'SET/{}' . format ( item ) , dict ( value = value ) ) if doc is None : return None return doc . status == 'FS_OK'
7986	def registration_success ( self , stanza ) : _unused = stanza self . lock . acquire ( ) try : self . state_change ( "registered" , self . registration_form ) if ( 'FORM_TYPE' in self . registration_form and self . registration_form [ 'FORM_TYPE' ] . value == 'jabber:iq:register' ) : if 'username' in self . registration_form : self . my_jid = JID ( self . registration_form [ 'username' ] . value , self . my_jid . domain , self . my_jid . resource ) if 'password' in self . registration_form : self . password = self . registration_form [ 'password' ] . value self . registration_callback = None self . _post_connect ( ) finally : self . lock . release ( )
673	def runHotgym ( numRecords ) : dataSource = FileRecordStream ( streamID = _INPUT_FILE_PATH ) numRecords = min ( numRecords , dataSource . getDataRowCount ( ) ) network = createNetwork ( dataSource ) network . regions [ "sensor" ] . setParameter ( "predictedField" , "consumption" ) network . regions [ "SP" ] . setParameter ( "learningMode" , 1 ) network . regions [ "TM" ] . setParameter ( "learningMode" , 1 ) network . regions [ "classifier" ] . setParameter ( "learningMode" , 1 ) network . regions [ "SP" ] . setParameter ( "inferenceMode" , 1 ) network . regions [ "TM" ] . setParameter ( "inferenceMode" , 1 ) network . regions [ "classifier" ] . setParameter ( "inferenceMode" , 1 ) results = [ ] N = 1 for iteration in range ( 0 , numRecords , N ) : network . run ( N ) predictionResults = getPredictionResults ( network , "classifier" ) oneStep = predictionResults [ 1 ] [ "predictedValue" ] oneStepConfidence = predictionResults [ 1 ] [ "predictionConfidence" ] fiveStep = predictionResults [ 5 ] [ "predictedValue" ] fiveStepConfidence = predictionResults [ 5 ] [ "predictionConfidence" ] result = ( oneStep , oneStepConfidence * 100 , fiveStep , fiveStepConfidence * 100 ) print "1-step: {:16} ({:4.4}%)\t 5-step: {:16} ({:4.4}%)" . format ( * result ) results . append ( result ) return results
12235	def generate_versionwarning_data_json ( app , config = None , ** kwargs ) : config = config or kwargs . pop ( 'config' , None ) if config is None : config = app . config if config . versionwarning_project_version in config . versionwarning_messages : custom = True message = config . versionwarning_messages . get ( config . versionwarning_project_version ) else : custom = False message = config . versionwarning_default_message banner_html = config . versionwarning_banner_html . format ( id_div = config . versionwarning_banner_id_div , banner_title = config . versionwarning_banner_title , message = message . format ( ** { config . versionwarning_message_placeholder : '<a href="#"></a>' } , ) , admonition_type = config . versionwarning_admonition_type , ) data = json . dumps ( { 'meta' : { 'api_url' : config . versionwarning_api_url , } , 'banner' : { 'html' : banner_html , 'id_div' : config . versionwarning_banner_id_div , 'body_selector' : config . versionwarning_body_selector , 'custom' : custom , } , 'project' : { 'slug' : config . versionwarning_project_slug , } , 'version' : { 'slug' : config . versionwarning_project_version , } , } , indent = 4 ) data_path = os . path . join ( STATIC_PATH , 'data' ) if not os . path . exists ( data_path ) : os . mkdir ( data_path ) with open ( os . path . join ( data_path , JSON_DATA_FILENAME ) , 'w' ) as f : f . write ( data ) config . html_static_path . append ( STATIC_PATH )
5005	def get_enterprise_customer_for_running_pipeline ( request , pipeline ) : sso_provider_id = request . GET . get ( 'tpa_hint' ) if pipeline : sso_provider_id = Registry . get_from_pipeline ( pipeline ) . provider_id return get_enterprise_customer_for_sso ( sso_provider_id )
9795	def group ( ctx , project , group ) : ctx . obj = ctx . obj or { } ctx . obj [ 'project' ] = project ctx . obj [ 'group' ] = group
3206	def get ( self , batch_id , ** queryparams ) : self . batch_id = batch_id self . operation_status = None return self . _mc_client . _get ( url = self . _build_path ( batch_id ) , ** queryparams )
1084	def time ( self ) : "Return the time part, with tzinfo None." return time ( self . hour , self . minute , self . second , self . microsecond )
5536	def write ( self , process_tile , data ) : if isinstance ( process_tile , tuple ) : process_tile = self . config . process_pyramid . tile ( * process_tile ) elif not isinstance ( process_tile , BufferedTile ) : raise ValueError ( "invalid process_tile type: %s" % type ( process_tile ) ) if self . config . mode not in [ "continue" , "overwrite" ] : raise ValueError ( "cannot write output in current process mode" ) if self . config . mode == "continue" and ( self . config . output . tiles_exist ( process_tile ) ) : message = "output exists, not overwritten" logger . debug ( ( process_tile . id , message ) ) return ProcessInfo ( tile = process_tile , processed = False , process_msg = None , written = False , write_msg = message ) elif data is None : message = "output empty, nothing written" logger . debug ( ( process_tile . id , message ) ) return ProcessInfo ( tile = process_tile , processed = False , process_msg = None , written = False , write_msg = message ) else : with Timer ( ) as t : self . config . output . write ( process_tile = process_tile , data = data ) message = "output written in %s" % t logger . debug ( ( process_tile . id , message ) ) return ProcessInfo ( tile = process_tile , processed = False , process_msg = None , written = True , write_msg = message )
1884	def concretize ( self , symbolic , policy , maxcount = 7 ) : assert self . constraints == self . platform . constraints symbolic = self . migrate_expression ( symbolic ) vals = [ ] if policy == 'MINMAX' : vals = self . _solver . minmax ( self . _constraints , symbolic ) elif policy == 'MAX' : vals = self . _solver . max ( self . _constraints , symbolic ) elif policy == 'MIN' : vals = self . _solver . min ( self . _constraints , symbolic ) elif policy == 'SAMPLED' : m , M = self . _solver . minmax ( self . _constraints , symbolic ) vals += [ m , M ] if M - m > 3 : if self . _solver . can_be_true ( self . _constraints , symbolic == ( m + M ) // 2 ) : vals . append ( ( m + M ) // 2 ) if M - m > 100 : for i in ( 0 , 1 , 2 , 5 , 32 , 64 , 128 , 320 ) : if self . _solver . can_be_true ( self . _constraints , symbolic == m + i ) : vals . append ( m + i ) if maxcount <= len ( vals ) : break if M - m > 1000 and maxcount > len ( vals ) : vals += self . _solver . get_all_values ( self . _constraints , symbolic , maxcnt = maxcount - len ( vals ) , silent = True ) elif policy == 'ONE' : vals = [ self . _solver . get_value ( self . _constraints , symbolic ) ] else : assert policy == 'ALL' vals = solver . get_all_values ( self . _constraints , symbolic , maxcnt = maxcount , silent = True ) return tuple ( set ( vals ) )
5378	def _build_pipeline_docker_command ( cls , script_name , inputs , outputs , envs ) : recursive_input_dirs = [ var for var in inputs if var . recursive and var . value ] recursive_output_dirs = [ var for var in outputs if var . recursive and var . value ] install_cloud_sdk = '' if recursive_input_dirs or recursive_output_dirs : install_cloud_sdk = INSTALL_CLOUD_SDK export_input_dirs = '' copy_input_dirs = '' if recursive_input_dirs : export_input_dirs = providers_util . build_recursive_localize_env ( providers_util . DATA_MOUNT_POINT , inputs ) copy_input_dirs = providers_util . build_recursive_localize_command ( providers_util . DATA_MOUNT_POINT , inputs , job_model . P_GCS ) export_output_dirs = '' copy_output_dirs = '' if recursive_output_dirs : export_output_dirs = providers_util . build_recursive_gcs_delocalize_env ( providers_util . DATA_MOUNT_POINT , outputs ) copy_output_dirs = providers_util . build_recursive_delocalize_command ( providers_util . DATA_MOUNT_POINT , outputs , job_model . P_GCS ) docker_paths = [ var . docker_path if var . recursive else os . path . dirname ( var . docker_path ) for var in outputs if var . value ] mkdirs = '\n' . join ( [ 'mkdir -p {0}/{1}' . format ( providers_util . DATA_MOUNT_POINT , path ) for path in docker_paths ] ) inputs_with_wildcards = [ var for var in inputs if not var . recursive and var . docker_path and '*' in os . path . basename ( var . docker_path ) ] export_inputs_with_wildcards = '\n' . join ( [ 'export {0}="{1}/{2}"' . format ( var . name , providers_util . DATA_MOUNT_POINT , var . docker_path ) for var in inputs_with_wildcards ] ) export_empty_envs = '\n' . join ( [ 'export {0}=""' . format ( var . name ) for var in envs | inputs | outputs if not var . value ] ) return DOCKER_COMMAND . format ( mk_runtime_dirs = MK_RUNTIME_DIRS_COMMAND , script_path = '%s/%s' % ( providers_util . SCRIPT_DIR , script_name ) , install_cloud_sdk = install_cloud_sdk , export_inputs_with_wildcards = export_inputs_with_wildcards , export_input_dirs = export_input_dirs , copy_input_dirs = copy_input_dirs , mk_output_dirs = mkdirs , export_output_dirs = export_output_dirs , export_empty_envs = export_empty_envs , tmpdir = providers_util . TMP_DIR , working_dir = providers_util . WORKING_DIR , copy_output_dirs = copy_output_dirs )
2195	def write ( self , msg ) : if self . redirect is not None : self . redirect . write ( msg ) if six . PY2 : from xdoctest . utils . util_str import ensure_unicode msg = ensure_unicode ( msg ) super ( TeeStringIO , self ) . write ( msg )
1935	def get_source_for ( self , asm_offset , runtime = True ) : srcmap = self . get_srcmap ( runtime ) try : beg , size , _ , _ = srcmap [ asm_offset ] except KeyError : return '' output = '' nl = self . source_code [ : beg ] . count ( '\n' ) + 1 snippet = self . source_code [ beg : beg + size ] for l in snippet . split ( '\n' ) : output += ' %s %s\n' % ( nl , l ) nl += 1 return output
12399	def parse ( cls , s , required = False ) : req = pkg_resources . Requirement . parse ( s ) return cls ( req , required = required )
2834	def platform_detect ( ) : pi = pi_version ( ) if pi is not None : return RASPBERRY_PI plat = platform . platform ( ) if plat . lower ( ) . find ( 'armv7l-with-debian' ) > - 1 : return BEAGLEBONE_BLACK elif plat . lower ( ) . find ( 'armv7l-with-ubuntu' ) > - 1 : return BEAGLEBONE_BLACK elif plat . lower ( ) . find ( 'armv7l-with-glibc2.4' ) > - 1 : return BEAGLEBONE_BLACK elif plat . lower ( ) . find ( 'tegra-aarch64-with-ubuntu' ) > - 1 : return JETSON_NANO try : import mraa if mraa . getPlatformName ( ) == 'MinnowBoard MAX' : return MINNOWBOARD except ImportError : pass return UNKNOWN
7755	def set_response_handlers ( self , stanza , res_handler , err_handler , timeout_handler = None , timeout = None ) : self . lock . acquire ( ) try : self . _set_response_handlers ( stanza , res_handler , err_handler , timeout_handler , timeout ) finally : self . lock . release ( )
7526	def get_quick_depths ( data , sample ) : sample . files . clusters = os . path . join ( data . dirs . clusts , sample . name + ".clustS.gz" ) fclust = data . samples [ sample . name ] . files . clusters clusters = gzip . open ( fclust , 'r' ) pairdealer = itertools . izip ( * [ iter ( clusters ) ] * 2 ) depths = [ ] maxlen = [ ] tdepth = 0 tlen = 0 while 1 : try : name , seq = pairdealer . next ( ) except StopIteration : break if name . strip ( ) == seq . strip ( ) : depths . append ( tdepth ) maxlen . append ( tlen ) tlen = 0 tdepth = 0 else : tdepth += int ( name . split ( ";" ) [ - 2 ] [ 5 : ] ) tlen = len ( seq ) clusters . close ( ) return np . array ( maxlen ) , np . array ( depths )
2251	def color_text ( text , color ) : r if color is None : return text try : import pygments import pygments . console if sys . platform . startswith ( 'win32' ) : import colorama colorama . init ( ) ansi_text = pygments . console . colorize ( color , text ) return ansi_text except ImportError : import warnings warnings . warn ( 'pygments is not installed, text will not be colored' ) return text
996	def _updateStatsInferEnd ( self , stats , bottomUpNZ , predictedState , colConfidence ) : if not self . collectStats : return stats [ 'nInfersSinceReset' ] += 1 ( numExtra2 , numMissing2 , confidences2 ) = self . _checkPrediction ( patternNZs = [ bottomUpNZ ] , output = predictedState , colConfidence = colConfidence ) predictionScore , positivePredictionScore , negativePredictionScore = ( confidences2 [ 0 ] ) stats [ 'curPredictionScore2' ] = float ( predictionScore ) stats [ 'curFalseNegativeScore' ] = 1.0 - float ( positivePredictionScore ) stats [ 'curFalsePositiveScore' ] = float ( negativePredictionScore ) stats [ 'curMissing' ] = numMissing2 stats [ 'curExtra' ] = numExtra2 if stats [ 'nInfersSinceReset' ] <= self . burnIn : return stats [ 'nPredictions' ] += 1 numExpected = max ( 1.0 , float ( len ( bottomUpNZ ) ) ) stats [ 'totalMissing' ] += numMissing2 stats [ 'totalExtra' ] += numExtra2 stats [ 'pctExtraTotal' ] += 100.0 * numExtra2 / numExpected stats [ 'pctMissingTotal' ] += 100.0 * numMissing2 / numExpected stats [ 'predictionScoreTotal2' ] += float ( predictionScore ) stats [ 'falseNegativeScoreTotal' ] += 1.0 - float ( positivePredictionScore ) stats [ 'falsePositiveScoreTotal' ] += float ( negativePredictionScore ) if self . collectSequenceStats : cc = self . cellConfidence [ 't-1' ] * self . infActiveState [ 't' ] sconf = cc . sum ( axis = 1 ) for c in range ( self . numberOfCols ) : if sconf [ c ] > 0 : cc [ c , : ] /= sconf [ c ] self . _internalStats [ 'confHistogram' ] += cc
11041	def get_single_header ( headers , key ) : raw_headers = headers . getRawHeaders ( key ) if raw_headers is None : return None header , _ = cgi . parse_header ( raw_headers [ - 1 ] ) return header
2988	def ensure_iterable ( inst ) : if isinstance ( inst , string_types ) : return [ inst ] elif not isinstance ( inst , collections . Iterable ) : return [ inst ] else : return inst
7181	def fix_remaining_type_comments ( node ) : assert node . type == syms . file_input last_n = None for n in node . post_order ( ) : if last_n is not None : if n . type == token . NEWLINE and is_assignment ( last_n ) : fix_variable_annotation_type_comment ( n , last_n ) elif n . type == syms . funcdef and last_n . type == syms . suite : fix_signature_annotation_type_comment ( n , last_n , offset = 1 ) elif n . type == syms . async_funcdef and last_n . type == syms . suite : fix_signature_annotation_type_comment ( n , last_n , offset = 2 ) last_n = n
7168	def remove_intent ( self , name ) : self . intents . remove ( name ) self . padaos . remove_intent ( name ) self . must_train = True
12567	def create_empty_dataset ( self , ds_name , dtype = np . float32 ) : if ds_name in self . _datasets : return self . _datasets [ ds_name ] ds = self . _group . create_dataset ( ds_name , ( 1 , 1 ) , maxshape = None , dtype = dtype ) self . _datasets [ ds_name ] = ds return ds
5263	def capitalcase ( string ) : string = str ( string ) if not string : return string return uppercase ( string [ 0 ] ) + string [ 1 : ]
3246	def get_managed_policies ( group , ** conn ) : managed_policies = list_attached_group_managed_policies ( group [ 'GroupName' ] , ** conn ) managed_policy_names = [ ] for policy in managed_policies : managed_policy_names . append ( policy [ 'PolicyName' ] ) return managed_policy_names
3724	def load_group_assignments_DDBST ( ) : if DDBST_UNIFAC_assignments : return None with open ( os . path . join ( folder , 'DDBST UNIFAC assignments.tsv' ) ) as f : _group_assignments = [ DDBST_UNIFAC_assignments , DDBST_MODIFIED_UNIFAC_assignments , DDBST_PSRK_assignments ] for line in f . readlines ( ) : key , valids , original , modified , PSRK = line . split ( '\t' ) valids = [ True if i == '1' else False for i in valids . split ( ' ' ) ] for groups , storage , valid in zip ( [ original , modified , PSRK ] , _group_assignments , valids ) : if valid : groups = groups . rstrip ( ) . split ( ' ' ) d_data = { } for i in range ( int ( len ( groups ) / 2 ) ) : d_data [ int ( groups [ i * 2 ] ) ] = int ( groups [ i * 2 + 1 ] ) storage [ key ] = d_data
9713	def validate_response ( expected_responses ) : def internal_decorator ( function ) : @ wraps ( function ) async def wrapper ( * args , ** kwargs ) : response = await function ( * args , ** kwargs ) for expected_response in expected_responses : if response . startswith ( expected_response ) : return response raise QRTCommandException ( "Expected %s but got %s" % ( expected_responses , response ) ) return wrapper return internal_decorator
6841	def supported_locales ( ) : family = distrib_family ( ) if family == 'debian' : return _parse_locales ( '/usr/share/i18n/SUPPORTED' ) elif family == 'arch' : return _parse_locales ( '/etc/locale.gen' ) elif family == 'redhat' : return _supported_locales_redhat ( ) else : raise UnsupportedFamily ( supported = [ 'debian' , 'arch' , 'redhat' ] )
212	def from_uint8 ( arr_uint8 , shape , min_value = 0.0 , max_value = 1.0 ) : arr_0to1 = arr_uint8 . astype ( np . float32 ) / 255.0 return HeatmapsOnImage . from_0to1 ( arr_0to1 , shape , min_value = min_value , max_value = max_value )
11853	def add_edge ( self , edge ) : "Add edge to chart, and see if it extends or predicts another edge." start , end , lhs , found , expects = edge if edge not in self . chart [ end ] : self . chart [ end ] . append ( edge ) if self . trace : print '%10s: added %s' % ( caller ( 2 ) , edge ) if not expects : self . extender ( edge ) else : self . predictor ( edge )
12941	def pprint ( self , stream = None ) : pprint . pprint ( self . asDict ( includeMeta = True , forStorage = False , strKeys = True ) , stream = stream )
1148	def _keep_alive ( x , memo ) : try : memo [ id ( memo ) ] . append ( x ) except KeyError : memo [ id ( memo ) ] = [ x ]
10125	def flip_y ( self , center = None ) : if center is None : self . poly . flop ( ) else : self . poly . flop ( center [ 1 ] ) return self
13669	def check_readable ( self , timeout ) : rlist , wlist , xlist = select . select ( [ self . _stdout ] , [ ] , [ ] , timeout ) return bool ( len ( rlist ) )
3080	def get_token ( http , service_account = 'default' ) : token_json = get ( http , 'instance/service-accounts/{0}/token' . format ( service_account ) ) token_expiry = client . _UTCNOW ( ) + datetime . timedelta ( seconds = token_json [ 'expires_in' ] ) return token_json [ 'access_token' ] , token_expiry
4365	def decode ( rawstr , json_loads = default_json_loads ) : decoded_msg = { } try : rawstr = rawstr . decode ( 'utf-8' ) except AttributeError : pass split_data = rawstr . split ( ":" , 3 ) msg_type = split_data [ 0 ] msg_id = split_data [ 1 ] endpoint = split_data [ 2 ] data = '' if msg_id != '' : if "+" in msg_id : msg_id = msg_id . split ( '+' ) [ 0 ] decoded_msg [ 'id' ] = int ( msg_id ) decoded_msg [ 'ack' ] = 'data' else : decoded_msg [ 'id' ] = int ( msg_id ) decoded_msg [ 'ack' ] = True msg_type_id = int ( msg_type ) if msg_type_id in MSG_VALUES : decoded_msg [ 'type' ] = MSG_VALUES [ int ( msg_type ) ] else : raise Exception ( "Unknown message type: %s" % msg_type ) decoded_msg [ 'endpoint' ] = endpoint if len ( split_data ) > 3 : data = split_data [ 3 ] if msg_type == "0" : pass elif msg_type == "1" : decoded_msg [ 'qs' ] = data elif msg_type == "2" : pass elif msg_type == "3" : decoded_msg [ 'data' ] = data elif msg_type == "4" : decoded_msg [ 'data' ] = json_loads ( data ) elif msg_type == "5" : try : data = json_loads ( data ) except ValueError : print ( "Invalid JSON event message" , data ) decoded_msg [ 'args' ] = [ ] else : decoded_msg [ 'name' ] = data . pop ( 'name' ) if 'args' in data : decoded_msg [ 'args' ] = data [ 'args' ] else : decoded_msg [ 'args' ] = [ ] elif msg_type == "6" : if '+' in data : ackId , data = data . split ( '+' ) decoded_msg [ 'ackId' ] = int ( ackId ) decoded_msg [ 'args' ] = json_loads ( data ) else : decoded_msg [ 'ackId' ] = int ( data ) decoded_msg [ 'args' ] = [ ] elif msg_type == "7" : if '+' in data : reason , advice = data . split ( '+' ) decoded_msg [ 'reason' ] = REASONS_VALUES [ int ( reason ) ] decoded_msg [ 'advice' ] = ADVICES_VALUES [ int ( advice ) ] else : decoded_msg [ 'advice' ] = '' if data != '' : decoded_msg [ 'reason' ] = REASONS_VALUES [ int ( data ) ] else : decoded_msg [ 'reason' ] = '' elif msg_type == "8" : pass return decoded_msg
3253	def get_store ( self , name , workspace = None ) : stores = self . get_stores ( workspaces = workspace , names = name ) return self . _return_first_item ( stores )
4705	def write ( self , path ) : with open ( path , "wb" ) as fout : fout . write ( self . m_buf )
8175	def update ( self , shuffled = True , cohesion = 100 , separation = 10 , alignment = 5 , goal = 20 , limit = 30 ) : from random import shuffle if shuffled : shuffle ( self ) m1 = 1.0 m2 = 1.0 m3 = 1.0 m4 = 1.0 if not self . scattered and _ctx . random ( ) < self . _scatter : self . scattered = True if self . scattered : m1 = - m1 m3 *= 0.25 self . _scatter_i += 1 if self . _scatter_i >= self . _scatter_t : self . scattered = False self . _scatter_i = 0 if not self . has_goal : m4 = 0 if self . flee : m4 = - m4 for b in self : if b . is_perching : if b . _perch_t > 0 : b . _perch_t -= 1 continue else : b . is_perching = False vx1 , vy1 , vz1 = b . cohesion ( cohesion ) vx2 , vy2 , vz2 = b . separation ( separation ) vx3 , vy3 , vz3 = b . alignment ( alignment ) vx4 , vy4 , vz4 = b . goal ( self . _gx , self . _gy , self . _gz , goal ) b . vx += m1 * vx1 + m2 * vx2 + m3 * vx3 + m4 * vx4 b . vy += m1 * vy1 + m2 * vy2 + m3 * vy3 + m4 * vy4 b . vz += m1 * vz1 + m2 * vz2 + m3 * vz3 + m4 * vz4 b . limit ( limit ) b . x += b . vx b . y += b . vy b . z += b . vz self . constrain ( )
10328	def rank_causalr_hypothesis ( graph , node_to_regulation , regulator_node ) : upregulation_hypothesis = { 'correct' : 0 , 'incorrect' : 0 , 'ambiguous' : 0 } downregulation_hypothesis = { 'correct' : 0 , 'incorrect' : 0 , 'ambiguous' : 0 } targets = [ node for node in node_to_regulation if node != regulator_node ] predicted_regulations = run_cna ( graph , regulator_node , targets ) for _ , target_node , predicted_regulation in predicted_regulations : if ( predicted_regulation is Effect . inhibition or predicted_regulation is Effect . activation ) and ( predicted_regulation . value == node_to_regulation [ target_node ] ) : upregulation_hypothesis [ 'correct' ] += 1 downregulation_hypothesis [ 'incorrect' ] += 1 elif predicted_regulation is Effect . ambiguous : upregulation_hypothesis [ 'ambiguous' ] += 1 downregulation_hypothesis [ 'ambiguous' ] += 1 elif predicted_regulation is Effect . no_effect : continue else : downregulation_hypothesis [ 'correct' ] += 1 upregulation_hypothesis [ 'incorrect' ] += 1 upregulation_hypothesis [ 'score' ] = upregulation_hypothesis [ 'correct' ] - upregulation_hypothesis [ 'incorrect' ] downregulation_hypothesis [ 'score' ] = downregulation_hypothesis [ 'correct' ] - downregulation_hypothesis [ 'incorrect' ] return upregulation_hypothesis , downregulation_hypothesis
1607	def spec ( cls , name = None , inputs = None , par = 1 , config = None , optional_outputs = None ) : python_class_path = "%s.%s" % ( cls . __module__ , cls . __name__ ) if hasattr ( cls , 'outputs' ) : _outputs = copy . copy ( cls . outputs ) else : _outputs = [ ] if optional_outputs is not None : assert isinstance ( optional_outputs , ( list , tuple ) ) for out in optional_outputs : assert isinstance ( out , ( str , Stream ) ) _outputs . append ( out ) return HeronComponentSpec ( name , python_class_path , is_spout = False , par = par , inputs = inputs , outputs = _outputs , config = config )
4956	def get_object ( self , name , description ) : return Activity ( id = X_API_ACTIVITY_COURSE , definition = ActivityDefinition ( name = LanguageMap ( { 'en-US' : ( name or '' ) . encode ( "ascii" , "ignore" ) . decode ( 'ascii' ) } ) , description = LanguageMap ( { 'en-US' : ( description or '' ) . encode ( "ascii" , "ignore" ) . decode ( 'ascii' ) } ) , ) , )
1534	def create_socket_options ( ) : sys_config = system_config . get_sys_config ( ) opt_list = [ const . INSTANCE_NETWORK_WRITE_BATCH_SIZE_BYTES , const . INSTANCE_NETWORK_WRITE_BATCH_TIME_MS , const . INSTANCE_NETWORK_READ_BATCH_SIZE_BYTES , const . INSTANCE_NETWORK_READ_BATCH_TIME_MS , const . INSTANCE_NETWORK_OPTIONS_SOCKET_RECEIVED_BUFFER_SIZE_BYTES , const . INSTANCE_NETWORK_OPTIONS_SOCKET_SEND_BUFFER_SIZE_BYTES ] Log . debug ( "In create_socket_options()" ) try : value_lst = [ int ( sys_config [ opt ] ) for opt in opt_list ] sock_opt = SocketOptions ( * value_lst ) return sock_opt except ValueError as e : raise ValueError ( "Invalid value in sys_config: %s" % str ( e ) ) except KeyError as e : raise KeyError ( "Incomplete sys_config: %s" % str ( e ) )
5164	def __intermediate_bridge ( self , interface , i ) : if interface [ 'type' ] == 'bridge' and i < 2 : bridge_members = ' ' . join ( interface . pop ( 'bridge_members' ) ) if bridge_members : interface [ 'ifname' ] = bridge_members else : interface [ 'bridge_empty' ] = True del interface [ 'ifname' ] elif interface [ 'type' ] == 'bridge' and i >= 2 : if 'br-' not in interface [ 'ifname' ] : interface [ 'ifname' ] = 'br-{ifname}' . format ( ** interface ) for attr in [ 'type' , 'bridge_members' , 'stp' , 'gateway' ] : if attr in interface : del interface [ attr ] elif interface [ 'type' ] != 'bridge' : del interface [ 'type' ] return interface
5881	def is_highlink_density ( self , element ) : links = self . parser . getElementsByTag ( element , tag = 'a' ) if not links : return False text = self . parser . getText ( element ) words = text . split ( ' ' ) words_number = float ( len ( words ) ) link_text_parts = [ ] for link in links : link_text_parts . append ( self . parser . getText ( link ) ) link_text = '' . join ( link_text_parts ) link_words = link_text . split ( ' ' ) number_of_link_words = float ( len ( link_words ) ) number_of_links = float ( len ( links ) ) link_divisor = float ( number_of_link_words / words_number ) score = float ( link_divisor * number_of_links ) if score >= 1.0 : return True return False
9011	def index_of_first_consumed_mesh_in_row ( self ) : index = 0 for instruction in self . row_instructions : if instruction is self : break index += instruction . number_of_consumed_meshes else : self . _raise_not_found_error ( ) return index
12949	def copyModel ( mdl ) : copyNum = _modelCopyMap [ mdl ] _modelCopyMap [ mdl ] += 1 mdlCopy = type ( mdl . __name__ + '_Copy' + str ( copyNum ) , mdl . __bases__ , copy . deepcopy ( dict ( mdl . __dict__ ) ) ) mdlCopy . FIELDS = [ field . copy ( ) for field in mdl . FIELDS ] mdlCopy . INDEXED_FIELDS = [ str ( idxField ) for idxField in mdl . INDEXED_FIELDS ] mdlCopy . validateModel ( ) return mdlCopy
11423	def print_recs ( listofrec , format = 1 , tags = None ) : if tags is None : tags = [ ] text = "" if type ( listofrec ) . __name__ != 'list' : return "" else : for rec in listofrec : text = "%s\n%s" % ( text , print_rec ( rec , format , tags ) ) return text
7977	def _post_connect ( self ) : if not self . initiator : if "plain" in self . auth_methods or "digest" in self . auth_methods : self . set_iq_get_handler ( "query" , "jabber:iq:auth" , self . auth_in_stage1 ) self . set_iq_set_handler ( "query" , "jabber:iq:auth" , self . auth_in_stage2 ) elif self . registration_callback : iq = Iq ( stanza_type = "get" ) iq . set_content ( Register ( ) ) self . set_response_handlers ( iq , self . registration_form_received , self . registration_error ) self . send ( iq ) return ClientStream . _post_connect ( self )
12966	def allOnlyIndexedFields ( self ) : matchedKeys = self . getPrimaryKeys ( ) if matchedKeys : return self . getMultipleOnlyIndexedFields ( matchedKeys ) return IRQueryableList ( [ ] , mdl = self . mdl )
4439	async def _playnow ( self , ctx , * , query : str ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . queue and not player . is_playing : return await ctx . invoke ( self . _play , query = query ) query = query . strip ( '<>' ) if not url_rx . match ( query ) : query = f'ytsearch:{query}' results = await self . bot . lavalink . get_tracks ( query ) if not results or not results [ 'tracks' ] : return await ctx . send ( 'Nothing found!' ) tracks = results [ 'tracks' ] track = tracks . pop ( 0 ) if results [ 'loadType' ] == 'PLAYLIST_LOADED' : for _track in tracks : player . add ( requester = ctx . author . id , track = _track ) await player . play_now ( requester = ctx . author . id , track = track )
3756	def Tautoignition ( CASRN , AvailableMethods = False , Method = None ) : r def list_methods ( ) : methods = [ ] if CASRN in IEC_2010 . index and not np . isnan ( IEC_2010 . at [ CASRN , 'Tautoignition' ] ) : methods . append ( IEC ) if CASRN in NFPA_2008 . index and not np . isnan ( NFPA_2008 . at [ CASRN , 'Tautoignition' ] ) : methods . append ( NFPA ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == IEC : return float ( IEC_2010 . at [ CASRN , 'Tautoignition' ] ) elif Method == NFPA : return float ( NFPA_2008 . at [ CASRN , 'Tautoignition' ] ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' )
3018	def _generate_assertion ( self ) : now = int ( time . time ( ) ) payload = { 'aud' : self . token_uri , 'scope' : self . _scopes , 'iat' : now , 'exp' : now + self . MAX_TOKEN_LIFETIME_SECS , 'iss' : self . _service_account_email , } payload . update ( self . _kwargs ) return crypt . make_signed_jwt ( self . _signer , payload , key_id = self . _private_key_id )
2227	def _digest_hasher ( hasher , hashlen , base ) : hex_text = hasher . hexdigest ( ) base_text = _convert_hexstr_base ( hex_text , base ) text = base_text [ : hashlen ] return text
2674	def init ( src , minimal = False ) : templates_path = os . path . join ( os . path . dirname ( os . path . abspath ( __file__ ) ) , 'project_templates' , ) for filename in os . listdir ( templates_path ) : if ( minimal and filename == 'event.json' ) or filename . endswith ( '.pyc' ) : continue dest_path = os . path . join ( templates_path , filename ) if not os . path . isdir ( dest_path ) : copy ( dest_path , src )
7963	def _feed_reader ( self , data ) : IN_LOGGER . debug ( "IN: %r" , data ) if data : self . lock . release ( ) try : self . _reader . feed ( data ) finally : self . lock . acquire ( ) else : self . _eof = True self . lock . release ( ) try : self . _stream . stream_eof ( ) finally : self . lock . acquire ( ) if not self . _serializer : if self . _state != "closed" : self . event ( DisconnectedEvent ( self . _dst_addr ) ) self . _set_state ( "closed" )
8534	def read ( cls , data , protocol = None , fallback_protocol = TBinaryProtocol , finagle_thrift = False , max_fields = MAX_FIELDS , max_list_size = MAX_LIST_SIZE , max_map_size = MAX_MAP_SIZE , max_set_size = MAX_SET_SIZE , read_values = False ) : if len ( data ) < cls . MIN_MESSAGE_SIZE : raise ValueError ( 'not enough data' ) if protocol is None : protocol = cls . detect_protocol ( data , fallback_protocol ) trans = TTransport . TMemoryBuffer ( data ) proto = protocol ( trans ) header = None if finagle_thrift : try : header = ThriftStruct . read ( proto , max_fields , max_list_size , max_map_size , max_set_size , read_values ) except : trans = TTransport . TMemoryBuffer ( data ) proto = protocol ( trans ) method , mtype , seqid = proto . readMessageBegin ( ) mtype = cls . message_type_to_str ( mtype ) if len ( method ) == 0 or method . isspace ( ) or method . startswith ( ' ' ) : raise ValueError ( 'no method name' ) if len ( method ) > cls . MAX_METHOD_LENGTH : raise ValueError ( 'method name too long' ) valid = range ( 33 , 127 ) if any ( ord ( char ) not in valid for char in method ) : raise ValueError ( 'invalid method name' % method ) args = ThriftStruct . read ( proto , max_fields , max_list_size , max_map_size , max_set_size , read_values ) proto . readMessageEnd ( ) msglen = trans . _buffer . tell ( ) return cls ( method , mtype , seqid , args , header , msglen ) , msglen
4478	def split_storage ( path , default = 'osfstorage' ) : path = norm_remote_path ( path ) for provider in KNOWN_PROVIDERS : if path . startswith ( provider + '/' ) : if six . PY3 : return path . split ( '/' , maxsplit = 1 ) else : return path . split ( '/' , 1 ) return ( default , path )
12363	def get ( self , id , ** kwargs ) : return ( super ( MutableCollection , self ) . get ( ( id , ) , ** kwargs ) . get ( self . singular , None ) )
4299	def create_project ( config_data ) : env = deepcopy ( dict ( os . environ ) ) env [ str ( 'DJANGO_SETTINGS_MODULE' ) ] = str ( '{0}.settings' . format ( config_data . project_name ) ) env [ str ( 'PYTHONPATH' ) ] = str ( os . pathsep . join ( map ( shlex_quote , sys . path ) ) ) kwargs = { } args = [ ] if config_data . template : kwargs [ 'template' ] = config_data . template args . append ( config_data . project_name ) if config_data . project_directory : args . append ( config_data . project_directory ) if not os . path . exists ( config_data . project_directory ) : os . makedirs ( config_data . project_directory ) base_cmd = 'django-admin.py' start_cmds = [ os . path . join ( os . path . dirname ( sys . executable ) , base_cmd ) ] start_cmd_pnodes = [ 'Scripts' ] start_cmds . extend ( [ os . path . join ( os . path . dirname ( sys . executable ) , pnode , base_cmd ) for pnode in start_cmd_pnodes ] ) start_cmd = [ base_cmd ] for p in start_cmds : if os . path . exists ( p ) : start_cmd = [ sys . executable , p ] break cmd_args = start_cmd + [ 'startproject' ] + args if config_data . verbose : sys . stdout . write ( 'Project creation command: {0}\n' . format ( ' ' . join ( cmd_args ) ) ) try : output = subprocess . check_output ( cmd_args , stderr = subprocess . STDOUT ) sys . stdout . write ( output . decode ( 'utf-8' ) ) except subprocess . CalledProcessError as e : if config_data . verbose : sys . stdout . write ( e . output . decode ( 'utf-8' ) ) raise
11695	def verify_words ( self ) : if self . comment : if find_words ( self . comment , self . suspect_words , self . excluded_words ) : self . label_suspicious ( 'suspect_word' ) if self . source : for word in self . illegal_sources : if word in self . source . lower ( ) : self . label_suspicious ( 'suspect_word' ) break if self . imagery_used : for word in self . illegal_sources : if word in self . imagery_used . lower ( ) : self . label_suspicious ( 'suspect_word' ) break self . suspicion_reasons = list ( set ( self . suspicion_reasons ) )
4301	def setup_database ( config_data ) : with chdir ( config_data . project_directory ) : env = deepcopy ( dict ( os . environ ) ) env [ str ( 'DJANGO_SETTINGS_MODULE' ) ] = str ( '{0}.settings' . format ( config_data . project_name ) ) env [ str ( 'PYTHONPATH' ) ] = str ( os . pathsep . join ( map ( shlex_quote , sys . path ) ) ) commands = [ ] commands . append ( [ sys . executable , '-W' , 'ignore' , 'manage.py' , 'migrate' ] , ) if config_data . verbose : sys . stdout . write ( 'Database setup commands: {0}\n' . format ( ', ' . join ( [ ' ' . join ( cmd ) for cmd in commands ] ) ) ) for command in commands : try : output = subprocess . check_output ( command , env = env , stderr = subprocess . STDOUT ) sys . stdout . write ( output . decode ( 'utf-8' ) ) except subprocess . CalledProcessError as e : if config_data . verbose : sys . stdout . write ( e . output . decode ( 'utf-8' ) ) raise if not config_data . no_user : sys . stdout . write ( 'Creating admin user\n' ) if config_data . noinput : create_user ( config_data ) else : subprocess . check_call ( ' ' . join ( [ sys . executable , '-W' , 'ignore' , 'manage.py' , 'createsuperuser' ] ) , shell = True , stderr = subprocess . STDOUT )
6459	def _has_vowel ( self , term ) : for letter in term : if letter in self . _vowels : return True return False
6582	def play_station ( self , station ) : for song in iterate_forever ( station . get_playlist ) : try : self . play ( song ) except StopIteration : self . stop ( ) return
3850	def fetch_raw ( self , method , url , params = None , headers = None , data = None ) : if not urllib . parse . urlparse ( url ) . hostname . endswith ( '.google.com' ) : raise Exception ( 'expected google.com domain' ) headers = headers or { } headers . update ( self . _authorization_headers ) return self . _session . request ( method , url , params = params , headers = headers , data = data , proxy = self . _proxy )
13075	def create_blueprint ( self ) : self . register_plugins ( ) self . blueprint = Blueprint ( self . name , "nemo" , url_prefix = self . prefix , template_folder = self . template_folder , static_folder = self . static_folder , static_url_path = self . static_url_path ) for url , name , methods , instance in self . _urls : self . blueprint . add_url_rule ( url , view_func = self . view_maker ( name , instance ) , endpoint = _plugin_endpoint_rename ( name , instance ) , methods = methods ) for url , name , methods , instance in self . _semantic_url : self . blueprint . add_url_rule ( url , view_func = self . view_maker ( name , instance ) , endpoint = _plugin_endpoint_rename ( name , instance ) + "_semantic" , methods = methods ) self . register_assets ( ) self . register_filters ( ) self . __templates_namespaces__ . extend ( self . __instance_templates__ ) for namespace , directory in self . __templates_namespaces__ [ : : - 1 ] : if namespace not in self . __template_loader__ : self . __template_loader__ [ namespace ] = [ ] self . __template_loader__ [ namespace ] . append ( jinja2 . FileSystemLoader ( op . abspath ( directory ) ) ) self . blueprint . jinja_loader = jinja2 . PrefixLoader ( { namespace : jinja2 . ChoiceLoader ( paths ) for namespace , paths in self . __template_loader__ . items ( ) } , "::" ) if self . cache is not None : for func , instance in self . cached : setattr ( instance , func . __name__ , self . cache . memoize ( ) ( func ) ) return self . blueprint
4710	def get_chunk_information ( self , chk , lun , chunk_name ) : cmd = [ "nvm_cmd rprt_lun" , self . envs , "%d %d > %s" % ( chk , lun , chunk_name ) ] status , _ , _ = cij . ssh . command ( cmd , shell = True ) return status
9192	def _get_file_sha1 ( file ) : bits = file . read ( ) file . seek ( 0 ) h = hashlib . new ( 'sha1' , bits ) . hexdigest ( ) return h
4896	def get_enterprise_sso_uid ( self , obj ) : enterprise_learner = EnterpriseCustomerUser . objects . filter ( user_id = obj . id ) . first ( ) return enterprise_learner and enterprise_learner . get_remote_id ( )
7009	def _fourier_residual ( fourierparams , phase , mags ) : f = _fourier_func ( fourierparams , phase , mags ) residual = mags - f return residual
6923	def aovhm_theta ( times , mags , errs , frequency , nharmonics , magvariance ) : period = 1.0 / frequency ndet = times . size two_nharmonics = nharmonics + nharmonics phasedseries = phase_magseries_with_errs ( times , mags , errs , period , times [ 0 ] , sort = True , wrap = False ) phase = phasedseries [ 'phase' ] pmags = phasedseries [ 'mags' ] perrs = phasedseries [ 'errs' ] pweights = 1.0 / perrs phase = phase * 2.0 * pi_value z = npcos ( phase ) + 1.0j * npsin ( phase ) phase = nharmonics * phase psi = pmags * pweights * ( npcos ( phase ) + 1j * npsin ( phase ) ) zn = 1.0 + 0.0j phi = pweights + 0.0j theta_aov = 0.0 for _ in range ( two_nharmonics ) : phi_dot_phi = npsum ( phi * phi . conjugate ( ) ) alpha = npsum ( pweights * z * phi ) phi_dot_psi = npvdot ( phi , psi ) phi_dot_phi = npmax ( [ phi_dot_phi , 10.0e-9 ] ) alpha = alpha / phi_dot_phi theta_aov = ( theta_aov + npabs ( phi_dot_psi ) * npabs ( phi_dot_psi ) / phi_dot_phi ) phi = phi * z - alpha * zn * phi . conjugate ( ) zn = zn * z theta_aov = ( ( ndet - two_nharmonics - 1.0 ) * theta_aov / ( two_nharmonics * npmax ( [ magvariance - theta_aov , 1.0e-9 ] ) ) ) return theta_aov
1495	def find_closing_braces ( self , query ) : if query [ 0 ] != '(' : raise Exception ( "Trying to find closing braces for no opening braces" ) num_open_braces = 0 for i in range ( len ( query ) ) : c = query [ i ] if c == '(' : num_open_braces += 1 elif c == ')' : num_open_braces -= 1 if num_open_braces == 0 : return i raise Exception ( "No closing braces found" )
11040	def write ( self , path , ** data ) : d = self . request ( 'PUT' , '/v1/' + path , json = data ) return d . addCallback ( self . _handle_response , check_cas = True )
8346	def findAll ( self , name = None , attrs = { } , recursive = True , text = None , limit = None , ** kwargs ) : generator = self . recursiveChildGenerator if not recursive : generator = self . childGenerator return self . _findAll ( name , attrs , text , limit , generator , ** kwargs )
2709	def limit_sentences ( path , word_limit = 100 ) : word_count = 0 if isinstance ( path , str ) : path = json_iter ( path ) for meta in path : if not isinstance ( meta , SummarySent ) : p = SummarySent ( ** meta ) else : p = meta sent_text = p . text . strip ( ) . split ( " " ) sent_len = len ( sent_text ) if ( word_count + sent_len ) > word_limit : break else : word_count += sent_len yield sent_text , p . idx
9491	def _get_name_info ( name_index , name_list ) : argval = name_index if name_list is not None : try : argval = name_list [ name_index ] except IndexError : raise ValidationError ( "Names value out of range: {}" . format ( name_index ) ) from None argrepr = argval else : argrepr = repr ( argval ) return argval , argrepr
5983	def output_subplot_array ( output_path , output_filename , output_format ) : if output_format is 'show' : plt . show ( ) elif output_format is 'png' : plt . savefig ( output_path + output_filename + '.png' , bbox_inches = 'tight' ) elif output_format is 'fits' : raise exc . PlottingException ( 'You cannot output a subplots with format .fits' )
6924	def open ( self , database , user , password , host ) : try : self . connection = pg . connect ( user = user , password = password , database = database , host = host ) LOGINFO ( 'postgres connection successfully ' 'created, using DB %s, user %s' % ( database , user ) ) self . database = database self . user = user except Exception as e : LOGEXCEPTION ( 'postgres connection failed, ' 'using DB %s, user %s' % ( database , user ) ) self . database = None self . user = None
4605	def upgrade ( self ) : assert callable ( self . blockchain . upgrade_account ) return self . blockchain . upgrade_account ( account = self )
8464	def deploy ( target ) : if not os . getenv ( CIRCLECI_ENV_VAR ) : raise EnvironmentError ( 'Must be on CircleCI to run this script' ) current_branch = os . getenv ( 'CIRCLE_BRANCH' ) if ( target == 'PROD' ) and ( current_branch != 'master' ) : raise EnvironmentError ( ( 'Refusing to deploy to production from branch {current_branch!r}. ' 'Production deploys can only be made from master.' ) . format ( current_branch = current_branch ) ) if target in ( 'PROD' , 'TEST' ) : pypi_username = os . getenv ( '{target}_PYPI_USERNAME' . format ( target = target ) ) pypi_password = os . getenv ( '{target}_PYPI_PASSWORD' . format ( target = target ) ) else : raise ValueError ( "Deploy target must be 'PROD' or 'TEST', got {target!r}." . format ( target = target ) ) if not ( pypi_username and pypi_password ) : raise EnvironmentError ( ( "Missing '{target}_PYPI_USERNAME' and/or '{target}_PYPI_PASSWORD' " "environment variables. These are required to push to PyPI." ) . format ( target = target ) ) os . environ [ 'TWINE_USERNAME' ] = pypi_username os . environ [ 'TWINE_PASSWORD' ] = pypi_password _shell ( 'git config --global user.email "oss@cloverhealth.com"' ) _shell ( 'git config --global user.name "Circle CI"' ) _shell ( 'git config push.default current' ) ret = _shell ( 'make version' , stdout = subprocess . PIPE ) version = ret . stdout . decode ( 'utf-8' ) . strip ( ) print ( 'Deploying version {version!r}...' . format ( version = version ) ) _shell ( 'git tag -f -a {version} -m "Version {version}"' . format ( version = version ) ) _shell ( 'sed -i.bak "s/^__version__ = .*/__version__ = {version!r}/" */version.py' . format ( version = version ) ) _shell ( 'python setup.py sdist bdist_wheel' ) _shell ( 'git add ChangeLog AUTHORS */version.py' ) _shell ( 'git commit --no-verify -m "Merge autogenerated files [skip ci]"' ) _pypi_push ( 'dist' ) _shell ( 'git push --follow-tags' ) print ( 'Deployment complete. Latest version is {version}.' . format ( version = version ) )
2566	def udp_messenger ( domain_name , UDP_IP , UDP_PORT , sock_timeout , message ) : try : if message is None : raise ValueError ( "message was none" ) encoded_message = bytes ( message , "utf-8" ) if encoded_message is None : raise ValueError ( "utf-8 encoding of message failed" ) if domain_name : try : UDP_IP = socket . gethostbyname ( domain_name ) except Exception : pass if UDP_IP is None : raise Exception ( "UDP_IP is None" ) if UDP_PORT is None : raise Exception ( "UDP_PORT is None" ) sock = socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) sock . settimeout ( sock_timeout ) sock . sendto ( bytes ( message , "utf-8" ) , ( UDP_IP , UDP_PORT ) ) sock . close ( ) except socket . timeout : logger . debug ( "Failed to send usage tracking data: socket timeout" ) except OSError as e : logger . debug ( "Failed to send usage tracking data: OSError: {}" . format ( e ) ) except Exception as e : logger . debug ( "Failed to send usage tracking data: Exception: {}" . format ( e ) )
5973	def generate_submit_array ( templates , directories , ** kwargs ) : dirname = kwargs . setdefault ( 'dirname' , os . path . curdir ) reldirs = [ relpath ( p , start = dirname ) for p in asiterable ( directories ) ] missing = [ p for p in ( os . path . join ( dirname , subdir ) for subdir in reldirs ) if not os . path . exists ( p ) ] if len ( missing ) > 0 : logger . debug ( "template=%(template)r: dirname=%(dirname)r reldirs=%(reldirs)r" , vars ( ) ) logger . error ( "Some directories are not accessible from the array script: " "%(missing)r" , vars ( ) ) def write_script ( template ) : qsystem = detect_queuing_system ( template ) if qsystem is None or not qsystem . has_arrays ( ) : logger . warning ( "Not known how to make a job array for %(template)r; skipping..." , vars ( ) ) return None kwargs [ 'jobarray_string' ] = qsystem . array ( reldirs ) return generate_submit_scripts ( template , ** kwargs ) [ 0 ] return [ write_script ( template ) for template in config . get_templates ( templates ) ]
5192	def send_select_and_operate_command ( self , command , index , callback = asiodnp3 . PrintingCommandCallback . Get ( ) , config = opendnp3 . TaskConfig ( ) . Default ( ) ) : self . master . SelectAndOperate ( command , index , callback , config )
7239	def window_at ( self , geom , window_shape ) : y_size , x_size = window_shape [ 0 ] , window_shape [ 1 ] bounds = box ( * geom . bounds ) px = ops . transform ( self . __geo_transform__ . rev , bounds ) . centroid miny , maxy = int ( px . y - y_size / 2 ) , int ( px . y + y_size / 2 ) minx , maxx = int ( px . x - x_size / 2 ) , int ( px . x + x_size / 2 ) _ , y_max , x_max = self . shape if minx < 0 or miny < 0 or maxx > x_max or maxy > y_max : raise ValueError ( "Input geometry resulted in a window outside of the image" ) return self [ : , miny : maxy , minx : maxx ]
12604	def duplicated ( values : Sequence ) : vals = pd . Series ( values ) return vals [ vals . duplicated ( ) ]
9846	def resample_factor ( self , factor ) : newlengths = [ ( N - 1 ) * float ( factor ) + 1 for N in self . _len_edges ( ) ] edges = [ numpy . linspace ( start , stop , num = int ( N ) , endpoint = True ) for ( start , stop , N ) in zip ( self . _min_edges ( ) , self . _max_edges ( ) , newlengths ) ] return self . resample ( edges )
1513	def wait_for_master_to_start ( single_master ) : i = 0 while True : try : r = requests . get ( "http://%s:4646/v1/status/leader" % single_master ) if r . status_code == 200 : break except : Log . debug ( sys . exc_info ( ) [ 0 ] ) Log . info ( "Waiting for cluster to come up... %s" % i ) time . sleep ( 1 ) if i > 10 : Log . error ( "Failed to start Nomad Cluster!" ) sys . exit ( - 1 ) i = i + 1
1099	def _count_leading ( line , ch ) : i , n = 0 , len ( line ) while i < n and line [ i ] == ch : i += 1 return i
12072	def show ( self ) : copied = self . copy ( ) enumerated = [ el for el in enumerate ( copied ) ] for ( group_ind , specs ) in enumerated : if len ( enumerated ) > 1 : print ( "Group %d" % group_ind ) ordering = self . constant_keys + self . varying_keys spec_lines = [ ', ' . join ( [ '%s=%s' % ( k , s [ k ] ) for k in ordering ] ) for s in specs ] print ( '\n' . join ( [ '%d: %s' % ( i , l ) for ( i , l ) in enumerate ( spec_lines ) ] ) ) print ( 'Remaining arguments not available for %s' % self . __class__ . __name__ )
3472	def subtract_metabolites ( self , metabolites , combine = True , reversibly = True ) : self . add_metabolites ( { k : - v for k , v in iteritems ( metabolites ) } , combine = combine , reversibly = reversibly )
9327	def valid_content_type ( self , content_type , accept ) : accept_tokens = accept . replace ( ' ' , '' ) . split ( ';' ) content_type_tokens = content_type . replace ( ' ' , '' ) . split ( ';' ) return ( all ( elem in content_type_tokens for elem in accept_tokens ) and ( content_type_tokens [ 0 ] == 'application/vnd.oasis.taxii+json' or content_type_tokens [ 0 ] == 'application/vnd.oasis.stix+json' ) )
1406	def validated_formatter ( self , url_format ) : valid_parameters = { "${CLUSTER}" : "cluster" , "${ENVIRON}" : "environ" , "${TOPOLOGY}" : "topology" , "${ROLE}" : "role" , "${USER}" : "user" , } dummy_formatted_url = url_format for key , value in valid_parameters . items ( ) : dummy_formatted_url = dummy_formatted_url . replace ( key , value ) if '$' in dummy_formatted_url : raise Exception ( "Invalid viz.url.format: %s" % ( url_format ) ) return url_format
4613	def blocks ( self , start = None , stop = None ) : self . block_interval = self . get_block_interval ( ) if not start : start = self . get_current_block_num ( ) while True : if stop : head_block = stop else : head_block = self . get_current_block_num ( ) for blocknum in range ( start , head_block + 1 ) : block = self . wait_for_and_get_block ( blocknum ) block . update ( { "block_num" : blocknum } ) yield block start = head_block + 1 if stop and start > stop : return time . sleep ( self . block_interval )
13278	def update_desc_rcin_path ( desc , sibs_len , pdesc_level ) : psibs_len = pdesc_level . __len__ ( ) parent_breadth = desc [ 'parent_breadth_path' ] [ - 1 ] if ( desc [ 'sib_seq' ] == ( sibs_len - 1 ) ) : if ( parent_breadth == ( psibs_len - 1 ) ) : pass else : parent_rsib_breadth = parent_breadth + 1 prsib_desc = pdesc_level [ parent_rsib_breadth ] if ( prsib_desc [ 'leaf' ] ) : pass else : rcin_path = copy . deepcopy ( prsib_desc [ 'path' ] ) rcin_path . append ( 0 ) desc [ 'rcin_path' ] = rcin_path else : pass return ( desc )
4580	def toggle ( s ) : is_numeric = ',' in s or s . startswith ( '0x' ) or s . startswith ( '#' ) c = name_to_color ( s ) return color_to_name ( c ) if is_numeric else str ( c )
999	def printColConfidence ( self , aState , maxCols = 20 ) : def formatFPRow ( var ) : s = '' for c in range ( min ( maxCols , self . numberOfCols ) ) : if c > 0 and c % 10 == 0 : s += ' ' s += ' %5.3f' % var [ c ] s += ' ' return s print formatFPRow ( aState )
5802	def _convert_filetime_to_timestamp ( filetime ) : hundreds_nano_seconds = struct . unpack ( b'>Q' , struct . pack ( b'>LL' , filetime . dwHighDateTime , filetime . dwLowDateTime ) ) [ 0 ] seconds_since_1601 = hundreds_nano_seconds / 10000000 return seconds_since_1601 - 11644473600
2824	def convert_sigmoid ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting sigmoid ...' ) if names == 'short' : tf_name = 'SIGM' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) sigmoid = keras . layers . Activation ( 'sigmoid' , name = tf_name ) layers [ scope_name ] = sigmoid ( layers [ inputs [ 0 ] ] )
774	def main ( ) : initLogging ( verbose = True ) initExperimentPrng ( ) @ staticmethod def _mockCreate ( * args , ** kwargs ) : kwargs . pop ( 'implementation' , None ) return SDRClassifierDiff ( * args , ** kwargs ) SDRClassifierFactory . create = _mockCreate runExperiment ( sys . argv [ 1 : ] )
9045	def gradient ( self ) : self . _init_svd ( ) C0 = self . _C0 . gradient ( ) [ "Lu" ] . T C1 = self . _C1 . gradient ( ) [ "Lu" ] . T grad = { "C0.Lu" : kron ( C0 , self . _X ) . T , "C1.Lu" : kron ( C1 , self . _I ) . T } return grad
3524	def intercom ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return IntercomNode ( )
9176	def with_db_cursor ( func ) : @ functools . wraps ( func ) def wrapped ( * args , ** kwargs ) : if 'cursor' in kwargs or func . func_code . co_argcount == len ( args ) : return func ( * args , ** kwargs ) with db_connect ( ) as db_connection : with db_connection . cursor ( ) as cursor : kwargs [ 'cursor' ] = cursor return func ( * args , ** kwargs ) return wrapped
5319	def readattr ( path , name ) : try : f = open ( USB_SYS_PREFIX + path + "/" + name ) return f . readline ( ) . rstrip ( "\n" ) except IOError : return None
13691	def status ( self ) : peer = random . choice ( self . PEERS ) formatted_peer = 'http://{}:4001' . format ( peer ) peerdata = requests . get ( url = formatted_peer + '/api/peers/' ) . json ( ) [ 'peers' ] peers_status = { } networkheight = max ( [ x [ 'height' ] for x in peerdata ] ) for i in peerdata : if 'http://{}:4001' . format ( i [ 'ip' ] ) in self . PEERS : peers_status . update ( { i [ 'ip' ] : { 'height' : i [ 'height' ] , 'status' : i [ 'status' ] , 'version' : i [ 'version' ] , 'delay' : i [ 'delay' ] , } } ) return { 'network_height' : networkheight , 'peer_status' : peers_status }
5387	def _convert_suffix_to_docker_chars ( suffix ) : accepted_characters = string . ascii_letters + string . digits + '_.-' def label_char_transform ( char ) : if char in accepted_characters : return char return '-' return '' . join ( label_char_transform ( c ) for c in suffix )
13687	def assert_equal_files ( self , obtained_fn , expected_fn , fix_callback = lambda x : x , binary = False , encoding = None ) : import os from zerotk . easyfs import GetFileContents , GetFileLines __tracebackhide__ = True import io def FindFile ( filename ) : data_filename = self . get_filename ( filename ) if os . path . isfile ( data_filename ) : return data_filename if os . path . isfile ( filename ) : return filename from . _exceptions import MultipleFilesNotFound raise MultipleFilesNotFound ( [ filename , data_filename ] ) obtained_fn = FindFile ( obtained_fn ) expected_fn = FindFile ( expected_fn ) if binary : obtained_lines = GetFileContents ( obtained_fn , binary = True ) expected_lines = GetFileContents ( expected_fn , binary = True ) assert obtained_lines == expected_lines else : obtained_lines = fix_callback ( GetFileLines ( obtained_fn , encoding = encoding ) ) expected_lines = GetFileLines ( expected_fn , encoding = encoding ) if obtained_lines != expected_lines : html_fn = os . path . splitext ( obtained_fn ) [ 0 ] + '.diff.html' html_diff = self . _generate_html_diff ( expected_fn , expected_lines , obtained_fn , obtained_lines ) with io . open ( html_fn , 'w' ) as f : f . write ( html_diff ) import difflib diff = [ 'FILES DIFFER:' , obtained_fn , expected_fn ] diff += [ 'HTML DIFF: %s' % html_fn ] diff += difflib . context_diff ( obtained_lines , expected_lines ) raise AssertionError ( '\n' . join ( diff ) + '\n' )
11719	def pipelines ( self ) : if not self . response : return set ( ) elif self . _pipelines is None and self . response : self . _pipelines = set ( ) for group in self . response . payload : for pipeline in group [ 'pipelines' ] : self . _pipelines . add ( pipeline [ 'name' ] ) return self . _pipelines
4314	def validate_input_file ( input_filepath ) : if not os . path . exists ( input_filepath ) : raise IOError ( "input_filepath {} does not exist." . format ( input_filepath ) ) ext = file_extension ( input_filepath ) if ext not in VALID_FORMATS : logger . info ( "Valid formats: %s" , " " . join ( VALID_FORMATS ) ) logger . warning ( "This install of SoX cannot process .{} files." . format ( ext ) )
8551	def delete_image ( self , image_id ) : response = self . _perform_request ( url = '/images/' + image_id , method = 'DELETE' ) return response
8355	def _subMSChar ( self , orig ) : sub = self . MS_CHARS . get ( orig ) if type ( sub ) == types . TupleType : if self . smartQuotesTo == 'xml' : sub = '&#x%s;' % sub [ 1 ] else : sub = '&%s;' % sub [ 0 ] return sub
6662	def generate_csr ( self , domain = '' , r = None ) : r = r or self . local_renderer r . env . domain = domain or r . env . domain role = self . genv . ROLE or ALL site = self . genv . SITE or self . genv . default_site print ( 'self.genv.default_site:' , self . genv . default_site , file = sys . stderr ) print ( 'site.csr0:' , site , file = sys . stderr ) ssl_dst = 'roles/%s/ssl' % ( role , ) print ( 'ssl_dst:' , ssl_dst ) if not os . path . isdir ( ssl_dst ) : os . makedirs ( ssl_dst ) for site , site_data in self . iter_sites ( ) : print ( 'site.csr1:' , site , file = sys . stderr ) assert r . env . domain , 'No SSL domain defined.' r . env . ssl_base_dst = '%s/%s' % ( ssl_dst , r . env . domain . replace ( '*.' , '' ) ) r . env . ssl_csr_year = date . today ( ) . year r . local ( 'openssl req -nodes -newkey rsa:{ssl_length} ' '-subj "/C={ssl_country}/ST={ssl_state}/L={ssl_city}/O={ssl_organization}/CN={ssl_domain}" ' '-keyout {ssl_base_dst}.{ssl_csr_year}.key -out {ssl_base_dst}.{ssl_csr_year}.csr' )
13527	def clean ( options , info ) : info ( "Cleaning patterns %s" , options . paved . clean . patterns ) for wd in options . paved . clean . dirs : info ( "Cleaning in %s" , wd ) for p in options . paved . clean . patterns : for f in wd . walkfiles ( p ) : f . remove ( )
7691	def validate ( schema_file = None , jams_files = None ) : schema = load_json ( schema_file ) for jams_file in jams_files : try : jams = load_json ( jams_file ) jsonschema . validate ( jams , schema ) print '{:s} was successfully validated' . format ( jams_file ) except jsonschema . ValidationError as exc : print '{:s} was NOT successfully validated' . format ( jams_file ) print exc
9307	def get_canonical_request ( self , req , cano_headers , signed_headers ) : url = urlparse ( req . url ) path = self . amz_cano_path ( url . path ) split = req . url . split ( '?' , 1 ) qs = split [ 1 ] if len ( split ) == 2 else '' qs = self . amz_cano_querystring ( qs ) payload_hash = req . headers [ 'x-amz-content-sha256' ] req_parts = [ req . method . upper ( ) , path , qs , cano_headers , signed_headers , payload_hash ] cano_req = '\n' . join ( req_parts ) return cano_req
11576	def sonar_data ( self , data ) : val = int ( ( data [ self . MSB ] << 7 ) + data [ self . LSB ] ) pin_number = data [ 0 ] with self . pymata . data_lock : sonar_pin_entry = self . active_sonar_map [ pin_number ] self . digital_response_table [ data [ self . RESPONSE_TABLE_MODE ] ] [ self . RESPONSE_TABLE_PIN_DATA_VALUE ] = val if sonar_pin_entry [ 0 ] is not None : if sonar_pin_entry [ 1 ] != val : self . active_sonar_map [ pin_number ] [ 0 ] ( [ self . pymata . SONAR , pin_number , val ] ) sonar_pin_entry [ 1 ] = val self . active_sonar_map [ pin_number ] = sonar_pin_entry
12818	def _build_chunk_headers ( self ) : if hasattr ( self , "_chunk_headers" ) and self . _chunk_headers : return self . _chunk_headers = { } for field in self . _files : self . _chunk_headers [ field ] = self . _headers ( field , True ) for field in self . _data : self . _chunk_headers [ field ] = self . _headers ( field )
8424	def husl_palette ( n_colors = 6 , h = .01 , s = .9 , l = .65 ) : hues = np . linspace ( 0 , 1 , n_colors + 1 ) [ : - 1 ] hues += h hues %= 1 hues *= 359 s *= 99 l *= 99 palette = [ husl . husl_to_rgb ( h_i , s , l ) for h_i in hues ] return palette
5277	def query ( self , i , j ) : "Query the oracle to find out whether i and j should be must-linked" if self . queries_cnt < self . max_queries_cnt : self . queries_cnt += 1 return self . labels [ i ] == self . labels [ j ] else : raise MaximumQueriesExceeded
12034	def kernel_gaussian ( self , sizeMS , sigmaMS = None , forwardOnly = False ) : sigmaMS = sizeMS / 10 if sigmaMS is None else sigmaMS size , sigma = sizeMS * self . pointsPerMs , sigmaMS * self . pointsPerMs self . kernel = swhlab . common . kernel_gaussian ( size , sigma , forwardOnly ) return self . kernel
2037	def JUMPI ( self , dest , cond ) : self . pc = Operators . ITEBV ( 256 , cond != 0 , dest , self . pc + self . instruction . size ) self . _set_check_jmpdest ( cond != 0 )
468	def sample_top ( a = None , top_k = 10 ) : if a is None : a = [ ] idx = np . argpartition ( a , - top_k ) [ - top_k : ] probs = a [ idx ] probs = probs / np . sum ( probs ) choice = np . random . choice ( idx , p = probs ) return choice
5098	def graph2dict ( g , return_dict_of_dict = True ) : if not isinstance ( g , nx . DiGraph ) : g = QueueNetworkDiGraph ( g ) dict_of_dicts = nx . to_dict_of_dicts ( g ) if return_dict_of_dict : return dict_of_dicts else : return { k : list ( val . keys ( ) ) for k , val in dict_of_dicts . items ( ) }
5810	def _parse_hello_extensions ( data ) : if data == b'' : return extentions_length = int_from_bytes ( data [ 0 : 2 ] ) extensions_start = 2 extensions_end = 2 + extentions_length pointer = extensions_start while pointer < extensions_end : extension_type = int_from_bytes ( data [ pointer : pointer + 2 ] ) extension_length = int_from_bytes ( data [ pointer + 2 : pointer + 4 ] ) yield ( extension_type , data [ pointer + 4 : pointer + 4 + extension_length ] ) pointer += 4 + extension_length
983	def mmGetMetricFromTrace ( self , trace ) : return Metric . createFromTrace ( trace . makeCountsTrace ( ) , excludeResets = self . mmGetTraceResets ( ) )
4837	def get_catalog_courses ( self , catalog_id ) : return self . _load_data ( self . CATALOGS_COURSES_ENDPOINT . format ( catalog_id ) , default = [ ] )
10311	def prepare_c3 ( data : Union [ List [ Tuple [ str , int ] ] , Mapping [ str , int ] ] , y_axis_label : str = 'y' , x_axis_label : str = 'x' , ) -> str : if not isinstance ( data , list ) : data = sorted ( data . items ( ) , key = itemgetter ( 1 ) , reverse = True ) try : labels , values = zip ( * data ) except ValueError : log . info ( f'no values found for {x_axis_label}, {y_axis_label}' ) labels , values = [ ] , [ ] return json . dumps ( [ [ x_axis_label ] + list ( labels ) , [ y_axis_label ] + list ( values ) , ] )
7485	def concat_reads ( data , subsamples , ipyclient ) : if any ( [ len ( i . files . fastqs ) > 1 for i in subsamples ] ) : start = time . time ( ) printstr = " concatenating inputs | {} | s2 |" finished = 0 catjobs = { } for sample in subsamples : if len ( sample . files . fastqs ) > 1 : catjobs [ sample . name ] = ipyclient [ 0 ] . apply ( concat_multiple_inputs , * ( data , sample ) ) else : sample . files . concat = sample . files . fastqs while 1 : finished = sum ( [ i . ready ( ) for i in catjobs . values ( ) ] ) elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( len ( catjobs ) , finished , printstr . format ( elapsed ) , spacer = data . _spacer ) time . sleep ( 0.1 ) if finished == len ( catjobs ) : print ( "" ) break for async in catjobs : if catjobs [ async ] . successful ( ) : data . samples [ async ] . files . concat = catjobs [ async ] . result ( ) else : error = catjobs [ async ] . result ( ) LOGGER . error ( "error in step2 concat %s" , error ) raise IPyradWarningExit ( "error in step2 concat: {}" . format ( error ) ) else : for sample in subsamples : sample . files . concat = sample . files . fastqs return subsamples
7459	def paramname ( param = "" ) : try : name = pinfo [ str ( param ) ] [ 0 ] . strip ( ) . split ( " " ) [ 1 ] except ( KeyError , ValueError ) as err : print ( "\tKey name/number not recognized - " . format ( param ) , err ) raise return name
2933	def write_meta_data ( self ) : config = configparser . ConfigParser ( ) config . add_section ( 'MetaData' ) config . set ( 'MetaData' , 'entry_point_process' , self . wf_spec . name ) if self . editor : config . set ( 'MetaData' , 'editor' , self . editor ) for k , v in self . meta_data : config . set ( 'MetaData' , k , v ) if not self . PARSER_CLASS == BpmnParser : config . set ( 'MetaData' , 'parser_class_module' , inspect . getmodule ( self . PARSER_CLASS ) . __name__ ) config . set ( 'MetaData' , 'parser_class' , self . PARSER_CLASS . __name__ ) ini = StringIO ( ) config . write ( ini ) self . write_to_package_zip ( self . METADATA_FILE , ini . getvalue ( ) )
9629	def split_docstring ( value ) : docstring = textwrap . dedent ( getattr ( value , '__doc__' , '' ) ) if not docstring : return None pieces = docstring . strip ( ) . split ( '\n\n' , 1 ) try : body = pieces [ 1 ] except IndexError : body = None return Docstring ( pieces [ 0 ] , body )
8074	def image ( self , path , x , y , width = None , height = None , alpha = 1.0 , data = None , draw = True , ** kwargs ) : return self . Image ( path , x , y , width , height , alpha , data , ** kwargs )
4021	def _start_docker_vm ( ) : is_running = docker_vm_is_running ( ) if not is_running : log_to_client ( 'Starting docker-machine VM {}' . format ( constants . VM_MACHINE_NAME ) ) _apply_nat_dns_host_resolver ( ) _apply_nat_net_less_greedy_subnet ( ) check_and_log_output_and_error_demoted ( [ 'docker-machine' , 'start' , constants . VM_MACHINE_NAME ] , quiet_on_success = True ) return is_running
11718	def delete ( self ) : headers = self . _default_headers ( ) return self . _request ( self . name , ok_status = None , data = None , headers = headers , method = "DELETE" )
1096	def free_temp ( self , v ) : self . used_temps . remove ( v ) self . free_temps . add ( v )
3565	def write_value ( self , value , write_type = 0 ) : data = NSData . dataWithBytes_length_ ( value , len ( value ) ) self . _device . _peripheral . writeValue_forCharacteristic_type_ ( data , self . _characteristic , write_type )
9573	def unpack ( endian , fmt , data ) : if fmt == 's' : val = struct . unpack ( '' . join ( [ endian , str ( len ( data ) ) , 's' ] ) , data ) [ 0 ] else : num = len ( data ) // struct . calcsize ( fmt ) val = struct . unpack ( '' . join ( [ endian , str ( num ) , fmt ] ) , data ) if len ( val ) == 1 : val = val [ 0 ] return val
8100	def apply ( self ) : sorted = self . order + self . keys ( ) unique = [ ] [ unique . append ( x ) for x in sorted if x not in unique ] for node in self . graph . nodes : for s in unique : if self . has_key ( s ) and self [ s ] ( self . graph , node ) : node . style = s
2427	def set_doc_comment ( self , doc , comment ) : if not self . doc_comment_set : self . doc_comment_set = True if validations . validate_doc_comment ( comment ) : doc . comment = str_from_text ( comment ) return True else : raise SPDXValueError ( 'Document::Comment' ) else : raise CardinalityError ( 'Document::Comment' )
4977	def course_or_program_exist ( self , course_id , program_uuid ) : course_exists = course_id and CourseApiClient ( ) . get_course_details ( course_id ) program_exists = program_uuid and CourseCatalogApiServiceClient ( ) . program_exists ( program_uuid ) return course_exists or program_exists
13277	def update_desc_lcin_path ( desc , pdesc_level ) : parent_breadth = desc [ 'parent_breadth_path' ] [ - 1 ] if ( desc [ 'sib_seq' ] == 0 ) : if ( parent_breadth == 0 ) : pass else : parent_lsib_breadth = parent_breadth - 1 plsib_desc = pdesc_level [ parent_lsib_breadth ] if ( plsib_desc [ 'leaf' ] ) : pass else : lcin_path = copy . deepcopy ( plsib_desc [ 'path' ] ) lcin_path . append ( plsib_desc [ 'sons_count' ] - 1 ) desc [ 'lcin_path' ] = lcin_path else : pass return ( desc )
804	def modelAdoptNextOrphan ( self , jobId , maxUpdateInterval ) : @ g_retrySQL def findCandidateModelWithRetries ( ) : modelID = None with ConnectionFactory . get ( ) as conn : query = 'SELECT model_id FROM %s ' ' WHERE status=%%s ' ' AND job_id=%%s ' ' AND TIMESTAMPDIFF(SECOND, ' ' _eng_last_update_time, ' ' UTC_TIMESTAMP()) > %%s ' ' LIMIT 1 ' % ( self . modelsTableName , ) sqlParams = [ self . STATUS_RUNNING , jobId , maxUpdateInterval ] numRows = conn . cursor . execute ( query , sqlParams ) rows = conn . cursor . fetchall ( ) assert numRows <= 1 , "Unexpected numRows: %r" % numRows if numRows == 1 : ( modelID , ) = rows [ 0 ] return modelID @ g_retrySQL def adoptModelWithRetries ( modelID ) : adopted = False with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET _eng_worker_conn_id=%%s, ' ' _eng_last_update_time=UTC_TIMESTAMP() ' ' WHERE model_id=%%s ' ' AND status=%%s' ' AND TIMESTAMPDIFF(SECOND, ' ' _eng_last_update_time, ' ' UTC_TIMESTAMP()) > %%s ' ' LIMIT 1 ' % ( self . modelsTableName , ) sqlParams = [ self . _connectionID , modelID , self . STATUS_RUNNING , maxUpdateInterval ] numRowsAffected = conn . cursor . execute ( query , sqlParams ) assert numRowsAffected <= 1 , 'Unexpected numRowsAffected=%r' % ( numRowsAffected , ) if numRowsAffected == 1 : adopted = True else : ( status , connectionID ) = self . _getOneMatchingRowNoRetries ( self . _models , conn , { 'model_id' : modelID } , [ 'status' , '_eng_worker_conn_id' ] ) adopted = ( status == self . STATUS_RUNNING and connectionID == self . _connectionID ) return adopted adoptedModelID = None while True : modelID = findCandidateModelWithRetries ( ) if modelID is None : break if adoptModelWithRetries ( modelID ) : adoptedModelID = modelID break return adoptedModelID
1669	def FlagCxx14Features ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] include = Match ( r'\s*#\s*include\s+[<"]([^<"]+)[">]' , line ) if include and include . group ( 1 ) in ( 'scoped_allocator' , 'shared_mutex' ) : error ( filename , linenum , 'build/c++14' , 5 , ( '<%s> is an unapproved C++14 header.' ) % include . group ( 1 ) )
10690	def render ( self , format = ReportFormat . printout ) : table = self . _generate_table_ ( ) if format == ReportFormat . printout : print ( tabulate ( table , headers = "firstrow" , tablefmt = "simple" ) ) elif format == ReportFormat . latex : self . _render_latex_ ( table ) elif format == ReportFormat . txt : self . _render_txt_ ( table ) elif format == ReportFormat . csv : self . _render_csv_ ( table ) elif format == ReportFormat . string : return str ( tabulate ( table , headers = "firstrow" , tablefmt = "simple" ) ) elif format == ReportFormat . matplotlib : self . _render_matplotlib_ ( ) elif format == ReportFormat . png : if self . output_path is None : self . _render_matplotlib_ ( ) else : self . _render_matplotlib_ ( True )
1496	def get_sub_parts ( self , query ) : parts = [ ] num_open_braces = 0 delimiter = ',' last_starting_index = 0 for i in range ( len ( query ) ) : if query [ i ] == '(' : num_open_braces += 1 elif query [ i ] == ')' : num_open_braces -= 1 elif query [ i ] == delimiter and num_open_braces == 0 : parts . append ( query [ last_starting_index : i ] . strip ( ) ) last_starting_index = i + 1 parts . append ( query [ last_starting_index : ] . strip ( ) ) return parts
6980	def _epd_function ( coeffs , fluxes , xcc , ycc , bgv , bge ) : epdf = ( coeffs [ 0 ] + coeffs [ 1 ] * npsin ( 2 * MPI * xcc ) + coeffs [ 2 ] * npcos ( 2 * MPI * xcc ) + coeffs [ 3 ] * npsin ( 2 * MPI * ycc ) + coeffs [ 4 ] * npcos ( 2 * MPI * ycc ) + coeffs [ 5 ] * npsin ( 4 * MPI * xcc ) + coeffs [ 6 ] * npcos ( 4 * MPI * xcc ) + coeffs [ 7 ] * npsin ( 4 * MPI * ycc ) + coeffs [ 8 ] * npcos ( 4 * MPI * ycc ) + coeffs [ 9 ] * bgv + coeffs [ 10 ] * bge ) return epdf
1488	def save_module ( self , obj ) : self . modules . add ( obj ) self . save_reduce ( subimport , ( obj . __name__ , ) , obj = obj )
1649	def GetLineWidth ( line ) : if isinstance ( line , unicode ) : width = 0 for uc in unicodedata . normalize ( 'NFC' , line ) : if unicodedata . east_asian_width ( uc ) in ( 'W' , 'F' ) : width += 2 elif not unicodedata . combining ( uc ) : width += 1 return width else : return len ( line )
11359	def add_nations_field ( authors_subfields ) : from . config import NATIONS_DEFAULT_MAP result = [ ] for field in authors_subfields : if field [ 0 ] == 'v' : values = [ x . replace ( '.' , '' ) for x in field [ 1 ] . split ( ', ' ) ] possible_affs = filter ( lambda x : x is not None , map ( NATIONS_DEFAULT_MAP . get , values ) ) if 'CERN' in possible_affs and 'Switzerland' in possible_affs : possible_affs = [ x for x in possible_affs if x != 'Switzerland' ] result . extend ( possible_affs ) result = sorted ( list ( set ( result ) ) ) if result : authors_subfields . extend ( [ ( 'w' , res ) for res in result ] ) else : authors_subfields . append ( ( 'w' , 'HUMAN CHECK' ) )
11860	def all_events ( vars , bn , e ) : "Yield every way of extending e with values for all vars." if not vars : yield e else : X , rest = vars [ 0 ] , vars [ 1 : ] for e1 in all_events ( rest , bn , e ) : for x in bn . variable_values ( X ) : yield extend ( e1 , X , x )
12122	def filter_gaussian ( self , sigmaMs = 100 , applyFiltered = False , applyBaseline = False ) : if sigmaMs == 0 : return self . dataY filtered = cm . filter_gaussian ( self . dataY , sigmaMs ) if applyBaseline : self . dataY = self . dataY - filtered elif applyFiltered : self . dataY = filtered else : return filtered
1612	def ProcessGlobalSuppresions ( lines ) : for line in lines : if _SEARCH_C_FILE . search ( line ) : for category in _DEFAULT_C_SUPPRESSED_CATEGORIES : _global_error_suppressions [ category ] = True if _SEARCH_KERNEL_FILE . search ( line ) : for category in _DEFAULT_KERNEL_SUPPRESSED_CATEGORIES : _global_error_suppressions [ category ] = True
7808	def from_ssl_socket ( cls , ssl_socket ) : cert = cls ( ) try : data = ssl_socket . getpeercert ( ) except AttributeError : return cert logger . debug ( "Certificate data from ssl module: {0!r}" . format ( data ) ) if not data : return cert cert . validated = True cert . subject_name = data . get ( 'subject' ) cert . alt_names = defaultdict ( list ) if 'subjectAltName' in data : for name , value in data [ 'subjectAltName' ] : cert . alt_names [ name ] . append ( value ) if 'notAfter' in data : tstamp = ssl . cert_time_to_seconds ( data [ 'notAfter' ] ) cert . not_after = datetime . utcfromtimestamp ( tstamp ) if sys . version_info . major < 3 : cert . _decode_names ( ) cert . common_names = [ ] if cert . subject_name : for part in cert . subject_name : for name , value in part : if name == 'commonName' : cert . common_names . append ( value ) return cert
12311	def record ( self , localStreamName , pathToFile , ** kwargs ) : return self . protocol . execute ( 'record' , localStreamName = localStreamName , pathToFile = pathToFile , ** kwargs )
12438	def deserialize ( self , request = None , text = None , format = None ) : if isinstance ( self , Resource ) : if not request : request = self . _request Deserializer = None if format : Deserializer = self . meta . deserializers [ format ] if not Deserializer : media_ranges = request . get ( 'Content-Type' ) if media_ranges : media_types = six . iterkeys ( self . _deserializer_map ) media_type = mimeparse . best_match ( media_types , media_ranges ) if media_type : format = self . _deserializer_map [ media_type ] Deserializer = self . meta . deserializers [ format ] else : pass if Deserializer : try : deserializer = Deserializer ( ) data = deserializer . deserialize ( request = request , text = text ) return data , deserializer except ValueError : pass raise http . exceptions . UnsupportedMediaType ( )
6286	def supports_file ( cls , meta ) : path = Path ( meta . path ) for ext in cls . file_extensions : if path . suffixes [ : len ( ext ) ] == ext : return True return False
5350	def compose_projects_json ( projects , data ) : projects = compose_git ( projects , data ) projects = compose_mailing_lists ( projects , data ) projects = compose_bugzilla ( projects , data ) projects = compose_github ( projects , data ) projects = compose_gerrit ( projects ) projects = compose_mbox ( projects ) return projects
6788	def push ( self , components = None , yes = 0 ) : from burlap import notifier service = self . get_satchel ( 'service' ) self . lock ( ) try : yes = int ( yes ) if not yes : if self . genv . host_string == self . genv . hosts [ 0 ] : execute ( partial ( self . preview , components = components , ask = 1 ) ) notifier . notify_pre_deployment ( ) component_order , plan_funcs = self . get_component_funcs ( components = components ) service . pre_deploy ( ) for func_name , plan_func in plan_funcs : print ( 'Executing %s...' % func_name ) plan_func ( ) self . fake ( components = components ) service . post_deploy ( ) notifier . notify_post_deployment ( ) finally : self . unlock ( )
7928	def stop ( self ) : with self . lock : for dummy in self . threads : self . queue . put ( None )
7589	def call_fastq_dump_on_SRRs ( self , srr , outname , paired ) : fd_cmd = [ "fastq-dump" , srr , "--accession" , outname , "--outdir" , self . workdir , "--gzip" , ] if paired : fd_cmd += [ "--split-files" ] proc = sps . Popen ( fd_cmd , stderr = sps . STDOUT , stdout = sps . PIPE ) o , e = proc . communicate ( ) srafile = os . path . join ( self . workdir , "sra" , srr + ".sra" ) if os . path . exists ( srafile ) : os . remove ( srafile )
1677	def ResetSection ( self , directive ) : self . _section = self . _INITIAL_SECTION self . _last_header = '' if directive in ( 'if' , 'ifdef' , 'ifndef' ) : self . include_list . append ( [ ] ) elif directive in ( 'else' , 'elif' ) : self . include_list [ - 1 ] = [ ]
8421	def _format ( formatter , x ) : formatter . create_dummy_axis ( ) formatter . set_locs ( [ val for val in x if ~ np . isnan ( val ) ] ) try : oom = int ( formatter . orderOfMagnitude ) except AttributeError : oom = 0 labels = [ formatter ( tick ) for tick in x ] pattern = re . compile ( r'\.0+$' ) for i , label in enumerate ( labels ) : match = pattern . search ( label ) if match : labels [ i ] = pattern . sub ( '' , label ) if oom : labels = [ '{}e{}' . format ( s , oom ) if s != '0' else s for s in labels ] return labels
9384	def parse_xml_jtl ( self , granularity ) : data = defaultdict ( list ) processed_data = defaultdict ( lambda : defaultdict ( lambda : defaultdict ( list ) ) ) for input_file in self . infile_list : logger . info ( 'Processing : %s' , input_file ) timestamp_format = None tree = ElementTree . parse ( input_file ) samples = tree . findall ( './httpSample' ) + tree . findall ( './sample' ) for sample in samples : if not timestamp_format or timestamp_format == 'unknown' : timestamp_format = naarad . utils . detect_timestamp_format ( sample . get ( 'ts' ) ) if timestamp_format == 'unknown' : continue ts = naarad . utils . get_standardized_timestamp ( sample . get ( 'ts' ) , timestamp_format ) if ts == - 1 : continue ts = naarad . utils . reconcile_timezones ( ts , self . timezone , self . graph_timezone ) aggregate_timestamp , averaging_factor = self . get_aggregation_timestamp ( ts , granularity ) self . aggregate_count_over_time ( processed_data , sample , [ self . _sanitize_label ( sample . get ( 'lb' ) ) , 'Overall_Summary' ] , aggregate_timestamp ) self . aggregate_values_over_time ( processed_data , sample , [ self . _sanitize_label ( sample . get ( 'lb' ) ) , 'Overall_Summary' ] , [ 't' , 'by' ] , aggregate_timestamp ) logger . info ( 'Finished parsing : %s' , input_file ) logger . info ( 'Processing metrics for output to csv' ) self . average_values_for_plot ( processed_data , data , averaging_factor ) logger . info ( 'Writing time series csv' ) for csv in data . keys ( ) : self . csv_files . append ( csv ) with open ( csv , 'w' ) as csvf : csvf . write ( '\n' . join ( sorted ( data [ csv ] ) ) ) logger . info ( 'Processing raw data for stats' ) self . calculate_key_stats ( processed_data ) return True
5493	def validate_config_key ( ctx , param , value ) : if not value : return value try : section , item = value . split ( "." , 1 ) except ValueError : raise click . BadArgumentUsage ( "Given key does not contain a section name." ) else : return section , item
9603	def from_object ( cls , obj ) : return cls ( obj . get ( 'sessionId' , None ) , obj . get ( 'status' , 0 ) , obj . get ( 'value' , None ) )
3539	def hubspot ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return HubSpotNode ( )
10191	def get_anonymization_salt ( ts ) : salt_key = 'stats:salt:{}' . format ( ts . date ( ) . isoformat ( ) ) salt = current_cache . get ( salt_key ) if not salt : salt_bytes = os . urandom ( 32 ) salt = b64encode ( salt_bytes ) . decode ( 'utf-8' ) current_cache . set ( salt_key , salt , timeout = 60 * 60 * 24 ) return salt
5651	def _scan_footpaths_to_departure_stop ( self , connection_dep_stop , connection_dep_time , arrival_time_target ) : for _ , neighbor , data in self . _walk_network . edges_iter ( nbunch = [ connection_dep_stop ] , data = True ) : d_walk = data [ 'd_walk' ] neighbor_dep_time = connection_dep_time - d_walk / self . _walk_speed pt = LabelTimeSimple ( departure_time = neighbor_dep_time , arrival_time_target = arrival_time_target ) self . _stop_profiles [ neighbor ] . update_pareto_optimal_tuples ( pt )
10075	def merge_with_published ( self ) : pid , first = self . fetch_published ( ) lca = first . revisions [ self [ '_deposit' ] [ 'pid' ] [ 'revision_id' ] ] args = [ lca . dumps ( ) , first . dumps ( ) , self . dumps ( ) ] for arg in args : del arg [ '$schema' ] , arg [ '_deposit' ] args . append ( { } ) m = Merger ( * args ) try : m . run ( ) except UnresolvedConflictsException : raise MergeConflict ( ) return patch ( m . unified_patches , lca )
3895	def print_table ( col_tuple , row_tuples ) : col_widths = [ max ( len ( str ( row [ col ] ) ) for row in [ col_tuple ] + row_tuples ) for col in range ( len ( col_tuple ) ) ] format_str = ' ' . join ( '{{:<{}}}' . format ( col_width ) for col_width in col_widths ) header_border = ' ' . join ( '=' * col_width for col_width in col_widths ) print ( header_border ) print ( format_str . format ( * col_tuple ) ) print ( header_border ) for row_tuple in row_tuples : print ( format_str . format ( * row_tuple ) ) print ( header_border ) print ( )
260	def create_perf_attrib_stats ( perf_attrib , risk_exposures ) : summary = OrderedDict ( ) total_returns = perf_attrib [ 'total_returns' ] specific_returns = perf_attrib [ 'specific_returns' ] common_returns = perf_attrib [ 'common_returns' ] summary [ 'Annualized Specific Return' ] = ep . annual_return ( specific_returns ) summary [ 'Annualized Common Return' ] = ep . annual_return ( common_returns ) summary [ 'Annualized Total Return' ] = ep . annual_return ( total_returns ) summary [ 'Specific Sharpe Ratio' ] = ep . sharpe_ratio ( specific_returns ) summary [ 'Cumulative Specific Return' ] = ep . cum_returns_final ( specific_returns ) summary [ 'Cumulative Common Return' ] = ep . cum_returns_final ( common_returns ) summary [ 'Total Returns' ] = ep . cum_returns_final ( total_returns ) summary = pd . Series ( summary , name = '' ) annualized_returns_by_factor = [ ep . annual_return ( perf_attrib [ c ] ) for c in risk_exposures . columns ] cumulative_returns_by_factor = [ ep . cum_returns_final ( perf_attrib [ c ] ) for c in risk_exposures . columns ] risk_exposure_summary = pd . DataFrame ( data = OrderedDict ( [ ( 'Average Risk Factor Exposure' , risk_exposures . mean ( axis = 'rows' ) ) , ( 'Annualized Return' , annualized_returns_by_factor ) , ( 'Cumulative Return' , cumulative_returns_by_factor ) , ] ) , index = risk_exposures . columns , ) return summary , risk_exposure_summary
1128	def SeqN ( n , * inner_rules , ** kwargs ) : @ action ( Seq ( * inner_rules ) , loc = kwargs . get ( "loc" , None ) ) def rule ( parser , * values ) : return values [ n ] return rule
2604	def close ( self ) : if self . reuse : logger . debug ( "Ipcontroller not shutting down: reuse enabled" ) return if self . mode == "manual" : logger . debug ( "Ipcontroller not shutting down: Manual mode" ) return try : pgid = os . getpgid ( self . proc . pid ) os . killpg ( pgid , signal . SIGTERM ) time . sleep ( 0.2 ) os . killpg ( pgid , signal . SIGKILL ) try : self . proc . wait ( timeout = 1 ) x = self . proc . returncode if x == 0 : logger . debug ( "Controller exited with {0}" . format ( x ) ) else : logger . error ( "Controller exited with {0}. May require manual cleanup" . format ( x ) ) except subprocess . TimeoutExpired : logger . warn ( "Ipcontroller process:{0} cleanup failed. May require manual cleanup" . format ( self . proc . pid ) ) except Exception as e : logger . warn ( "Failed to kill the ipcontroller process[{0}]: {1}" . format ( self . proc . pid , e ) )
8829	def security_group_rule_update ( context , rule , ** kwargs ) : rule . update ( kwargs ) context . session . add ( rule ) return rule
13392	def paginate_update ( update ) : from happenings . models import Update time = update . pub_time event = update . event try : next = Update . objects . filter ( event = event , pub_time__gt = time ) . order_by ( 'pub_time' ) . only ( 'title' ) [ 0 ] except : next = None try : previous = Update . objects . filter ( event = event , pub_time__lt = time ) . order_by ( '-pub_time' ) . only ( 'title' ) [ 0 ] except : previous = None return { 'next' : next , 'previous' : previous , 'event' : event }
2086	def get ( self , pk ) : try : return next ( s for s in self . list ( ) [ 'results' ] if s [ 'id' ] == pk ) except StopIteration : raise exc . NotFound ( 'The requested object could not be found.' )
6721	def get_combined_requirements ( self , requirements = None ) : requirements = requirements or self . env . requirements def iter_lines ( fn ) : with open ( fn , 'r' ) as fin : for line in fin . readlines ( ) : line = line . strip ( ) if not line or line . startswith ( '#' ) : continue yield line content = [ ] if isinstance ( requirements , ( tuple , list ) ) : for f in requirements : f = self . find_template ( f ) content . extend ( list ( iter_lines ( f ) ) ) else : assert isinstance ( requirements , six . string_types ) f = self . find_template ( requirements ) content . extend ( list ( iter_lines ( f ) ) ) return '\n' . join ( content )
11768	def weighted_sampler ( seq , weights ) : "Return a random-sample function that picks from seq weighted by weights." totals = [ ] for w in weights : totals . append ( w + totals [ - 1 ] if totals else w ) return lambda : seq [ bisect . bisect ( totals , random . uniform ( 0 , totals [ - 1 ] ) ) ]
5829	def check_for_rate_limiting ( response , response_lambda , timeout = 1 , attempts = 0 ) : if attempts >= 3 : raise RateLimitingException ( ) if response . status_code == 429 : sleep ( timeout ) new_timeout = timeout + 1 new_attempts = attempts + 1 return check_for_rate_limiting ( response_lambda ( timeout , attempts ) , response_lambda , timeout = new_timeout , attempts = new_attempts ) return response
7596	def search_tournaments ( self , ** params : keys ) : url = self . api . TOURNAMENT + '/search' return self . _get_model ( url , PartialClan , ** params )
7674	def slice ( self , start_time , end_time , strict = False ) : if self . file_metadata . duration is None : raise JamsError ( 'Duration must be set (jam.file_metadata.duration) before ' 'slicing can be performed.' ) if ( start_time < 0 or start_time > float ( self . file_metadata . duration ) or end_time < start_time or end_time > float ( self . file_metadata . duration ) ) : raise ParameterError ( 'start_time and end_time must be within the original file ' 'duration ({:f}) and end_time cannot be smaller than ' 'start_time.' . format ( float ( self . file_metadata . duration ) ) ) jam_sliced = JAMS ( annotations = None , file_metadata = self . file_metadata , sandbox = self . sandbox ) jam_sliced . annotations = self . annotations . slice ( start_time , end_time , strict = strict ) jam_sliced . file_metadata . duration = end_time - start_time if 'slice' not in jam_sliced . sandbox . keys ( ) : jam_sliced . sandbox . update ( slice = [ { 'start_time' : start_time , 'end_time' : end_time } ] ) else : jam_sliced . sandbox . slice . append ( { 'start_time' : start_time , 'end_time' : end_time } ) return jam_sliced
2994	def _getJson ( url , token = '' , version = '' ) : if token : return _getJsonIEXCloud ( url , token , version ) return _getJsonOrig ( url )
7206	def generate_workflow_description ( self ) : if not self . tasks : raise WorkflowError ( 'Workflow contains no tasks, and cannot be executed.' ) self . definition = self . workflow_skeleton ( ) if self . batch_values : self . definition [ "batch_values" ] = self . batch_values all_input_port_values = [ t . inputs . __getattribute__ ( input_port_name ) . value for t in self . tasks for input_port_name in t . inputs . _portnames ] for task in self . tasks : output_multiplex_ports_to_exclude = [ ] multiplex_output_port_names = [ portname for portname in task . outputs . _portnames if task . outputs . __getattribute__ ( portname ) . is_multiplex ] for p in multiplex_output_port_names : output_port_reference = 'source:' + task . name + ':' + p if output_port_reference not in all_input_port_values : output_multiplex_ports_to_exclude . append ( p ) task_def = task . generate_task_workflow_json ( output_multiplex_ports_to_exclude = output_multiplex_ports_to_exclude ) self . definition [ 'tasks' ] . append ( task_def ) if self . callback : self . definition [ 'callback' ] = self . callback return self . definition
7170	def train ( self , debug = True , force = False , single_thread = False , timeout = 20 ) : if not self . must_train and not force : return self . padaos . compile ( ) self . train_thread = Thread ( target = self . _train , kwargs = dict ( debug = debug , single_thread = single_thread , timeout = timeout ) , daemon = True ) self . train_thread . start ( ) self . train_thread . join ( timeout ) self . must_train = False return not self . train_thread . is_alive ( )
4846	def get_content_metadata ( self , enterprise_customer ) : content_metadata = OrderedDict ( ) if enterprise_customer . catalog : response = self . _load_data ( self . ENTERPRISE_CUSTOMER_ENDPOINT , detail_resource = 'courses' , resource_id = str ( enterprise_customer . uuid ) , traverse_pagination = True , ) for course in response [ 'results' ] : for course_run in course [ 'course_runs' ] : course_run [ 'content_type' ] = 'courserun' content_metadata [ course_run [ 'key' ] ] = course_run for enterprise_customer_catalog in enterprise_customer . enterprise_customer_catalogs . all ( ) : response = self . _load_data ( self . ENTERPRISE_CUSTOMER_CATALOGS_ENDPOINT , resource_id = str ( enterprise_customer_catalog . uuid ) , traverse_pagination = True , querystring = { 'page_size' : 1000 } , ) for item in response [ 'results' ] : content_id = utils . get_content_metadata_item_id ( item ) content_metadata [ content_id ] = item return content_metadata . values ( )
7494	def count_snps ( mat ) : snps = np . zeros ( 4 , dtype = np . uint32 ) snps [ 0 ] = np . uint32 ( mat [ 0 , 5 ] + mat [ 0 , 10 ] + mat [ 0 , 15 ] + mat [ 5 , 0 ] + mat [ 5 , 10 ] + mat [ 5 , 15 ] + mat [ 10 , 0 ] + mat [ 10 , 5 ] + mat [ 10 , 15 ] + mat [ 15 , 0 ] + mat [ 15 , 5 ] + mat [ 15 , 10 ] ) for i in range ( 16 ) : if i % 5 : snps [ 1 ] += mat [ i , i ] snps [ 2 ] = mat [ 1 , 4 ] + mat [ 2 , 8 ] + mat [ 3 , 12 ] + mat [ 4 , 1 ] + mat [ 6 , 9 ] + mat [ 7 , 13 ] + mat [ 8 , 2 ] + mat [ 9 , 6 ] + mat [ 11 , 14 ] + mat [ 12 , 3 ] + mat [ 13 , 7 ] + mat [ 14 , 11 ] snps [ 3 ] = ( mat . sum ( ) - np . diag ( mat ) . sum ( ) ) - snps [ 2 ] return snps
1791	def IMUL ( cpu , * operands ) : dest = operands [ 0 ] OperandSize = dest . size reg_name_h = { 8 : 'AH' , 16 : 'DX' , 32 : 'EDX' , 64 : 'RDX' } [ OperandSize ] reg_name_l = { 8 : 'AL' , 16 : 'AX' , 32 : 'EAX' , 64 : 'RAX' } [ OperandSize ] arg0 = dest . read ( ) arg1 = None arg2 = None res = None if len ( operands ) == 1 : arg1 = cpu . read_register ( reg_name_l ) temp = ( Operators . SEXTEND ( arg0 , OperandSize , OperandSize * 2 ) * Operators . SEXTEND ( arg1 , OperandSize , OperandSize * 2 ) ) temp = temp & ( ( 1 << ( OperandSize * 2 ) ) - 1 ) cpu . write_register ( reg_name_l , Operators . EXTRACT ( temp , 0 , OperandSize ) ) cpu . write_register ( reg_name_h , Operators . EXTRACT ( temp , OperandSize , OperandSize ) ) res = Operators . EXTRACT ( temp , 0 , OperandSize ) elif len ( operands ) == 2 : arg1 = operands [ 1 ] . read ( ) arg1 = Operators . SEXTEND ( arg1 , OperandSize , OperandSize * 2 ) temp = Operators . SEXTEND ( arg0 , OperandSize , OperandSize * 2 ) * arg1 temp = temp & ( ( 1 << ( OperandSize * 2 ) ) - 1 ) res = dest . write ( Operators . EXTRACT ( temp , 0 , OperandSize ) ) else : arg1 = operands [ 1 ] . read ( ) arg2 = operands [ 2 ] . read ( ) temp = ( Operators . SEXTEND ( arg1 , OperandSize , OperandSize * 2 ) * Operators . SEXTEND ( arg2 , operands [ 2 ] . size , OperandSize * 2 ) ) temp = temp & ( ( 1 << ( OperandSize * 2 ) ) - 1 ) res = dest . write ( Operators . EXTRACT ( temp , 0 , OperandSize ) ) cpu . CF = ( Operators . SEXTEND ( res , OperandSize , OperandSize * 2 ) != temp ) cpu . OF = cpu . CF
5388	def _task_sort_function ( task ) : return ( task . get_field ( 'create-time' ) , int ( task . get_field ( 'task-id' , 0 ) ) , int ( task . get_field ( 'task-attempt' , 0 ) ) )
1998	def visit ( self , node , use_fixed_point = False ) : cache = self . _cache visited = set ( ) stack = [ ] stack . append ( node ) while stack : node = stack . pop ( ) if node in cache : self . push ( cache [ node ] ) elif isinstance ( node , Operation ) : if node in visited : operands = [ self . pop ( ) for _ in range ( len ( node . operands ) ) ] value = self . _method ( node , * operands ) visited . remove ( node ) self . push ( value ) cache [ node ] = value else : visited . add ( node ) stack . append ( node ) stack . extend ( node . operands ) else : self . push ( self . _method ( node ) ) if use_fixed_point : old_value = None new_value = self . pop ( ) while old_value is not new_value : self . visit ( new_value ) old_value = new_value new_value = self . pop ( ) self . push ( new_value )
617	def parseTimestamp ( s ) : s = s . strip ( ) for pattern in DATETIME_FORMATS : try : return datetime . datetime . strptime ( s , pattern ) except ValueError : pass raise ValueError ( 'The provided timestamp %s is malformed. The supported ' 'formats are: [%s]' % ( s , ', ' . join ( DATETIME_FORMATS ) ) )
882	def activateCells ( self , activeColumns , learn = True ) : prevActiveCells = self . activeCells prevWinnerCells = self . winnerCells self . activeCells = [ ] self . winnerCells = [ ] segToCol = lambda segment : int ( segment . cell / self . cellsPerColumn ) identity = lambda x : x for columnData in groupby2 ( activeColumns , identity , self . activeSegments , segToCol , self . matchingSegments , segToCol ) : ( column , activeColumns , columnActiveSegments , columnMatchingSegments ) = columnData if activeColumns is not None : if columnActiveSegments is not None : cellsToAdd = self . activatePredictedColumn ( column , columnActiveSegments , columnMatchingSegments , prevActiveCells , prevWinnerCells , learn ) self . activeCells += cellsToAdd self . winnerCells += cellsToAdd else : ( cellsToAdd , winnerCell ) = self . burstColumn ( column , columnMatchingSegments , prevActiveCells , prevWinnerCells , learn ) self . activeCells += cellsToAdd self . winnerCells . append ( winnerCell ) else : if learn : self . punishPredictedColumn ( column , columnActiveSegments , columnMatchingSegments , prevActiveCells , prevWinnerCells )
8676	def migrate_stash ( source_stash_path , source_passphrase , source_backend , destination_stash_path , destination_passphrase , destination_backend ) : click . echo ( 'Migrating all keys from {0} to {1}...' . format ( source_stash_path , destination_stash_path ) ) try : migrate ( src_path = source_stash_path , src_passphrase = source_passphrase , src_backend = source_backend , dst_path = destination_stash_path , dst_passphrase = destination_passphrase , dst_backend = destination_backend ) except GhostError as ex : sys . exit ( ex ) click . echo ( 'Migration complete!' )
8067	def loadGrammar ( self , grammar , searchpaths = None ) : self . grammar = self . _load ( grammar , searchpaths = searchpaths ) self . refs = { } for ref in self . grammar . getElementsByTagName ( "ref" ) : self . refs [ ref . attributes [ "id" ] . value ] = ref
5857	def get_available_columns ( self , dataset_ids ) : if not isinstance ( dataset_ids , list ) : dataset_ids = [ dataset_ids ] data = { "dataset_ids" : dataset_ids } failure_message = "Failed to get available columns in dataset(s) {}" . format ( dataset_ids ) return self . _get_success_json ( self . _post_json ( 'v1/datasets/get-available-columns' , data , failure_message = failure_message ) ) [ 'data' ]
4693	def cmd ( command ) : env ( ) ipmi = cij . env_to_dict ( PREFIX , EXPORTED + REQUIRED ) command = "ipmitool -U %s -P %s -H %s -p %s %s" % ( ipmi [ "USER" ] , ipmi [ "PASS" ] , ipmi [ "HOST" ] , ipmi [ "PORT" ] , command ) cij . info ( "ipmi.command: %s" % command ) return cij . util . execute ( command , shell = True , echo = True )
5236	def file_modified_time ( file_name ) -> pd . Timestamp : return pd . to_datetime ( time . ctime ( os . path . getmtime ( filename = file_name ) ) )
977	def _newRepresentationOK ( self , newRep , newIndex ) : if newRep . size != self . w : return False if ( newIndex < self . minIndex - 1 ) or ( newIndex > self . maxIndex + 1 ) : raise ValueError ( "newIndex must be within one of existing indices" ) newRepBinary = numpy . array ( [ False ] * self . n ) newRepBinary [ newRep ] = True midIdx = self . _maxBuckets / 2 runningOverlap = self . _countOverlap ( self . bucketMap [ self . minIndex ] , newRep ) if not self . _overlapOK ( self . minIndex , newIndex , overlap = runningOverlap ) : return False for i in range ( self . minIndex + 1 , midIdx + 1 ) : newBit = ( i - 1 ) % self . w if newRepBinary [ self . bucketMap [ i - 1 ] [ newBit ] ] : runningOverlap -= 1 if newRepBinary [ self . bucketMap [ i ] [ newBit ] ] : runningOverlap += 1 if not self . _overlapOK ( i , newIndex , overlap = runningOverlap ) : return False for i in range ( midIdx + 1 , self . maxIndex + 1 ) : newBit = i % self . w if newRepBinary [ self . bucketMap [ i - 1 ] [ newBit ] ] : runningOverlap -= 1 if newRepBinary [ self . bucketMap [ i ] [ newBit ] ] : runningOverlap += 1 if not self . _overlapOK ( i , newIndex , overlap = runningOverlap ) : return False return True
6438	def dist ( self , src , tar , weights = 'exponential' , max_length = 8 ) : return self . dist_abs ( src , tar , weights , max_length , True )
5931	def scale_dihedrals ( mol , dihedrals , scale , banned_lines = None ) : if banned_lines is None : banned_lines = [ ] new_dihedrals = [ ] for dh in mol . dihedrals : atypes = dh . atom1 . get_atomtype ( ) , dh . atom2 . get_atomtype ( ) , dh . atom3 . get_atomtype ( ) , dh . atom4 . get_atomtype ( ) atypes = [ a . replace ( "_" , "" ) . replace ( "=" , "" ) for a in atypes ] if dh . gromacs [ 'param' ] != [ ] : for p in dh . gromacs [ 'param' ] : p [ 'kch' ] *= scale new_dihedrals . append ( dh ) continue for iswitch in range ( 32 ) : if ( iswitch % 2 == 0 ) : a1 = atypes [ 0 ] a2 = atypes [ 1 ] a3 = atypes [ 2 ] a4 = atypes [ 3 ] else : a1 = atypes [ 3 ] a2 = atypes [ 2 ] a3 = atypes [ 1 ] a4 = atypes [ 0 ] if ( ( iswitch // 2 ) % 2 == 1 ) : a1 = "X" if ( ( iswitch // 4 ) % 2 == 1 ) : a2 = "X" if ( ( iswitch // 8 ) % 2 == 1 ) : a3 = "X" if ( ( iswitch // 16 ) % 2 == 1 ) : a4 = "X" key = "{0}-{1}-{2}-{3}-{4}" . format ( a1 , a2 , a3 , a4 , dh . gromacs [ 'func' ] ) if ( key in dihedrals ) : for i , dt in enumerate ( dihedrals [ key ] ) : dhA = copy . deepcopy ( dh ) param = copy . deepcopy ( dt . gromacs [ 'param' ] ) if not dihedrals [ key ] [ 0 ] . line in banned_lines : for p in param : p [ 'kchi' ] *= scale dhA . gromacs [ 'param' ] = param if i == 0 : dhA . comment = "; banned lines {0} found={1}\n" . format ( " " . join ( map ( str , banned_lines ) ) , 1 if dt . line in banned_lines else 0 ) dhA . comment += "; parameters for types {}-{}-{}-{}-9 at LINE({})\n" . format ( dhA . atom1 . atomtype , dhA . atom2 . atomtype , dhA . atom3 . atomtype , dhA . atom4 . atomtype , dt . line ) . replace ( "_" , "" ) name = "{}-{}-{}-{}-9" . format ( dhA . atom1 . atomtype , dhA . atom2 . atomtype , dhA . atom3 . atomtype , dhA . atom4 . atomtype ) . replace ( "_" , "" ) new_dihedrals . append ( dhA ) break mol . dihedrals = new_dihedrals return mol
13475	def start ( self ) : assert not self . has_started ( ) , "called start() on an active GeventLoop" self . _stop_event = Event ( ) self . _greenlet = gevent . spawn ( self . _loop )
11629	def _loadNamelistIncludes ( item , unique_glyphs , cache ) : includes = item [ "includes" ] = [ ] charset = item [ "charset" ] = set ( ) | item [ "ownCharset" ] noCharcode = item [ "noCharcode" ] = set ( ) | item [ "ownNoCharcode" ] dirname = os . path . dirname ( item [ "fileName" ] ) for include in item [ "header" ] [ "includes" ] : includeFile = os . path . join ( dirname , include ) try : includedItem = readNamelist ( includeFile , unique_glyphs , cache ) except NamelistRecursionError : continue if includedItem in includes : continue includes . append ( includedItem ) charset |= includedItem [ "charset" ] noCharcode |= includedItem [ "ownNoCharcode" ] return item
8938	def confluence ( ctx , no_publish = False , clean = False , opts = '' ) : cfg = config . load ( ) if clean : ctx . run ( "invoke clean --docs" ) cmd = [ 'sphinx-build' , '-b' , 'confluence' ] cmd . extend ( [ '-E' , '-a' ] ) if opts : cmd . append ( opts ) cmd . extend ( [ '.' , ctx . rituals . docs . build + '_cf' ] ) if no_publish : cmd . extend ( [ '-Dconfluence_publish=False' ] ) notify . info ( "Starting Sphinx build..." ) with pushd ( ctx . rituals . docs . sources ) : ctx . run ( ' ' . join ( cmd ) , pty = True )
2573	def _create_task_log_info ( self , task_id , fail_mode = None ) : info_to_monitor = [ 'func_name' , 'fn_hash' , 'memoize' , 'checkpoint' , 'fail_count' , 'fail_history' , 'status' , 'id' , 'time_submitted' , 'time_returned' , 'executor' ] task_log_info = { "task_" + k : self . tasks [ task_id ] [ k ] for k in info_to_monitor } task_log_info [ 'run_id' ] = self . run_id task_log_info [ 'timestamp' ] = datetime . datetime . now ( ) task_log_info [ 'task_status_name' ] = self . tasks [ task_id ] [ 'status' ] . name task_log_info [ 'tasks_failed_count' ] = self . tasks_failed_count task_log_info [ 'tasks_completed_count' ] = self . tasks_completed_count task_log_info [ 'task_inputs' ] = str ( self . tasks [ task_id ] [ 'kwargs' ] . get ( 'inputs' , None ) ) task_log_info [ 'task_outputs' ] = str ( self . tasks [ task_id ] [ 'kwargs' ] . get ( 'outputs' , None ) ) task_log_info [ 'task_stdin' ] = self . tasks [ task_id ] [ 'kwargs' ] . get ( 'stdin' , None ) task_log_info [ 'task_stdout' ] = self . tasks [ task_id ] [ 'kwargs' ] . get ( 'stdout' , None ) task_log_info [ 'task_depends' ] = None if self . tasks [ task_id ] [ 'depends' ] is not None : task_log_info [ 'task_depends' ] = "," . join ( [ str ( t . _tid ) for t in self . tasks [ task_id ] [ 'depends' ] ] ) task_log_info [ 'task_elapsed_time' ] = None if self . tasks [ task_id ] [ 'time_returned' ] is not None : task_log_info [ 'task_elapsed_time' ] = ( self . tasks [ task_id ] [ 'time_returned' ] - self . tasks [ task_id ] [ 'time_submitted' ] ) . total_seconds ( ) if fail_mode is not None : task_log_info [ 'task_fail_mode' ] = fail_mode return task_log_info
403	def ramp ( x , v_min = 0 , v_max = 1 , name = None ) : return tf . clip_by_value ( x , clip_value_min = v_min , clip_value_max = v_max , name = name )
6135	def _fix_docs ( this_abc , child_class ) : if sys . version_info >= ( 3 , 5 ) : return child_class if not issubclass ( child_class , this_abc ) : raise KappaError ( 'Cannot fix docs of class that is not decendent.' ) for name , child_func in vars ( child_class ) . items ( ) : if callable ( child_func ) and not child_func . __doc__ : if name in this_abc . __abstractmethods__ : parent_func = getattr ( this_abc , name ) child_func . __doc__ = parent_func . __doc__ return child_class
10192	def get_geoip ( ip ) : reader = geolite2 . reader ( ) ip_data = reader . get ( ip ) or { } return ip_data . get ( 'country' , { } ) . get ( 'iso_code' )
810	def _finishLearning ( self ) : if self . _doSphering : self . _finishSphering ( ) self . _knn . finishLearning ( ) self . _accuracy = None
2451	def set_pkg_down_location ( self , doc , location ) : self . assert_package_exists ( ) if not self . package_down_location_set : self . package_down_location_set = True doc . package . download_location = location return True else : raise CardinalityError ( 'Package::DownloadLocation' )
6237	def draw_buffers ( self , near , far ) : self . ctx . disable ( moderngl . DEPTH_TEST ) helper . draw ( self . gbuffer . color_attachments [ 0 ] , pos = ( 0.0 , 0.0 ) , scale = ( 0.25 , 0.25 ) ) helper . draw ( self . gbuffer . color_attachments [ 1 ] , pos = ( 0.5 , 0.0 ) , scale = ( 0.25 , 0.25 ) ) helper . draw_depth ( self . gbuffer . depth_attachment , near , far , pos = ( 1.0 , 0.0 ) , scale = ( 0.25 , 0.25 ) ) helper . draw ( self . lightbuffer . color_attachments [ 0 ] , pos = ( 1.5 , 0.0 ) , scale = ( 0.25 , 0.25 ) )
2705	def collect_phrases ( sent , ranks , spacy_nlp ) : tail = 0 last_idx = sent [ 0 ] . idx - 1 phrase = [ ] while tail < len ( sent ) : w = sent [ tail ] if ( w . word_id > 0 ) and ( w . root in ranks ) and ( ( w . idx - last_idx ) == 1 ) : rl = RankedLexeme ( text = w . raw . lower ( ) , rank = ranks [ w . root ] , ids = w . word_id , pos = w . pos . lower ( ) , count = 1 ) phrase . append ( rl ) else : for text , p in enumerate_chunks ( phrase , spacy_nlp ) : if p : id_list = [ rl . ids for rl in p ] rank_list = [ rl . rank for rl in p ] np_rl = RankedLexeme ( text = text , rank = rank_list , ids = id_list , pos = "np" , count = 1 ) if DEBUG : print ( np_rl ) yield np_rl phrase = [ ] last_idx = w . idx tail += 1
5434	def parse_tasks_file_header ( header , input_file_param_util , output_file_param_util ) : job_params = [ ] for col in header : col_type = '--env' col_value = col if col . startswith ( '-' ) : col_type , col_value = split_pair ( col , ' ' , 1 ) if col_type == '--env' : job_params . append ( job_model . EnvParam ( col_value ) ) elif col_type == '--label' : job_params . append ( job_model . LabelParam ( col_value ) ) elif col_type == '--input' or col_type == '--input-recursive' : name = input_file_param_util . get_variable_name ( col_value ) job_params . append ( job_model . InputFileParam ( name , recursive = ( col_type . endswith ( 'recursive' ) ) ) ) elif col_type == '--output' or col_type == '--output-recursive' : name = output_file_param_util . get_variable_name ( col_value ) job_params . append ( job_model . OutputFileParam ( name , recursive = ( col_type . endswith ( 'recursive' ) ) ) ) else : raise ValueError ( 'Unrecognized column header: %s' % col ) return job_params
6122	def zoom_region ( self ) : where = np . array ( np . where ( np . invert ( self . astype ( 'bool' ) ) ) ) y0 , x0 = np . amin ( where , axis = 1 ) y1 , x1 = np . amax ( where , axis = 1 ) return [ y0 , y1 + 1 , x0 , x1 + 1 ]
10859	def update ( self , params , values ) : global_update , particles = self . _update_type ( params ) if global_update : self . set_values ( params , values ) self . initialize ( ) return oldargs = self . _drawargs ( ) for n in particles : self . _draw_particle ( self . pos [ n ] , * listify ( oldargs [ n ] ) , sign = - 1 ) self . set_values ( params , values ) newargs = self . _drawargs ( ) for n in particles : self . _draw_particle ( self . pos [ n ] , * listify ( newargs [ n ] ) , sign = + 1 )
2992	def mutualFundSymbolsDF ( token = '' , version = '' ) : df = pd . DataFrame ( mutualFundSymbols ( token , version ) ) _toDatetime ( df ) _reindex ( df , 'symbol' ) return df
6514	def get_gender ( self , name , country = None ) : if not self . case_sensitive : name = name . lower ( ) if name not in self . names : return self . unknown_value elif not country : def counter ( country_values ) : country_values = map ( ord , country_values . replace ( " " , "" ) ) return ( len ( country_values ) , sum ( map ( lambda c : c > 64 and c - 55 or c - 48 , country_values ) ) ) return self . _most_popular_gender ( name , counter ) elif country in self . __class__ . COUNTRIES : index = self . __class__ . COUNTRIES . index ( country ) counter = lambda e : ( ord ( e [ index ] ) - 32 , 0 ) return self . _most_popular_gender ( name , counter ) else : raise NoCountryError ( "No such country: %s" % country )
6178	def map_chunk ( func , array , out_array ) : for slice in iter_chunk_slice ( array . shape [ - 1 ] , array . chunkshape [ - 1 ] ) : out_array . append ( func ( array [ ... , slice ] ) ) return out_array
6782	def get_previous_thumbprint ( self , components = None ) : components = str_to_component_list ( components ) tp_fn = self . manifest_filename tp_text = None if self . file_exists ( tp_fn ) : fd = six . BytesIO ( ) get ( tp_fn , fd ) tp_text = fd . getvalue ( ) manifest_data = { } raw_data = yaml . load ( tp_text ) for k , v in raw_data . items ( ) : manifest_key = assert_valid_satchel ( k ) service_name = clean_service_name ( k ) if components and service_name not in components : continue manifest_data [ manifest_key ] = v return manifest_data
11321	def update_languages ( self ) : language_fields = record_get_field_instances ( self . record , '041' ) language = "eng" record_delete_fields ( self . record , "041" ) for field in language_fields : subs = field_get_subfields ( field ) if 'a' in subs : language = self . get_config_item ( subs [ 'a' ] [ 0 ] , "languages" ) break new_subs = [ ( 'a' , language ) ] record_add_field ( self . record , "041" , subfields = new_subs )
4758	def main ( args ) : trun = cij . runner . trun_from_file ( args . trun_fpath ) rehome ( trun [ "conf" ] [ "OUTPUT" ] , args . output , trun ) postprocess ( trun ) cij . emph ( "main: reports are uses tmpl_fpath: %r" % args . tmpl_fpath ) cij . emph ( "main: reports are here args.output: %r" % args . output ) html_fpath = os . sep . join ( [ args . output , "%s.html" % args . tmpl_name ] ) cij . emph ( "html_fpath: %r" % html_fpath ) try : with open ( html_fpath , 'w' ) as html_file : html_file . write ( dset_to_html ( trun , args . tmpl_fpath ) ) except ( IOError , OSError , ValueError ) as exc : import traceback traceback . print_exc ( ) cij . err ( "rprtr:main: exc: %s" % exc ) return 1 return 0
3761	def draw_2d ( self , Hs = False ) : r try : from rdkit . Chem import Draw from rdkit . Chem . Draw import IPythonConsole if Hs : mols = [ i . rdkitmol_Hs for i in self . Chemicals ] else : mols = [ i . rdkitmol for i in self . Chemicals ] return Draw . MolsToImage ( mols ) except : return 'Rdkit is required for this feature.'
5893	def handle_upload ( self , request ) : if request . method != 'POST' : raise Http404 if request . is_ajax ( ) : try : filename = request . GET [ 'quillUploadFile' ] data = request is_raw = True except KeyError : return HttpResponseBadRequest ( "Invalid file upload." ) else : if len ( request . FILES ) != 1 : return HttpResponseBadRequest ( "Can only upload 1 file at a time." ) try : data = request . FILES [ 'quillUploadFile' ] filename = data . name is_raw = False except KeyError : return HttpResponseBadRequest ( 'Missing image `quillUploadFile`.' ) url = save_file ( data , filename , is_raw , default_storage ) response_data = { } response_data [ 'url' ] = url return HttpResponse ( json . dumps ( response_data ) , content_type = "text/html; charset=utf-8" )
5026	def get_result ( self , course_grade ) : return Result ( score = Score ( scaled = course_grade . percent , raw = course_grade . percent * 100 , min = MIN_SCORE , max = MAX_SCORE , ) , success = course_grade . passed , completion = course_grade . passed )
12594	def select_arg_verify ( endpoint , cert , key , pem , ca , aad , no_verify ) : if not ( endpoint . lower ( ) . startswith ( 'http' ) or endpoint . lower ( ) . startswith ( 'https' ) ) : raise CLIError ( 'Endpoint must be HTTP or HTTPS' ) usage = ( 'Valid syntax : --endpoint [ [ --key --cert | --pem | --aad] ' '[ --ca | --no-verify ] ]' ) if ca and not ( pem or all ( [ key , cert ] ) ) : raise CLIError ( usage ) if no_verify and not ( pem or all ( [ key , cert ] ) or aad ) : raise CLIError ( usage ) if no_verify and ca : raise CLIError ( usage ) if any ( [ cert , key ] ) and not all ( [ cert , key ] ) : raise CLIError ( usage ) if aad and any ( [ pem , cert , key ] ) : raise CLIError ( usage ) if pem and any ( [ cert , key ] ) : raise CLIError ( usage )
4064	def add_tags ( self , item , * tags ) : try : assert item [ "data" ] [ "tags" ] except AssertionError : item [ "data" ] [ "tags" ] = list ( ) for tag in tags : item [ "data" ] [ "tags" ] . append ( { "tag" : "%s" % tag } ) assert self . check_items ( [ item ] ) return self . update_item ( item )
13488	def create ( self , server ) : for chunk in self . __cut_to_size ( ) : server . post ( 'tasks_admin' , chunk . as_payload ( ) , replacements = { 'slug' : chunk . challenge . slug } )
10616	def clear ( self ) : self . _compound_masses = self . _compound_masses * 0.0 self . _P = 1.0 self . _T = 25.0 self . _H = 0.0
5248	def custom_req ( session , request ) : while ( session . tryNextEvent ( ) ) : pass print ( "Sending Request:\n %s" % request ) session . sendRequest ( request ) messages = [ ] while ( True ) : ev = session . nextEvent ( 500 ) for msg in ev : print ( "Message Received:\n %s" % msg ) messages . append ( msg ) if ev . eventType ( ) == blpapi . Event . RESPONSE : break return messages
3825	async def get_group_conversation_url ( self , get_group_conversation_url_request ) : response = hangouts_pb2 . GetGroupConversationUrlResponse ( ) await self . _pb_request ( 'conversations/getgroupconversationurl' , get_group_conversation_url_request , response ) return response
11571	def set_bit_map ( self , shape , color ) : for row in range ( 0 , 8 ) : data = shape [ row ] bit_mask = 0x80 for column in range ( 0 , 8 ) : if data & bit_mask : self . set_pixel ( row , column , color , True ) bit_mask >>= 1 self . output_entire_buffer ( )
3512	def optimizely ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return OptimizelyNode ( )
6091	def cache ( func ) : def wrapper ( instance : GeometryProfile , grid : np . ndarray , * args , ** kwargs ) : if not hasattr ( instance , "cache" ) : instance . cache = { } key = ( func . __name__ , grid . tobytes ( ) ) if key not in instance . cache : instance . cache [ key ] = func ( instance , grid ) return instance . cache [ key ] return wrapper
4436	def destroy ( self ) : self . ws . destroy ( ) self . bot . remove_listener ( self . on_socket_response ) self . hooks . clear ( )
8057	def do_restart ( self , line ) : self . bot . _frame = 0 self . bot . _namespace . clear ( ) self . bot . _namespace . update ( self . bot . _initial_namespace )
3016	def from_json_keyfile_name ( cls , filename , scopes = '' , token_uri = None , revoke_uri = None ) : with open ( filename , 'r' ) as file_obj : client_credentials = json . load ( file_obj ) return cls . _from_parsed_json_keyfile ( client_credentials , scopes , token_uri = token_uri , revoke_uri = revoke_uri )
12296	def post ( repo , args = [ ] ) : mgr = plugins_get_mgr ( ) keys = mgr . search ( what = 'metadata' ) keys = keys [ 'metadata' ] if len ( keys ) == 0 : return if 'pipeline' in repo . options : for name , details in repo . options [ 'pipeline' ] . items ( ) : patterns = details [ 'files' ] matching_files = repo . find_matching_files ( patterns ) matching_files . sort ( ) details [ 'files' ] = matching_files for i , f in enumerate ( matching_files ) : r = repo . get_resource ( f ) if 'pipeline' not in r : r [ 'pipeline' ] = [ ] r [ 'pipeline' ] . append ( name + " [Step {}]" . format ( i ) ) if 'metadata-management' in repo . options : print ( "Collecting all the required metadata to post" ) metadata = repo . options [ 'metadata-management' ] if 'include-data-history' in metadata and metadata [ 'include-data-history' ] : repo . package [ 'history' ] = get_history ( repo . rootdir ) if 'include-action-history' in metadata and metadata [ 'include-action-history' ] : annotate_metadata_action ( repo ) if 'include-preview' in metadata : annotate_metadata_data ( repo , task = 'preview' , patterns = metadata [ 'include-preview' ] [ 'files' ] , size = metadata [ 'include-preview' ] [ 'length' ] ) if ( ( 'include-schema' in metadata ) and metadata [ 'include-schema' ] ) : annotate_metadata_data ( repo , task = 'schema' ) if 'include-code-history' in metadata : annotate_metadata_code ( repo , files = metadata [ 'include-code-history' ] ) if 'include-platform' in metadata : annotate_metadata_platform ( repo ) if 'include-validation' in metadata : annotate_metadata_validation ( repo ) if 'include-dependencies' in metadata : annotate_metadata_dependencies ( repo ) history = repo . package . get ( 'history' , None ) if ( ( 'include-tab-diffs' in metadata ) and metadata [ 'include-tab-diffs' ] and history is not None ) : annotate_metadata_diffs ( repo ) repo . package [ 'config' ] = repo . options try : for k in keys : metadatamgr = mgr . get_by_key ( 'metadata' , k ) url = metadatamgr . url o = urlparse ( url ) print ( "Posting to " , o . netloc ) response = metadatamgr . post ( repo ) if isinstance ( response , str ) : print ( "Error while posting:" , response ) elif response . status_code in [ 400 ] : content = response . json ( ) print ( "Error while posting:" ) for k in content : print ( " " , k , "- " , "," . join ( content [ k ] ) ) except NetworkError as e : print ( "Unable to reach metadata server!" ) except NetworkInvalidConfiguration as e : print ( "Invalid network configuration in the INI file" ) print ( e . message ) except Exception as e : print ( "Could not post. Unknown error" ) print ( e )
9985	def _reload ( self , module = None ) : if self . module is None : raise RuntimeError elif module is None : import importlib module = ModuleSource ( importlib . reload ( module ) ) elif module . name != self . module : raise RuntimeError if self . name in module . funcs : func = module . funcs [ self . name ] self . __init__ ( func = func ) else : self . __init__ ( func = NULL_FORMULA ) return self
6551	def from_configurations ( cls , configurations , variables , vartype , name = None ) : def func ( * args ) : return args in configurations return cls ( func , configurations , variables , vartype , name )
8354	def start_meta ( self , attrs ) : httpEquiv = None contentType = None contentTypeIndex = None tagNeedsEncodingSubstitution = False for i in range ( 0 , len ( attrs ) ) : key , value = attrs [ i ] key = key . lower ( ) if key == 'http-equiv' : httpEquiv = value elif key == 'content' : contentType = value contentTypeIndex = i if httpEquiv and contentType : match = self . CHARSET_RE . search ( contentType ) if match : if ( self . declaredHTMLEncoding is not None or self . originalEncoding == self . fromEncoding ) : def rewrite ( match ) : return match . group ( 1 ) + "%SOUP-ENCODING%" newAttr = self . CHARSET_RE . sub ( rewrite , contentType ) attrs [ contentTypeIndex ] = ( attrs [ contentTypeIndex ] [ 0 ] , newAttr ) tagNeedsEncodingSubstitution = True else : newCharset = match . group ( 3 ) if newCharset and newCharset != self . originalEncoding : self . declaredHTMLEncoding = newCharset self . _feed ( self . declaredHTMLEncoding ) raise StopParsing pass tag = self . unknown_starttag ( "meta" , attrs ) if tag and tagNeedsEncodingSubstitution : tag . containsSubstitutions = True
12866	def startup ( self , app ) : self . database . init_async ( app . loop ) if not self . cfg . connection_manual : app . middlewares . insert ( 0 , self . _middleware )
7428	def _subsample ( self ) : spans = self . maparr samp = np . zeros ( spans . shape [ 0 ] , dtype = np . uint64 ) for i in xrange ( spans . shape [ 0 ] ) : samp [ i ] = np . random . randint ( spans [ i , 0 ] , spans [ i , 1 ] , 1 ) return samp
5152	def merge_list ( list1 , list2 , identifiers = None ) : identifiers = identifiers or [ ] dict_map = { 'list1' : OrderedDict ( ) , 'list2' : OrderedDict ( ) } counter = 1 for list_ in [ list1 , list2 ] : container = dict_map [ 'list{0}' . format ( counter ) ] for el in list_ : key = id ( el ) if isinstance ( el , dict ) : for id_key in identifiers : if id_key in el : key = el [ id_key ] break container [ key ] = deepcopy ( el ) counter += 1 merged = merge_config ( dict_map [ 'list1' ] , dict_map [ 'list2' ] ) return list ( merged . values ( ) )
3653	def represent_pixel_location ( self ) : if self . data is None : return None return self . _data . reshape ( self . height + self . y_padding , int ( self . width * self . _num_components_per_pixel + self . x_padding ) )
4157	def arma_estimate ( X , P , Q , lag ) : R = CORRELATION ( X , maxlags = lag , norm = 'unbiased' ) R0 = R [ 0 ] MPQ = lag - Q + P N = len ( X ) Y = np . zeros ( N - P , dtype = complex ) for K in range ( 0 , MPQ ) : KPQ = K + Q - P + 1 if KPQ < 0 : Y [ K ] = R [ - KPQ ] . conjugate ( ) if KPQ == 0 : Y [ K ] = R0 if KPQ > 0 : Y [ K ] = R [ KPQ ] Y . resize ( lag ) if P <= 4 : res = arcovar_marple ( Y . copy ( ) , P ) ar_params = res [ 0 ] else : res = arcovar ( Y . copy ( ) , P ) ar_params = res [ 0 ] Y . resize ( N - P ) for k in range ( P , N ) : SUM = X [ k ] for j in range ( 0 , P ) : SUM = SUM + ar_params [ j ] * X [ k - j - 1 ] Y [ k - P ] = SUM ma_params , rho = ma ( Y , Q , 2 * Q ) return ar_params , ma_params , rho
7279	def play ( self ) : if not self . is_playing ( ) : self . play_pause ( ) self . _is_playing = True self . playEvent ( self )
6504	def decorate_matches ( match_in , match_word ) : matches = re . finditer ( match_word , match_in , re . IGNORECASE ) for matched_string in set ( [ match . group ( ) for match in matches ] ) : match_in = match_in . replace ( matched_string , getattr ( settings , "SEARCH_MATCH_DECORATION" , u"<b>{}</b>" ) . format ( matched_string ) ) return match_in
12117	def ndist ( data , Xs ) : sigma = np . sqrt ( np . var ( data ) ) center = np . average ( data ) curve = mlab . normpdf ( Xs , center , sigma ) curve *= len ( data ) * HIST_RESOLUTION return curve
1237	def from_spec ( spec , kwargs = None ) : network = util . get_object ( obj = spec , default_object = LayeredNetwork , kwargs = kwargs ) assert isinstance ( network , Network ) return network
3039	def has_scopes ( self , scopes ) : scopes = _helpers . string_to_scopes ( scopes ) return set ( scopes ) . issubset ( self . scopes )
5372	def _file_exists_in_gcs ( gcs_file_path , credentials = None ) : gcs_service = _get_storage_service ( credentials ) bucket_name , object_name = gcs_file_path [ len ( 'gs://' ) : ] . split ( '/' , 1 ) request = gcs_service . objects ( ) . get ( bucket = bucket_name , object = object_name , projection = 'noAcl' ) try : request . execute ( ) return True except errors . HttpError : return False
3716	def legal_status ( CASRN , Method = None , AvailableMethods = False , CASi = None ) : r load_law_data ( ) if not CASi : CASi = CAS2int ( CASRN ) methods = [ COMBINED , DSL , TSCA , EINECS , NLP , SPIN ] if AvailableMethods : return methods if not Method : Method = methods [ 0 ] if Method == DSL : if CASi in DSL_data . index : status = CAN_DSL_flags [ DSL_data . at [ CASi , 'Registry' ] ] else : status = UNLISTED elif Method == TSCA : if CASi in TSCA_data . index : data = TSCA_data . loc [ CASi ] . to_dict ( ) if any ( data . values ( ) ) : status = sorted ( [ TSCA_flags [ i ] for i in data . keys ( ) if data [ i ] ] ) else : status = LISTED else : status = UNLISTED elif Method == EINECS : if CASi in EINECS_data . index : status = LISTED else : status = UNLISTED elif Method == NLP : if CASi in NLP_data . index : status = LISTED else : status = UNLISTED elif Method == SPIN : if CASi in SPIN_data . index : status = LISTED else : status = UNLISTED elif Method == COMBINED : status = { } for method in methods [ 1 : ] : status [ method ] = legal_status ( CASRN , Method = method , CASi = CASi ) else : raise Exception ( 'Failure in in function' ) return status
11778	def leave1out ( learner , dataset ) : "Leave one out cross-validation over the dataset." return cross_validation ( learner , dataset , k = len ( dataset . examples ) )
4497	def project ( self , project_id ) : type_ = self . guid ( project_id ) url = self . _build_url ( type_ , project_id ) if type_ in Project . _types : return Project ( self . _json ( self . _get ( url ) , 200 ) , self . session ) raise OSFException ( '{} is unrecognized type {}. Clone supports projects and registrations' . format ( project_id , type_ ) )
13487	def get_or_create_index ( self , index_ratio , index_width ) : if not self . index_path . exists ( ) or not self . filepath . stat ( ) . st_mtime == self . index_path . stat ( ) . st_mtime : create_index ( self . filepath , self . index_path , index_ratio = index_ratio , index_width = index_width ) return IndexFile ( str ( self . index_path ) )
5367	def compact_interval_string ( value_list ) : if not value_list : return '' value_list . sort ( ) interval_list = [ ] curr = [ ] for val in value_list : if curr and ( val > curr [ - 1 ] + 1 ) : interval_list . append ( ( curr [ 0 ] , curr [ - 1 ] ) ) curr = [ val ] else : curr . append ( val ) if curr : interval_list . append ( ( curr [ 0 ] , curr [ - 1 ] ) ) return ',' . join ( [ '{}-{}' . format ( pair [ 0 ] , pair [ 1 ] ) if pair [ 0 ] != pair [ 1 ] else str ( pair [ 0 ] ) for pair in interval_list ] )
3089	def locked_get ( self ) : credentials = None if self . _cache : json = self . _cache . get ( self . _key_name ) if json : credentials = client . Credentials . new_from_json ( json ) if credentials is None : entity = self . _get_entity ( ) if entity is not None : credentials = getattr ( entity , self . _property_name ) if self . _cache : self . _cache . set ( self . _key_name , credentials . to_json ( ) ) if credentials and hasattr ( credentials , 'set_store' ) : credentials . set_store ( self ) return credentials
1426	def getInstanceJstack ( self , topology_info , instance_id ) : pid_response = yield getInstancePid ( topology_info , instance_id ) try : http_client = tornado . httpclient . AsyncHTTPClient ( ) pid_json = json . loads ( pid_response ) pid = pid_json [ 'stdout' ] . strip ( ) if pid == '' : raise Exception ( 'Failed to get pid' ) endpoint = utils . make_shell_endpoint ( topology_info , instance_id ) url = "%s/jstack/%s" % ( endpoint , pid ) response = yield http_client . fetch ( url ) Log . debug ( "HTTP call for url: %s" , url ) raise tornado . gen . Return ( response . body ) except tornado . httpclient . HTTPError as e : raise Exception ( str ( e ) )
6101	def intensities_from_grid_radii ( self , grid_radii ) : np . seterr ( all = 'ignore' ) return np . multiply ( self . intensity , np . exp ( np . multiply ( - self . sersic_constant , np . add ( np . power ( np . divide ( grid_radii , self . effective_radius ) , 1. / self . sersic_index ) , - 1 ) ) ) )
6491	def _process_exclude_dictionary ( exclude_dictionary ) : not_properties = [ ] for exclude_property in exclude_dictionary : exclude_values = exclude_dictionary [ exclude_property ] if not isinstance ( exclude_values , list ) : exclude_values = [ exclude_values ] not_properties . extend ( [ { "term" : { exclude_property : exclude_value } } for exclude_value in exclude_values ] ) if not not_properties : return { } return { "not" : { "filter" : { "or" : not_properties } } }
6021	def from_fits_renormalized ( cls , file_path , hdu , pixel_scale ) : psf = PSF . from_fits_with_scale ( file_path , hdu , pixel_scale ) psf [ : , : ] = np . divide ( psf , np . sum ( psf ) ) return psf
1102	def restore ( delta , which ) : r try : tag = { 1 : "- " , 2 : "+ " } [ int ( which ) ] except KeyError : raise ValueError , ( 'unknown delta choice (must be 1 or 2): %r' % which ) prefixes = ( " " , tag ) for line in delta : if line [ : 2 ] in prefixes : yield line [ 2 : ]
796	def getActiveJobsForClientInfo ( self , clientInfo , fields = [ ] ) : dbFields = [ self . _jobs . pubToDBNameDict [ x ] for x in fields ] dbFieldsStr = ',' . join ( [ 'job_id' ] + dbFields ) with ConnectionFactory . get ( ) as conn : query = 'SELECT %s FROM %s ' 'WHERE client_info = %%s ' ' AND status != %%s' % ( dbFieldsStr , self . jobsTableName ) conn . cursor . execute ( query , [ clientInfo , self . STATUS_COMPLETED ] ) rows = conn . cursor . fetchall ( ) return rows
12836	def render_vars ( self ) : return { 'records' : [ { 'message' : record . getMessage ( ) , 'time' : dt . datetime . fromtimestamp ( record . created ) . strftime ( '%H:%M:%S' ) , } for record in self . handler . records ] }
8565	def update_loadbalancer ( self , datacenter_id , loadbalancer_id , ** kwargs ) : data = { } for attr , value in kwargs . items ( ) : data [ self . _underscore_to_camelcase ( attr ) ] = value response = self . _perform_request ( url = '/datacenters/%s/loadbalancers/%s' % ( datacenter_id , loadbalancer_id ) , method = 'PATCH' , data = json . dumps ( data ) ) return response
3421	def model_to_pymatbridge ( model , variable_name = "model" , matlab = None ) : if scipy_sparse is None : raise ImportError ( "`model_to_pymatbridge` requires scipy!" ) if matlab is None : from IPython import get_ipython matlab = get_ipython ( ) . magics_manager . registry [ "MatlabMagics" ] . Matlab model_info = create_mat_dict ( model ) S = model_info [ "S" ] . todok ( ) model_info [ "S" ] = 0 temp_S_name = "cobra_pymatbridge_temp_" + uuid4 ( ) . hex _check ( matlab . set_variable ( variable_name , model_info ) ) _check ( matlab . set_variable ( temp_S_name , S ) ) _check ( matlab . run_code ( "%s.S = %s;" % ( variable_name , temp_S_name ) ) ) for i in model_info . keys ( ) : if i == "S" : continue _check ( matlab . run_code ( "{0}.{1} = {0}.{1}';" . format ( variable_name , i ) ) ) _check ( matlab . run_code ( "clear %s;" % temp_S_name ) )
2453	def set_pkg_verif_code ( self , doc , code ) : self . assert_package_exists ( ) if not self . package_verif_set : self . package_verif_set = True match = self . VERIF_CODE_REGEX . match ( code ) if match : doc . package . verif_code = match . group ( self . VERIF_CODE_CODE_GRP ) if match . group ( self . VERIF_CODE_EXC_FILES_GRP ) is not None : doc . package . verif_exc_files = match . group ( self . VERIF_CODE_EXC_FILES_GRP ) . split ( ',' ) return True else : raise SPDXValueError ( 'Package::VerificationCode' ) else : raise CardinalityError ( 'Package::VerificationCode' )
4239	def config_finish ( self ) : _LOGGER . info ( "Config finish" ) if not self . config_started : return True success , _ = self . _make_request ( SERVICE_DEVICE_CONFIG , "ConfigurationFinished" , { "NewStatus" : "ChangesApplied" } ) self . config_started = not success return success
8501	def as_namespace ( self , namespace = None ) : key = self . key if namespace and key . startswith ( namespace ) : key = key [ len ( namespace ) + 1 : ] return "%s = %s" % ( self . get_key ( ) , self . _default ( ) or NotSet ( ) )
10119	def circle ( cls , center , radius , n_vertices = 50 , ** kwargs ) : return cls . regular_polygon ( center , radius , n_vertices , ** kwargs )
2864	def readS8 ( self , register ) : result = self . readU8 ( register ) if result > 127 : result -= 256 return result
9780	def build ( ctx , project , build ) : ctx . obj = ctx . obj or { } ctx . obj [ 'project' ] = project ctx . obj [ 'build' ] = build
10991	def makestate ( im , pos , rad , slab = None , mem_level = 'hi' ) : if slab is not None : o = comp . ComponentCollection ( [ objs . PlatonicSpheresCollection ( pos , rad , zscale = zscale ) , slab ] , category = 'obj' ) else : o = objs . PlatonicSpheresCollection ( pos , rad , zscale = zscale ) p = exactpsf . FixedSSChebLinePSF ( ) npts , iorder = _calc_ilm_order ( im . get_image ( ) . shape ) i = ilms . BarnesStreakLegPoly2P1D ( npts = npts , zorder = iorder ) b = ilms . LegendrePoly2P1D ( order = ( 9 , 3 , 5 ) , category = 'bkg' ) c = comp . GlobalScalar ( 'offset' , 0.0 ) s = states . ImageState ( im , [ o , i , b , c , p ] ) runner . link_zscale ( s ) if mem_level != 'hi' : s . set_mem_level ( mem_level ) opt . do_levmarq ( s , [ 'ilm-scale' ] , max_iter = 1 , run_length = 6 , max_mem = 1e4 ) return s
9348	def year ( past = False , min_delta = 0 , max_delta = 20 ) : return dt . date . today ( ) . year + _delta ( past , min_delta , max_delta )
12370	def get ( self , id , ** kwargs ) : return super ( DomainRecords , self ) . get ( id , ** kwargs )
12203	def _compile ( self , parselet_node , level = 0 ) : if self . DEBUG : debug_offset = "" . join ( [ " " for x in range ( level ) ] ) if self . DEBUG : print ( debug_offset , "%s::compile(%s)" % ( self . __class__ . __name__ , parselet_node ) ) if isinstance ( parselet_node , dict ) : parselet_tree = ParsleyNode ( ) for k , v in list ( parselet_node . items ( ) ) : try : m = self . REGEX_PARSELET_KEY . match ( k ) if not m : if self . DEBUG : print ( debug_offset , "could not parse key" , k ) raise InvalidKeySyntax ( k ) except : raise InvalidKeySyntax ( "Key %s is not valid" % k ) key = m . group ( 'key' ) key_required = True operator = m . group ( 'operator' ) if operator == '?' : key_required = False scope = m . group ( 'scope' ) if isinstance ( v , ( list , tuple ) ) : v = v [ 0 ] iterate = True else : iterate = False try : parsley_context = ParsleyContext ( key , operator = operator , required = key_required , scope = self . selector_handler . make ( scope ) if scope else None , iterate = iterate ) except SyntaxError : if self . DEBUG : print ( "Invalid scope:" , k , scope ) raise if self . DEBUG : print ( debug_offset , "current context:" , parsley_context ) try : child_tree = self . _compile ( v , level = level + 1 ) except SyntaxError : if self . DEBUG : print ( "Invalid value: " , v ) raise except : raise if self . DEBUG : print ( debug_offset , "child tree:" , child_tree ) parselet_tree [ parsley_context ] = child_tree return parselet_tree elif isstr ( parselet_node ) : return self . selector_handler . make ( parselet_node ) else : raise ValueError ( "Unsupported type(%s) for Parselet node <%s>" % ( type ( parselet_node ) , parselet_node ) )
3566	def read_value ( self ) : pass self . _value_read . clear ( ) self . _device . _peripheral . readValueForDescriptor ( self . _descriptor ) if not self . _value_read . wait ( timeout_sec ) : raise RuntimeError ( 'Exceeded timeout waiting to read characteristic value!' ) return self . _value
5890	def force_unicode ( string , encoding = 'utf-8' , strings_only = False , errors = 'strict' ) : if isinstance ( string , str ) : return string if strings_only and is_protected_type ( string ) : return string try : if not isinstance ( string , str ) : if hasattr ( string , '__unicode__' ) : string = string . __unicode__ ( ) else : try : string = str ( string , encoding , errors ) except UnicodeEncodeError : if not isinstance ( string , Exception ) : raise string = ' ' . join ( [ force_unicode ( arg , encoding , strings_only , errors ) for arg in string ] ) elif not isinstance ( string , str ) : string = string . decode ( encoding , errors ) except UnicodeDecodeError as ex : if not isinstance ( string , Exception ) : raise DjangoUnicodeDecodeError ( string , * ex . args ) else : string = ' ' . join ( [ force_unicode ( arg , encoding , strings_only , errors ) for arg in string ] ) return string
11578	def send_command ( self , command ) : send_message = "" for i in command : send_message += chr ( i ) for data in send_message : self . pymata . transport . write ( data )
4624	def _get_encrypted_masterpassword ( self ) : if not self . unlocked ( ) : raise WalletLocked aes = AESCipher ( self . password ) return "{}${}" . format ( self . _derive_checksum ( self . masterkey ) , aes . encrypt ( self . masterkey ) )
3487	def _check ( value , message ) : if value is None : LOGGER . error ( 'Error: LibSBML returned a null value trying ' 'to <' + message + '>.' ) elif type ( value ) is int : if value == libsbml . LIBSBML_OPERATION_SUCCESS : return else : LOGGER . error ( 'Error encountered trying to <' + message + '>.' ) LOGGER . error ( 'LibSBML error code {}: {}' . format ( str ( value ) , libsbml . OperationReturnValue_toString ( value ) . strip ( ) ) ) else : return
6160	def eye_plot ( x , L , S = 0 ) : plt . figure ( figsize = ( 6 , 4 ) ) idx = np . arange ( 0 , L + 1 ) plt . plot ( idx , x [ S : S + L + 1 ] , 'b' ) k_max = int ( ( len ( x ) - S ) / L ) - 1 for k in range ( 1 , k_max ) : plt . plot ( idx , x [ S + k * L : S + L + 1 + k * L ] , 'b' ) plt . grid ( ) plt . xlabel ( 'Time Index - n' ) plt . ylabel ( 'Amplitude' ) plt . title ( 'Eye Plot' ) return 0
6138	def set_default_sim_param ( self , * args , ** kwargs ) : if len ( args ) is 1 and isinstance ( args [ 0 ] , SimulationParameter ) : self . __default_param = args [ 0 ] else : self . __default_param = SimulationParameter ( * args , ** kwargs ) return
307	def show_profit_attribution ( round_trips ) : total_pnl = round_trips [ 'pnl' ] . sum ( ) pnl_attribution = round_trips . groupby ( 'symbol' ) [ 'pnl' ] . sum ( ) / total_pnl pnl_attribution . name = '' pnl_attribution . index = pnl_attribution . index . map ( utils . format_asset ) utils . print_table ( pnl_attribution . sort_values ( inplace = False , ascending = False , ) , name = 'Profitability (PnL / PnL total) per name' , float_format = '{:.2%}' . format , )
6862	def drop_views ( self , name = None , site = None ) : r = self . database_renderer result = r . sudo ( "mysql --batch -v -h {db_host} " "-u {db_user} -p'{db_password}' " "--execute=\"SELECT GROUP_CONCAT(CONCAT(TABLE_SCHEMA,'.',table_name) SEPARATOR ', ') AS views " "FROM INFORMATION_SCHEMA.views WHERE TABLE_SCHEMA = '{db_name}' ORDER BY table_name DESC;\"" ) result = re . findall ( r'^views[\s\t\r\n]+(.*)' , result , flags = re . IGNORECASE | re . DOTALL | re . MULTILINE ) if not result : return r . env . db_view_list = result [ 0 ] r . sudo ( "mysql -v -h {db_host} -u {db_user} -p'{db_password}' " "--execute=\"DROP VIEW {db_view_list} CASCADE;\"" )
788	def jobInfoWithModels ( self , jobID ) : combinedResults = None with ConnectionFactory . get ( ) as conn : query = ' ' . join ( [ 'SELECT %s.*, %s.*' % ( self . jobsTableName , self . modelsTableName ) , 'FROM %s' % self . jobsTableName , 'LEFT JOIN %s USING(job_id)' % self . modelsTableName , 'WHERE job_id=%s' ] ) conn . cursor . execute ( query , ( jobID , ) ) if conn . cursor . rowcount > 0 : combinedResults = [ ClientJobsDAO . _combineResults ( result , self . _jobs . jobInfoNamedTuple , self . _models . modelInfoNamedTuple ) for result in conn . cursor . fetchall ( ) ] if combinedResults is not None : return combinedResults raise RuntimeError ( "jobID=%s not found within the jobs table" % ( jobID ) )
5709	def redirect ( self , request ) : url = request . path querystring = request . GET . copy ( ) if self . logout_key and self . logout_key in request . GET : del querystring [ self . logout_key ] if querystring : url = '%s?%s' % ( url , querystring . urlencode ( ) ) return HttpResponseRedirect ( url )
9492	def compile_bytecode ( code : list ) -> bytes : bc = b"" for i , op in enumerate ( code ) : try : if isinstance ( op , _PyteOp ) or isinstance ( op , _PyteAugmentedComparator ) : bc_op = op . to_bytes ( bc ) elif isinstance ( op , int ) : bc_op = op . to_bytes ( 1 , byteorder = "little" ) elif isinstance ( op , bytes ) : bc_op = op else : raise CompileError ( "Could not compile code of type {}" . format ( type ( op ) ) ) bc += bc_op except Exception as e : print ( "Fatal compiliation error on operator {i} ({op})." . format ( i = i , op = op ) ) raise e return bc
10416	def data_contains_key_builder ( key : str ) -> NodePredicate : def data_contains_key ( _ : BELGraph , node : BaseEntity ) -> bool : return key in node return data_contains_key
10980	def remove ( group_id , user_id ) : group = Group . query . get_or_404 ( group_id ) user = User . query . get_or_404 ( user_id ) if group . can_edit ( current_user ) : try : group . remove_member ( user ) except Exception as e : flash ( str ( e ) , "error" ) return redirect ( urlparse ( request . referrer ) . path ) flash ( _ ( 'User %(user_email)s was removed from %(group_name)s group.' , user_email = user . email , group_name = group . name ) , 'success' ) return redirect ( urlparse ( request . referrer ) . path ) flash ( _ ( 'You cannot delete users of the group %(group_name)s' , group_name = group . name ) , 'error' ) return redirect ( url_for ( '.index' ) )
11401	def create_field ( subfields = None , ind1 = ' ' , ind2 = ' ' , controlfield_value = '' , global_position = - 1 ) : if subfields is None : subfields = [ ] ind1 , ind2 = _wash_indicators ( ind1 , ind2 ) field = ( subfields , ind1 , ind2 , controlfield_value , global_position ) _check_field_validity ( field ) return field
10272	def get_unweighted_sources ( graph : BELGraph , key : Optional [ str ] = None ) -> Iterable [ BaseEntity ] : if key is None : key = WEIGHT for node in graph : if is_unweighted_source ( graph , node , key ) : yield node
1316	def GetAllPixelColors ( self ) -> ctypes . Array : return self . GetPixelColorsOfRect ( 0 , 0 , self . Width , self . Height )
7865	def handle_authorized ( self , event ) : request_software_version ( self . client , self . target_jid , self . success , self . failure )
6005	def generate_poisson_noise ( image , exposure_time_map , seed = - 1 ) : setup_random_seed ( seed ) image_counts = np . multiply ( image , exposure_time_map ) return image - np . divide ( np . random . poisson ( image_counts , image . shape ) , exposure_time_map )
582	def removeLabels ( self , start = None , end = None , labelFilter = None ) : if len ( self . saved_states ) == 0 : raise HTMPredictionModelInvalidRangeError ( "Invalid supplied range for " "'removeLabels'. Model has no saved records." ) startID = self . saved_states [ 0 ] . ROWID clippedStart = 0 if start is None else max ( 0 , start - startID ) clippedEnd = len ( self . saved_states ) if end is None else max ( 0 , min ( len ( self . saved_states ) , end - startID ) ) if clippedEnd <= clippedStart : raise HTMPredictionModelInvalidRangeError ( "Invalid supplied range for " "'removeLabels'." , debugInfo = { 'requestRange' : { 'startRecordID' : start , 'endRecordID' : end } , 'clippedRequestRange' : { 'startRecordID' : clippedStart , 'endRecordID' : clippedEnd } , 'validRange' : { 'startRecordID' : startID , 'endRecordID' : self . saved_states [ len ( self . saved_states ) - 1 ] . ROWID } , 'numRecordsStored' : len ( self . saved_states ) } ) recordsToDelete = [ ] for state in self . saved_states [ clippedStart : clippedEnd ] : if labelFilter is not None : if labelFilter in state . anomalyLabel : state . anomalyLabel . remove ( labelFilter ) else : state . anomalyLabel = [ ] state . setByUser = False recordsToDelete . append ( state ) self . _deleteRecordsFromKNN ( recordsToDelete ) self . _deleteRangeFromKNN ( start , end ) for state in self . saved_states [ clippedEnd : ] : self . _updateState ( state ) return { 'status' : 'success' }
12711	def add_force ( self , force , relative = False , position = None , relative_position = None ) : b = self . ode_body if relative_position is not None : op = b . addRelForceAtRelPos if relative else b . addForceAtRelPos op ( force , relative_position ) elif position is not None : op = b . addRelForceAtPos if relative else b . addForceAtPos op ( force , position ) else : op = b . addRelForce if relative else b . addForce op ( force )
8325	def buildTagMap ( default , * args ) : built = { } for portion in args : if hasattr ( portion , 'items' ) : for k , v in portion . items ( ) : built [ k ] = v elif isList ( portion ) : for k in portion : built [ k ] = default else : built [ portion ] = default return built
8228	def show ( self , format = 'png' , as_data = False ) : from io import BytesIO b = BytesIO ( ) if format == 'png' : from IPython . display import Image surface = cairo . ImageSurface ( cairo . FORMAT_ARGB32 , self . WIDTH , self . HEIGHT ) self . snapshot ( surface ) surface . write_to_png ( b ) b . seek ( 0 ) data = b . read ( ) if as_data : return data else : return Image ( data ) elif format == 'svg' : from IPython . display import SVG surface = cairo . SVGSurface ( b , self . WIDTH , self . HEIGHT ) surface . finish ( ) b . seek ( 0 ) data = b . read ( ) if as_data : return data else : return SVG ( data )
8660	def is_valid_identifier ( name ) : if not isinstance ( name , str ) : return False if '\n' in name : return False if name . strip ( ) != name : return False try : code = compile ( '\n{0}=None' . format ( name ) , filename = '<string>' , mode = 'single' ) exec ( code ) return True except SyntaxError : return False
76	def normalize_shape ( shape ) : if isinstance ( shape , tuple ) : return shape assert ia . is_np_array ( shape ) , ( "Expected tuple of ints or array, got %s." % ( type ( shape ) , ) ) return shape . shape
7976	def _reset ( self ) : ClientStream . _reset ( self ) self . available_auth_methods = None self . auth_stanza = None self . registration_callback = None
13436	def _setup_positions ( self , positions ) : updated_positions = [ ] for i , position in enumerate ( positions ) : ranger = re . search ( r'(?P<start>-?\d*):(?P<end>\d*)' , position ) if ranger : if i > 0 : updated_positions . append ( self . separator ) start = group_val ( ranger . group ( 'start' ) ) end = group_val ( ranger . group ( 'end' ) ) if start and end : updated_positions . extend ( self . _extendrange ( start , end + 1 ) ) elif ranger . group ( 'start' ) : updated_positions . append ( [ start ] ) else : updated_positions . extend ( self . _extendrange ( 1 , end + 1 ) ) else : updated_positions . append ( positions [ i ] ) try : if int ( position ) and int ( positions [ i + 1 ] ) : updated_positions . append ( self . separator ) except ( ValueError , IndexError ) : pass return updated_positions
7028	def objectid_search ( gaiaid , gaia_mirror = None , columns = ( 'source_id' , 'ra' , 'dec' , 'phot_g_mean_mag' , 'phot_bp_mean_mag' , 'phot_rp_mean_mag' , 'l' , 'b' , 'parallax, parallax_error' , 'pmra' , 'pmra_error' , 'pmdec' , 'pmdec_error' ) , returnformat = 'csv' , forcefetch = False , cachedir = '~/.astrobase/gaia-cache' , verbose = True , timeout = 15.0 , refresh = 2.0 , maxtimeout = 300.0 , maxtries = 3 , complete_query_later = True ) : query = ( "select {columns} from {{table}} where " "source_id = {gaiaid}" ) formatted_query = query . format ( columns = ', ' . join ( columns ) , gaiaid = gaiaid ) return tap_query ( formatted_query , gaia_mirror = gaia_mirror , returnformat = returnformat , forcefetch = forcefetch , cachedir = cachedir , verbose = verbose , timeout = timeout , refresh = refresh , maxtimeout = maxtimeout , maxtries = maxtries , complete_query_later = complete_query_later )
4854	def _create_transmissions ( self , content_metadata_item_map ) : ContentMetadataItemTransmission = apps . get_model ( 'integrated_channel' , 'ContentMetadataItemTransmission' ) transmissions = [ ] for content_id , channel_metadata in content_metadata_item_map . items ( ) : transmissions . append ( ContentMetadataItemTransmission ( enterprise_customer = self . enterprise_configuration . enterprise_customer , integrated_channel_code = self . enterprise_configuration . channel_code ( ) , content_id = content_id , channel_metadata = channel_metadata ) ) ContentMetadataItemTransmission . objects . bulk_create ( transmissions )
1938	def get_func_argument_types ( self , hsh : bytes ) : if not isinstance ( hsh , ( bytes , bytearray ) ) : raise TypeError ( 'The selector argument must be a concrete byte array' ) sig = self . _function_signatures_by_selector . get ( hsh ) return '()' if sig is None else sig [ sig . find ( '(' ) : ]
7856	def __response ( self , stanza ) : try : d = self . disco_class ( stanza . get_query ( ) ) self . got_it ( d ) except ValueError , e : self . error ( e )
4492	def fetch ( args ) : storage , remote_path = split_storage ( args . remote ) local_path = args . local if local_path is None : _ , local_path = os . path . split ( remote_path ) local_path_exists = os . path . exists ( local_path ) if local_path_exists and not args . force and not args . update : sys . exit ( "Local file %s already exists, not overwriting." % local_path ) directory , _ = os . path . split ( local_path ) if directory : makedirs ( directory , exist_ok = True ) osf = _setup_osf ( args ) project = osf . project ( args . project ) store = project . storage ( storage ) for file_ in store . files : if norm_remote_path ( file_ . path ) == remote_path : if local_path_exists and not args . force and args . update : if file_ . hashes . get ( 'md5' ) == checksum ( local_path ) : print ( "Local file %s already matches remote." % local_path ) break with open ( local_path , 'wb' ) as fp : file_ . write_to ( fp ) break
11554	def disable_digital_reporting ( self , pin ) : port = pin // 8 command = [ self . _command_handler . REPORT_DIGITAL + port , self . REPORTING_DISABLE ] self . _command_handler . send_command ( command )
5196	def configure_stack ( ) : stack_config = asiodnp3 . OutstationStackConfig ( opendnp3 . DatabaseSizes . AllTypes ( 10 ) ) stack_config . outstation . eventBufferConfig = opendnp3 . EventBufferConfig ( ) . AllTypes ( 10 ) stack_config . outstation . params . allowUnsolicited = True stack_config . link . LocalAddr = 10 stack_config . link . RemoteAddr = 1 stack_config . link . KeepAliveTimeout = openpal . TimeDuration ( ) . Max ( ) return stack_config
135	def change_first_point_by_coords ( self , x , y , max_distance = 1e-4 , raise_if_too_far_away = True ) : if len ( self . exterior ) == 0 : raise Exception ( "Cannot reorder polygon points, because it contains no points." ) closest_idx , closest_dist = self . find_closest_point_index ( x = x , y = y , return_distance = True ) if max_distance is not None and closest_dist > max_distance : if not raise_if_too_far_away : return self . deepcopy ( ) closest_point = self . exterior [ closest_idx , : ] raise Exception ( "Closest found point (%.9f, %.9f) exceeds max_distance of %.9f exceeded" % ( closest_point [ 0 ] , closest_point [ 1 ] , closest_dist ) ) return self . change_first_point_by_index ( closest_idx )
7185	def copy_type_comments_to_annotations ( args ) : for arg in args . args : copy_type_comment_to_annotation ( arg ) if args . vararg : copy_type_comment_to_annotation ( args . vararg ) for arg in args . kwonlyargs : copy_type_comment_to_annotation ( arg ) if args . kwarg : copy_type_comment_to_annotation ( args . kwarg )
6816	def optimize_wsgi_processes ( self ) : r = self . local_renderer r . env . wsgi_server_memory_gb = 8 verbose = self . verbose all_sites = list ( self . iter_sites ( site = ALL , setter = self . set_site_specifics ) )
9185	def _reassemble_binder ( id , tree , metadata ) : binder = cnxepub . Binder ( id , metadata = metadata ) for item in tree [ 'contents' ] : node = _node_to_model ( item , parent = binder ) if node . metadata [ 'title' ] != item [ 'title' ] : binder . set_title_for_node ( node , item [ 'title' ] ) return binder
2842	def write_gppu ( self , gppu = None ) : if gppu is not None : self . gppu = gppu self . _device . writeList ( self . GPPU , self . gppu )
7139	def spend_key ( self ) : key = self . _backend . spend_key ( ) if key == numbers . EMPTY_KEY : return None return key
11781	def compare ( algorithms = [ PluralityLearner , NaiveBayesLearner , NearestNeighborLearner , DecisionTreeLearner ] , datasets = [ iris , orings , zoo , restaurant , SyntheticRestaurant ( 20 ) , Majority ( 7 , 100 ) , Parity ( 7 , 100 ) , Xor ( 100 ) ] , k = 10 , trials = 1 ) : print_table ( [ [ a . __name__ . replace ( 'Learner' , '' ) ] + [ cross_validation ( a , d , k , trials ) for d in datasets ] for a in algorithms ] , header = [ '' ] + [ d . name [ 0 : 7 ] for d in datasets ] , numfmt = '%.2f' )
72	def deepcopy ( self ) : bbs = [ bb . deepcopy ( ) for bb in self . bounding_boxes ] return BoundingBoxesOnImage ( bbs , tuple ( self . shape ) )
844	def _rebuildPartitionIdMap ( self , partitionIdList ) : self . _partitionIdMap = { } for row , partitionId in enumerate ( partitionIdList ) : indices = self . _partitionIdMap . get ( partitionId , [ ] ) indices . append ( row ) self . _partitionIdMap [ partitionId ] = indices
3245	def get_inline_policies ( group , ** conn ) : policy_list = list_group_policies ( group [ 'GroupName' ] ) policy_documents = { } for policy in policy_list : policy_documents [ policy ] = get_group_policy_document ( group [ 'GroupName' ] , policy , ** conn ) return policy_documents
13479	def _hyphens_to_dashes ( self ) : problematic_hyphens = [ ( r'-([.,!)])' , r'---\1' ) , ( r'(?<=\d)-(?=\d)' , '--' ) , ( r'(?<=\s)-(?=\s)' , '---' ) ] for problem_case in problematic_hyphens : self . _regex_replacement ( * problem_case )
490	def acquireConnection ( self ) : self . _logger . debug ( "Acquiring connection" ) self . _conn . _ping_check ( ) connWrap = ConnectionWrapper ( dbConn = self . _conn , cursor = self . _conn . cursor ( ) , releaser = self . _releaseConnection , logger = self . _logger ) return connWrap
11503	def folder_children ( self , token , folder_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'id' ] = folder_id response = self . request ( 'midas.folder.children' , parameters ) return response
9561	def _apply_check_methods ( self , i , r , summarize = False , report_unexpected_exceptions = True , context = None ) : for a in dir ( self ) : if a . startswith ( 'check' ) : rdict = self . _as_dict ( r ) f = getattr ( self , a ) try : f ( rdict ) except RecordError as e : code = e . code if e . code is not None else RECORD_CHECK_FAILED p = { 'code' : code } if not summarize : message = e . message if e . message is not None else MESSAGES [ RECORD_CHECK_FAILED ] p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r if context is not None : p [ 'context' ] = context if e . details is not None : p [ 'details' ] = e . details yield p except Exception as e : if report_unexpected_exceptions : p = { 'code' : UNEXPECTED_EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED_EXCEPTION ] % ( e . __class__ . __name__ , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( f . __name__ , f . __doc__ ) if context is not None : p [ 'context' ] = context yield p
8203	def set_size ( self , size ) : if self . size is None : self . size = size return size else : return self . size
1863	def STOS ( cpu , dest , src ) : size = src . size dest . write ( src . read ( ) ) dest_reg = dest . mem . base increment = Operators . ITEBV ( { 'RDI' : 64 , 'EDI' : 32 , 'DI' : 16 } [ dest_reg ] , cpu . DF , - size // 8 , size // 8 ) cpu . write_register ( dest_reg , cpu . read_register ( dest_reg ) + increment )
1516	def make_tarfile ( output_filename , source_dir ) : with tarfile . open ( output_filename , "w:gz" ) as tar : tar . add ( source_dir , arcname = os . path . basename ( source_dir ) )
954	def getCallerInfo ( depth = 2 ) : f = sys . _getframe ( depth ) method_name = f . f_code . co_name filename = f . f_code . co_filename arg_class = None args = inspect . getargvalues ( f ) if len ( args [ 0 ] ) > 0 : arg_name = args [ 0 ] [ 0 ] arg_class = args [ 3 ] [ arg_name ] . __class__ . __name__ return ( method_name , filename , arg_class )
7960	def handle_err ( self ) : with self . lock : if self . _state == 'connecting' and self . _dst_addrs : self . _hup = False self . _set_state ( "connect" ) return self . _socket . close ( ) self . _socket = None self . _set_state ( "aborted" ) self . _write_queue . clear ( ) self . _write_queue_cond . notify ( ) raise PyXMPPIOError ( "Unhandled error on socket" )
2285	def predict ( self , df_data , graph = None , ** kwargs ) : if graph is None : return self . create_graph_from_data ( df_data , ** kwargs ) elif isinstance ( graph , nx . DiGraph ) : return self . orient_directed_graph ( df_data , graph , ** kwargs ) elif isinstance ( graph , nx . Graph ) : return self . orient_undirected_graph ( df_data , graph , ** kwargs ) else : print ( 'Unknown Graph type' ) raise ValueError
7045	def all_nonperiodic_features ( times , mags , errs , magsarefluxes = False , stetson_weightbytimediff = True ) : finiteind = npisfinite ( times ) & npisfinite ( mags ) & npisfinite ( errs ) ftimes , fmags , ferrs = times [ finiteind ] , mags [ finiteind ] , errs [ finiteind ] nzind = npnonzero ( ferrs ) ftimes , fmags , ferrs = ftimes [ nzind ] , fmags [ nzind ] , ferrs [ nzind ] xfeatures = nonperiodic_lightcurve_features ( times , mags , errs , magsarefluxes = magsarefluxes ) stetj = stetson_jindex ( ftimes , fmags , ferrs , weightbytimediff = stetson_weightbytimediff ) stetk = stetson_kindex ( fmags , ferrs ) xfeatures . update ( { 'stetsonj' : stetj , 'stetsonk' : stetk } ) return xfeatures
1109	def compare ( self , a , b ) : r cruncher = SequenceMatcher ( self . linejunk , a , b ) for tag , alo , ahi , blo , bhi in cruncher . get_opcodes ( ) : if tag == 'replace' : g = self . _fancy_replace ( a , alo , ahi , b , blo , bhi ) elif tag == 'delete' : g = self . _dump ( '-' , a , alo , ahi ) elif tag == 'insert' : g = self . _dump ( '+' , b , blo , bhi ) elif tag == 'equal' : g = self . _dump ( ' ' , a , alo , ahi ) else : raise ValueError , 'unknown tag %r' % ( tag , ) for line in g : yield line
9958	def restore_ipython ( self ) : if not self . is_ipysetup : return shell_class = type ( self . shell ) shell_class . showtraceback = shell_class . default_showtraceback del shell_class . default_showtraceback self . is_ipysetup = False
5552	def clip_bounds ( bounds = None , clip = None ) : bounds = Bounds ( * bounds ) clip = Bounds ( * clip ) return Bounds ( max ( bounds . left , clip . left ) , max ( bounds . bottom , clip . bottom ) , min ( bounds . right , clip . right ) , min ( bounds . top , clip . top ) )
8516	def _assert_all_finite ( X ) : X = np . asanyarray ( X ) if ( X . dtype . char in np . typecodes [ 'AllFloat' ] and not np . isfinite ( X . sum ( ) ) and not np . isfinite ( X ) . all ( ) ) : raise ValueError ( "Input contains NaN, infinity" " or a value too large for %r." % X . dtype )
8242	def outline ( path , colors , precision = 0.4 , continuous = True ) : def _point_count ( path , precision ) : return max ( int ( path . length * precision * 0.5 ) , 10 ) n = sum ( [ _point_count ( contour , precision ) for contour in path . contours ] ) contour_i = 0 contour_n = len ( path . contours ) - 1 if contour_n == 0 : continuous = False i = 0 for contour in path . contours : if not continuous : i = 0 j = _point_count ( contour , precision ) first = True for pt in contour . points ( j ) : if first : first = False else : if not continuous : clr = float ( i ) / j * len ( colors ) else : clr = float ( i ) / n * len ( colors ) - 1 * contour_i / contour_n _ctx . stroke ( colors [ int ( clr ) ] ) _ctx . line ( x0 , y0 , pt . x , pt . y ) x0 = pt . x y0 = pt . y i += 1 pt = contour . point ( 0.9999999 ) _ctx . line ( x0 , y0 , pt . x , pt . y ) contour_i += 1
13760	def _format_iso_time ( self , time ) : if isinstance ( time , str ) : return time elif isinstance ( time , datetime ) : return time . strftime ( '%Y-%m-%dT%H:%M:%S.%fZ' ) else : return None
4347	def tempo ( self , factor , audio_type = None , quick = False ) : if not is_number ( factor ) or factor <= 0 : raise ValueError ( "factor must be a positive number" ) if factor < 0.5 or factor > 2 : logger . warning ( "Using an extreme time stretching factor. " "Quality of results will be poor" ) if abs ( factor - 1.0 ) <= 0.1 : logger . warning ( "For this stretch factor, " "the stretch effect has better performance." ) if audio_type not in [ None , 'm' , 's' , 'l' ] : raise ValueError ( "audio_type must be one of None, 'm', 's', or 'l'." ) if not isinstance ( quick , bool ) : raise ValueError ( "quick must be a boolean." ) effect_args = [ 'tempo' ] if quick : effect_args . append ( '-q' ) if audio_type is not None : effect_args . append ( '-{}' . format ( audio_type ) ) effect_args . append ( '{:f}' . format ( factor ) ) self . effects . extend ( effect_args ) self . effects_log . append ( 'tempo' ) return self
11733	def isValidClass ( self , class_ ) : module = inspect . getmodule ( class_ ) valid = ( module in self . _valid_modules or ( hasattr ( module , '__file__' ) and module . __file__ in self . _valid_named_modules ) ) return valid and not private ( class_ )
5024	def get_enterprise_customer ( uuid ) : if uuid is None : return None try : return EnterpriseCustomer . active_customers . get ( uuid = uuid ) except EnterpriseCustomer . DoesNotExist : raise CommandError ( _ ( 'Enterprise customer {uuid} not found, or not active' ) . format ( uuid = uuid ) )
9589	def init ( self ) : resp = self . _execute ( Command . NEW_SESSION , { 'desiredCapabilities' : self . desired_capabilities } , False ) resp . raise_for_status ( ) self . session_id = str ( resp . session_id ) self . capabilities = resp . value
12938	def setDefaultRedisConnectionParams ( connectionParams ) : global _defaultRedisConnectionParams _defaultRedisConnectionParams . clear ( ) for key , value in connectionParams . items ( ) : _defaultRedisConnectionParams [ key ] = value clearRedisPools ( )
2102	def configure_model ( self , attrs , field_name ) : self . relationship = field_name self . _set_method_names ( relationship = field_name ) if self . res_name is None : self . res_name = grammar . singularize ( attrs . get ( 'endpoint' , 'unknown' ) . strip ( '/' ) )
4743	def exists ( ) : if env ( ) : cij . err ( "cij.nvm.exists: Invalid NVMe ENV." ) return 1 nvm = cij . env_to_dict ( PREFIX , EXPORTED + REQUIRED ) cmd = [ '[[ -b "%s" ]]' % nvm [ "DEV_PATH" ] ] rcode , _ , _ = cij . ssh . command ( cmd , shell = True , echo = False ) return rcode
1629	def GetHeaderGuardCPPVariable ( filename ) : filename = re . sub ( r'_flymake\.h$' , '.h' , filename ) filename = re . sub ( r'/\.flymake/([^/]*)$' , r'/\1' , filename ) filename = filename . replace ( 'C++' , 'cpp' ) . replace ( 'c++' , 'cpp' ) fileinfo = FileInfo ( filename ) file_path_from_root = fileinfo . RepositoryName ( ) if _root : suffix = os . sep if suffix == '\\' : suffix += '\\' file_path_from_root = re . sub ( '^' + _root + suffix , '' , file_path_from_root ) return re . sub ( r'[^a-zA-Z0-9]' , '_' , file_path_from_root ) . upper ( ) + '_'
13572	def download ( course , tid = None , dl_all = False , force = False , upgradejava = False , update = False ) : def dl ( id ) : download_exercise ( Exercise . get ( Exercise . tid == id ) , force = force , update_java = upgradejava , update = update ) if dl_all : for exercise in list ( course . exercises ) : dl ( exercise . tid ) elif tid is not None : dl ( int ( tid ) ) else : for exercise in list ( course . exercises ) : if not exercise . is_completed : dl ( exercise . tid ) else : exercise . update_downloaded ( )
12586	def get_nifti1hdr_from_h5attrs ( h5attrs ) : hdr = nib . Nifti1Header ( ) for k in list ( h5attrs . keys ( ) ) : hdr [ str ( k ) ] = np . array ( h5attrs [ k ] ) return hdr
4644	def get ( self , key , default = None ) : if key in self : return self . __getitem__ ( key ) else : return default
12043	def algo_exp ( x , m , t , b ) : return m * np . exp ( - t * x ) + b
2611	def serialize_object ( obj , buffer_threshold = MAX_BYTES , item_threshold = MAX_ITEMS ) : buffers = [ ] if istype ( obj , sequence_types ) and len ( obj ) < item_threshold : cobj = can_sequence ( obj ) for c in cobj : buffers . extend ( _extract_buffers ( c , buffer_threshold ) ) elif istype ( obj , dict ) and len ( obj ) < item_threshold : cobj = { } for k in sorted ( obj ) : c = can ( obj [ k ] ) buffers . extend ( _extract_buffers ( c , buffer_threshold ) ) cobj [ k ] = c else : cobj = can ( obj ) buffers . extend ( _extract_buffers ( cobj , buffer_threshold ) ) buffers . insert ( 0 , pickle . dumps ( cobj , PICKLE_PROTOCOL ) ) return buffers
1901	def get_value ( self , constraints , expression ) : if not issymbolic ( expression ) : return expression assert isinstance ( expression , ( Bool , BitVec , Array ) ) with constraints as temp_cs : if isinstance ( expression , Bool ) : var = temp_cs . new_bool ( ) elif isinstance ( expression , BitVec ) : var = temp_cs . new_bitvec ( expression . size ) elif isinstance ( expression , Array ) : var = [ ] result = [ ] for i in range ( expression . index_max ) : subvar = temp_cs . new_bitvec ( expression . value_bits ) var . append ( subvar ) temp_cs . add ( subvar == simplify ( expression [ i ] ) ) self . _reset ( temp_cs ) if not self . _is_sat ( ) : raise SolverError ( 'Model is not available' ) for i in range ( expression . index_max ) : self . _send ( '(get-value (%s))' % var [ i ] . name ) ret = self . _recv ( ) assert ret . startswith ( '((' ) and ret . endswith ( '))' ) pattern , base = self . _get_value_fmt m = pattern . match ( ret ) expr , value = m . group ( 'expr' ) , m . group ( 'value' ) result . append ( int ( value , base ) ) return bytes ( result ) temp_cs . add ( var == expression ) self . _reset ( temp_cs ) if not self . _is_sat ( ) : raise SolverError ( 'Model is not available' ) self . _send ( '(get-value (%s))' % var . name ) ret = self . _recv ( ) if not ( ret . startswith ( '((' ) and ret . endswith ( '))' ) ) : raise SolverError ( 'SMTLIB error parsing response: %s' % ret ) if isinstance ( expression , Bool ) : return { 'true' : True , 'false' : False } [ ret [ 2 : - 2 ] . split ( ' ' ) [ 1 ] ] if isinstance ( expression , BitVec ) : pattern , base = self . _get_value_fmt m = pattern . match ( ret ) expr , value = m . group ( 'expr' ) , m . group ( 'value' ) return int ( value , base ) raise NotImplementedError ( "get_value only implemented for Bool and BitVec" )
8597	def delete_group ( self , group_id ) : response = self . _perform_request ( url = '/um/groups/%s' % group_id , method = 'DELETE' ) return response
6584	def station_selection_menu ( self , error = None ) : self . screen . clear ( ) if error : self . screen . print_error ( "{}\n" . format ( error ) ) for i , station in enumerate ( self . stations ) : i = "{:>3}" . format ( i ) print ( "{}: {}" . format ( Colors . yellow ( i ) , station . name ) ) return self . stations [ self . screen . get_integer ( "Station: " ) ]
7697	def from_xml ( cls , element ) : if element . tag != ITEM_TAG : raise ValueError ( "{0!r} is not a roster item" . format ( element ) ) try : jid = JID ( element . get ( "jid" ) ) except ValueError : raise BadRequestProtocolError ( u"Bad item JID" ) subscription = element . get ( "subscription" ) ask = element . get ( "ask" ) name = element . get ( "name" ) duplicate_group = False groups = set ( ) for child in element : if child . tag != GROUP_TAG : continue group = child . text if group is None : group = u"" if group in groups : duplicate_group = True else : groups . add ( group ) approved = element . get ( "approved" ) if approved == "true" : approved = True elif approved in ( "false" , None ) : approved = False else : logger . debug ( "RosterItem.from_xml: got unknown 'approved':" " {0!r}, changing to False" . format ( approved ) ) approved = False result = cls ( jid , name , groups , subscription , ask , approved ) result . _duplicate_group = duplicate_group return result
4584	def image_to_colorlist ( image , container = list ) : deprecated . deprecated ( 'util.gif.image_to_colorlist' ) return container ( convert_mode ( image ) . getdata ( ) )
526	def _inhibitColumnsLocal ( self , overlaps , density ) : activeArray = numpy . zeros ( self . _numColumns , dtype = "bool" ) for column , overlap in enumerate ( overlaps ) : if overlap >= self . _stimulusThreshold : neighborhood = self . _getColumnNeighborhood ( column ) neighborhoodOverlaps = overlaps [ neighborhood ] numBigger = numpy . count_nonzero ( neighborhoodOverlaps > overlap ) ties = numpy . where ( neighborhoodOverlaps == overlap ) tiedNeighbors = neighborhood [ ties ] numTiesLost = numpy . count_nonzero ( activeArray [ tiedNeighbors ] ) numActive = int ( 0.5 + density * len ( neighborhood ) ) if numBigger + numTiesLost < numActive : activeArray [ column ] = True return activeArray . nonzero ( ) [ 0 ]
7683	def display_multi ( annotations , fig_kw = None , meta = True , ** kwargs ) : if fig_kw is None : fig_kw = dict ( ) fig_kw . setdefault ( 'sharex' , True ) fig_kw . setdefault ( 'squeeze' , True ) display_annotations = [ ] for ann in annotations : for namespace in VIZ_MAPPING : if can_convert ( ann , namespace ) : display_annotations . append ( ann ) break if not len ( display_annotations ) : raise ParameterError ( 'No displayable annotations found' ) fig , axs = plt . subplots ( nrows = len ( display_annotations ) , ncols = 1 , ** fig_kw ) if len ( display_annotations ) == 1 : axs = [ axs ] for ann , ax in zip ( display_annotations , axs ) : kwargs [ 'ax' ] = ax display ( ann , meta = meta , ** kwargs ) return fig , axs
7870	def _decode_error ( self ) : error_qname = self . _ns_prefix + "error" for child in self . _element : if child . tag == error_qname : self . _error = StanzaErrorElement ( child ) return raise BadRequestProtocolError ( "Error element missing in" " an error stanza" )
2440	def add_annotator ( self , doc , annotator ) : self . reset_annotations ( ) if validations . validate_annotator ( annotator ) : doc . add_annotation ( annotation . Annotation ( annotator = annotator ) ) return True else : raise SPDXValueError ( 'Annotation::Annotator' )
1577	def make_shell_logfiles_url ( host , shell_port , _ , instance_id = None ) : if not shell_port : return None if not instance_id : return "http://%s:%d/browse/log-files" % ( host , shell_port ) else : return "http://%s:%d/file/log-files/%s.log.0" % ( host , shell_port , instance_id )
1189	def _randbelow ( self , n ) : k = _int_bit_length ( n ) r = self . getrandbits ( k ) while r >= n : r = self . getrandbits ( k ) return r
2832	def get_platform_pwm ( ** keywords ) : plat = Platform . platform_detect ( ) if plat == Platform . RASPBERRY_PI : import RPi . GPIO return RPi_PWM_Adapter ( RPi . GPIO , ** keywords ) elif plat == Platform . BEAGLEBONE_BLACK : import Adafruit_BBIO . PWM return BBIO_PWM_Adapter ( Adafruit_BBIO . PWM , ** keywords ) elif plat == Platform . UNKNOWN : raise RuntimeError ( 'Could not determine platform.' )
7287	def set_form_fields ( self , form_field_dict , parent_key = None , field_type = None ) : for form_key , field_value in form_field_dict . items ( ) : form_key = make_key ( parent_key , form_key ) if parent_key is not None else form_key if isinstance ( field_value , tuple ) : set_list_class = False base_key = form_key if ListField in ( field_value . field_type , field_type ) : if parent_key is None or ListField == field_value . field_type : if field_type != EmbeddedDocumentField : field_value . widget . attrs [ 'class' ] += ' listField {0}' . format ( form_key ) set_list_class = True else : field_value . widget . attrs [ 'class' ] += ' listField' list_keys = [ field_key for field_key in self . form . fields . keys ( ) if has_digit ( field_key ) ] key_int = 0 while form_key in list_keys : key_int += 1 form_key = make_key ( form_key , key_int ) if parent_key is not None : valid_base_keys = [ model_key for model_key in self . model_map_dict . keys ( ) if not model_key . startswith ( "_" ) ] while base_key not in valid_base_keys and base_key : base_key = make_key ( base_key , exclude_last_string = True ) embedded_key_class = None if set_list_class : field_value . widget . attrs [ 'class' ] += " listField" . format ( base_key ) embedded_key_class = make_key ( field_key , exclude_last_string = True ) field_value . widget . attrs [ 'class' ] += " embeddedField" if base_key == parent_key : field_value . widget . attrs [ 'class' ] += ' {0}' . format ( base_key ) else : field_value . widget . attrs [ 'class' ] += ' {0} {1}' . format ( base_key , parent_key ) if embedded_key_class is not None : field_value . widget . attrs [ 'class' ] += ' {0}' . format ( embedded_key_class ) default_value = self . get_field_value ( form_key ) if isinstance ( default_value , list ) and len ( default_value ) > 0 : key_index = int ( form_key . split ( "_" ) [ - 1 ] ) new_base_key = make_key ( form_key , exclude_last_string = True ) for list_value in default_value : list_widget = deepcopy ( field_value . widget ) new_key = make_key ( new_base_key , six . text_type ( key_index ) ) list_widget . attrs [ 'class' ] += " {0}" . format ( make_key ( base_key , key_index ) ) self . set_form_field ( list_widget , field_value . document_field , new_key , list_value ) key_index += 1 else : self . set_form_field ( field_value . widget , field_value . document_field , form_key , default_value ) elif isinstance ( field_value , dict ) : self . set_form_fields ( field_value , form_key , field_value . get ( "_field_type" , None ) )
1461	def import_and_get_class ( path_to_pex , python_class_name ) : abs_path_to_pex = os . path . abspath ( path_to_pex ) Log . debug ( "Add a pex to the path: %s" % abs_path_to_pex ) Log . debug ( "In import_and_get_class with cls_name: %s" % python_class_name ) split = python_class_name . split ( '.' ) from_path = '.' . join ( split [ : - 1 ] ) import_name = python_class_name . split ( '.' ) [ - 1 ] Log . debug ( "From path: %s, import name: %s" % ( from_path , import_name ) ) if python_class_name . startswith ( "heron." ) : try : mod = resolve_heron_suffix_issue ( abs_path_to_pex , python_class_name ) return getattr ( mod , import_name ) except : Log . error ( "Could not resolve class %s with special handling" % python_class_name ) mod = __import__ ( from_path , fromlist = [ import_name ] , level = - 1 ) Log . debug ( "Imported module: %s" % str ( mod ) ) return getattr ( mod , import_name )
4751	def run ( self , shell = True , cmdline = False , echo = True ) : if env ( ) : return 1 cmd = [ "fio" ] + self . __parse_parms ( ) if cmdline : cij . emph ( "cij.fio.run: shell: %r, cmd: %r" % ( shell , cmd ) ) return cij . ssh . command ( cmd , shell , echo )
9832	def value ( self , ascode = None ) : if ascode is None : ascode = self . code return self . cast [ ascode ] ( self . text )
8324	def isString ( s ) : try : return isinstance ( s , unicode ) or isinstance ( s , basestring ) except NameError : return isinstance ( s , str )
4569	def dump ( data , file = sys . stdout , use_yaml = None , ** kwds ) : if use_yaml is None : use_yaml = ALWAYS_DUMP_YAML def dump ( fp ) : if use_yaml : yaml . safe_dump ( data , stream = fp , ** kwds ) else : json . dump ( data , fp , indent = 4 , sort_keys = True , ** kwds ) if not isinstance ( file , str ) : return dump ( file ) if os . path . isabs ( file ) : parent = os . path . dirname ( file ) if not os . path . exists ( parent ) : os . makedirs ( parent , exist_ok = True ) with open ( file , 'w' ) as fp : return dump ( fp )
9803	def set ( verbose , host , http_port , ws_port , use_https , verify_ssl ) : _config = GlobalConfigManager . get_config_or_default ( ) if verbose is not None : _config . verbose = verbose if host is not None : _config . host = host if http_port is not None : _config . http_port = http_port if ws_port is not None : _config . ws_port = ws_port if use_https is not None : _config . use_https = use_https if verify_ssl is False : _config . verify_ssl = verify_ssl GlobalConfigManager . set_config ( _config ) Printer . print_success ( 'Config was updated.' ) CliConfigManager . purge ( )
6704	def togroups ( self , user , groups ) : r = self . local_renderer if isinstance ( groups , six . string_types ) : groups = [ _ . strip ( ) for _ in groups . split ( ',' ) if _ . strip ( ) ] for group in groups : r . env . username = user r . env . group = group r . sudo ( 'groupadd --force {group}' ) r . sudo ( 'adduser {username} {group}' )
12326	def init ( globalvars = None , show = False ) : global config profileini = getprofileini ( ) if os . path . exists ( profileini ) : config = configparser . ConfigParser ( ) config . read ( profileini ) mgr = plugins_get_mgr ( ) mgr . update_configs ( config ) if show : for source in config : print ( "[%s] :" % ( source ) ) for k in config [ source ] : print ( " %s : %s" % ( k , config [ source ] [ k ] ) ) else : print ( "Profile does not exist. So creating one" ) if not show : update ( globalvars ) print ( "Complete init" )
12908	def load ( cls , fh ) : dat = fh . read ( ) try : ret = cls . from_json ( dat ) except : ret = cls . from_yaml ( dat ) return ret
12482	def find_in_sections ( var_name , app_name ) : sections = get_sections ( app_name ) if not sections : raise ValueError ( 'No sections found in {} rcfiles.' . format ( app_name ) ) for s in sections : try : var_value = get_rcfile_variable_value ( var_name , section_name = s , app_name = app_name ) except : pass else : return s , var_value raise KeyError ( 'No variable {} has been found in {} ' 'rcfiles.' . format ( var_name , app_name ) )
5233	def all_folders ( path_name , keyword = '' , has_date = False , date_fmt = DATE_FMT ) -> list : if not os . path . exists ( path = path_name ) : return [ ] path_name = path_name . replace ( '\\' , '/' ) if keyword : folders = sort_by_modified ( [ f . replace ( '\\' , '/' ) for f in glob . iglob ( f'{path_name}/*{keyword}*' ) if os . path . isdir ( f ) and ( f . replace ( '\\' , '/' ) . split ( '/' ) [ - 1 ] [ 0 ] != '~' ) ] ) else : folders = sort_by_modified ( [ f'{path_name}/{f}' for f in os . listdir ( path = path_name ) if os . path . isdir ( f'{path_name}/{f}' ) and ( f [ 0 ] != '~' ) ] ) if has_date : folders = filter_by_dates ( folders , date_fmt = date_fmt ) return folders
11416	def record_add_subfield_into ( rec , tag , subfield_code , value , subfield_position = None , field_position_global = None , field_position_local = None ) : subfields = record_get_subfields ( rec , tag , field_position_global = field_position_global , field_position_local = field_position_local ) if subfield_position is None : subfields . append ( ( subfield_code , value ) ) else : subfields . insert ( subfield_position , ( subfield_code , value ) )
1034	def decode ( input , output ) : while True : line = input . readline ( ) if not line : break s = binascii . a2b_base64 ( line ) output . write ( s )
5271	def lcs ( self , stringIdxs = - 1 ) : if stringIdxs == - 1 or not isinstance ( stringIdxs , list ) : stringIdxs = set ( range ( len ( self . word_starts ) ) ) else : stringIdxs = set ( stringIdxs ) deepestNode = self . _find_lcs ( self . root , stringIdxs ) start = deepestNode . idx end = deepestNode . idx + deepestNode . depth return self . word [ start : end ]
1597	def format_prefix ( filename , sres ) : try : pwent = pwd . getpwuid ( sres . st_uid ) user = pwent . pw_name except KeyError : user = sres . st_uid try : grent = grp . getgrgid ( sres . st_gid ) group = grent . gr_name except KeyError : group = sres . st_gid return '%s %3d %10s %10s %10d %s' % ( format_mode ( sres ) , sres . st_nlink , user , group , sres . st_size , format_mtime ( sres . st_mtime ) , )
11546	def SIRode ( y0 , time , beta , gamma ) : Xsim = rk4 ( SIR_D , y0 , time , args = ( beta , gamma , ) ) Xsim = Xsim . transpose ( ) return Xsim
9787	def init ( project , polyaxonfile ) : user , project_name = get_project_or_local ( project ) try : project_config = PolyaxonClient ( ) . project . get_project ( user , project_name ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Make sure you have a project with this name `{}`' . format ( project ) ) Printer . print_error ( 'You can a create new project with this command: ' 'polyaxon project create ' '--name={} [--description=...] [--tags=...]' . format ( project_name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) init_project = False if ProjectManager . is_initialized ( ) : local_project = ProjectManager . get_config ( ) click . echo ( 'Warning! This project is already initialized with the following project:' ) with clint . textui . indent ( 4 ) : clint . textui . puts ( 'User: {}' . format ( local_project . user ) ) clint . textui . puts ( 'Project: {}' . format ( local_project . name ) ) if click . confirm ( 'Would you like to override this current config?' , default = False ) : init_project = True else : init_project = True if init_project : ProjectManager . purge ( ) ProjectManager . set_config ( project_config , init = True ) Printer . print_success ( 'Project was initialized' ) else : Printer . print_header ( 'Project config was not changed.' ) init_ignore = False if IgnoreManager . is_initialized ( ) : click . echo ( 'Warning! Found a .polyaxonignore file.' ) if click . confirm ( 'Would you like to override it?' , default = False ) : init_ignore = True else : init_ignore = True if init_ignore : IgnoreManager . init_config ( ) Printer . print_success ( 'New .polyaxonignore file was created.' ) else : Printer . print_header ( '.polyaxonignore file was not changed.' ) if polyaxonfile : create_polyaxonfile ( )
7382	def has_edge_within_group ( self , group ) : assert group in self . nodes . keys ( ) , "{0} not one of the group of nodes" . format ( group ) nodelist = self . nodes [ group ] for n1 , n2 in self . simplified_edges ( ) : if n1 in nodelist and n2 in nodelist : return True
13636	def contentEncoding ( requestHeaders , encoding = None ) : if encoding is None : encoding = b'utf-8' headers = _splitHeaders ( requestHeaders . getRawHeaders ( b'Content-Type' , [ ] ) ) if headers : return headers [ 0 ] [ 1 ] . get ( b'charset' , encoding ) return encoding
3341	def parse_xml_body ( environ , allow_empty = False ) : clHeader = environ . get ( "CONTENT_LENGTH" , "" ) . strip ( ) if clHeader == "" : requestbody = "" else : try : content_length = int ( clHeader ) if content_length < 0 : raise DAVError ( HTTP_BAD_REQUEST , "Negative content-length." ) except ValueError : raise DAVError ( HTTP_BAD_REQUEST , "content-length is not numeric." ) if content_length == 0 : requestbody = "" else : requestbody = environ [ "wsgi.input" ] . read ( content_length ) environ [ "wsgidav.all_input_read" ] = 1 if requestbody == "" : if allow_empty : return None else : raise DAVError ( HTTP_BAD_REQUEST , "Body must not be empty." ) try : rootEL = etree . fromstring ( requestbody ) except Exception as e : raise DAVError ( HTTP_BAD_REQUEST , "Invalid XML format." , src_exception = e ) if environ . get ( "wsgidav.dump_request_body" ) : _logger . info ( "{} XML request body:\n{}" . format ( environ [ "REQUEST_METHOD" ] , compat . to_native ( xml_to_bytes ( rootEL , pretty_print = True ) ) , ) ) environ [ "wsgidav.dump_request_body" ] = False return rootEL
11256	def flatten ( prev , depth = sys . maxsize ) : def inner_flatten ( iterable , curr_level , max_levels ) : for i in iterable : if hasattr ( i , '__iter__' ) and curr_level < max_levels : for j in inner_flatten ( i , curr_level + 1 , max_levels ) : yield j else : yield i for d in prev : if hasattr ( d , '__iter__' ) and depth > 0 : for inner_d in inner_flatten ( d , 1 , depth ) : yield inner_d else : yield d
1410	def filter_bolts ( table , header ) : bolts_info = [ ] for row in table : if row [ 0 ] == 'bolt' : bolts_info . append ( row ) return bolts_info , header
5910	def delete_frames ( self ) : for frame in glob . glob ( self . frameglob ) : os . unlink ( frame )
2283	def check_R_package ( self , package ) : test_package = not bool ( launch_R_script ( "{}/R_templates/test_import.R" . format ( os . path . dirname ( os . path . realpath ( __file__ ) ) ) , { "{package}" : package } , verbose = True ) ) return test_package
13763	def _wrap_color ( self , code , text , format = None , style = None ) : color = None if code [ : 3 ] == self . bg . PREFIX : color = self . bg . COLORS . get ( code , None ) if not color : color = self . fg . COLORS . get ( code , None ) if not color : raise Exception ( 'Color code not found' ) if format and format not in self . formats : raise Exception ( 'Color format not found' ) fmt = "0;" if format == 'bold' : fmt = "1;" elif format == 'underline' : fmt = "4;" parts = color . split ( '[' ) color = '{0}[{1}{2}' . format ( parts [ 0 ] , fmt , parts [ 1 ] ) if self . has_colors and self . colors_enabled : st = '' if style : st = self . st . COLORS . get ( style , '' ) return "{0}{1}{2}{3}" . format ( st , color , text , self . st . COLORS [ 'reset_all' ] ) else : return text
7713	def handle_roster_push ( self , stanza ) : if self . server is None and stanza . from_jid : logger . debug ( u"Server address not known, cannot verify roster push" " from {0}" . format ( stanza . from_jid ) ) return stanza . make_error_response ( u"service-unavailable" ) if self . server and stanza . from_jid and stanza . from_jid != self . server : logger . debug ( u"Roster push from invalid source: {0}" . format ( stanza . from_jid ) ) return stanza . make_error_response ( u"service-unavailable" ) payload = stanza . get_payload ( RosterPayload ) if len ( payload ) != 1 : logger . warning ( "Bad roster push received ({0} items)" . format ( len ( payload ) ) ) return stanza . make_error_response ( u"bad-request" ) if self . roster is None : logger . debug ( "Dropping roster push - no roster here" ) return True item = payload [ 0 ] item . verify_roster_push ( True ) old_item = self . roster . get ( item . jid ) if item . subscription == "remove" : if old_item : self . roster . remove_item ( item . jid ) else : self . roster . add_item ( item , replace = True ) self . _event_queue . put ( RosterUpdatedEvent ( self , old_item , item ) ) return stanza . make_result_response ( )
12520	def _load_images_and_labels ( self , images , labels = None ) : if not isinstance ( images , ( list , tuple ) ) : raise ValueError ( 'Expected an iterable (list or tuple) of strings or img-like objects. ' 'Got a {}.' . format ( type ( images ) ) ) if not len ( images ) > 0 : raise ValueError ( 'Expected an iterable (list or tuple) of strings or img-like objects ' 'of size higher than 0. Got {} items.' . format ( len ( images ) ) ) if labels is not None and len ( labels ) != len ( images ) : raise ValueError ( 'Expected the same length for image set ({}) and ' 'labels list ({}).' . format ( len ( images ) , len ( labels ) ) ) first_file = images [ 0 ] if first_file : first_img = NeuroImage ( first_file ) else : raise ( 'Error reading image {}.' . format ( repr_imgs ( first_file ) ) ) for idx , image in enumerate ( images ) : try : img = NeuroImage ( image ) self . check_compatibility ( img , first_img ) except : log . exception ( 'Error reading image {}.' . format ( repr_imgs ( image ) ) ) raise else : self . items . append ( img ) self . set_labels ( labels )
4913	def courses ( self , request , pk = None ) : enterprise_customer = self . get_object ( ) self . check_object_permissions ( request , enterprise_customer ) self . ensure_data_exists ( request , enterprise_customer . catalog , error_message = "No catalog is associated with Enterprise {enterprise_name} from endpoint '{path}'." . format ( enterprise_name = enterprise_customer . name , path = request . get_full_path ( ) ) ) catalog_api = CourseCatalogApiClient ( request . user , enterprise_customer . site ) courses = catalog_api . get_paginated_catalog_courses ( enterprise_customer . catalog , request . GET ) self . ensure_data_exists ( request , courses , error_message = ( "Unable to fetch API response for catalog courses for " "Enterprise {enterprise_name} from endpoint '{path}'." . format ( enterprise_name = enterprise_customer . name , path = request . get_full_path ( ) ) ) ) serializer = serializers . EnterpriseCatalogCoursesReadOnlySerializer ( courses ) serializer . update_enterprise_courses ( enterprise_customer , catalog_id = enterprise_customer . catalog ) return get_paginated_response ( serializer . data , request )
6019	def from_inverse_noise_map ( cls , pixel_scale , inverse_noise_map ) : noise_map = 1.0 / inverse_noise_map return NoiseMap ( array = noise_map , pixel_scale = pixel_scale )
2337	def remove_indirect_links ( g , alg = "aracne" , ** kwargs ) : alg = { "aracne" : aracne , "nd" : network_deconvolution , "clr" : clr } [ alg ] mat = np . array ( nx . adjacency_matrix ( g ) . todense ( ) ) return nx . relabel_nodes ( nx . DiGraph ( alg ( mat , ** kwargs ) ) , { idx : i for idx , i in enumerate ( list ( g . nodes ( ) ) ) } )
12781	def set_name ( self , name ) : if not self . _campfire . get_user ( ) . admin : return False result = self . _connection . put ( "room/%s" % self . id , { "room" : { "name" : name } } ) if result [ "success" ] : self . _load ( ) return result [ "success" ]
959	def aggregationDivide ( dividend , divisor ) : dividendMonthSec = aggregationToMonthsSeconds ( dividend ) divisorMonthSec = aggregationToMonthsSeconds ( divisor ) if ( dividendMonthSec [ 'months' ] != 0 and divisorMonthSec [ 'seconds' ] != 0 ) or ( dividendMonthSec [ 'seconds' ] != 0 and divisorMonthSec [ 'months' ] != 0 ) : raise RuntimeError ( "Aggregation dicts with months/years can only be " "inter-operated with other aggregation dicts that contain " "months/years" ) if dividendMonthSec [ 'months' ] > 0 : return float ( dividendMonthSec [ 'months' ] ) / divisor [ 'months' ] else : return float ( dividendMonthSec [ 'seconds' ] ) / divisorMonthSec [ 'seconds' ]
1381	def getComponentExceptionSummary ( self , tmaster , component_name , instances = [ ] , callback = None ) : if not tmaster or not tmaster . host or not tmaster . stats_port : return exception_request = tmaster_pb2 . ExceptionLogRequest ( ) exception_request . component_name = component_name if len ( instances ) > 0 : exception_request . instances . extend ( instances ) request_str = exception_request . SerializeToString ( ) port = str ( tmaster . stats_port ) host = tmaster . host url = "http://{0}:{1}/exceptionsummary" . format ( host , port ) Log . debug ( "Creating request object." ) request = tornado . httpclient . HTTPRequest ( url , body = request_str , method = 'POST' , request_timeout = 5 ) Log . debug ( 'Making HTTP call to fetch exceptionsummary url: %s' , url ) try : client = tornado . httpclient . AsyncHTTPClient ( ) result = yield client . fetch ( request ) Log . debug ( "HTTP call complete." ) except tornado . httpclient . HTTPError as e : raise Exception ( str ( e ) ) responseCode = result . code if responseCode >= 400 : message = "Error in getting exceptions from Tmaster, code: " + responseCode Log . error ( message ) raise tornado . gen . Return ( { "message" : message } ) exception_response = tmaster_pb2 . ExceptionLogResponse ( ) exception_response . ParseFromString ( result . body ) if exception_response . status . status == common_pb2 . NOTOK : if exception_response . status . HasField ( "message" ) : raise tornado . gen . Return ( { "message" : exception_response . status . message } ) ret = [ ] for exception_log in exception_response . exceptions : ret . append ( { 'class_name' : exception_log . stacktrace , 'lasttime' : exception_log . lasttime , 'firsttime' : exception_log . firsttime , 'count' : str ( exception_log . count ) } ) raise tornado . gen . Return ( ret )
4578	def set_one ( desc , name , value ) : old_value = desc . get ( name ) if old_value is None : raise KeyError ( 'No section "%s"' % name ) if value is None : value = type ( old_value ) ( ) elif name in CLASS_SECTIONS : if isinstance ( value , str ) : value = { 'typename' : aliases . resolve ( value ) } elif isinstance ( value , type ) : value = { 'typename' : class_name . class_name ( value ) } elif not isinstance ( value , dict ) : raise TypeError ( 'Expected dict, str or type, got "%s"' % value ) typename = value . get ( 'typename' ) if typename : s = 's' if name == 'driver' else '' path = 'bibliopixel.' + name + s importer . import_symbol ( typename , path ) elif name == 'shape' : if not isinstance ( value , ( list , int , tuple , str ) ) : raise TypeError ( 'Expected shape, got "%s"' % value ) elif type ( old_value ) is not type ( value ) : raise TypeError ( 'Expected %s but got "%s" of type %s' % ( type ( old_value ) , value , type ( value ) ) ) desc [ name ] = value
12442	def require_accessibility ( self , user , method ) : if method == 'OPTIONS' : return authz = self . meta . authorization if not authz . is_accessible ( user , method , self ) : authz . unaccessible ( )
12971	def getMultiple ( self , pks , cascadeFetch = False ) : if type ( pks ) == set : pks = list ( pks ) if len ( pks ) == 1 : return IRQueryableList ( [ self . get ( pks [ 0 ] , cascadeFetch = cascadeFetch ) ] , mdl = self . mdl ) conn = self . _get_connection ( ) pipeline = conn . pipeline ( ) for pk in pks : key = self . _get_key_for_id ( pk ) pipeline . hgetall ( key ) res = pipeline . execute ( ) ret = IRQueryableList ( mdl = self . mdl ) i = 0 pksLen = len ( pks ) while i < pksLen : if res [ i ] is None : ret . append ( None ) i += 1 continue res [ i ] [ '_id' ] = pks [ i ] obj = self . _redisResultToObj ( res [ i ] ) ret . append ( obj ) i += 1 if cascadeFetch is True : for obj in ret : if not obj : continue self . _doCascadeFetch ( obj ) return ret
12012	def do_photometry ( self ) : std_f = np . zeros ( 4 ) data_save = np . zeros_like ( self . postcard ) self . obs_flux = np . zeros_like ( self . reference_flux ) for i in range ( 4 ) : g = np . where ( self . qs == i ) [ 0 ] wh = np . where ( self . times [ g ] > 54947 ) data_save [ g ] = np . roll ( self . postcard [ g ] , int ( self . roll_best [ i , 0 ] ) , axis = 1 ) data_save [ g ] = np . roll ( data_save [ g ] , int ( self . roll_best [ i , 1 ] ) , axis = 2 ) self . target_flux_pixels = data_save [ : , self . targets == 1 ] self . target_flux = np . sum ( self . target_flux_pixels , axis = 1 ) self . obs_flux [ g ] = self . target_flux [ g ] / self . reference_flux [ g ] self . obs_flux [ g ] /= np . median ( self . obs_flux [ g [ wh ] ] ) fitline = np . polyfit ( self . times [ g ] [ wh ] , self . obs_flux [ g ] [ wh ] , 1 ) std_f [ i ] = np . max ( [ np . std ( self . obs_flux [ g ] [ wh ] / ( fitline [ 0 ] * self . times [ g ] [ wh ] + fitline [ 1 ] ) ) , 0.001 ] ) self . flux_uncert = std_f
79	def Dropout ( p = 0 , per_channel = False , name = None , deterministic = False , random_state = None ) : if ia . is_single_number ( p ) : p2 = iap . Binomial ( 1 - p ) elif ia . is_iterable ( p ) : ia . do_assert ( len ( p ) == 2 ) ia . do_assert ( p [ 0 ] < p [ 1 ] ) ia . do_assert ( 0 <= p [ 0 ] <= 1.0 ) ia . do_assert ( 0 <= p [ 1 ] <= 1.0 ) p2 = iap . Binomial ( iap . Uniform ( 1 - p [ 1 ] , 1 - p [ 0 ] ) ) elif isinstance ( p , iap . StochasticParameter ) : p2 = p else : raise Exception ( "Expected p to be float or int or StochasticParameter, got %s." % ( type ( p ) , ) ) if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return MultiplyElementwise ( p2 , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )
10935	def check_update_J ( self ) : self . _J_update_counter += 1 update = self . _J_update_counter >= self . update_J_frequency return update & ( not self . _fresh_JTJ )
9693	def cut ( self , by , from_start = True ) : s , e = copy ( self . start ) , copy ( self . end ) if from_start : e = s + by else : s = e - by return Range ( s , e )
4607	def blacklist ( self , account ) : assert callable ( self . blockchain . account_whitelist ) return self . blockchain . account_whitelist ( account , lists = [ "black" ] , account = self )
12249	def get_key ( self , * args , ** kwargs ) : if kwargs . pop ( 'force' , None ) : headers = kwargs . get ( 'headers' , { } ) headers [ 'force' ] = True kwargs [ 'headers' ] = headers return super ( Bucket , self ) . get_key ( * args , ** kwargs )
9913	def _create ( cls , model_class , * args , ** kwargs ) : manager = cls . _get_manager ( model_class ) return manager . create_user ( * args , ** kwargs )
8910	def owsproxy_delegate ( request ) : twitcher_url = request . registry . settings . get ( 'twitcher.url' ) protected_path = request . registry . settings . get ( 'twitcher.ows_proxy_protected_path' , '/ows' ) url = twitcher_url + protected_path + '/proxy' if request . matchdict . get ( 'service_name' ) : url += '/' + request . matchdict . get ( 'service_name' ) if request . matchdict . get ( 'access_token' ) : url += '/' + request . matchdict . get ( 'service_name' ) url += '?' + urlparse . urlencode ( request . params ) LOGGER . debug ( "delegate to owsproxy: %s" , url ) resp = requests . request ( method = request . method . upper ( ) , url = url , data = request . body , headers = request . headers , verify = False ) return Response ( resp . content , status = resp . status_code , headers = resp . headers )
9355	def money ( min = 0 , max = 10 ) : value = random . choice ( range ( min * 100 , max * 100 ) ) return "%1.2f" % ( float ( value ) / 100 )
881	def compute ( self , activeColumns , learn = True ) : self . activateCells ( sorted ( activeColumns ) , learn ) self . activateDendrites ( learn )
8206	def overlap ( self , x1 , y1 , x2 , y2 , r = 5 ) : if abs ( x2 - x1 ) < r and abs ( y2 - y1 ) < r : return True else : return False
10904	def compare_data_model_residuals ( s , tile , data_vmin = 'calc' , data_vmax = 'calc' , res_vmin = - 0.1 , res_vmax = 0.1 , edgepts = 'calc' , do_imshow = True , data_cmap = plt . cm . bone , res_cmap = plt . cm . RdBu ) : residuals = s . residuals [ tile . slicer ] . squeeze ( ) data = s . data [ tile . slicer ] . squeeze ( ) model = s . model [ tile . slicer ] . squeeze ( ) if data . ndim != 2 : raise ValueError ( 'tile does not give a 2D slice' ) im = np . zeros ( [ data . shape [ 0 ] , data . shape [ 1 ] , 4 ] ) if data_vmin == 'calc' : data_vmin = 0.5 * ( data . min ( ) + model . min ( ) ) if data_vmax == 'calc' : data_vmax = 0.5 * ( data . max ( ) + model . max ( ) ) upper_mask , center_mask , lower_mask = trisect_image ( im . shape , edgepts ) gm = data_cmap ( center_data ( model , data_vmin , data_vmax ) ) dt = data_cmap ( center_data ( data , data_vmin , data_vmax ) ) rs = res_cmap ( center_data ( residuals , res_vmin , res_vmax ) ) for a in range ( 4 ) : im [ : , : , a ] [ upper_mask ] = rs [ : , : , a ] [ upper_mask ] im [ : , : , a ] [ center_mask ] = gm [ : , : , a ] [ center_mask ] im [ : , : , a ] [ lower_mask ] = dt [ : , : , a ] [ lower_mask ] if do_imshow : return plt . imshow ( im ) else : return im
11526	def create_small_thumbnail ( self , token , item_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'itemId' ] = item_id response = self . request ( 'midas.thumbnailcreator.create.small.thumbnail' , parameters ) return response
11137	def path_required ( func ) : @ wraps ( func ) def wrapper ( self , * args , ** kwargs ) : if self . path is None : warnings . warn ( 'Must load (Repository.load_repository) or initialize (Repository.create_repository) the repository first !' ) return return func ( self , * args , ** kwargs ) return wrapper
746	def anomalyAddLabel ( self , start , end , labelName ) : self . _getAnomalyClassifier ( ) . getSelf ( ) . addLabel ( start , end , labelName )
10475	def _sendKeyWithModifiers ( self , keychr , modifiers , globally = False ) : if not self . _isSingleCharacter ( keychr ) : raise ValueError ( 'Please provide only one character to send' ) if not hasattr ( self , 'keyboard' ) : self . keyboard = AXKeyboard . loadKeyboard ( ) modFlags = self . _pressModifiers ( modifiers , globally = globally ) self . _sendKey ( keychr , modFlags , globally = globally ) self . _releaseModifiers ( modifiers , globally = globally ) self . _postQueuedEvents ( )
2041	def SELFDESTRUCT ( self , recipient ) : recipient = Operators . EXTRACT ( recipient , 0 , 160 ) address = self . address if issymbolic ( recipient ) : logger . info ( "Symbolic recipient on self destruct" ) recipient = solver . get_value ( self . constraints , recipient ) if recipient not in self . world : self . world . create_account ( address = recipient ) self . world . send_funds ( address , recipient , self . world . get_balance ( address ) ) self . world . delete_account ( address ) raise EndTx ( 'SELFDESTRUCT' )
8761	def delete_subnet ( context , id ) : LOG . info ( "delete_subnet %s for tenant %s" % ( id , context . tenant_id ) ) with context . session . begin ( ) : subnet = db_api . subnet_find ( context , id = id , scope = db_api . ONE ) if not subnet : raise n_exc . SubnetNotFound ( subnet_id = id ) if not context . is_admin : if STRATEGY . is_provider_network ( subnet . network_id ) : if subnet . tenant_id == context . tenant_id : raise n_exc . NotAuthorized ( subnet_id = id ) else : raise n_exc . SubnetNotFound ( subnet_id = id ) _delete_subnet ( context , subnet )
9971	def _get_range ( book , range_ , sheet ) : filename = None if isinstance ( book , str ) : filename = book book = opxl . load_workbook ( book , data_only = True ) elif isinstance ( book , opxl . Workbook ) : pass else : raise TypeError if _is_range_address ( range_ ) : sheet_names = [ name . upper ( ) for name in book . sheetnames ] index = sheet_names . index ( sheet . upper ( ) ) data = book . worksheets [ index ] [ range_ ] else : data = _get_namedrange ( book , range_ , sheet ) if data is None : raise ValueError ( "Named range '%s' not found in %s" % ( range_ , filename or book ) ) return data
3212	def _update_cache_stats ( self , key , result ) : if result is None : self . _CACHE_STATS [ 'access_stats' ] . setdefault ( key , { 'hit' : 0 , 'miss' : 0 , 'expired' : 0 } ) else : self . _CACHE_STATS [ 'access_stats' ] [ key ] [ result ] += 1
5464	def get_action_by_id ( op , action_id ) : actions = get_actions ( op ) if actions and 1 <= action_id < len ( actions ) : return actions [ action_id - 1 ]
8308	def ensure_pycairo_context ( self , ctx ) : if self . cairocffi and isinstance ( ctx , self . cairocffi . Context ) : from shoebot . util . cairocffi . cairocffi_to_pycairo import _UNSAFE_cairocffi_context_to_pycairo return _UNSAFE_cairocffi_context_to_pycairo ( ctx ) else : return ctx
4288	def read_settings ( filename = None ) : logger = logging . getLogger ( __name__ ) logger . info ( "Reading settings ..." ) settings = _DEFAULT_CONFIG . copy ( ) if filename : logger . debug ( "Settings file: %s" , filename ) settings_path = os . path . dirname ( filename ) tempdict = { } with open ( filename ) as f : code = compile ( f . read ( ) , filename , 'exec' ) exec ( code , tempdict ) settings . update ( ( k , v ) for k , v in tempdict . items ( ) if k not in [ '__builtins__' ] ) paths = [ 'source' , 'destination' , 'watermark' ] if os . path . isdir ( join ( settings_path , settings [ 'theme' ] ) ) and os . path . isdir ( join ( settings_path , settings [ 'theme' ] , 'templates' ) ) : paths . append ( 'theme' ) for p in paths : path = settings [ p ] if path and not isabs ( path ) : settings [ p ] = abspath ( normpath ( join ( settings_path , path ) ) ) logger . debug ( "Rewrite %s : %s -> %s" , p , path , settings [ p ] ) for key in ( 'img_size' , 'thumb_size' , 'video_size' ) : w , h = settings [ key ] if h > w : settings [ key ] = ( h , w ) logger . warning ( "The %s setting should be specified with the " "largest value first." , key ) if not settings [ 'img_processor' ] : logger . info ( 'No Processor, images will not be resized' ) logger . debug ( 'Settings:\n%s' , pformat ( settings , width = 120 ) ) return settings
797	def jobUpdateResults ( self , jobID , results ) : with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET _eng_last_update_time=UTC_TIMESTAMP(), ' ' results=%%s ' ' WHERE job_id=%%s' % ( self . jobsTableName , ) conn . cursor . execute ( query , [ results , jobID ] )
6649	def inheritsFrom ( self , target_name ) : for t in self . hierarchy : if t and t . getName ( ) == target_name or target_name in t . description . get ( 'inherits' , { } ) : return True return False
1591	def _setup_custom_grouping ( self , topology ) : for i in range ( len ( topology . bolts ) ) : for in_stream in topology . bolts [ i ] . inputs : if in_stream . stream . component_name == self . my_component_name and in_stream . gtype == topology_pb2 . Grouping . Value ( "CUSTOM" ) : if in_stream . type == topology_pb2 . CustomGroupingObjectType . Value ( "PYTHON_OBJECT" ) : custom_grouping_obj = default_serializer . deserialize ( in_stream . custom_grouping_object ) if isinstance ( custom_grouping_obj , str ) : pex_loader . load_pex ( self . topology_pex_abs_path ) grouping_cls = pex_loader . import_and_get_class ( self . topology_pex_abs_path , custom_grouping_obj ) custom_grouping_obj = grouping_cls ( ) assert isinstance ( custom_grouping_obj , ICustomGrouping ) self . custom_grouper . add ( in_stream . stream . id , self . _get_taskids_for_component ( topology . bolts [ i ] . comp . name ) , custom_grouping_obj , self . my_component_name ) elif in_stream . type == topology_pb2 . CustomGroupingObjectType . Value ( "JAVA_OBJECT" ) : raise NotImplementedError ( "Java-serialized custom grouping is not yet supported " "for python topology" ) else : raise ValueError ( "Unrecognized custom grouping type found: %s" % str ( in_stream . type ) )
5142	def make_index ( self ) : for prev , block in zip ( self . blocks [ : - 1 ] , self . blocks [ 1 : ] ) : if not block . is_comment : self . index [ block . start_lineno ] = prev
3527	def piwik ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return PiwikNode ( )
3530	def get_identity ( context , prefix = None , identity_func = None , user = None ) : if prefix is not None : try : return context [ '%s_identity' % prefix ] except KeyError : pass try : return context [ 'analytical_identity' ] except KeyError : pass if getattr ( settings , 'ANALYTICAL_AUTO_IDENTIFY' , True ) : try : if user is None : user = get_user_from_context ( context ) if get_user_is_authenticated ( user ) : if identity_func is not None : return identity_func ( user ) else : return user . get_username ( ) except ( KeyError , AttributeError ) : pass return None
11785	def attrnum ( self , attr ) : "Returns the number used for attr, which can be a name, or -n .. n-1." if attr < 0 : return len ( self . attrs ) + attr elif isinstance ( attr , str ) : return self . attrnames . index ( attr ) else : return attr
3063	def parse_unique_urlencoded ( content ) : urlencoded_params = urllib . parse . parse_qs ( content ) params = { } for key , value in six . iteritems ( urlencoded_params ) : if len ( value ) != 1 : msg = ( 'URL-encoded content contains a repeated value:' '%s -> %s' % ( key , ', ' . join ( value ) ) ) raise ValueError ( msg ) params [ key ] = value [ 0 ] return params
5449	def validate_param_name ( name , param_type ) : if not re . match ( r'^[a-zA-Z_][a-zA-Z0-9_]*$' , name ) : raise ValueError ( 'Invalid %s: %s' % ( param_type , name ) )
3684	def solve ( self ) : self . check_sufficient_inputs ( ) if self . V : if self . P : self . T = self . solve_T ( self . P , self . V ) self . a_alpha , self . da_alpha_dT , self . d2a_alpha_dT2 = self . a_alpha_and_derivatives ( self . T ) else : self . a_alpha , self . da_alpha_dT , self . d2a_alpha_dT2 = self . a_alpha_and_derivatives ( self . T ) self . P = R * self . T / ( self . V - self . b ) - self . a_alpha / ( self . V * self . V + self . delta * self . V + self . epsilon ) Vs = [ self . V , 1j , 1j ] else : self . a_alpha , self . da_alpha_dT , self . d2a_alpha_dT2 = self . a_alpha_and_derivatives ( self . T ) Vs = self . volume_solutions ( self . T , self . P , self . b , self . delta , self . epsilon , self . a_alpha ) self . set_from_PT ( Vs )
3616	def _should_really_index ( self , instance ) : if self . _should_index_is_method : is_method = inspect . ismethod ( self . should_index ) try : count_args = len ( inspect . signature ( self . should_index ) . parameters ) except AttributeError : count_args = len ( inspect . getargspec ( self . should_index ) . args ) if is_method or count_args is 1 : return self . should_index ( instance ) else : return self . should_index ( ) else : attr_type = type ( self . should_index ) if attr_type is DeferredAttribute : attr_value = self . should_index . __get__ ( instance , None ) elif attr_type is str : attr_value = getattr ( instance , self . should_index ) elif attr_type is property : attr_value = self . should_index . __get__ ( instance ) else : raise AlgoliaIndexError ( '{} should be a boolean attribute or a method that returns a boolean.' . format ( self . should_index ) ) if type ( attr_value ) is not bool : raise AlgoliaIndexError ( "%s's should_index (%s) should be a boolean" % ( instance . __class__ . __name__ , self . should_index ) ) return attr_value
13133	def parse_domain_users ( domain_users_file , domain_groups_file ) : with open ( domain_users_file ) as f : users = json . loads ( f . read ( ) ) domain_groups = { } if domain_groups_file : with open ( domain_groups_file ) as f : groups = json . loads ( f . read ( ) ) for group in groups : sid = get_field ( group , 'objectSid' ) domain_groups [ int ( sid . split ( '-' ) [ - 1 ] ) ] = get_field ( group , 'cn' ) user_search = UserSearch ( ) count = 0 total = len ( users ) print_notification ( "Importing {} users" . format ( total ) ) for entry in users : result = parse_user ( entry , domain_groups ) user = user_search . id_to_object ( result [ 'username' ] ) user . name = result [ 'name' ] user . domain . append ( result [ 'domain' ] ) user . description = result [ 'description' ] user . groups . extend ( result [ 'groups' ] ) user . flags . extend ( result [ 'flags' ] ) user . sid = result [ 'sid' ] user . add_tag ( "domaindump" ) user . save ( ) count += 1 sys . stdout . write ( '\r' ) sys . stdout . write ( "[{}/{}]" . format ( count , total ) ) sys . stdout . flush ( ) sys . stdout . write ( '\r' ) return count
13833	def ParseInteger ( text , is_signed = False , is_long = False ) : try : if is_long : result = long ( text , 0 ) else : result = int ( text , 0 ) except ValueError : raise ValueError ( 'Couldn\'t parse integer: %s' % text ) checker = _INTEGER_CHECKERS [ 2 * int ( is_long ) + int ( is_signed ) ] checker . CheckValue ( result ) return result
11400	def update_collaboration ( self ) : for field in record_get_field_instances ( self . record , '710' ) : subs = field_get_subfield_instances ( field ) for idx , ( key , value ) in enumerate ( subs [ : ] ) : if key == '5' : subs . pop ( idx ) elif value . startswith ( 'CERN. Geneva' ) : subs . pop ( idx ) if len ( subs ) == 0 : record_delete_field ( self . record , tag = '710' , field_position_global = field [ 4 ] )
1149	def warnpy3k ( message , category = None , stacklevel = 1 ) : if sys . py3kwarning : if category is None : category = DeprecationWarning warn ( message , category , stacklevel + 1 )
6239	def render_lights ( self , camera_matrix , projection ) : self . ctx . front_face = 'cw' self . ctx . blend_func = moderngl . ONE , moderngl . ONE helper . _depth_sampler . use ( location = 1 ) with self . lightbuffer_scope : for light in self . point_lights : light_size = light . radius m_light = matrix44 . multiply ( light . matrix , camera_matrix ) self . point_light_shader [ "m_proj" ] . write ( projection . tobytes ( ) ) self . point_light_shader [ "m_light" ] . write ( m_light . astype ( 'f4' ) . tobytes ( ) ) self . gbuffer . color_attachments [ 1 ] . use ( location = 0 ) self . point_light_shader [ "g_normal" ] . value = 0 self . gbuffer . depth_attachment . use ( location = 1 ) self . point_light_shader [ "g_depth" ] . value = 1 self . point_light_shader [ "screensize" ] . value = ( self . width , self . height ) self . point_light_shader [ "proj_const" ] . value = projection . projection_constants self . point_light_shader [ "radius" ] . value = light_size self . unit_cube . render ( self . point_light_shader ) helper . _depth_sampler . clear ( location = 1 )
6659	def _bias_correction ( V_IJ , inbag , pred_centered , n_trees ) : n_train_samples = inbag . shape [ 0 ] n_var = np . mean ( np . square ( inbag [ 0 : n_trees ] ) . mean ( axis = 1 ) . T . view ( ) - np . square ( inbag [ 0 : n_trees ] . mean ( axis = 1 ) ) . T . view ( ) ) boot_var = np . square ( pred_centered ) . sum ( axis = 1 ) / n_trees bias_correction = n_train_samples * n_var * boot_var / n_trees V_IJ_unbiased = V_IJ - bias_correction return V_IJ_unbiased
7586	def _get_boots ( arr , nboots ) : boots = np . zeros ( ( nboots , ) ) for bidx in xrange ( nboots ) : lidx = np . random . randint ( 0 , arr . shape [ 0 ] , arr . shape [ 0 ] ) tarr = arr [ lidx ] _ , _ , dst = _prop_dstat ( tarr ) boots [ bidx ] = dst return boots
8050	def load_source ( self ) : if self . filename in self . STDIN_NAMES : self . filename = "stdin" if sys . version_info [ 0 ] < 3 : self . source = sys . stdin . read ( ) else : self . source = TextIOWrapper ( sys . stdin . buffer , errors = "ignore" ) . read ( ) else : handle = tokenize_open ( self . filename ) self . source = handle . read ( ) handle . close ( )
730	def _generate ( self ) : candidates = np . array ( range ( self . _n ) , np . uint32 ) for i in xrange ( self . _num ) : self . _random . shuffle ( candidates ) pattern = candidates [ 0 : self . _getW ( ) ] self . _patterns [ i ] = set ( pattern )
5996	def plot_border ( mask , should_plot_border , units , kpc_per_arcsec , pointsize , zoom_offset_pixels ) : if should_plot_border and mask is not None : plt . gca ( ) border_pixels = mask . masked_grid_index_to_pixel [ mask . border_pixels ] if zoom_offset_pixels is not None : border_pixels -= zoom_offset_pixels border_arcsec = mask . grid_pixels_to_grid_arcsec ( grid_pixels = border_pixels ) border_units = convert_grid_units ( array = mask , grid_arcsec = border_arcsec , units = units , kpc_per_arcsec = kpc_per_arcsec ) plt . scatter ( y = border_units [ : , 0 ] , x = border_units [ : , 1 ] , s = pointsize , c = 'y' )
9584	def write_var_data ( fd , data ) : fd . write ( struct . pack ( 'b3xI' , etypes [ 'miMATRIX' ] [ 'n' ] , len ( data ) ) ) fd . write ( data )
2920	def _send_call ( self , my_task ) : args , kwargs = None , None if self . args : args = _eval_args ( self . args , my_task ) if self . kwargs : kwargs = _eval_kwargs ( self . kwargs , my_task ) LOG . debug ( "%s (task id %s) calling %s" % ( self . name , my_task . id , self . call ) , extra = dict ( data = dict ( args = args , kwargs = kwargs ) ) ) async_call = default_app . send_task ( self . call , args = args , kwargs = kwargs ) my_task . _set_internal_data ( task_id = async_call . task_id ) my_task . async_call = async_call LOG . debug ( "'%s' called: %s" % ( self . call , my_task . async_call . task_id ) )
11198	def tzname_in_python2 ( namefunc ) : def adjust_encoding ( * args , ** kwargs ) : name = namefunc ( * args , ** kwargs ) if name is not None and not PY3 : name = name . encode ( ) return name return adjust_encoding
2245	def hzcat ( args , sep = '' ) : import unicodedata if '\n' in sep or '\r' in sep : raise ValueError ( '`sep` cannot contain newline characters' ) args = [ unicodedata . normalize ( 'NFC' , ensure_unicode ( val ) ) for val in args ] arglines = [ a . split ( '\n' ) for a in args ] height = max ( map ( len , arglines ) ) arglines = [ lines + [ '' ] * ( height - len ( lines ) ) for lines in arglines ] all_lines = [ '' for _ in range ( height ) ] width = 0 n_args = len ( args ) for sx , lines in enumerate ( arglines ) : for lx , line in enumerate ( lines ) : all_lines [ lx ] += line width = max ( width , max ( map ( len , all_lines ) ) ) if sx < n_args - 1 : for lx , line in list ( enumerate ( all_lines ) ) : residual = width - len ( line ) all_lines [ lx ] = line + ( ' ' * residual ) + sep width += len ( sep ) all_lines = [ line . rstrip ( ' ' ) for line in all_lines ] ret = '\n' . join ( all_lines ) return ret
12010	def getTableOfContents ( self ) : self . directory_size = self . getDirectorySize ( ) if self . directory_size > 65536 : self . directory_size += 2 self . requestContentDirectory ( ) directory_start = unpack ( "i" , self . raw_bytes [ self . directory_end + 16 : self . directory_end + 20 ] ) [ 0 ] self . raw_bytes = self . raw_bytes current_start = directory_start - self . start filestart = 0 compressedsize = 0 tableOfContents = [ ] try : while True : zip_n = unpack ( "H" , self . raw_bytes [ current_start + 28 : current_start + 28 + 2 ] ) [ 0 ] zip_m = unpack ( "H" , self . raw_bytes [ current_start + 30 : current_start + 30 + 2 ] ) [ 0 ] zip_k = unpack ( "H" , self . raw_bytes [ current_start + 32 : current_start + 32 + 2 ] ) [ 0 ] filename = self . raw_bytes [ current_start + 46 : current_start + 46 + zip_n ] filestart = unpack ( "I" , self . raw_bytes [ current_start + 42 : current_start + 42 + 4 ] ) [ 0 ] compressedsize = unpack ( "I" , self . raw_bytes [ current_start + 20 : current_start + 20 + 4 ] ) [ 0 ] uncompressedsize = unpack ( "I" , self . raw_bytes [ current_start + 24 : current_start + 24 + 4 ] ) [ 0 ] tableItem = { 'filename' : filename , 'compressedsize' : compressedsize , 'uncompressedsize' : uncompressedsize , 'filestart' : filestart } tableOfContents . append ( tableItem ) current_start = current_start + 46 + zip_n + zip_m + zip_k except : pass self . tableOfContents = tableOfContents return tableOfContents
8554	def reserve_ipblock ( self , ipblock ) : properties = { "name" : ipblock . name } if ipblock . location : properties [ 'location' ] = ipblock . location if ipblock . size : properties [ 'size' ] = str ( ipblock . size ) raw = { "properties" : properties , } response = self . _perform_request ( url = '/ipblocks' , method = 'POST' , data = json . dumps ( raw ) ) return response
6107	def trace_grid_stack_to_next_plane ( self ) : def minus ( grid , deflections ) : return grid - deflections return self . grid_stack . map_function ( minus , self . deflection_stack )
12774	def inverse_kinematics ( self , start = 0 , end = 1e100 , states = None , max_force = 20 ) : zeros = None if max_force > 0 : self . skeleton . enable_motors ( max_force ) zeros = np . zeros ( self . skeleton . num_dofs ) for _ in self . follow_markers ( start , end , states ) : if zeros is not None : self . skeleton . set_target_angles ( zeros ) yield self . skeleton . joint_angles
4882	def create_switch ( apps , schema_editor ) : Switch = apps . get_model ( 'waffle' , 'Switch' ) Switch . objects . get_or_create ( name = 'SAP_USE_ENTERPRISE_ENROLLMENT_PAGE' , defaults = { 'active' : False } )
12925	def match_to_clinvar ( genome_file , clin_file ) : clin_curr_line = _next_line ( clin_file ) genome_curr_line = _next_line ( genome_file ) while clin_curr_line . startswith ( '#' ) : clin_curr_line = _next_line ( clin_file ) while genome_curr_line . startswith ( '#' ) : genome_curr_line = _next_line ( genome_file ) while clin_curr_line and genome_curr_line : clin_curr_pos = VCFLine . get_pos ( clin_curr_line ) genome_curr_pos = VCFLine . get_pos ( genome_curr_line ) try : if clin_curr_pos [ 'chrom' ] > genome_curr_pos [ 'chrom' ] : genome_curr_line = _next_line ( genome_file ) continue elif clin_curr_pos [ 'chrom' ] < genome_curr_pos [ 'chrom' ] : clin_curr_line = _next_line ( clin_file ) continue if clin_curr_pos [ 'pos' ] > genome_curr_pos [ 'pos' ] : genome_curr_line = _next_line ( genome_file ) continue elif clin_curr_pos [ 'pos' ] < genome_curr_pos [ 'pos' ] : clin_curr_line = _next_line ( clin_file ) continue except StopIteration : break genome_vcf_line = GenomeVCFLine ( vcf_line = genome_curr_line , skip_info = True ) if not genome_vcf_line . genotype_allele_indexes : genome_curr_line = _next_line ( genome_file ) continue clinvar_vcf_line = ClinVarVCFLine ( vcf_line = clin_curr_line ) if not genome_vcf_line . ref_allele == clinvar_vcf_line . ref_allele : try : genome_curr_line = _next_line ( genome_file ) clin_curr_line = _next_line ( clin_file ) continue except StopIteration : break genotype_allele_indexes = genome_vcf_line . genotype_allele_indexes genome_alleles = [ genome_vcf_line . alleles [ x ] for x in genotype_allele_indexes ] if len ( genome_alleles ) == 1 : zygosity = 'Hem' elif len ( genome_alleles ) == 2 : if genome_alleles [ 0 ] . sequence == genome_alleles [ 1 ] . sequence : zygosity = 'Hom' genome_alleles = [ genome_alleles [ 0 ] ] else : zygosity = 'Het' else : raise ValueError ( 'This code only expects to work on genomes ' + 'with one or two alleles called at each ' + 'location. The following line violates this:' + str ( genome_vcf_line ) ) for genome_allele in genome_alleles : for allele in clinvar_vcf_line . alleles : if genome_allele . sequence == allele . sequence : if hasattr ( allele , 'records' ) : yield ( genome_vcf_line , allele , zygosity ) try : genome_curr_line = _next_line ( genome_file ) clin_curr_line = _next_line ( clin_file ) except StopIteration : break
9890	def _boottime_linux ( ) : global __boottime try : f = open ( '/proc/stat' , 'r' ) for line in f : if line . startswith ( 'btime' ) : __boottime = int ( line . split ( ) [ 1 ] ) if datetime is None : raise NotImplementedError ( 'datetime module required.' ) return datetime . fromtimestamp ( __boottime ) except ( IOError , IndexError ) : return None
5713	def is_safe_path ( path ) : contains_windows_var = lambda val : re . match ( r'%.+%' , val ) contains_posix_var = lambda val : re . match ( r'\$.+' , val ) unsafeness_conditions = [ os . path . isabs ( path ) , ( '..%s' % os . path . sep ) in path , path . startswith ( '~' ) , os . path . expandvars ( path ) != path , contains_windows_var ( path ) , contains_posix_var ( path ) , ] return not any ( unsafeness_conditions )
6786	def get_component_funcs ( self , components = None ) : current_tp = self . get_current_thumbprint ( components = components ) or { } previous_tp = self . get_previous_thumbprint ( components = components ) or { } if self . verbose : print ( 'Current thumbprint:' ) pprint ( current_tp , indent = 4 ) print ( 'Previous thumbprint:' ) pprint ( previous_tp , indent = 4 ) differences = list ( iter_dict_differences ( current_tp , previous_tp ) ) if self . verbose : print ( 'Differences:' ) pprint ( differences , indent = 4 ) component_order = get_component_order ( [ k for k , ( _ , _ ) in differences ] ) if self . verbose : print ( 'component_order:' ) pprint ( component_order , indent = 4 ) plan_funcs = list ( get_deploy_funcs ( component_order , current_tp , previous_tp ) ) return component_order , plan_funcs
12744	def get_internal_urls ( self ) : internal_urls = self . get_subfields ( "856" , "u" , i1 = "4" , i2 = "0" ) internal_urls . extend ( self . get_subfields ( "998" , "a" ) ) internal_urls . extend ( self . get_subfields ( "URL" , "u" ) ) return map ( lambda x : x . replace ( "&amp;" , "&" ) , internal_urls )
130	def is_fully_within_image ( self , image ) : return not self . is_out_of_image ( image , fully = True , partly = True )
3088	def _delete_entity ( self ) : if self . _is_ndb ( ) : _NDB_KEY ( self . _model , self . _key_name ) . delete ( ) else : entity_key = db . Key . from_path ( self . _model . kind ( ) , self . _key_name ) db . delete ( entity_key )
7711	def _get_success ( self , stanza ) : payload = stanza . get_payload ( RosterPayload ) if payload is None : if "versioning" in self . server_features and self . roster : logger . debug ( "Server will send roster delta in pushes" ) else : logger . warning ( "Bad roster response (no payload)" ) self . _event_queue . put ( RosterNotReceivedEvent ( self , stanza ) ) return else : items = list ( payload ) for item in items : item . verify_roster_result ( True ) self . roster = Roster ( items , payload . version ) self . _event_queue . put ( RosterReceivedEvent ( self , self . roster ) )
5304	def sanitize_color_palette ( colorpalette ) : new_palette = { } def __make_valid_color_name ( name ) : if len ( name ) == 1 : name = name [ 0 ] return name [ : 1 ] . lower ( ) + name [ 1 : ] return name [ 0 ] . lower ( ) + '' . join ( word . capitalize ( ) for word in name [ 1 : ] ) for key , value in colorpalette . items ( ) : if isinstance ( value , str ) : value = utils . hex_to_rgb ( value ) new_palette [ __make_valid_color_name ( key . split ( ) ) ] = value return new_palette
6765	def load_table ( self , table_name , src , dst = 'localhost' , name = None , site = None ) : r = self . database_renderer ( name = name , site = site ) r . env . table_name = table_name r . run ( 'psql --user={dst_db_user} --host={dst_db_host} --command="DROP TABLE IF EXISTS {table_name} CASCADE;"' ) r . run ( 'pg_dump -t {table_name} --user={dst_db_user} --host={dst_db_host} | psql --user={src_db_user} --host={src_db_host}' )
13433	def showfig ( fig , aspect = "auto" ) : ax = fig . gca ( ) alim = list ( ax . axis ( ) ) if alim [ 3 ] < alim [ 2 ] : temp = alim [ 2 ] alim [ 2 ] = alim [ 3 ] alim [ 3 ] = temp ax . axis ( alim ) ax . set_aspect ( aspect ) fig . show ( )
8143	def rotate ( self , angle ) : from math import sqrt , pow , sin , cos , degrees , radians , asin w0 , h0 = self . img . size d = sqrt ( pow ( w0 , 2 ) + pow ( h0 , 2 ) ) d_angle = degrees ( asin ( ( w0 * 0.5 ) / ( d * 0.5 ) ) ) angle = angle % 360 if angle > 90 and angle <= 270 : d_angle += 180 w = sin ( radians ( d_angle + angle ) ) * d w = max ( w , sin ( radians ( d_angle - angle ) ) * d ) w = int ( abs ( w ) ) h = cos ( radians ( d_angle + angle ) ) * d h = max ( h , cos ( radians ( d_angle - angle ) ) * d ) h = int ( abs ( h ) ) dx = int ( ( w - w0 ) / 2 ) dy = int ( ( h - h0 ) / 2 ) d = int ( d ) bg = ImageStat . Stat ( self . img ) . mean bg = ( int ( bg [ 0 ] ) , int ( bg [ 1 ] ) , int ( bg [ 2 ] ) , 0 ) box = Image . new ( "RGBA" , ( d , d ) , bg ) box . paste ( self . img , ( ( d - w0 ) / 2 , ( d - h0 ) / 2 ) ) box = box . rotate ( angle , INTERPOLATION ) box = box . crop ( ( ( d - w ) / 2 + 2 , ( d - h ) / 2 , d - ( d - w ) / 2 , d - ( d - h ) / 2 ) ) self . img = box self . x += ( self . w - w ) / 2 self . y += ( self . h - h ) / 2 self . w = w self . h = h
2508	def get_extr_lics_comment ( self , extr_lics ) : comment_list = list ( self . graph . triples ( ( extr_lics , RDFS . comment , None ) ) ) if len ( comment_list ) > 1 : self . more_than_one_error ( 'extracted license comment' ) return elif len ( comment_list ) == 1 : return comment_list [ 0 ] [ 2 ] else : return
3144	def update ( self , file_id , data ) : self . file_id = file_id if 'name' not in data : raise KeyError ( 'The file must have a name' ) if 'file_data' not in data : raise KeyError ( 'The file must have file_data' ) return self . _mc_client . _patch ( url = self . _build_path ( file_id ) , data = data )
5395	def _get_input_target_path ( self , local_file_path ) : path , filename = os . path . split ( local_file_path ) if '*' in filename : return path + '/' else : return local_file_path
12145	def doStuff ( ABFfolder , analyze = False , convert = False , index = True , overwrite = True , launch = True ) : IN = INDEX ( ABFfolder ) if analyze : IN . analyzeAll ( ) if convert : IN . convertImages ( )
1606	def run_containers ( command , parser , cl_args , unknown_args ) : cluster , role , env = cl_args [ 'cluster' ] , cl_args [ 'role' ] , cl_args [ 'environ' ] topology = cl_args [ 'topology-name' ] container_id = cl_args [ 'id' ] try : result = tracker_access . get_topology_info ( cluster , env , topology , role ) except : Log . error ( "Fail to connect to tracker: \'%s\'" , cl_args [ "tracker_url" ] ) return False containers = result [ 'physical_plan' ] [ 'stmgrs' ] all_bolts , all_spouts = set ( ) , set ( ) for _ , bolts in result [ 'physical_plan' ] [ 'bolts' ] . items ( ) : all_bolts = all_bolts | set ( bolts ) for _ , spouts in result [ 'physical_plan' ] [ 'spouts' ] . items ( ) : all_spouts = all_spouts | set ( spouts ) stmgrs = containers . keys ( ) stmgrs . sort ( ) if container_id is not None : try : normalized_cid = container_id - 1 if normalized_cid < 0 : raise stmgrs = [ stmgrs [ normalized_cid ] ] except : Log . error ( 'Invalid container id: %d' % container_id ) return False table = [ ] for sid , name in enumerate ( stmgrs ) : cid = sid + 1 host = containers [ name ] [ "host" ] port = containers [ name ] [ "port" ] pid = containers [ name ] [ "pid" ] instances = containers [ name ] [ "instance_ids" ] bolt_nums = len ( [ instance for instance in instances if instance in all_bolts ] ) spout_nums = len ( [ instance for instance in instances if instance in all_spouts ] ) table . append ( [ cid , host , port , pid , bolt_nums , spout_nums , len ( instances ) ] ) headers = [ "container" , "host" , "port" , "pid" , "#bolt" , "#spout" , "#instance" ] sys . stdout . flush ( ) print ( tabulate ( table , headers = headers ) ) return True
6714	def _install_from_scratch ( python_cmd , use_sudo ) : with cd ( "/tmp" ) : download ( EZ_SETUP_URL ) command = '%(python_cmd)s ez_setup.py' % locals ( ) if use_sudo : run_as_root ( command ) else : run ( command ) run ( 'rm -f ez_setup.py' )
5680	def get_trip_counts_per_day ( self ) : query = "SELECT date, count(*) AS number_of_trips FROM day_trips GROUP BY date" trip_counts_per_day = pd . read_sql_query ( query , self . conn , index_col = "date" ) max_day = trip_counts_per_day . index . max ( ) min_day = trip_counts_per_day . index . min ( ) min_date = datetime . datetime . strptime ( min_day , '%Y-%m-%d' ) max_date = datetime . datetime . strptime ( max_day , '%Y-%m-%d' ) num_days = ( max_date - min_date ) . days dates = [ min_date + datetime . timedelta ( days = x ) for x in range ( num_days + 1 ) ] trip_counts = [ ] date_strings = [ ] for date in dates : date_string = date . strftime ( "%Y-%m-%d" ) date_strings . append ( date_string ) try : value = trip_counts_per_day . loc [ date_string , 'number_of_trips' ] except KeyError : value = 0 trip_counts . append ( value ) for date_string in trip_counts_per_day . index : assert date_string in date_strings data = { "date" : dates , "date_str" : date_strings , "trip_counts" : trip_counts } return pd . DataFrame ( data )
2525	def get_reviewer ( self , r_term ) : reviewer_list = list ( self . graph . triples ( ( r_term , self . spdx_namespace [ 'reviewer' ] , None ) ) ) if len ( reviewer_list ) != 1 : self . error = True msg = 'Review must have exactly one reviewer' self . logger . log ( msg ) return try : return self . builder . create_entity ( self . doc , six . text_type ( reviewer_list [ 0 ] [ 2 ] ) ) except SPDXValueError : self . value_error ( 'REVIEWER_VALUE' , reviewer_list [ 0 ] [ 2 ] )
413	def find_top_model ( self , sess , sort = None , model_name = 'model' , ** kwargs ) : kwargs . update ( { 'model_name' : model_name } ) self . _fill_project_info ( kwargs ) s = time . time ( ) d = self . db . Model . find_one ( filter = kwargs , sort = sort ) _temp_file_name = '_find_one_model_ztemp_file' if d is not None : params_id = d [ 'params_id' ] graphs = d [ 'architecture' ] _datetime = d [ 'time' ] exists_or_mkdir ( _temp_file_name , False ) with open ( os . path . join ( _temp_file_name , 'graph.pkl' ) , 'wb' ) as file : pickle . dump ( graphs , file , protocol = pickle . HIGHEST_PROTOCOL ) else : print ( "[Database] FAIL! Cannot find model: {}" . format ( kwargs ) ) return False try : params = self . _deserialization ( self . model_fs . get ( params_id ) . read ( ) ) np . savez ( os . path . join ( _temp_file_name , 'params.npz' ) , params = params ) network = load_graph_and_params ( name = _temp_file_name , sess = sess ) del_folder ( _temp_file_name ) pc = self . db . Model . find ( kwargs ) print ( "[Database] Find one model SUCCESS. kwargs:{} sort:{} save time:{} took: {}s" . format ( kwargs , sort , _datetime , round ( time . time ( ) - s , 2 ) ) ) for key in d : network . __dict__ . update ( { "_%s" % key : d [ key ] } ) params_id_list = pc . distinct ( 'params_id' ) n_params = len ( params_id_list ) if n_params != 1 : print ( " Note that there are {} models match the kwargs" . format ( n_params ) ) return network except Exception as e : exc_type , exc_obj , exc_tb = sys . exc_info ( ) fname = os . path . split ( exc_tb . tb_frame . f_code . co_filename ) [ 1 ] logging . info ( "{} {} {} {} {}" . format ( exc_type , exc_obj , fname , exc_tb . tb_lineno , e ) ) return False
6618	def _expand_tuple ( path_cfg , alias_dict , overriding_kargs ) : new_path_cfg = path_cfg [ 0 ] new_overriding_kargs = path_cfg [ 1 ] . copy ( ) new_overriding_kargs . update ( overriding_kargs ) return expand_path_cfg ( new_path_cfg , overriding_kargs = new_overriding_kargs , alias_dict = alias_dict )
7531	def _plot_dag ( dag , results , snames ) : try : import matplotlib . pyplot as plt from matplotlib . dates import date2num from matplotlib . cm import gist_rainbow plt . figure ( "dag_layout" , figsize = ( 10 , 10 ) ) nx . draw ( dag , pos = nx . spring_layout ( dag ) , node_color = 'pink' , with_labels = True ) plt . savefig ( "./dag_layout.png" , bbox_inches = 'tight' , dpi = 200 ) pos = { } colors = { } for node in dag : mtd = results [ node ] . metadata start = date2num ( mtd . started ) _ , _ , sname = node . split ( "-" , 2 ) sid = snames . index ( sname ) pos [ node ] = ( start + sid , start * 1e6 ) colors [ node ] = mtd . engine_id plt . figure ( "dag_starttimes" , figsize = ( 10 , 16 ) ) nx . draw ( dag , pos , node_list = colors . keys ( ) , node_color = colors . values ( ) , cmap = gist_rainbow , with_labels = True ) plt . savefig ( "./dag_starttimes.png" , bbox_inches = 'tight' , dpi = 200 ) except Exception as inst : LOGGER . warning ( inst )
2900	def get_tasks ( self , state = Task . ANY_MASK ) : return [ t for t in Task . Iterator ( self . task_tree , state ) ]
6739	def check_settings_for_differences ( old , new , as_bool = False , as_tri = False ) : assert not as_bool or not as_tri old = old or { } new = new or { } changes = set ( k for k in set ( new . iterkeys ( ) ) . intersection ( old . iterkeys ( ) ) if new [ k ] != old [ k ] ) if changes and as_bool : return True added_keys = set ( new . iterkeys ( ) ) . difference ( old . iterkeys ( ) ) if added_keys and as_bool : return True if not as_tri : changes . update ( added_keys ) deled_keys = set ( old . iterkeys ( ) ) . difference ( new . iterkeys ( ) ) if deled_keys and as_bool : return True if as_bool : return False if not as_tri : changes . update ( deled_keys ) if as_tri : return added_keys , changes , deled_keys return changes
9818	def upgrade ( self ) : if not self . is_valid : raise PolyaxonDeploymentConfigError ( 'Deployment type `{}` not supported' . format ( self . deployment_type ) ) if self . is_kubernetes : self . upgrade_on_kubernetes ( ) elif self . is_docker_compose : self . upgrade_on_docker_compose ( ) elif self . is_docker : self . upgrade_on_docker ( ) elif self . is_heroku : self . upgrade_on_heroku ( )
8225	def _key_pressed ( self , key , keycode ) : self . _namespace [ 'key' ] = key self . _namespace [ 'keycode' ] = keycode self . _namespace [ 'keydown' ] = True
13242	def period ( self ) : start_time = self . root . findtext ( 'daily_start_time' ) if start_time : return Period ( text_to_time ( start_time ) , text_to_time ( self . root . findtext ( 'daily_end_time' ) ) ) return Period ( datetime . time ( 0 , 0 ) , datetime . time ( 23 , 59 ) )
7681	def piano_roll ( annotation , ** kwargs ) : times , midi = annotation . to_interval_values ( ) return mir_eval . display . piano_roll ( times , midi = midi , ** kwargs )
6249	def get_effect_class ( self , effect_name : str , package_name : str = None ) -> Type [ 'Effect' ] : return self . _project . get_effect_class ( effect_name , package_name = package_name )
9943	def delete_file ( self , path , prefixed_path , source_storage ) : if self . storage . exists ( prefixed_path ) : try : target_last_modified = self . storage . modified_time ( prefixed_path ) except ( OSError , NotImplementedError , AttributeError ) : pass else : try : source_last_modified = source_storage . modified_time ( path ) except ( OSError , NotImplementedError , AttributeError ) : pass else : if self . local : full_path = self . storage . path ( prefixed_path ) else : full_path = None if ( target_last_modified . replace ( microsecond = 0 ) >= source_last_modified . replace ( microsecond = 0 ) ) : if not ( ( self . symlink and full_path and not os . path . islink ( full_path ) ) or ( not self . symlink and full_path and os . path . islink ( full_path ) ) ) : if prefixed_path not in self . unmodified_files : self . unmodified_files . append ( prefixed_path ) self . log ( "Skipping '%s' (not modified)" % path ) return False if self . dry_run : self . log ( "Pretending to delete '%s'" % path ) else : self . log ( "Deleting '%s'" % path ) self . storage . delete ( prefixed_path ) return True
7842	def set_category ( self , category ) : if not category : raise ValueError ( "Category is required in DiscoIdentity" ) category = unicode ( category ) self . xmlnode . setProp ( "category" , category . encode ( "utf-8" ) )
8585	def get_attached_cdroms ( self , datacenter_id , server_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/servers/%s/cdroms?depth=%s' % ( datacenter_id , server_id , str ( depth ) ) ) return response
12869	async def manage ( self ) : cm = _ContextManager ( self . database ) if isinstance ( self . database . obj , AIODatabase ) : cm . connection = await self . database . async_connect ( ) else : cm . connection = self . database . connect ( ) return cm
13403	def acceptedUser ( self , logType ) : from urllib2 import urlopen , URLError , HTTPError import json isApproved = False userName = str ( self . logui . userName . text ( ) ) if userName == "" : return False if logType == "MCC" : networkFault = False data = [ ] log_url = "https://mccelog.slac.stanford.edu/elog/dev/mgibbs/dev_json_user_list.php/?username=" + userName try : data = urlopen ( log_url , None , 5 ) . read ( ) data = json . loads ( data ) except URLError as error : print ( "URLError: " + str ( error . reason ) ) networkFault = True except HTTPError as error : print ( "HTTPError: " + str ( error . reason ) ) networkFault = True if networkFault : msgBox = QMessageBox ( ) msgBox . setText ( "Cannot connect to MCC Log Server!" ) msgBox . setInformativeText ( "Use entered User name anyway?" ) msgBox . setStandardButtons ( QMessageBox . Ok | QMessageBox . Cancel ) msgBox . setDefaultButton ( QMessageBox . Ok ) if msgBox . exec_ ( ) == QMessageBox . Ok : isApproved = True if data != [ ] and ( data is not None ) : isApproved = True else : isApproved = True return isApproved
4433	async def dispatch_event ( self , event ) : log . debug ( 'Dispatching event of type {} to {} hooks' . format ( event . __class__ . __name__ , len ( self . hooks ) ) ) for hook in self . hooks : try : if asyncio . iscoroutinefunction ( hook ) : await hook ( event ) else : hook ( event ) except Exception as e : log . warning ( 'Encountered exception while dispatching an event to hook `{}` ({})' . format ( hook . __name__ , str ( e ) ) ) if isinstance ( event , ( TrackEndEvent , TrackExceptionEvent , TrackStuckEvent ) ) and event . player : await event . player . handle_event ( event )
11161	def autopep8 ( self , ** kwargs ) : self . assert_is_dir_and_exists ( ) for p in self . select_by_ext ( ".py" ) : with open ( p . abspath , "rb" ) as f : code = f . read ( ) . decode ( "utf-8" ) formatted_code = autopep8 . fix_code ( code , ** kwargs ) with open ( p . abspath , "wb" ) as f : f . write ( formatted_code . encode ( "utf-8" ) )
5197	def configure_database ( db_config ) : db_config . analog [ 1 ] . clazz = opendnp3 . PointClass . Class2 db_config . analog [ 1 ] . svariation = opendnp3 . StaticAnalogVariation . Group30Var1 db_config . analog [ 1 ] . evariation = opendnp3 . EventAnalogVariation . Group32Var7 db_config . analog [ 2 ] . clazz = opendnp3 . PointClass . Class2 db_config . analog [ 2 ] . svariation = opendnp3 . StaticAnalogVariation . Group30Var1 db_config . analog [ 2 ] . evariation = opendnp3 . EventAnalogVariation . Group32Var7 db_config . binary [ 1 ] . clazz = opendnp3 . PointClass . Class2 db_config . binary [ 1 ] . svariation = opendnp3 . StaticBinaryVariation . Group1Var2 db_config . binary [ 1 ] . evariation = opendnp3 . EventBinaryVariation . Group2Var2 db_config . binary [ 2 ] . clazz = opendnp3 . PointClass . Class2 db_config . binary [ 2 ] . svariation = opendnp3 . StaticBinaryVariation . Group1Var2 db_config . binary [ 2 ] . evariation = opendnp3 . EventBinaryVariation . Group2Var2
734	def _calculateError ( self , recordNum , bucketIdxList ) : error = dict ( ) targetDist = numpy . zeros ( self . _maxBucketIdx + 1 ) numCategories = len ( bucketIdxList ) for bucketIdx in bucketIdxList : targetDist [ bucketIdx ] = 1.0 / numCategories for ( learnRecordNum , learnPatternNZ ) in self . _patternNZHistory : nSteps = recordNum - learnRecordNum if nSteps in self . steps : predictDist = self . inferSingleStep ( learnPatternNZ , self . _weightMatrix [ nSteps ] ) error [ nSteps ] = targetDist - predictDist return error
2896	def is_completed ( self ) : mask = Task . NOT_FINISHED_MASK iter = Task . Iterator ( self . task_tree , mask ) try : next ( iter ) except StopIteration : return True return False
3984	def get_same_container_repos ( app_or_library_name ) : specs = get_expanded_libs_specs ( ) spec = specs . get_app_or_lib ( app_or_library_name ) return get_same_container_repos_from_spec ( spec )
2255	def unique_flags ( items , key = None ) : len_ = len ( items ) if key is None : item_to_index = dict ( zip ( reversed ( items ) , reversed ( range ( len_ ) ) ) ) indices = item_to_index . values ( ) else : indices = argunique ( items , key = key ) flags = boolmask ( indices , len_ ) return flags
