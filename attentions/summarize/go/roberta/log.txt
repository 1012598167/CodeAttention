12/14/2021 09:28:28 - WARNING - configs -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, cpu count: 48
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
12/14/2021 09:28:45 - INFO - models -   Finish loading model [173.0M] parameters from roberta
12/14/2021 09:28:59 - INFO - utils -   Read 5000 examples, avg src len: 88, avg trg len: 15, max src len: 512, max trg len: 157
12/14/2021 09:28:59 - INFO - utils -   Sample 5k data for computing bleu/attention from /data/pretrain-attention/CodeAttention/data/summarize/go/train.jsonl
  0%|          | 0/5000 [00:00<?, ?it/s]  1%|          | 54/5000 [00:00<00:13, 358.51it/s]  2%|▏         | 108/5000 [00:00<00:11, 425.74it/s]  4%|▍         | 189/5000 [00:00<00:08, 556.94it/s]  5%|▌         | 270/5000 [00:00<00:07, 634.29it/s]  7%|▋         | 351/5000 [00:00<00:06, 673.12it/s]  9%|▉         | 459/5000 [00:00<00:06, 739.22it/s] 11%|█         | 540/5000 [00:00<00:05, 759.84it/s] 12%|█▏        | 621/5000 [00:00<00:05, 763.83it/s] 14%|█▍        | 702/5000 [00:01<00:05, 745.86it/s] 16%|█▌        | 783/5000 [00:01<00:05, 729.09it/s] 17%|█▋        | 864/5000 [00:01<00:05, 730.15it/s] 19%|█▉        | 945/5000 [00:01<00:05, 715.42it/s] 21%|██        | 1026/5000 [00:01<00:05, 713.34it/s] 22%|██▏       | 1107/5000 [00:01<00:05, 694.01it/s] 24%|██▍       | 1188/5000 [00:01<00:05, 674.38it/s] 25%|██▌       | 1269/5000 [00:01<00:05, 693.64it/s] 27%|██▋       | 1350/5000 [00:01<00:05, 712.99it/s] 29%|██▊       | 1431/5000 [00:02<00:04, 736.42it/s] 30%|███       | 1512/5000 [00:02<00:04, 753.89it/s] 32%|███▏      | 1593/5000 [00:02<00:04, 756.04it/s] 34%|███▍      | 1701/5000 [00:02<00:04, 791.52it/s] 36%|███▌      | 1809/5000 [00:02<00:04, 782.48it/s] 38%|███▊      | 1888/5000 [00:02<00:03, 779.43it/s] 39%|███▉      | 1966/5000 [00:02<00:03, 770.23it/s] 41%|████      | 2052/5000 [00:02<00:03, 760.65it/s] 43%|████▎     | 2160/5000 [00:02<00:03, 794.39it/s] 45%|████▌     | 2268/5000 [00:03<00:03, 828.33it/s] 48%|████▊     | 2376/5000 [00:03<00:03, 858.22it/s] 50%|████▉     | 2484/5000 [00:03<00:02, 880.89it/s] 51%|█████▏    | 2573/5000 [00:03<00:02, 880.51it/s] 53%|█████▎    | 2662/5000 [00:03<00:02, 799.43it/s] 55%|█████▌    | 2754/5000 [00:03<00:02, 765.44it/s] 57%|█████▋    | 2835/5000 [00:03<00:02, 775.84it/s] 58%|█████▊    | 2916/5000 [00:03<00:02, 776.80it/s] 60%|█████▉    | 2997/5000 [00:04<00:02, 782.50it/s] 62%|██████▏   | 3105/5000 [00:04<00:02, 802.49it/s] 64%|██████▎   | 3186/5000 [00:04<00:02, 794.49it/s] 65%|██████▌   | 3266/5000 [00:04<00:02, 763.77it/s] 67%|██████▋   | 3343/5000 [00:04<00:02, 736.42it/s] 69%|██████▊   | 3429/5000 [00:04<00:02, 739.74it/s] 71%|███████   | 3537/5000 [00:04<00:01, 808.91it/s] 73%|███████▎  | 3645/5000 [00:04<00:01, 875.28it/s] 76%|███████▌  | 3780/5000 [00:04<00:01, 955.35it/s] 78%|███████▊  | 3888/5000 [00:05<00:01, 949.11it/s] 80%|███████▉  | 3996/5000 [00:05<00:01, 945.45it/s] 82%|████████▏ | 4104/5000 [00:05<00:00, 930.93it/s] 84%|████████▍ | 4212/5000 [00:05<00:00, 943.61it/s] 86%|████████▋ | 4320/5000 [00:05<00:00, 959.17it/s] 89%|████████▊ | 4428/5000 [00:05<00:00, 959.92it/s] 91%|█████████ | 4536/5000 [00:05<00:00, 956.57it/s] 93%|█████████▎| 4644/5000 [00:05<00:00, 966.23it/s] 95%|█████████▌| 4752/5000 [00:05<00:00, 906.79it/s] 97%|█████████▋| 4844/5000 [00:06<00:00, 864.26it/s] 99%|█████████▊| 4932/5000 [00:06<00:00, 828.80it/s]100%|██████████| 5000/5000 [00:06<00:00, 789.82it/s]
Computing attention:   0%|          | 0/50 [00:00<?, ?it/s]Computing attention:   2%|▏         | 1/50 [00:00<00:47,  1.03it/s]Computing attention:   4%|▍         | 2/50 [00:01<00:38,  1.24it/s]Computing attention:   6%|▌         | 3/50 [00:02<00:35,  1.31it/s]Computing attention:   8%|▊         | 4/50 [00:03<00:33,  1.36it/s]Computing attention:  10%|█         | 5/50 [00:03<00:32,  1.38it/s]Computing attention:  12%|█▏        | 6/50 [00:04<00:31,  1.40it/s]Computing attention:  14%|█▍        | 7/50 [00:05<00:30,  1.41it/s]Computing attention:  16%|█▌        | 8/50 [00:05<00:29,  1.42it/s]Computing attention:  18%|█▊        | 9/50 [00:06<00:28,  1.42it/s]Computing attention:  20%|██        | 10/50 [00:07<00:28,  1.42it/s]Computing attention:  22%|██▏       | 11/50 [00:07<00:27,  1.42it/s]Computing attention:  24%|██▍       | 12/50 [00:08<00:26,  1.42it/s]Computing attention:  26%|██▌       | 13/50 [00:09<00:25,  1.42it/s]Computing attention:  28%|██▊       | 14/50 [00:10<00:25,  1.42it/s]Computing attention:  30%|███       | 15/50 [00:10<00:24,  1.42it/s]Computing attention:  32%|███▏      | 16/50 [00:11<00:23,  1.42it/s]Computing attention:  34%|███▍      | 17/50 [00:12<00:23,  1.42it/s]Computing attention:  36%|███▌      | 18/50 [00:12<00:22,  1.42it/s]Computing attention:  38%|███▊      | 19/50 [00:13<00:21,  1.42it/s]Computing attention:  40%|████      | 20/50 [00:14<00:21,  1.42it/s]Computing attention:  42%|████▏     | 21/50 [00:15<00:20,  1.42it/s]Computing attention:  44%|████▍     | 22/50 [00:15<00:19,  1.42it/s]Computing attention:  46%|████▌     | 23/50 [00:16<00:18,  1.42it/s]Computing attention:  48%|████▊     | 24/50 [00:17<00:18,  1.42it/s]Computing attention:  50%|█████     | 25/50 [00:17<00:17,  1.42it/s]Computing attention:  52%|█████▏    | 26/50 [00:18<00:16,  1.42it/s]Computing attention:  54%|█████▍    | 27/50 [00:19<00:16,  1.42it/s]Computing attention:  56%|█████▌    | 28/50 [00:19<00:15,  1.42it/s]Computing attention:  58%|█████▊    | 29/50 [00:20<00:14,  1.42it/s]Computing attention:  60%|██████    | 30/50 [00:21<00:14,  1.42it/s]Computing attention:  62%|██████▏   | 31/50 [00:22<00:13,  1.42it/s]Computing attention:  64%|██████▍   | 32/50 [00:22<00:12,  1.42it/s]Computing attention:  66%|██████▌   | 33/50 [00:23<00:11,  1.42it/s]Computing attention:  68%|██████▊   | 34/50 [00:24<00:11,  1.42it/s]Computing attention:  70%|███████   | 35/50 [00:24<00:10,  1.42it/s]Computing attention:  72%|███████▏  | 36/50 [00:25<00:09,  1.42it/s]Computing attention:  74%|███████▍  | 37/50 [00:26<00:09,  1.42it/s]Computing attention:  76%|███████▌  | 38/50 [00:26<00:08,  1.42it/s]Computing attention:  78%|███████▊  | 39/50 [00:27<00:07,  1.42it/s]Computing attention:  80%|████████  | 40/50 [00:28<00:07,  1.42it/s]Computing attention:  82%|████████▏ | 41/50 [00:29<00:06,  1.42it/s]Computing attention:  84%|████████▍ | 42/50 [00:29<00:05,  1.42it/s]Computing attention:  86%|████████▌ | 43/50 [00:30<00:04,  1.42it/s]Computing attention:  88%|████████▊ | 44/50 [00:31<00:04,  1.40it/s]Computing attention:  90%|█████████ | 45/50 [00:31<00:03,  1.40it/s]Computing attention:  92%|█████████▏| 46/50 [00:32<00:02,  1.40it/s]Computing attention:  94%|█████████▍| 47/50 [00:33<00:02,  1.41it/s]Computing attention:  96%|█████████▌| 48/50 [00:34<00:01,  1.41it/s]Computing attention:  98%|█████████▊| 49/50 [00:34<00:00,  1.41it/s]Computing attention: 100%|██████████| 50/50 [00:35<00:00,  1.41it/s]Computing attention: 100%|██████████| 50/50 [00:35<00:00,  1.41it/s]
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
torch.Size([12, 100, 12, 256, 256])
Traceback (most recent call last):
  File "/data/pretrain-attention/CodeAttention/attention.py", line 81, in <module>
    main()
  File "/data/pretrain-attention/CodeAttention/attention.py", line 77, in main
    print(attention_list[0].shape)
IndexError: list index out of range
