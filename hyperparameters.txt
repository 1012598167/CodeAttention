12/09/2021 09:16:14 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, add_lang_ids=False, add_task_prefix=False, always_save_model=True, beam_size=10, block_size=512, cache_path='saved_models/summarize/ruby/codebert_all_lr5_bs16_src256_trg128_pat2_e15/cache_data', config_name='', data_dir='/data/code/prompt-code-summarization/CodeT5/data', data_num=-1, dev_filename=None, do_eval=True, do_eval_bleu=True, do_lower_case=False, do_test=True, do_train=True, eval_batch_size=16, eval_steps=-1, eval_task='', gradient_accumulation_steps=1, lang='ruby', learning_rate=5e-05, load_model_path=None, local_rank=-1, log_steps=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=128, model_name_or_path='microsoft/codebert-base', model_type='roberta', no_cuda=False, num_train_epochs=15, output_dir='saved_models/summarize/ruby/codebert_all_lr5_bs16_src256_trg128_pat2_e15', patience=2, res_dir='saved_models/summarize/ruby/codebert_all_lr5_bs16_src256_trg128_pat2_e15/prediction', res_fn='results/summarize_codebert.txt', save_last_checkpoints=True, save_steps=-1, seed=1234, start_epoch=0, sub_task='ruby', summary_dir='tensorboard', task='summarize', test_filename=None, tokenizer_name='roberta-base', tokenizer_path='/data/code/prompt-code-summarization/CodeT5/tokenizer/salesforce', train_batch_size=16, train_filename=None, train_steps=-1, warmup_steps=1000, weight_decay=0.0)
12/09/2021 09:16:14 - WARNING - configs -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, cpu count: 48
Downloading: 100%|██████████| 498/498 [00:00<00:00, 245kB/s]
Downloading:   5%|▌         | 25.0M/476M [00:03<00:41, 11.4MB/s]^C(base) root@d77de3d29788:/data/pretrain-attention/CodeT5/sh# python run_exp.py --model_tag codet5_base --task summarize --============================Start Running==========================
bash exp_with_args.sh summarize ruby codet5_base 0 -1 16 5 256 128 2 15 1000 saved_models tensorboard results/summarize_codet5_base.txt

12/09/2021 09:16:53 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, add_lang_ids=False, add_task_prefix=False, always_save_model=True, beam_size=10, block_size=512, cache_path='saved_models/summarize/ruby/codet5_base_all_lr5_bs16_src256_trg128_pat2_e15/cache_data', config_name='', data_dir='/data/code/prompt-code-summarization/CodeT5/data', data_num=-1, dev_filename=None, do_eval=True, do_eval_bleu=True, do_lower_case=False, do_test=True, do_train=True, eval_batch_size=16, eval_steps=-1, eval_task='', gradient_accumulation_steps=1, lang='ruby', learning_rate=5e-05, load_model_path=None, local_rank=-1, log_steps=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=128, model_name_or_path='/data/code/prompt-code-summarization/CodeT5/pretrained_models/codet5_base', model_type='codet5', no_cuda=False, num_train_epochs=15, output_dir='saved_models/summarize/ruby/codet5_base_all_lr5_bs16_src256_trg128_pat2_e15', patience=2, res_dir='saved_models/summarize/ruby/codet5_base_all_lr5_bs16_src256_trg128_pat2_e15/prediction', res_fn='results/summarize_codet5_base.txt', save_last_checkpoints=True, save_steps=-1, seed=1234, start_epoch=0, sub_task='ruby', summary_dir='tensorboard', task='summarize', test_filename=None, tokenizer_name='roberta-base', tokenizer_path='/data/code/prompt-code-summarization/CodeT5/tokenizer/salesforce', train_batch_size=16, train_filename=None, train_steps=-1, warmup_steps=1000, weight_decay=0.0)
12/09/2021 09:16:53 - WARNING - configs -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, cpu count: 48
12/09/2021 09:16:57 - INFO - models -   Finish loading model [223.0M] from /data/code/prompt-code-summarization/CodeT5/pretrained_models/codet5_base
